# Crawl4AI Custom LLM Context
Generated on: 2025-07-11T19:25:16.981Z
Total files: 21

---

## Core Functionality - Memory
Source: crawl4ai_core_memory_content.llm.md

Okay, I have read the objective and instructions. I will now generate the detailed Markdown outline for a "Foundational Memory" document for the `core` component of `crawl4ai`, using the provided `code_analysis_output.md` (the concatenated code snippets from the previous prompt) as the primary source of truth for API details.

```markdown
# Detailed Outline for crawl4ai - core Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_core.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Core Components
    * 1.1. Purpose: Provides the foundational classes, configurations, and data models for web crawling and scraping operations within the `crawl4ai` library.
    * 1.2. Key Functionalities:
        *   Orchestration of asynchronous web crawling (`AsyncWebCrawler`).
        *   Configuration of browser behavior and specific crawl runs (`BrowserConfig`, `CrawlerRunConfig`).
        *   Standardized data structures for crawl results and associated data (`CrawlResult`, `Media`, `Links`, etc.).
        *   Strategies for fetching web content (`AsyncPlaywrightCrawlerStrategy`, `AsyncHTTPCrawlerStrategy`).
        *   Management of browser instances and sessions (`BrowserManager`, `ManagedBrowser`).
        *   Asynchronous logging (`AsyncLogger`).
    * 1.3. Relationship with other `crawl4ai` components:
        *   The `core` component serves as the foundation upon which specialized strategies (e.g., PDF processing, Markdown generation, content extraction, chunking, content filtering) are built and integrated.

## 2. Main Class: `AsyncWebCrawler`
    * 2.1. Purpose: The primary class for orchestrating asynchronous web crawling operations. It manages browser instances (via a `BrowserManager`), applies crawling strategies, and processes HTML content to produce structured results.
    * 2.2. Initialization (`__init__`)
        * 2.2.1. Signature:
            ```python
            def __init__(
                self,
                crawler_strategy: Optional[AsyncCrawlerStrategy] = None,
                config: Optional[BrowserConfig] = None,
                base_directory: str = str(os.getenv("CRAWL4_AI_BASE_DIRECTORY", Path.home())),
                thread_safe: bool = False,
                logger: Optional[AsyncLoggerBase] = None,
                **kwargs,
            ):
            ```
        * 2.2.2. Parameters:
            * `crawler_strategy (Optional[AsyncCrawlerStrategy])`: The strategy to use for fetching web content. If `None`, defaults to `AsyncPlaywrightCrawlerStrategy` initialized with `config` and `logger`.
            * `config (Optional[BrowserConfig])`: Configuration object for browser settings. If `None`, a default `BrowserConfig()` is created.
            * `base_directory (str)`: The base directory for storing crawl4ai related files, such as cache and logs. Defaults to `os.getenv("CRAWL4_AI_BASE_DIRECTORY", Path.home())`.
            * `thread_safe (bool)`: If `True`, uses an `asyncio.Lock` for thread-safe operations, particularly relevant for `arun_many`. Default: `False`.
            * `logger (Optional[AsyncLoggerBase])`: An instance of a logger. If `None`, a default `AsyncLogger` is initialized using `base_directory` and `config.verbose`.
            * `**kwargs`: Additional keyword arguments, primarily for backward compatibility, passed to the `AsyncPlaywrightCrawlerStrategy` if `crawler_strategy` is not provided.
    * 2.3. Key Public Attributes/Properties:
        * `browser_config (BrowserConfig)`: Read-only. The browser configuration object used by the crawler.
        * `crawler_strategy (AsyncCrawlerStrategy)`: Read-only. The active crawling strategy instance.
        * `logger (AsyncLoggerBase)`: Read-only. The logger instance used by the crawler.
        * `ready (bool)`: Read-only. `True` if the crawler has been started and is ready to perform crawl operations, `False` otherwise.
    * 2.4. Lifecycle Methods:
        * 2.4.1. `async start() -> AsyncWebCrawler`:
            * Purpose: Asynchronously initializes the crawler strategy (e.g., launches the browser). This must be called before `arun` or `arun_many` if the crawler is not used as an asynchronous context manager.
            * Returns: The `AsyncWebCrawler` instance (`self`).
        * 2.4.2. `async close() -> None`:
            * Purpose: Asynchronously closes the crawler strategy and cleans up resources (e.g., closes the browser). This should be called if `start()` was used explicitly.
        * 2.4.3. `async __aenter__() -> AsyncWebCrawler`:
            * Purpose: Entry point for asynchronous context management. Calls `self.start()`.
            * Returns: The `AsyncWebCrawler` instance (`self`).
        * 2.4.4. `async __aexit__(exc_type, exc_val, exc_tb) -> None`:
            * Purpose: Exit point for asynchronous context management. Calls `self.close()`.
    * 2.5. Primary Crawl Methods:
        * 2.5.1. `async arun(url: str, config: Optional[CrawlerRunConfig] = None, **kwargs) -> RunManyReturn`:
            * Purpose: Performs a single crawl operation for the given URL or raw HTML content.
            * Parameters:
                * `url (str)`: The URL to crawl (e.g., "http://example.com", "file:///path/to/file.html") or raw HTML content prefixed with "raw:" (e.g., "raw:<html>...</html>").
                * `config (Optional[CrawlerRunConfig])`: Configuration for this specific crawl run. If `None`, a default `CrawlerRunConfig()` is used.
                * `**kwargs`: Additional parameters passed to the underlying `aprocess_html` method, can be used to override settings in `config`.
            * Returns: `RunManyReturn` (which resolves to `CrawlResultContainer` containing a single `CrawlResult`).
        * 2.5.2. `async arun_many(urls: List[str], config: Optional[CrawlerRunConfig] = None, dispatcher: Optional[BaseDispatcher] = None, **kwargs) -> RunManyReturn`:
            * Purpose: Crawls multiple URLs concurrently using a specified or default dispatcher strategy.
            * Parameters:
                * `urls (List[str])`: A list of URLs to crawl.
                * `config (Optional[CrawlerRunConfig])`: Configuration applied to all crawl runs in this batch.
                * `dispatcher (Optional[BaseDispatcher])`: The dispatcher strategy to manage concurrent crawls. Defaults to `MemoryAdaptiveDispatcher`.
                * `**kwargs`: Additional parameters passed to the underlying `arun` method for each URL.
            * Returns: `RunManyReturn`. If `config.stream` is `True`, returns an `AsyncGenerator[CrawlResult, None]`. Otherwise, returns a `CrawlResultContainer` (list-like) of `CrawlResult` objects.
    * 2.6. Internal Processing Method (User-Facing Effects):
        * 2.6.1. `async aprocess_html(url: str, html: str, extracted_content: Optional[str], config: CrawlerRunConfig, screenshot_data: Optional[str], pdf_data: Optional[bytes], verbose: bool, **kwargs) -> CrawlResult`:
            * Purpose: Processes the fetched HTML content. This method is called internally by `arun` after content is fetched (either from a live crawl or cache). It applies scraping strategies, content filtering, and Markdown generation based on the `config`.
            * Parameters:
                * `url (str)`: The URL of the content being processed.
                * `html (str)`: The raw HTML content.
                * `extracted_content (Optional[str])`: Pre-extracted content from a previous step or cache.
                * `config (CrawlerRunConfig)`: Configuration for this processing run.
                * `screenshot_data (Optional[str])`: Base64 encoded screenshot data, if available.
                * `pdf_data (Optional[bytes])`: PDF data, if available.
                * `verbose (bool)`: Verbosity setting for logging during processing.
                * `**kwargs`: Additional parameters, including `is_raw_html` and `redirected_url`.
            * Returns: A `CrawlResult` object containing the processed data.

## 3. Core Configuration Objects

    * 3.1. Class `BrowserConfig` (from `crawl4ai.async_configs`)
        * 3.1.1. Purpose: Configures the browser instance launched by Playwright, including its type, mode, display settings, proxy, user agent, and persistent storage options.
        * 3.1.2. Initialization (`__init__`)
            * Signature:
            ```python
            def __init__(
                self,
                browser_type: str = "chromium",
                headless: bool = True,
                browser_mode: str = "dedicated",
                use_managed_browser: bool = False,
                cdp_url: Optional[str] = None,
                use_persistent_context: bool = False,
                user_data_dir: Optional[str] = None,
                channel: Optional[str] = "chromium", # Note: 'channel' from code, outline had 'chrome_channel'
                proxy: Optional[str] = None, # Note: 'proxy' from code, outline had 'proxy_config' for this level
                proxy_config: Optional[Union[ProxyConfig, dict, None]] = None,
                viewport_width: int = 1080,
                viewport_height: int = 600,
                viewport: Optional[dict] = None,
                accept_downloads: bool = False,
                downloads_path: Optional[str] = None,
                storage_state: Optional[Union[str, dict, None]] = None,
                ignore_https_errors: bool = True,
                java_script_enabled: bool = True,
                sleep_on_close: bool = False,
                verbose: bool = True,
                cookies: Optional[list] = None,
                headers: Optional[dict] = None,
                user_agent: str = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
                user_agent_mode: str = "",
                user_agent_generator_config: Optional[dict] = None, # Note: 'user_agent_generator_config' from code
                text_mode: bool = False,
                light_mode: bool = False,
                extra_args: Optional[list] = None,
                debugging_port: int = 9222,
                host: str = "localhost",
            ):
            ```
            * Key Parameters:
                * `browser_type (str)`: Type of browser to launch ("chromium", "firefox", "webkit"). Default: "chromium".
                * `headless (bool)`: Whether to run the browser in headless mode. Default: `True`.
                * `browser_mode (str)`: How the browser should be initialized ("builtin", "dedicated", "cdp", "docker"). Default: "dedicated".
                * `use_managed_browser (bool)`: Whether to launch the browser using a managed approach (e.g., via CDP). Default: `False`.
                * `cdp_url (Optional[str])`: URL for Chrome DevTools Protocol endpoint. Default: `None`.
                * `use_persistent_context (bool)`: Use a persistent browser context (profile). Default: `False`.
                * `user_data_dir (Optional[str])`: Path to user data directory for persistent sessions. Default: `None`.
                * `channel (Optional[str])`: Browser channel (e.g., "chromium", "chrome", "msedge"). Default: "chromium".
                * `proxy (Optional[str])`: Simple proxy server URL string.
                * `proxy_config (Optional[Union[ProxyConfig, dict, None]])`: Detailed proxy configuration object or dictionary. Takes precedence over `proxy`.
                * `viewport_width (int)`: Default viewport width. Default: `1080`.
                * `viewport_height (int)`: Default viewport height. Default: `600`.
                * `viewport (Optional[dict])`: Dictionary to set viewport dimensions, overrides `viewport_width` and `viewport_height` if set (e.g., `{"width": 1920, "height": 1080}`). Default: `None`.
                * `accept_downloads (bool)`: Whether to allow file downloads. Default: `False`.
                * `downloads_path (Optional[str])`: Directory to store downloaded files. Default: `None`.
                * `storage_state (Optional[Union[str, dict, None]])`: Path to a file or a dictionary containing browser storage state (cookies, localStorage). Default: `None`.
                * `ignore_https_errors (bool)`: Ignore HTTPS certificate errors. Default: `True`.
                * `java_script_enabled (bool)`: Enable JavaScript execution. Default: `True`.
                * `user_agent (str)`: Custom User-Agent string. Default: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36".
                * `user_agent_mode (str)`: Mode for generating User-Agent (e.g., "random"). Default: `""` (uses provided `user_agent`).
                * `user_agent_generator_config (Optional[dict])`: Configuration for User-Agent generation if `user_agent_mode` is active. Default: `{}`.
                * `text_mode (bool)`: If `True`, disables images and rich content for faster loading. Default: `False`.
                * `light_mode (bool)`: Disables certain background features for performance. Default: `False`.
                * `extra_args (Optional[list])`: Additional command-line arguments for the browser. Default: `None` (resolves to `[]`).
                * `debugging_port (int)`: Port for browser debugging protocol. Default: `9222`.
                * `host (str)`: Host for browser debugging protocol. Default: "localhost".
        * 3.1.3. Key Public Methods:
            * `clone(**kwargs) -> BrowserConfig`: Creates a new `BrowserConfig` instance as a copy of the current one, with specified keyword arguments overriding existing values.
            * `to_dict() -> dict`: Returns a dictionary representation of the configuration object's attributes.
            * `dump() -> dict`: Serializes the configuration object to a JSON-serializable dictionary, including nested objects.
            * `static load(data: dict) -> BrowserConfig`: Deserializes a `BrowserConfig` instance from a dictionary (previously created by `dump`).
            * `static from_kwargs(kwargs: dict) -> BrowserConfig`: Creates a `BrowserConfig` instance directly from a dictionary of keyword arguments.

    * 3.2. Class `CrawlerRunConfig` (from `crawl4ai.async_configs`)
        * 3.2.1. Purpose: Specifies settings for an individual crawl operation initiated by `arun()` or `arun_many()`. These settings can override or augment the global `BrowserConfig`.
        * 3.2.2. Initialization (`__init__`)
            * Signature:
            ```python
            def __init__(
                self,
                # Content Processing Parameters
                word_count_threshold: int = MIN_WORD_THRESHOLD,
                extraction_strategy: Optional[ExtractionStrategy] = None,
                chunking_strategy: ChunkingStrategy = RegexChunking(),
                markdown_generator: MarkdownGenerationStrategy = DefaultMarkdownGenerator(),
                only_text: bool = False,
                css_selector: Optional[str] = None,
                target_elements: Optional[List[str]] = None,
                excluded_tags: Optional[list] = None,
                excluded_selector: Optional[str] = None,
                keep_data_attributes: bool = False,
                keep_attrs: Optional[list] = None,
                remove_forms: bool = False,
                prettify: bool = False,
                parser_type: str = "lxml",
                scraping_strategy: ContentScrapingStrategy = None, # Will default to WebScrapingStrategy
                proxy_config: Optional[Union[ProxyConfig, dict, None]] = None,
                proxy_rotation_strategy: Optional[ProxyRotationStrategy] = None,
                # Browser Location and Identity Parameters
                locale: Optional[str] = None,
                timezone_id: Optional[str] = None,
                geolocation: Optional[GeolocationConfig] = None,
                # SSL Parameters
                fetch_ssl_certificate: bool = False,
                # Caching Parameters
                cache_mode: CacheMode = CacheMode.BYPASS,
                session_id: Optional[str] = None,
                bypass_cache: bool = False, # Legacy
                disable_cache: bool = False, # Legacy
                no_cache_read: bool = False, # Legacy
                no_cache_write: bool = False, # Legacy
                shared_data: Optional[dict] = None,
                # Page Navigation and Timing Parameters
                wait_until: str = "domcontentloaded",
                page_timeout: int = PAGE_TIMEOUT,
                wait_for: Optional[str] = None,
                wait_for_timeout: Optional[int] = None,
                wait_for_images: bool = False,
                delay_before_return_html: float = 0.1,
                mean_delay: float = 0.1,
                max_range: float = 0.3,
                semaphore_count: int = 5,
                # Page Interaction Parameters
                js_code: Optional[Union[str, List[str]]] = None,
                js_only: bool = False,
                ignore_body_visibility: bool = True,
                scan_full_page: bool = False,
                scroll_delay: float = 0.2,
                process_iframes: bool = False,
                remove_overlay_elements: bool = False,
                simulate_user: bool = False,
                override_navigator: bool = False,
                magic: bool = False,
                adjust_viewport_to_content: bool = False,
                # Media Handling Parameters
                screenshot: bool = False,
                screenshot_wait_for: Optional[float] = None,
                screenshot_height_threshold: int = SCREENSHOT_HEIGHT_THRESHOLD,
                pdf: bool = False,
                capture_mhtml: bool = False,
                image_description_min_word_threshold: int = IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,
                image_score_threshold: int = IMAGE_SCORE_THRESHOLD,
                table_score_threshold: int = 7,
                exclude_external_images: bool = False,
                exclude_all_images: bool = False,
                # Link and Domain Handling Parameters
                exclude_social_media_domains: Optional[list] = None, # Note: 'exclude_social_media_domains' from code
                exclude_external_links: bool = False,
                exclude_social_media_links: bool = False,
                exclude_domains: Optional[list] = None,
                exclude_internal_links: bool = False,
                # Debugging and Logging Parameters
                verbose: bool = True,
                log_console: bool = False,
                # Network and Console Capturing Parameters
                capture_network_requests: bool = False,
                capture_console_messages: bool = False,
                # Connection Parameters (for HTTPCrawlerStrategy)
                method: str = "GET",
                stream: bool = False,
                url: Optional[str] = None,
                # Robots.txt Handling
                check_robots_txt: bool = False,
                # User Agent Parameters
                user_agent: Optional[str] = None,
                user_agent_mode: Optional[str] = None,
                user_agent_generator_config: Optional[dict] = None, # Note: 'user_agent_generator_config' from code
                # Deep Crawl Parameters
                deep_crawl_strategy: Optional[DeepCrawlStrategy] = None,
                # Experimental Parameters
                experimental: Optional[Dict[str, Any]] = None,
            ):
            ```
            * Key Parameters:
                * `word_count_threshold (int)`: Minimum word count for a content block to be considered. Default: `MIN_WORD_THRESHOLD` (200).
                * `extraction_strategy (Optional[ExtractionStrategy])`: Strategy for structured data extraction (e.g., `LLMExtractionStrategy`, `JsonCssExtractionStrategy`). Default: `None` (falls back to `NoExtractionStrategy`).
                * `chunking_strategy (ChunkingStrategy)`: Strategy for splitting content into chunks before extraction. Default: `RegexChunking()`.
                * `markdown_generator (MarkdownGenerationStrategy)`: Strategy for converting HTML to Markdown. Default: `DefaultMarkdownGenerator()`.
                * `cache_mode (CacheMode)`: Caching behavior for this run. Default: `CacheMode.BYPASS`.
                * `session_id (Optional[str])`: ID for session persistence (reusing browser tabs/contexts). Default: `None`.
                * `js_code (Optional[Union[str, List[str]]])`: JavaScript code snippets to execute on the page. Default: `None`.
                * `wait_for (Optional[str])`: CSS selector or JS condition (prefixed with "js:") to wait for before proceeding. Default: `None`.
                * `page_timeout (int)`: Timeout for page operations (e.g., navigation) in milliseconds. Default: `PAGE_TIMEOUT` (60000ms).
                * `screenshot (bool)`: If `True`, capture a screenshot of the page. Default: `False`.
                * `pdf (bool)`: If `True`, generate a PDF of the page. Default: `False`.
                * `capture_mhtml (bool)`: If `True`, capture an MHTML snapshot of the page. Default: `False`.
                * `exclude_external_links (bool)`: If `True`, exclude external links from results. Default: `False`.
                * `stream (bool)`: If `True` (used with `arun_many`), results are yielded as an `AsyncGenerator`. Default: `False`.
                * `check_robots_txt (bool)`: If `True`, crawler will check and respect `robots.txt` rules. Default: `False`.
                * `user_agent (Optional[str])`: Override the browser's User-Agent for this specific run.
        * 3.2.3. Key Public Methods:
            * `clone(**kwargs) -> CrawlerRunConfig`: Creates a new `CrawlerRunConfig` instance as a copy of the current one, with specified keyword arguments overriding existing values.
            * `to_dict() -> dict`: Returns a dictionary representation of the configuration object's attributes.
            * `dump() -> dict`: Serializes the configuration object to a JSON-serializable dictionary, including nested objects.
            * `static load(data: dict) -> CrawlerRunConfig`: Deserializes a `CrawlerRunConfig` instance from a dictionary (previously created by `dump`).
            * `static from_kwargs(kwargs: dict) -> CrawlerRunConfig`: Creates a `CrawlerRunConfig` instance directly from a dictionary of keyword arguments.

    * 3.3. Supporting Configuration Objects (from `crawl4ai.async_configs`)
        * 3.3.1. Class `GeolocationConfig`
            * Purpose: Defines geolocation (latitude, longitude, accuracy) to be emulated by the browser.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    latitude: float,
                    longitude: float,
                    accuracy: Optional[float] = 0.0
                ):
                ```
            * Parameters:
                * `latitude (float)`: Latitude coordinate (e.g., 37.7749).
                * `longitude (float)`: Longitude coordinate (e.g., -122.4194).
                * `accuracy (Optional[float])`: Accuracy in meters. Default: `0.0`.
            * Methods:
                * `static from_dict(geo_dict: Dict) -> GeolocationConfig`: Creates an instance from a dictionary.
                * `to_dict() -> Dict`: Converts the instance to a dictionary.
                * `clone(**kwargs) -> GeolocationConfig`: Creates a copy with updated values.
        * 3.3.2. Class `ProxyConfig`
            * Purpose: Defines the settings for a single proxy server, including server address, authentication credentials, and optional IP.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    server: str,
                    username: Optional[str] = None,
                    password: Optional[str] = None,
                    ip: Optional[str] = None,
                ):
                ```
            * Parameters:
                * `server (str)`: Proxy server URL (e.g., "http://127.0.0.1:8080", "socks5://user:pass@host:port").
                * `username (Optional[str])`: Username for proxy authentication.
                * `password (Optional[str])`: Password for proxy authentication.
                * `ip (Optional[str])`: Optional IP address associated with the proxy for verification.
            * Methods:
                * `static from_string(proxy_str: str) -> ProxyConfig`: Creates an instance from a string (e.g., "ip:port:username:password" or "ip:port").
                * `static from_dict(proxy_dict: Dict) -> ProxyConfig`: Creates an instance from a dictionary.
                * `static from_env(env_var: str = "PROXIES") -> List[ProxyConfig]`: Loads a list of proxies from a comma-separated environment variable.
                * `to_dict() -> Dict`: Converts the instance to a dictionary.
                * `clone(**kwargs) -> ProxyConfig`: Creates a copy with updated values.
        * 3.3.3. Class `HTTPCrawlerConfig`
            * Purpose: Configuration for the `AsyncHTTPCrawlerStrategy`, specifying HTTP method, headers, data/JSON payload, and redirect/SSL verification behavior.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    method: str = "GET",
                    headers: Optional[Dict[str, str]] = None,
                    data: Optional[Dict[str, Any]] = None,
                    json: Optional[Dict[str, Any]] = None,
                    follow_redirects: bool = True,
                    verify_ssl: bool = True,
                ):
                ```
            * Parameters:
                * `method (str)`: HTTP method (e.g., "GET", "POST"). Default: "GET".
                * `headers (Optional[Dict[str, str]])`: Dictionary of HTTP request headers. Default: `None`.
                * `data (Optional[Dict[str, Any]])`: Dictionary of form data to send in the request body. Default: `None`.
                * `json (Optional[Dict[str, Any]])`: JSON data to send in the request body. Default: `None`.
                * `follow_redirects (bool)`: Whether to automatically follow HTTP redirects. Default: `True`.
                * `verify_ssl (bool)`: Whether to verify SSL certificates. Default: `True`.
            * Methods:
                * `static from_kwargs(kwargs: dict) -> HTTPCrawlerConfig`: Creates an instance from keyword arguments.
                * `to_dict() -> dict`: Converts config to a dictionary.
                * `clone(**kwargs) -> HTTPCrawlerConfig`: Creates a copy with updated values.
                * `dump() -> dict`: Serializes the config to a dictionary.
                * `static load(data: dict) -> HTTPCrawlerConfig`: Deserializes from a dictionary.
        * 3.3.4. Class `LLMConfig`
            * Purpose: Configures settings for interacting with Large Language Models, including provider choice, API credentials, and generation parameters.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    provider: str = DEFAULT_PROVIDER,
                    api_token: Optional[str] = None,
                    base_url: Optional[str] = None,
                    temperature: Optional[float] = None,
                    max_tokens: Optional[int] = None,
                    top_p: Optional[float] = None,
                    frequency_penalty: Optional[float] = None,
                    presence_penalty: Optional[float] = None,
                    stop: Optional[List[str]] = None,
                    n: Optional[int] = None,
                ):
                ```
            * Key Parameters:
                * `provider (str)`: Name of the LLM provider (e.g., "openai/gpt-4o", "ollama/llama3.3", "groq/llama3-8b-8192"). Default: `DEFAULT_PROVIDER` (from `crawl4ai.config`).
                * `api_token (Optional[str])`: API token for the LLM provider. If prefixed with "env:", it reads from the specified environment variable (e.g., "env:OPENAI_API_KEY"). If not provided, it attempts to load from default environment variables based on the provider.
                * `base_url (Optional[str])`: Custom base URL for the LLM API endpoint.
                * `temperature (Optional[float])`: Sampling temperature for generation.
                * `max_tokens (Optional[int])`: Maximum number of tokens to generate.
                * `top_p (Optional[float])`: Nucleus sampling parameter.
                * `frequency_penalty (Optional[float])`: Penalty for token frequency.
                * `presence_penalty (Optional[float])`: Penalty for token presence.
                * `stop (Optional[List[str]])`: List of stop sequences for generation.
                * `n (Optional[int])`: Number of completions to generate.
            * Methods:
                * `static from_kwargs(kwargs: dict) -> LLMConfig`: Creates an instance from keyword arguments.
                * `to_dict() -> dict`: Converts config to a dictionary.
                * `clone(**kwargs) -> LLMConfig`: Creates a copy with updated values.

## 4. Core Data Models (Results & Payloads from `crawl4ai.models`)

    * 4.1. Class `CrawlResult(BaseModel)`
        * Purpose: A Pydantic model representing the comprehensive result of a single crawl and processing operation.
        * Key Fields:
            * `url (str)`: The final URL that was crawled (after any redirects).
            * `html (str)`: The raw HTML content fetched from the URL.
            * `success (bool)`: `True` if the crawl operation (fetching and initial processing) was successful, `False` otherwise.
            * `cleaned_html (Optional[str])`: HTML content after sanitization and removal of unwanted tags/attributes as per configuration. Default: `None`.
            * `_markdown (Optional[MarkdownGenerationResult])`: (Private Attribute) Holds the `MarkdownGenerationResult` object if Markdown generation was performed. Use the `markdown` property to access. Default: `None`.
            * `markdown (Optional[Union[str, MarkdownGenerationResult]])`: (Property) Provides access to Markdown content. Behaves as a string (raw markdown) by default but allows access to `MarkdownGenerationResult` attributes (e.g., `result.markdown.fit_markdown`).
            * `extracted_content (Optional[str])`: JSON string representation of structured data extracted by an `ExtractionStrategy`. Default: `None`.
            * `media (Media)`: An object containing lists of `MediaItem` for images, videos, audio, and extracted tables. Default: `Media()`.
            * `links (Links)`: An object containing lists of `Link` for internal and external hyperlinks found on the page. Default: `Links()`.
            * `downloaded_files (Optional[List[str]])`: A list of file paths if any files were downloaded during the crawl. Default: `None`.
            * `js_execution_result (Optional[Dict[str, Any]])`: The result of any JavaScript code executed on the page. Default: `None`.
            * `screenshot (Optional[str])`: Base64 encoded string of the page screenshot, if `screenshot=True` was set. Default: `None`.
            * `pdf (Optional[bytes])`: Raw bytes of the PDF generated from the page, if `pdf=True` was set. Default: `None`.
            * `mhtml (Optional[str])`: MHTML snapshot of the page, if `capture_mhtml=True` was set. Default: `None`.
            * `metadata (Optional[dict])`: Dictionary of metadata extracted from the page (e.g., title, description, OpenGraph tags, Twitter card data). Default: `None`.
            * `error_message (Optional[str])`: A message describing the error if `success` is `False`. Default: `None`.
            * `session_id (Optional[str])`: The session ID used for this crawl, if applicable. Default: `None`.
            * `response_headers (Optional[dict])`: HTTP response headers from the server. Default: `None`.
            * `status_code (Optional[int])`: HTTP status code of the response. Default: `None`.
            * `ssl_certificate (Optional[SSLCertificate])`: Information about the SSL certificate if `fetch_ssl_certificate=True`. Default: `None`.
            * `dispatch_result (Optional[DispatchResult])`: Metadata about the task execution from the dispatcher (e.g., timings, memory usage). Default: `None`.
            * `redirected_url (Optional[str])`: The original URL if the request was redirected. Default: `None`.
            * `network_requests (Optional[List[Dict[str, Any]]])`: List of captured network requests if `capture_network_requests=True`. Default: `None`.
            * `console_messages (Optional[List[Dict[str, Any]]])`: List of captured browser console messages if `capture_console_messages=True`. Default: `None`.
        * Methods:
            * `model_dump(*args, **kwargs)`: Serializes the `CrawlResult` model to a dictionary, ensuring the `_markdown` private attribute is correctly handled and included as "markdown" in the output if present.

    * 4.2. Class `MarkdownGenerationResult(BaseModel)`
        * Purpose: A Pydantic model that holds various forms of Markdown generated from HTML content.
        * Fields:
            * `raw_markdown (str)`: The basic, direct conversion of HTML to Markdown.
            * `markdown_with_citations (str)`: Markdown content with inline citations (e.g., [^1^]) and a references section.
            * `references_markdown (str)`: The Markdown content for the "References" section, listing all cited links.
            * `fit_markdown (Optional[str])`: Markdown generated specifically from content deemed "relevant" by a content filter (like `PruningContentFilter` or `LLMContentFilter`), if such a filter was applied. Default: `None`.
            * `fit_html (Optional[str])`: The filtered HTML content that was used to generate `fit_markdown`. Default: `None`.
        * Methods:
            * `__str__(self) -> str`: Returns `self.raw_markdown` when the object is cast to a string.

    * 4.3. Class `ScrapingResult(BaseModel)`
        * Purpose: A Pydantic model representing a standardized output from content scraping strategies.
        * Fields:
            * `cleaned_html (str)`: The primary sanitized and processed HTML content.
            * `success (bool)`: Indicates if the scraping operation was successful.
            * `media (Media)`: A `Media` object containing extracted images, videos, audio, and tables.
            * `links (Links)`: A `Links` object containing extracted internal and external links.
            * `metadata (Dict[str, Any])`: A dictionary of metadata extracted from the page (e.g., title, description).

    * 4.4. Class `MediaItem(BaseModel)`
        * Purpose: A Pydantic model representing a generic media item like an image, video, or audio file.
        * Fields:
            * `src (Optional[str])`: The source URL of the media item. Default: `""`.
            * `data (Optional[str])`: Base64 encoded data for inline media. Default: `""`.
            * `alt (Optional[str])`: Alternative text for the media item (e.g., image alt text). Default: `""`.
            * `desc (Optional[str])`: A description or surrounding text related to the media item. Default: `""`.
            * `score (Optional[int])`: A relevance or importance score, if calculated by a strategy. Default: `0`.
            * `type (str)`: The type of media (e.g., "image", "video", "audio"). Default: "image".
            * `group_id (Optional[int])`: An identifier to group related media variants (e.g., different resolutions of the same image from a srcset). Default: `0`.
            * `format (Optional[str])`: The detected file format (e.g., "jpeg", "png", "mp4"). Default: `None`.
            * `width (Optional[int])`: The width of the media item in pixels, if available. Default: `None`.

    * 4.5. Class `Link(BaseModel)`
        * Purpose: A Pydantic model representing an extracted hyperlink.
        * Fields:
            * `href (Optional[str])`: The URL (href attribute) of the link. Default: `""`.
            * `text (Optional[str])`: The anchor text of the link. Default: `""`.
            * `title (Optional[str])`: The title attribute of the link, if present. Default: `""`.
            * `base_domain (Optional[str])`: The base domain extracted from the `href`. Default: `""`.

    * 4.6. Class `Media(BaseModel)`
        * Purpose: A Pydantic model that acts as a container for lists of different types of media items found on a page.
        * Fields:
            * `images (List[MediaItem])`: A list of `MediaItem` objects representing images. Default: `[]`.
            * `videos (List[MediaItem])`: A list of `MediaItem` objects representing videos. Default: `[]`.
            * `audios (List[MediaItem])`: A list of `MediaItem` objects representing audio files. Default: `[]`.
            * `tables (List[Dict])`: A list of dictionaries, where each dictionary represents an extracted HTML table with keys like "headers", "rows", "caption", "summary". Default: `[]`.

    * 4.7. Class `Links(BaseModel)`
        * Purpose: A Pydantic model that acts as a container for lists of internal and external links.
        * Fields:
            * `internal (List[Link])`: A list of `Link` objects considered internal to the crawled site. Default: `[]`.
            * `external (List[Link])`: A list of `Link` objects pointing to external sites. Default: `[]`.

    * 4.8. Class `AsyncCrawlResponse(BaseModel)`
        * Purpose: A Pydantic model representing the raw response from a crawler strategy's `crawl` method. This data is then processed further to create a `CrawlResult`.
        * Fields:
            * `html (str)`: The raw HTML content of the page.
            * `response_headers (Dict[str, str])`: A dictionary of HTTP response headers.
            * `js_execution_result (Optional[Dict[str, Any]])`: The result from any JavaScript code executed on the page. Default: `None`.
            * `status_code (int)`: The HTTP status code of the response.
            * `screenshot (Optional[str])`: Base64 encoded screenshot data, if captured. Default: `None`.
            * `pdf_data (Optional[bytes])`: Raw PDF data, if captured. Default: `None`.
            * `mhtml_data (Optional[str])`: MHTML snapshot data, if captured. Default: `None`.
            * `downloaded_files (Optional[List[str]])`: A list of local file paths for any files downloaded during the crawl. Default: `None`.
            * `ssl_certificate (Optional[SSLCertificate])`: SSL certificate information for the site. Default: `None`.
            * `redirected_url (Optional[str])`: The original URL requested if the final URL is a result of redirection. Default: `None`.
            * `network_requests (Optional[List[Dict[str, Any]]])`: Captured network requests if enabled. Default: `None`.
            * `console_messages (Optional[List[Dict[str, Any]]])`: Captured console messages if enabled. Default: `None`.

    * 4.9. Class `TokenUsage(BaseModel)`
        * Purpose: A Pydantic model to track token usage statistics for interactions with Large Language Models.
        * Fields:
            * `completion_tokens (int)`: Number of tokens used for the LLM's completion/response. Default: `0`.
            * `prompt_tokens (int)`: Number of tokens used for the input prompt to the LLM. Default: `0`.
            * `total_tokens (int)`: Total number of tokens used (prompt + completion). Default: `0`.
            * `completion_tokens_details (Optional[dict])`: Provider-specific detailed breakdown of completion tokens. Default: `None`.
            * `prompt_tokens_details (Optional[dict])`: Provider-specific detailed breakdown of prompt tokens. Default: `None`.

    * 4.10. Class `SSLCertificate(dict)` (from `crawl4ai.ssl_certificate`)
        * Purpose: Represents an SSL certificate's information, behaving like a dictionary for direct JSON serialization and easy access to its fields.
        * Key Fields (accessed as dictionary keys):
            * `subject (dict)`: Dictionary of subject fields (e.g., `{"CN": "example.com", "O": "Example Inc."}`).
            * `issuer (dict)`: Dictionary of issuer fields.
            * `version (int)`: Certificate version number.
            * `serial_number (str)`: Certificate serial number (hexadecimal string).
            * `not_before (str)`: Validity start date and time (ASN.1/UTC format string, e.g., "YYYYMMDDHHMMSSZ").
            * `not_after (str)`: Validity end date and time (ASN.1/UTC format string).
            * `fingerprint (str)`: SHA-256 fingerprint of the certificate (lowercase hex string).
            * `signature_algorithm (str)`: The algorithm used to sign the certificate (e.g., "sha256WithRSAEncryption").
            * `raw_cert (str)`: Base64 encoded string of the raw DER-encoded certificate.
            * `extensions (List[dict])`: A list of dictionaries, each representing a certificate extension with "name" and "value" keys.
        * Static Methods:
            * `from_url(url: str, timeout: int = 10) -> Optional[SSLCertificate]`: Fetches the SSL certificate from the given URL and returns an `SSLCertificate` instance, or `None` on failure.
        * Instance Methods:
            * `to_json(filepath: Optional[str] = None) -> Optional[str]`: Exports the certificate information as a JSON string. If `filepath` is provided, writes to the file and returns `None`.
            * `to_pem(filepath: Optional[str] = None) -> Optional[str]`: Exports the certificate in PEM format as a string. If `filepath` is provided, writes to the file and returns `None`.
            * `to_der(filepath: Optional[str] = None) -> Optional[bytes]`: Exports the raw certificate in DER format as bytes. If `filepath` is provided, writes to the file and returns `None`.
        * Example:
            ```python
            # Assuming 'cert' is an SSLCertificate instance
            # print(cert["subject"]["CN"])
            # cert.to_pem("my_cert.pem")
            ```

    * 4.11. Class `DispatchResult(BaseModel)`
        * Purpose: Contains metadata about a task's execution when processed by a dispatcher (e.g., in `arun_many`).
        * Fields:
            * `task_id (str)`: A unique identifier for the dispatched task.
            * `memory_usage (float)`: Memory usage (in MB) recorded during the task's execution.
            * `peak_memory (float)`: Peak memory usage (in MB) recorded during the task's execution.
            * `start_time (Union[datetime, float])`: The start time of the task (can be a `datetime` object or a Unix timestamp float).
            * `end_time (Union[datetime, float])`: The end time of the task.
            * `error_message (str)`: Any error message if the task failed during dispatch or execution. Default: `""`.

    * 4.12. `CrawlResultContainer(Generic[CrawlResultT])`
        * Purpose: A generic container for `CrawlResult` objects, primarily used as the return type for `arun_many` when `stream=False`. It behaves like a list, allowing iteration, indexing, and length checking.
        * Methods:
            * `__iter__(self)`: Allows iteration over the contained `CrawlResult` objects.
            * `__getitem__(self, index)`: Allows accessing `CrawlResult` objects by index.
            * `__len__(self)`: Returns the number of `CrawlResult` objects contained.
            * `__repr__(self)`: Provides a string representation of the container.
        * Attribute:
            * `_results (List[CrawlResultT])`: The internal list holding the `CrawlResult` objects.

    * 4.13. `RunManyReturn` (Type Alias from `crawl4ai.models`)
        * Purpose: A type alias defining the possible return types for the `arun_many` method of `AsyncWebCrawler`.
        * Definition: `Union[CrawlResultContainer[CrawlResult], AsyncGenerator[CrawlResult, None]]`
            * This means `arun_many` will return a `CrawlResultContainer` (a list-like object of all `CrawlResult` instances) if `CrawlerRunConfig.stream` is `False` (the default).
            * It will return an `AsyncGenerator` yielding individual `CrawlResult` instances if `CrawlerRunConfig.stream` is `True`.

## 5. Core Crawler Strategies (from `crawl4ai.async_crawler_strategy`)

    * 5.1. Abstract Base Class `AsyncCrawlerStrategy(ABC)`
        * Purpose: Defines the common interface that all asynchronous crawler strategies must implement. This allows `AsyncWebCrawler` to use different fetching mechanisms (e.g., Playwright, HTTP requests) interchangeably.
        * Initialization (`__init__`):
            ```python
            def __init__(self, browser_config: BrowserConfig, logger: AsyncLoggerBase):
            ```
            * Parameters:
                * `browser_config (BrowserConfig)`: The browser configuration to be used by the strategy.
                * `logger (AsyncLoggerBase)`: The logger instance for logging strategy-specific events.
        * Key Abstract Methods (must be implemented by concrete subclasses):
            * `async crawl(self, url: str, config: CrawlerRunConfig) -> AsyncCrawlResponse`:
                * Purpose: Fetches the content from the given URL according to the `config`.
                * Returns: An `AsyncCrawlResponse` object containing the raw fetched data.
            * `async __aenter__(self)`:
                * Purpose: Asynchronous context manager entry, typically for initializing resources (e.g., launching a browser).
            * `async __aexit__(self, exc_type, exc_val, exc_tb)`:
                * Purpose: Asynchronous context manager exit, for cleaning up resources.
        * Key Concrete Methods (available to all strategies):
            * `set_custom_headers(self, headers: dict) -> None`:
                * Purpose: Sets custom HTTP headers to be used by the strategy for subsequent requests.
            * `update_user_agent(self, user_agent: str) -> None`:
                * Purpose: Updates the User-Agent string used by the strategy.
            * `set_hook(self, hook_name: str, callback: Callable) -> None`:
                * Purpose: Registers a callback function for a specific hook point in the crawling lifecycle.
            * `async_run_hook(self, hook_name: str, *args, **kwargs) -> Any`:
                * Purpose: Executes a registered hook with the given arguments.
            * `async_get_default_context(self) -> BrowserContext`:
                * Purpose: Retrieves the default browser context (Playwright specific, might raise `NotImplementedError` in non-Playwright strategies).
            * `async_create_new_page(self, context: BrowserContext) -> Page`:
                * Purpose: Creates a new page within a given browser context (Playwright specific).
            * `async_get_page(self, url: str, config: CrawlerRunConfig, session_id: Optional[str]) -> Tuple[Page, BrowserContext]`:
                * Purpose: Gets an existing page/context for a session or creates a new one (Playwright specific, managed by `BrowserManager`).
            * `async_close_page(self, page: Page, session_id: Optional[str]) -> None`:
                * Purpose: Closes a page, potentially keeping the associated context/session alive (Playwright specific).
            * `async_kill_session(self, session_id: str) -> None`:
                * Purpose: Kills (closes) a specific browser session, including its page and context (Playwright specific).

    * 5.2. Class `AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy)`
        * Purpose: The default crawler strategy, using Playwright to control a web browser for fetching and interacting with web pages. It supports complex JavaScript execution and provides hooks for various stages of the crawl.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                browser_config: Optional[BrowserConfig] = None,
                logger: Optional[AsyncLoggerBase] = None,
                browser_manager: Optional[BrowserManager] = None
            ):
            ```
            * Parameters:
                * `browser_config (Optional[BrowserConfig])`: Browser configuration. Defaults to a new `BrowserConfig()` if not provided.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance. Defaults to a new `AsyncLogger()`.
                * `browser_manager (Optional[BrowserManager])`: An instance of `BrowserManager` to manage browser lifecycles and contexts. If `None`, a new `BrowserManager` is created internally.
        * Key Overridden/Implemented Methods:
            * `async crawl(self, url: str, config: CrawlerRunConfig) -> AsyncCrawlResponse`:
                * Purpose: Implements the crawling logic using Playwright. It navigates to the URL, executes JavaScript if specified, waits for conditions, captures screenshots/PDFs if requested, and returns the page content and other metadata.
            * `async aprocess_html(self, url: str, html: str, config: CrawlerRunConfig, **kwargs) -> CrawlResult`:
                * Purpose: (Note: While `AsyncWebCrawler` calls this, the default implementation is in `AsyncPlaywrightCrawlerStrategy` for convenience, acting as a bridge to the scraping strategy.) Processes the fetched HTML to produce a `CrawlResult`. This involves using the `scraping_strategy` from the `config` (defaults to `WebScrapingStrategy`) to clean HTML, extract media/links, and then uses the `markdown_generator` to produce Markdown.
        * Specific Public Methods:
            * `async_create_new_context(self, config: Optional[CrawlerRunConfig] = None) -> BrowserContext`:
                * Purpose: Creates a new Playwright `BrowserContext` based on the global `BrowserConfig` and optional overrides from `CrawlerRunConfig`.
            * `async_setup_context_default(self, context: BrowserContext, config: Optional[CrawlerRunConfig] = None) -> None`:
                * Purpose: Applies default settings to a `BrowserContext`, such as viewport size, user agent, custom headers, locale, timezone, and geolocation, based on `BrowserConfig` and `CrawlerRunConfig`.
            * `async_setup_context_hooks(self, context: BrowserContext, config: CrawlerRunConfig) -> None`:
                * Purpose: Sets up event listeners on the context for capturing network requests and console messages if `config.capture_network_requests` or `config.capture_console_messages` is `True`.
            * `async_handle_storage_state(self, context: BrowserContext, config: CrawlerRunConfig) -> None`:
                * Purpose: Loads cookies and localStorage from a `storage_state` file or dictionary (specified in `BrowserConfig` or `CrawlerRunConfig`) into the given `BrowserContext`.
        * Hooks (Callable via `set_hook(hook_name, callback)` and executed by `async_run_hook`):
            * `on_browser_created`: Called after the Playwright browser instance is launched or connected. Callback receives `(browser, **kwargs)`.
            * `on_page_context_created`: Called after a new Playwright `BrowserContext` and `Page` are created. Callback receives `(page, context, **kwargs)`.
            * `before_goto`: Called just before `page.goto(url)` is executed. Callback receives `(page, context, url, **kwargs)`.
            * `after_goto`: Called after `page.goto(url)` completes successfully. Callback receives `(page, context, url, response, **kwargs)`.
            * `on_user_agent_updated`: Called when the User-Agent string is updated for a context. Callback receives `(page, context, user_agent, **kwargs)`.
            * `on_execution_started`: Called when `js_code` execution begins on a page. Callback receives `(page, context, **kwargs)`.
            * `before_retrieve_html`: Called just before the final HTML content is retrieved from the page. Callback receives `(page, context, **kwargs)`.
            * `before_return_html`: Called just before the `AsyncCrawlResponse` is returned by the `crawl()` method of the strategy. Callback receives `(page, context, html_content, **kwargs)`.

    * 5.3. Class `AsyncHTTPCrawlerStrategy(AsyncCrawlerStrategy)`
        * Purpose: A lightweight crawler strategy that uses direct HTTP requests (via `httpx`) instead of a full browser. Suitable for static sites or when JavaScript execution is not needed.
        * Initialization (`__init__`):
            ```python
            def __init__(self, http_config: Optional[HTTPCrawlerConfig] = None, logger: Optional[AsyncLoggerBase] = None):
            ```
            * Parameters:
                * `http_config (Optional[HTTPCrawlerConfig])`: Configuration for HTTP requests (method, headers, data, etc.). Defaults to a new `HTTPCrawlerConfig()`.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance. Defaults to a new `AsyncLogger()`.
        * Key Overridden/Implemented Methods:
            * `async crawl(self, url: str, http_config: Optional[HTTPCrawlerConfig] = None, **kwargs) -> AsyncCrawlResponse`:
                * Purpose: Fetches content from the URL using an HTTP GET or POST request via `httpx`. Does not execute JavaScript. Returns an `AsyncCrawlResponse` with HTML, status code, and headers. Screenshot, PDF, and MHTML capabilities are not available with this strategy.

## 6. Browser Management (from `crawl4ai.browser_manager`)

    * 6.1. Class `BrowserManager`
        * Purpose: Manages the lifecycle of Playwright browser instances and their contexts. It handles launching/connecting to browsers, creating new contexts with specific configurations, managing sessions for page reuse, and cleaning up resources.
        * Initialization (`__init__`):
            ```python
            def __init__(self, browser_config: BrowserConfig, logger: Optional[AsyncLoggerBase] = None):
            ```
            * Parameters:
                * `browser_config (BrowserConfig)`: The global browser configuration settings.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance for browser management events.
        * Key Methods:
            * `async start() -> None`: Initializes the Playwright instance and launches or connects to the browser based on `browser_config` (e.g., launches a new browser instance or connects to an existing CDP endpoint via `ManagedBrowser`).
            * `async create_browser_context(self, crawlerRunConfig: Optional[CrawlerRunConfig] = None) -> playwright.async_api.BrowserContext`: Creates a new browser context. If `crawlerRunConfig` is provided, its settings (e.g., locale, viewport, proxy) can override the global `BrowserConfig`.
            * `async setup_context(self, context: playwright.async_api.BrowserContext, crawlerRunConfig: Optional[CrawlerRunConfig] = None, is_default: bool = False) -> None`: Applies various settings to a given browser context, including headers, cookies, viewport, geolocation, permissions, and storage state, based on `BrowserConfig` and `CrawlerRunConfig`.
            * `async get_page(self, crawlerRunConfig: CrawlerRunConfig) -> Tuple[playwright.async_api.Page, playwright.async_api.BrowserContext]`: Retrieves an existing page and context for a given `session_id` (if present in `crawlerRunConfig` and the session is active) or creates a new page and context. Manages context reuse based on a signature derived from `CrawlerRunConfig` to ensure contexts with different core settings (like proxy, locale) are isolated.
            * `async kill_session(self, session_id: str) -> None`: Closes the page and browser context associated with the given `session_id`, effectively ending that session.
            * `async close() -> None`: Closes all managed browser contexts and the main browser instance.

    * 6.2. Class `ManagedBrowser`
        * Purpose: Manages the lifecycle of a single, potentially persistent, browser process. It's used when `BrowserConfig.use_managed_browser` is `True` or `BrowserConfig.use_persistent_context` is `True`. It handles launching the browser with a specific user data directory and connecting via CDP.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                browser_type: str = "chromium",
                user_data_dir: Optional[str] = None,
                headless: bool = False,
                logger=None,
                host: str = "localhost",
                debugging_port: int = 9222,
                cdp_url: Optional[str] = None, # Added as per code_analysis
                browser_config: Optional[BrowserConfig] = None # Added as per code_analysis
            ):
            ```
            * Parameters:
                * `browser_type (str)`: "chromium", "firefox", or "webkit". Default: "chromium".
                * `user_data_dir (Optional[str])`: Path to the user data directory for the browser profile. If `None`, a temporary directory might be created.
                * `headless (bool)`: Whether to launch the browser in headless mode. Default: `False` (typically for managed/persistent scenarios).
                * `logger`: Logger instance.
                * `host (str)`: Host for the debugging port. Default: "localhost".
                * `debugging_port (int)`: Port for the Chrome DevTools Protocol. Default: `9222`.
                * `cdp_url (Optional[str])`: If provided, attempts to connect to an existing browser at this CDP URL instead of launching a new one.
                * `browser_config (Optional[BrowserConfig])`: The `BrowserConfig` object providing overall browser settings.
        * Key Methods:
            * `async start() -> str`: Starts the browser process (if not connecting to an existing `cdp_url`). If a new browser is launched, it uses the specified `user_data_dir` and `debugging_port`.
            * Returns: The CDP endpoint URL (e.g., "http://localhost:9222").
            * `async cleanup() -> None`: Terminates the browser process (if launched by this instance) and removes any temporary user data directory created by it.
        * Static Methods:
            * `async create_profile(cls, browser_config: Optional[BrowserConfig] = None, profile_name: Optional[str] = None, logger=None) -> str`:
                * Purpose: Launches a browser instance with a new or existing user profile, allowing interactive setup (e.g., manual login, cookie acceptance). The browser remains open until the user closes it.
                * Parameters:
                    * `browser_config (Optional[BrowserConfig])`: Optional browser configuration to use.
                    * `profile_name (Optional[str])`: Name for the profile. If `None`, a default name is used.
                    * `logger`: Logger instance.
                * Returns: The path to the created/used user data directory, which can then be passed to `BrowserConfig.user_data_dir`.
            * `list_profiles(cls) -> List[str]`:
                * Purpose: Lists the names of all browser profiles stored in the default Crawl4AI profiles directory (`~/.crawl4ai/profiles`).
                * Returns: A list of profile name strings.
            * `delete_profile(cls, profile_name_or_path: str) -> bool`:
                * Purpose: Deletes a browser profile either by its name (if in the default directory) or by its full path.
                * Returns: `True` if deletion was successful, `False` otherwise.

    * 6.3. Function `clone_runtime_state(src: BrowserContext, dst: BrowserContext, crawlerRunConfig: Optional[CrawlerRunConfig] = None, browserConfig: Optional[BrowserConfig] = None) -> None`
        * Purpose: Asynchronously copies runtime state (cookies, localStorage, session storage) from a source `BrowserContext` to a destination `BrowserContext`. Can also apply headers and geolocation from `CrawlerRunConfig` or `BrowserConfig` to the destination context.
        * Parameters:
            * `src (BrowserContext)`: The source browser context.
            * `dst (BrowserContext)`: The destination browser context.
            * `crawlerRunConfig (Optional[CrawlerRunConfig])`: Optional run configuration to apply to `dst`.
            * `browserConfig (Optional[BrowserConfig])`: Optional browser configuration to apply to `dst`.

## 7. Proxy Rotation Strategies (from `crawl4ai.proxy_strategy`)

    * 7.1. Abstract Base Class `ProxyRotationStrategy(ABC)`
        * Purpose: Defines the interface for strategies that provide a sequence of proxy configurations, enabling proxy rotation.
        * Abstract Methods:
            * `async get_next_proxy(self) -> Optional[ProxyConfig]`:
                * Purpose: Asynchronously retrieves the next `ProxyConfig` from the strategy.
                * Returns: A `ProxyConfig` object or `None` if no more proxies are available or an error occurs.
            * `add_proxies(self, proxies: List[ProxyConfig]) -> None`:
                * Purpose: Adds a list of `ProxyConfig` objects to the strategy's pool of proxies.

    * 7.2. Class `RoundRobinProxyStrategy(ProxyRotationStrategy)`
        * Purpose: A simple proxy rotation strategy that cycles through a list of provided proxies in a round-robin fashion.
        * Initialization (`__init__`):
            ```python
            def __init__(self, proxies: Optional[List[ProxyConfig]] = None):
            ```
            * Parameters:
                * `proxies (Optional[List[ProxyConfig]])`: An initial list of `ProxyConfig` objects. If `None`, the list is empty and proxies must be added via `add_proxies`.
        * Methods:
            * `add_proxies(self, proxies: List[ProxyConfig]) -> None`: Adds new `ProxyConfig` objects to the internal list of proxies and reinitializes the cycle.
            * `async get_next_proxy(self) -> Optional[ProxyConfig]`: Returns the next `ProxyConfig` from the list, cycling back to the beginning when the end is reached. Returns `None` if the list is empty.

## 8. Logging (from `crawl4ai.async_logger`)

    * 8.1. Abstract Base Class `AsyncLoggerBase(ABC)`
        * Purpose: Defines the basic interface for an asynchronous logger. Concrete implementations should provide methods for logging messages at different levels.
    * 8.2. Class `AsyncLogger(AsyncLoggerBase)`
        * Purpose: The default asynchronous logger for `crawl4ai`. It provides structured logging to both the console and optionally to a file, with customizable icons, colors, and verbosity levels.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                log_file: Optional[str] = None,
                verbose: bool = True,
                tag_width: int = 15, # outline had 10, code has 15
                icons: Optional[Dict[str, str]] = None,
                colors: Optional[Dict[LogLevel, LogColor]] = None, # Corrected type annotation
                log_level: LogLevel = LogLevel.INFO # Assuming LogLevel.INFO is a typical default
            ):
            ```
            * Parameters:
                * `log_file (Optional[str])`: Path to a file where logs should be written. If `None`, logs only to console.
                * `verbose (bool)`: If `True`, enables more detailed logging (DEBUG level). Default: `True`.
                * `tag_width (int)`: Width for the tag part of the log message. Default: `15`.
                * `icons (Optional[Dict[str, str]])`: Custom icons for different log tags.
                * `colors (Optional[Dict[LogLevel, LogColor]])`: Custom colors for different log levels.
                * `log_level (LogLevel)`: Minimum log level to output.
        * Key Methods (for logging):
            * `info(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs an informational message.
            * `warning(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs a warning message.
            * `error(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs an error message.
            * `debug(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs a debug message (only if `verbose=True` or `log_level` is DEBUG).
            * `url_status(self, url: str, success: bool, timing: float, tag: str = "FETCH", **params) -> None`: Logs the status of a URL fetch operation, including success/failure and timing.
            * `error_status(self, url: str, error: str, tag: str = "ERROR", **params) -> None`: Logs an error encountered for a specific URL.

## 9. Core Utility Functions (from `crawl4ai.async_configs`)
    * 9.1. `to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`
        * Purpose: Recursively converts a Python object (often a Pydantic model or a dataclass instance used for configuration) into a dictionary that is safe for JSON serialization. It handles nested objects, enums, and basic types.
        * Parameters:
            * `obj (Any)`: The object to be serialized.
            * `ignore_default_value (bool)`: If `True`, fields whose current value is the same as their default value (if applicable, e.g., for Pydantic models) might be omitted from the resulting dictionary. Default: `False`.
        * Returns: `Dict` - A JSON-serializable dictionary representation of the object.
    * 9.2. `from_serializable_dict(data: Any) -> Any`
        * Purpose: Recursively reconstructs Python objects from a dictionary representation (typically one created by `to_serializable_dict`). It attempts to instantiate classes based on a "type" key in the dictionary if present.
        * Parameters:
            * `data (Any)`: The dictionary (or basic type) to be deserialized.
        * Returns: `Any` - The reconstructed Python object or the original data if no special deserialization rule applies.
    * 9.3. `is_empty_value(value: Any) -> bool`
        * Purpose: Checks if a given value is considered "empty" (e.g., `None`, an empty string, an empty list, an empty dictionary).
        * Returns: `bool` - `True` if the value is empty, `False` otherwise.

## 10. Enumerations (Key Enums used in Core)
    * 10.1. `CacheMode` (from `crawl4ai.cache_context`, defined in `crawl4ai.async_configs` as per provided code)
        * Purpose: Defines the caching behavior for crawl operations.
        * Members:
            * `ENABLE`: (Value: "enable") Normal caching behavior; read from cache if available, write to cache after fetching.
            * `DISABLE`: (Value: "disable") No caching at all; always fetch fresh content and do not write to cache.
            * `READ_ONLY`: (Value: "read_only") Only read from the cache; do not write new or updated content to the cache.
            * `WRITE_ONLY`: (Value: "write_only") Only write to the cache after fetching; do not read from the cache.
            * `BYPASS`: (Value: "bypass") Skip the cache entirely for this specific operation; fetch fresh content and do not write to cache. This is often the default for individual `CrawlerRunConfig` instances.
    * 10.2. `DisplayMode` (from `crawl4ai.models`, used by `CrawlerMonitor`)
        * Purpose: Defines the display mode for the `CrawlerMonitor`.
        * Members:
            * `DETAILED`: Shows detailed information for each task.
            * `AGGREGATED`: Shows summary statistics and overall progress.
    * 10.3. `CrawlStatus` (from `crawl4ai.models`, used by `CrawlStats`)
        * Purpose: Represents the status of a crawl task.
        * Members:
            * `QUEUED`: Task is waiting to be processed.
            * `IN_PROGRESS`: Task is currently being processed.
            * `COMPLETED`: Task finished successfully.
            * `FAILED`: Task failed.

## 11. Versioning
    * 11.1. Accessing Library Version:
        * The current version of the `crawl4ai` library can be accessed programmatically via the `__version__` attribute of the top-level `crawl4ai` package.
        * Example:
            ```python
            from crawl4ai import __version__ as crawl4ai_version
            print(f"Crawl4AI Version: {crawl4ai_version}")
            # Expected output based on provided code: Crawl4AI Version: 0.6.3
            ```

## 12. Basic Usage Examples

    * 12.1. Minimal Crawl:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler

        async def main():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="http://example.com")
                if result.success:
                    print("Markdown (first 300 chars):")
                    print(result.markdown.raw_markdown[:300]) # Accessing raw_markdown
                else:
                    print(f"Error: {result.error_message}")

        if __name__ == "__main__":
            asyncio.run(main())
        ```

    * 12.2. Crawl with Basic Configuration:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

        async def main():
            browser_cfg = BrowserConfig(headless=True, browser_type="firefox")
            run_cfg = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=50
            )
            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                result = await crawler.arun(url="http://example.com", config=run_cfg)
                if result.success:
                    print(f"Status Code: {result.status_code}")
                    print(f"Cleaned HTML length: {len(result.cleaned_html)}")
                else:
                    print(f"Error: {result.error_message}")
        
        if __name__ == "__main__":
            asyncio.run(main())
        ```

    * 12.3. Accessing Links and Images from Result:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler

        async def main():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="http://example.com")
                if result.success:
                    print(f"Found {len(result.links.internal)} internal links.")
                    if result.links.internal:
                        print(f"First internal link: {result.links.internal[0].href}")
                    
                    print(f"Found {len(result.media.images)} images.")
                    if result.media.images:
                        print(f"First image src: {result.media.images[0].src}")
                else:
                    print(f"Error: {result.error_message}")

        if __name__ == "__main__":
            asyncio.run(main())
        ```
```

---


## Core Functionality - Reasoning
Source: crawl4ai_core_reasoning_content.llm.md

Okay, I have read and understood the objective and the detailed outline provided for the `core` component. I will now generate the "Reasoning & Problem-Solving Framework" document in Markdown format, strictly adhering to the outline and incorporating the principles for a "reasoning" document.

I will use the code snippets and documentation details you provided in the previous turn as the basis for this document.

```markdown
# Detailed Outline for crawl4ai - core Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_core.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

# I. Introduction to the Crawl4ai Core

The `core` component of Crawl4ai is the engine that powers all web crawling and scraping activities. Understanding its architecture and how its main pieces fit together is crucial for leveraging the full potential of the library. This section provides a high-level overview to set the stage.

*   A. **Purpose of the Core Component:**
    *   **Explaining why the `core` is central to `crawl4ai`'s functionality:**
        The `core` component serves as the central nervous system of Crawl4ai. It's responsible for orchestrating the entire lifecycle of a web crawl, from launching and managing browser instances to fetching web content, processing it through various strategies (scraping, markdown generation, data extraction), and finally delivering structured results. Without the `core`, other specialized components like PDF processors or advanced extraction strategies would lack the fundamental web interaction capabilities.
    *   **Core problems the `core` component aims to solve for users:**
        *   **Simplified Browser Automation:** Abstracts away the complexities of browser automation libraries like Playwright, providing a cleaner and more focused API for web crawling.
        *   **Flexible Configuration:** Offers granular control over how browsers are launched (`BrowserConfig`) and how individual crawl operations are executed (`CrawlerRunConfig`), allowing users to tailor crawls to specific site behaviors and data needs.
        *   **Unified Result Processing:** Provides a consistent `CrawlResult` object, regardless of whether the content was fetched via a full browser or a simple HTTP request, making it easier to build downstream processing pipelines.
        *   **Efficient Resource Management:** Includes mechanisms for managing browser contexts, sessions, and caching to optimize performance and resource utilization, especially for larger crawls.
        *   **Extensibility:** Designed with a strategy pattern, allowing users to plug in custom behaviors for crawling, scraping, and content processing.

*   B. **Key Abstractions and Design Philosophy:**
    *   **Brief overview of the main classes (`AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`) and their roles:**
        *   `AsyncWebCrawler`: This is your primary interaction point. You instantiate it, (optionally) configure it with a `BrowserConfig`, and then use its methods (`arun`, `arun_many`) to perform crawls, passing in `CrawlerRunConfig` objects for per-crawl specifics.
        *   `BrowserConfig`: Defines *how the browser itself is set up*. This is typically a one-time configuration for a `AsyncWebCrawler` instance, covering aspects like which browser engine to use, whether to run headless, proxy settings, user agent strings, and persistent user profiles. Think of it as setting up your physical web browser application.
        *   `CrawlerRunConfig`: Defines *how a specific URL or set of URLs should be crawled and processed*. This is passed to `arun` or `arun_many` and can override some browser-level settings. It controls aspects like caching, JavaScript execution for that run, content selectors, media capture (screenshots/PDFs), and extraction strategies. Think of it as the instructions you give your browser for a particular browsing session on a specific site.
    *   **How the separation of configurations (browser vs. run-specific) aids flexibility:**
        This separation is a key design choice. It allows you to:
        1.  **Reuse Browser Setups:** Configure a browser once (e.g., with specific proxies or a logged-in profile via `BrowserConfig`) and then use that same setup for multiple different crawl tasks, each with its own `CrawlerRunConfig` (e.g., one task extracts text, another captures screenshots, another looks for specific data structures).
        2.  **Targeted Overrides:** For a specific `arun` call, you might need to temporarily use a different user-agent or a different proxy without altering the global browser setup. `CrawlerRunConfig` allows these granular overrides.
        3.  **Clarity:** Keeps concerns separate. Browser setup is distinct from the instructions for a particular crawling job.

*   C. **Common Workflows Involving the Core:**
    *   **Single Page Static Crawl:** Initialize `AsyncWebCrawler`, call `arun()` with a URL and a basic `CrawlerRunConfig` (often just defaults or `CacheMode.BYPASS`).
    *   **Single Page Dynamic Crawl:** Similar to static, but `CrawlerRunConfig` might include `js_code` to interact with the page (e.g., click buttons, scroll) and `wait_for` conditions to ensure dynamic content loads.
    *   **Multi-Page Batch Crawl:** Use `arun_many()` with a list of URLs. You might use a shared `CrawlerRunConfig` if processing is similar for all URLs, or provide custom logic to generate different `CrawlerRunConfig` objects per URL if needed.
    *   **Persistent Session Crawl:** Use `BrowserConfig` with `user_data_dir` for profile persistence or `storage_state`. Then, use `CrawlerRunConfig` with a consistent `session_id` across multiple `arun()` calls to maintain login state or navigate multi-step processes.
    *   **API/JSON Fetching:** Use `AsyncHTTPCrawlerStrategy` with `AsyncWebCrawler` and an appropriate `HTTPCrawlerConfig` (within `CrawlerRunConfig`) for lightweight, non-browser data retrieval.

# II. Mastering `AsyncWebCrawler`: The Heart of Crawling

The `AsyncWebCrawler` class is the cornerstone of Crawl4ai. It orchestrates browser interactions, manages configurations, and processes web content. Understanding its nuances will empower you to build sophisticated and efficient web crawlers.

*   A. **Understanding `AsyncWebCrawler`'s Role:**
    *   **Why `AsyncWebCrawler` is the primary entry point for most crawling tasks:**
        `AsyncWebCrawler` provides a high-level, user-friendly API that abstracts away the complexities of direct browser automation (like Playwright or Selenium). It integrates browser launching, context management, page navigation, content retrieval, and initial processing into a cohesive workflow. Whether you're fetching a single page or thousands, `AsyncWebCrawler` is designed to be your go-to tool.
    *   **Its responsibilities in managing browser instances and executing crawl operations:**
        *   **Browser Lifecycle:** Manages the launching and closing of browser instances based on the provided `BrowserConfig`.
        *   **Context and Page Management:** Handles the creation and isolation of browser contexts and pages, which is crucial for maintaining separate states (cookies, local storage) if needed, especially with `session_id` usage.
        *   **Strategy Execution:** Delegates the actual "crawling" (fetching content from a URL) to a configurable `AsyncCrawlerStrategy` (defaulting to `AsyncPlaywrightCrawlerStrategy`).
        *   **Configuration Orchestration:** Applies both `BrowserConfig` (global browser settings) and `CrawlerRunConfig` (per-crawl settings) to each operation.
        *   **Result Aggregation:** Takes the raw response from the crawler strategy and processes it through scraping and markdown generation (via a `ContentScrapingStrategy` and `MarkdownGenerationStrategy`) to produce the final `CrawlResult`.

*   B. **Initialization and Lifecycle Management:**
    *   1.  **Best Practices for Initializing `AsyncWebCrawler`:**
        *   **When to pass a `BrowserConfig` vs. relying on defaults:**
            *   **Rely on defaults:** For quick, simple crawls where standard browser behavior is sufficient (e.g., fetching a public, static webpage).
            *   **Pass a `BrowserConfig`:**
                *   When you need to specify a browser engine other than Chromium (e.g., Firefox, WebKit).
                *   To run in headed (non-headless) mode for debugging.
                *   To configure proxies.
                *   To set custom user agents or other HTTP headers globally.
                *   To use persistent browser profiles (`user_data_dir`) or load existing browser state (`storage_state`).
                *   For advanced launch arguments.
        *   **Considerations for `logger` and `thread_safe` parameters:**
            *   `logger`: Pass a custom `AsyncLogger` instance if you have a centralized logging setup or need specific log formatting/destinations. If `None`, a default logger is created. For reasoning documents, verbose logging is often helpful.
            *   `thread_safe`: The default `False` is generally fine for most `asyncio`-based applications. Set to `True` only if you are interacting with the same `AsyncWebCrawler` instance from multiple OS-level threads, which is a less common pattern in asyncio. It introduces a lock around critical sections.

    *   2.  **Understanding the Crawler Lifecycle (`start`, `close`, and Context Management):**
        *   **When and why to use explicit `crawler.start()` and `crawler.close()`:**
            *   **Scenarios:**
                *   **Long-running applications:** Where the crawler needs to be available for an extended period to process requests on demand.
                *   **Reusing crawler instances:** If you intend to make multiple, separate `arun()` or `arun_many()` calls with the same underlying browser instance and configuration over time.
                *   **Pre-warming/Setup:** If you need to perform setup tasks (like logging into a site) once and then run multiple crawls using that established session.
            *   **Benefits:**
                *   **Resource Control:** You explicitly manage when browser resources are allocated and released.
                *   **Performance:** Avoids the overhead of launching a new browser for every crawl operation if the instance is reused.
            *   **Code Example: Illustrating explicit start/close for a persistent crawler**
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

                async def long_running_crawler_task():
                    # Configure browser once
                    browser_cfg = BrowserConfig(headless=True, user_agent="MyPersistentCrawler/1.0")
                    crawler = AsyncWebCrawler(config=browser_cfg)

                    print("Starting crawler...")
                    await crawler.start() # Explicitly start the browser

                    try:
                        # First crawl
                        run_cfg1 = CrawlerRunConfig(url="https://example.com/page1")
                        result1 = await crawler.arun(config=run_cfg1)
                        if result1.success:
                            print(f"Page 1 ({result1.status_code}): {result1.markdown.raw_markdown[:100]}...")

                        # ... some time later, or another task ...

                        # Second crawl with the same browser instance
                        run_cfg2 = CrawlerRunConfig(url="https://example.com/page2")
                        result2 = await crawler.arun(config=run_cfg2)
                        if result2.success:
                            print(f"Page 2 ({result2.status_code}): {result2.markdown.raw_markdown[:100]}...")

                    finally:
                        print("Closing crawler...")
                        await crawler.close() # Explicitly close the browser and release resources

                if __name__ == "__main__":
                    asyncio.run(long_running_crawler_task())
                ```
        *   **Leveraging `async with AsyncWebCrawler(...)` (Context Manager):**
            *   **Benefits:**
                *   **Automatic Resource Cleanup:** Ensures `crawler.start()` is called at the beginning and `crawler.close()` is called at the end, even if errors occur within the block. This is the most common and recommended way for most use cases.
                *   **Cleaner Code:** Reduces boilerplate for resource management.
            *   **When it's most appropriate:**
                For most scripts or functions where the crawler's lifetime is confined to that specific block of code.
            *   **Code Example: Illustrating typical context manager usage**
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

                async def simple_crawl_with_context_manager():
                    browser_cfg = BrowserConfig(headless=True)
                    run_cfg = CrawlerRunConfig(url="https://example.com")

                    async with AsyncWebCrawler(config=browser_cfg) as crawler:
                        # crawler.start() is implicitly called
                        result = await crawler.arun(config=run_cfg)
                        if result.success:
                            print(f"Content from {result.url}: {result.markdown.raw_markdown[:100]}...")
                        else:
                            print(f"Failed: {result.error_message}")
                    # crawler.close() is implicitly called here, even if an exception occurred

                if __name__ == "__main__":
                    asyncio.run(simple_crawl_with_context_manager())
                ```
        *   **Potential pitfalls:**
            *   Forgetting to call `await crawler.close()` when using explicit `await crawler.start()`. This can lead to dangling browser processes and resource leaks. The context manager (`async with`) prevents this.

*   C. **Executing Crawl Operations:**
    *   1.  **Single URL Crawling with `arun()`:**
        *   **Common use cases for `arun()`:**
            *   Fetching and processing a single webpage.
            *   Performing a specific interaction on one page (e.g., filling a form, then extracting results).
            *   Testing configurations on a sample URL.
        *   **Essential `CrawlerRunConfig` parameters for `arun()`:**
            *   `url` (str): The URL to crawl. This is the most fundamental parameter.
            *   `js_code` (Optional[str | List[str]]): JavaScript to execute after the page loads.
            *   `wait_for` (Optional[str]): A CSS selector or JS condition to wait for before proceeding.
            *   See Section IV for a full dive into `CrawlerRunConfig`.
        *   **Workflow: Fetching and processing a single page:**
            1.  Initialize `AsyncWebCrawler` (potentially with a `BrowserConfig`).
            2.  Create a `CrawlerRunConfig` instance, setting the `url` and any other desired options.
            3.  Call `await crawler.arun(config=your_run_config)`.
            4.  Process the returned `CrawlResult` object.
        *   **Code Example: Basic `arun()` usage with a simple `CrawlerRunConfig`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_single_page():
                # Basic CrawlerRunConfig for a specific URL
                run_config = CrawlerRunConfig(
                    url="https://quotes.toscrape.com/",
                    cache_mode=CacheMode.BYPASS # Ensure fresh fetch for this example
                )

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(config=run_config)
                    if result.success:
                        print(f"Successfully crawled: {result.url}")
                        print(f"Markdown (first 200 chars): {result.markdown.raw_markdown[:200]}...")
                    else:
                        print(f"Failed to crawl {result.url}: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(crawl_single_page())
            ```
        *   **Troubleshooting common `arun()` issues:**
            *   **Timeouts:** The page might be taking too long to load, or a `wait_for` condition isn't met. Adjust `page_timeout` or `wait_for_timeout` in `CrawlerRunConfig`.
            *   **JavaScript Errors:** If `js_code` execution fails, check the browser console (run with `headless=False` in `BrowserConfig`) or enable `log_console=True` in `CrawlerRunConfig` to see console messages in Crawl4ai logs.
            *   **Content Not Found:** The `css_selector` or `target_elements` might be incorrect, or the content might load after the crawler has finished processing. Use `wait_for` or `delay_before_retrieve_html`.

    *   2.  **Batch Crawling with `arun_many()`:**
        *   **When to prefer `arun_many()` over looping `arun()`:**
            `arun_many()` is generally preferred when you have a list of URLs to process because it leverages an internal dispatcher (like `MemoryAdaptiveDispatcher` by default) to manage concurrency, rate limiting, and resource usage more effectively than a simple Python loop of `await crawler.arun()`. This leads to better performance and stability for bulk operations.
        *   **How `arun_many()` handles concurrency and dispatching:**
            It uses a dispatcher strategy. The default `MemoryAdaptiveDispatcher` attempts to run multiple crawl tasks concurrently, adapting the level of concurrency based on available system memory. You can also provide custom dispatchers for more fine-grained control.
        *   **Using a shared `CrawlerRunConfig` vs. per-URL configurations:**
            *   **Shared `CrawlerRunConfig`:** If all URLs require similar processing (e.g., same extraction schema, same JS interactions), you can pass a single `CrawlerRunConfig` to `arun_many()`. The `url` property within this config will be ignored as URLs are taken from the input list.
            *   **Per-URL Configurations:** `arun_many()` is not designed to take a list of `CrawlerRunConfig` objects directly. If you need vastly different configurations per URL, you might iterate and call `arun()` or, for more sophisticated needs, look into customizing the dispatcher or using higher-level orchestration tools. A common pattern is to use a base `CrawlerRunConfig` and then clone/modify it slightly for each URL within a loop if minor variations are needed, though this would be outside the direct `arun_many` call. For `arun_many`, the primary variation is the URL itself.
        *   **Strategies for rate limiting and error handling within `arun_many()` (via Dispatcher configuration):**
            The dispatcher, particularly its `RateLimiter`, handles rate limiting (delays between requests, exponential backoff on 429/503 errors) and retries. If a URL fails after retries, `arun_many` will still proceed with other URLs and the failed result will indicate the error.
        *   **Code Example: Using `arun_many()` with a list of URLs and a shared `CrawlerRunConfig`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_multiple_pages():
                urls_to_crawl = [
                    "http://quotes.toscrape.com/page/1/",
                    "http://quotes.toscrape.com/page/2/",
                    "http://quotes.toscrape.com/tag/humor/", # This might be a different structure
                ]

                # Shared config for all URLs in this batch
                shared_run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=50 # Only process pages with meaningful content
                )

                async with AsyncWebCrawler() as crawler:
                    # results will be a list of CrawlResult objects, in the same order as urls_to_crawl
                    results_container = await crawler.arun_many(urls=urls_to_crawl, config=shared_run_config)
                    
                    for result in results_container: # Iterate through CrawlResultContainer
                        if result.success:
                            print(f"Processed: {result.url}, Markdown Length: {len(result.markdown.raw_markdown)}")
                        else:
                            print(f"Failed: {result.url}, Error: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(crawl_multiple_pages())
            ```
        *   **Code Example: Using `arun_many()` with a custom dispatcher for advanced control**
            *(Note: Custom dispatchers are more advanced; the default `MemoryAdaptiveDispatcher` is usually sufficient.)*
            ```python
            import asyncio
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, CacheMode,
                MemoryAdaptiveDispatcher, RateLimiter, CrawlerMonitor, DisplayMode
            )

            async def crawl_with_custom_dispatcher():
                urls = [f"http://quotes.toscrape.com/page/{i}/" for i in range(1, 4)]
                
                # Custom rate limiter: 0.5-1.0s base delay, max 10s, 2 retries
                rate_limiter = RateLimiter(base_delay=(0.5, 1.0), max_delay=10.0, max_retries=2)
                
                # Custom monitor
                monitor = CrawlerMonitor(display_mode=DisplayMode.DETAILED)

                # Custom dispatcher with more conservative settings
                custom_dispatcher = MemoryAdaptiveDispatcher(
                    memory_threshold_percent=60.0, # Pause if memory > 60%
                    check_interval=2.0,           # Check memory every 2 seconds
                    max_session_permit=5,         # Max 5 concurrent crawls
                    rate_limiter=rate_limiter,
                    monitor=monitor
                )

                run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

                async with AsyncWebCrawler() as crawler:
                    results_container = await crawler.arun_many(
                        urls=urls,
                        config=run_config,
                        dispatcher=custom_dispatcher
                    )
                    for result in results_container:
                        if result.success:
                            print(f"URL: {result.url} - Success")
                        else:
                            print(f"URL: {result.url} - Failed: {result.error_message}")
                            
            if __name__ == "__main__":
                asyncio.run(crawl_with_custom_dispatcher())
            ```
*   D. **Understanding the Internal Processing Flow (`aprocess_html`):**
    *   **High-level explanation of what happens after HTML is fetched:**
        Once the `AsyncCrawlerStrategy` (e.g., `AsyncPlaywrightCrawlerStrategy`) successfully fetches the HTML content for a URL, `AsyncWebCrawler`'s `aprocess_html` method (or a similar internal handler) takes over. This method orchestrates several subsequent steps:
        1.  **Scraping Strategy Application:** The raw HTML is passed to a `ContentScrapingStrategy` (defaulting to `WebScrapingStrategy` or `LXMLExtractionStrategy` based on configuration). This strategy is responsible for:
            *   Cleaning the HTML (removing scripts, styles, comments).
            *   Applying `css_selector` or `target_elements` to scope the content if specified.
            *   Extracting links (`<a>` tags).
            *   Extracting media information (`<img>`, `<video>`, `<audio>` tags, and data tables).
            *   Extracting page metadata (title, meta description, OpenGraph tags, etc.).
            The output of this step is a `ScrapingResult` object containing `cleaned_html`, `media`, `links`, and `metadata`.
        2.  **Content Filtering (Optional):** If a `content_filter` (like `PruningContentFilter` or `LLMContentFilter`) is configured in the `MarkdownGenerationStrategy` (which is part of `CrawlerRunConfig`), the `cleaned_html` from the `ScrapingResult` is passed through this filter. This step aims to further refine the HTML to only include the most relevant content blocks. The output is "fit HTML".
        3.  **Markdown Generation:** The (potentially filtered) HTML is then passed to a `MarkdownGenerationStrategy` (defaulting to `DefaultMarkdownGenerator`). This strategy converts the HTML into Markdown. If content filtering occurred, it might generate both a `raw_markdown` (from the original cleaned HTML) and a `fit_markdown` (from the filtered HTML). It also handles citation generation for links. The output is a `MarkdownGenerationResult`.
        4.  **Structured Data Extraction (Optional):** If an `extraction_strategy` is provided in `CrawlerRunConfig`, it's applied *after* Markdown generation. The input to the extraction strategy depends on its `input_format` (e.g., "markdown", "html", "fit_markdown"). This strategy extracts structured data (like JSON) based on its specific logic (e.g., LLM prompting, CSS selectors via `JsonCssExtractionStrategy`).
        5.  **Result Aggregation:** Finally, all these pieces (`CrawlResult.html` (original), `cleaned_html`, `markdown` (the `MarkdownGenerationResult` object), `extracted_content`, `media`, `links`, `metadata`, etc.) are assembled into the final `CrawlResult` object.
    *   **How this internal flow influences the choice and impact of `CrawlerRunConfig` parameters:**
        *   `css_selector`: Applied early by the `ScrapingStrategy`. If set, *only* the HTML within this selector is processed for everything downstream (Markdown, extraction, etc.). This can significantly speed up processing and improve relevance.
        *   `target_elements`: Also applied by `ScrapingStrategy`, but its effect is primarily on what content is considered for *Markdown generation* and *structured data extraction* if the extraction strategy uses HTML as input. Links and media are still typically extracted from the whole (scoped by `css_selector` if present) page.
        *   `extraction_strategy`: This is one of the last steps. Its effectiveness depends on the quality and format of its input (e.g., clean Markdown or specific HTML structure).
        *   `markdown_generator`: Affects how `cleaned_html` (or `fit_html`) is converted to Markdown. The `content_filter` within it can drastically change the `fit_markdown`.
    *   **Decision Guide: When to rely on default processing vs. providing custom strategies:**
        *   **Default Processing:** Sufficient for many common use cases where you need the main content of a page converted to clean Markdown, along with standard metadata, links, and images.
        *   **Custom `ContentScrapingStrategy`:** Consider if you need highly specialized HTML cleaning, link/media extraction logic that differs significantly from the defaults, or if you're dealing with non-standard HTML structures.
        *   **Custom `MarkdownGenerationStrategy`:** If you need a different HTML-to-Markdown conversion engine, different citation styles, or very specific pre/post-processing of the Markdown.
        *   **Custom `ContentFilter` (within `MarkdownGenerationStrategy`):** If the default pruning/fitting logic isn't capturing the desired content accurately and you need a more sophisticated way (e.g., LLM-based, custom heuristics) to identify relevant sections.
        *   **Custom `ExtractionStrategy`:** Essential if you need to extract structured data beyond what `JsoupCssExtractionStrategy` can offer, or if you want to use LLMs for extraction based on natural language prompts or a defined schema.

# III. Configuring the Browser: `BrowserConfig` Deep Dive

`BrowserConfig` is your toolkit for defining the environment in which your web crawls will run. It dictates everything from the browser engine and its appearance (headless/headed) to network settings like proxies and crucial identity aspects like user agents and persistent storage. A well-configured `BrowserConfig` is often the key to successful and reliable crawling, especially on modern, dynamic websites.

*   A. **Purpose and Importance of `BrowserConfig`:**
    *   **Explaining how `BrowserConfig` defines the foundational browser environment:**
        Think of `BrowserConfig` as setting up your web browser application *before* you even type a URL. It's the global configuration for the `AsyncWebCrawler` instance. Changes here affect all crawl operations performed by that instance, unless specifically overridden by a `CrawlerRunConfig`.
    *   **Why getting this right is crucial for successful and stealthy crawling:**
        *   **Site Compatibility:** Some sites render or behave differently based on the browser engine or viewport size.
        *   **Stealth:** Websites employ various techniques to detect and block automated crawlers. A realistic `BrowserConfig` (e.g., common user agent, appropriate headers, possibly proxies) can significantly reduce the chances of detection.
        *   **Resource Management:** Settings like `headless` mode or `browser_mode` can impact how many resources your crawler consumes.
        *   **Session Persistence:** For sites requiring logins or multi-step interactions, `use_persistent_context` and `user_data_dir` are essential for maintaining state.

*   B. **Key `BrowserConfig` Decision Points and Workflows:**
    *   1.  **Choosing a `browser_type` (`chromium`, `firefox`, `webkit`):**
        *   **Trade-offs:**
            *   `chromium`: Most widely used, excellent DevTools support, generally good performance and compatibility. Often the default and a good starting point.
            *   `firefox`: Strong alternative, good privacy features, sometimes handles certain sites differently.
            *   `webkit`: Engine behind Safari. Useful for testing Safari-specific rendering or behavior.
        *   **When one might be preferred:**
            *   Start with `chromium`.
            *   If you encounter issues specific to Chromium-based browsers or need to emulate Firefox/Safari users, switch accordingly.
            *   Some sites might have better compatibility or less aggressive bot detection for one engine over others.
    *   2.  **Headless vs. Headed Mode (`headless`):**
        *   **Rationale for headless (`headless=True`, default):**
            *   No visible GUI, runs in the background.
            *   Essential for server environments or automated scripts.
            *   Generally consumes fewer resources than headed mode.
        *   **When to use headed mode (`headless=False`):**
            *   **Debugging:** Visually inspect what the crawler is doing, observe page rendering, and identify issues with selectors or interactions.
            *   **Complex Interactions:** For sites with very complex JavaScript, CAPTCHAs (though Crawl4ai doesn't solve these directly), or interactions that are hard to automate blindly.
            *   **Initial Setup:** When first developing a script for a new site, running in headed mode can be invaluable.
        *   **Impact on resource consumption and stealth:**
            *   Headed mode consumes more CPU and memory.
            *   Some rudimentary bot detection systems might flag headless browsers, though modern headless modes are much harder to detect than older versions.
    *   3.  **Browser Launch Modes (`browser_mode`):**
        *   **Understanding `"dedicated"` (default):**
            *   **Pros:** Each `AsyncWebCrawler` instance (or more accurately, its `BrowserManager`) launches and manages its own independent browser process. This provides strong isolation.
            *   **Cons:** Can be resource-intensive if you have many `AsyncWebCrawler` instances, as each spawns a new browser.
        *   **Understanding `"builtin"` (CDP based):**
            *   The `builtin` mode is designed to use a browser instance that Crawl4ai itself manages in the background, potentially shared across different `AsyncWebCrawler` instances if configured carefully (though this is an advanced use case). It connects via the Chrome DevTools Protocol (CDP).
            *   **When to use:** When you want Crawl4ai to manage the browser lifecycle but need to connect to it via CDP for more direct control or to share a browser instance.
            *   **Setup:** `use_managed_browser` is implicitly `True`. `cdp_url` is typically set by the internal `ManagedBrowser`.
        *   **Understanding `"cdp"` (external CDP):**
            *   **Scenarios:** You have an existing browser instance already running (e.g., a Chrome browser launched with `--remote-debugging-port=9222`) and you want Crawl4ai to connect to and control that instance.
            *   **Setup:** You must provide the `cdp_url` (e.g., `"ws://localhost:9222/devtools/browser/..."`).
        *   **Understanding `"docker"`:**
            *   **Benefits:** Runs the browser within a Docker container, providing excellent isolation, reproducibility, and easier dependency management, especially in CI/CD environments or when deploying to different systems. The core library handles launching a pre-configured Docker container (like `browserless/chrome`) and connecting to it.
            *   Refer to [Docker Deployment Guide](../../../basic/docker-deployment.md) for details.
        *   **Decision Guide: Selecting the appropriate `browser_mode`:**
            *   `"dedicated"`: Good default for simplicity and isolation when running a moderate number of crawlers.
            *   `"cdp"`: Use if you need to attach to an externally managed browser.
            *   `"docker"`: Excellent for reproducible environments, CI/CD, and avoiding local browser/driver issues.
            *   `"builtin"`: More of an internal mechanism, usually not directly set by users unless for advanced shared browser scenarios.
    *   4.  **User Agent Management (`user_agent`, `user_agent_mode`):**
        *   **Importance of a realistic User-Agent:** Many sites use the User-Agent string for basic bot detection or to serve different content. A missing or suspicious UA can lead to blocks or incorrect page versions.
        *   **Strategies for User-Agent rotation using `user_agent_mode="random"`:**
            Crawl4ai uses the `fake_useragent` library (via `ValidUAGenerator`) to pick from a list of common, valid user agents. This helps in making requests appear as if they are coming from different browsers/devices.
        *   **When to provide a specific `user_agent`:**
            If you need to emulate a very specific browser or device, or if a site requires a particular User-Agent to function correctly.
        *   **How `user_agent_generator_config` can be used for fine-tuning:**
            You can pass a dictionary to `user_agent_generator_config` to control aspects of the `ValidUAGenerator`, such as specifying browser types (`browsers=['chrome', 'edge']`), OS types, or min/max popularity.
        *   **Code Example: Setting a custom User-Agent**
            ```python
            from crawl4ai import BrowserConfig

            # To emulate a specific mobile browser
            custom_ua_config = BrowserConfig(
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
            )
            # Now use this custom_ua_config when initializing AsyncWebCrawler
            ```
        *   **Code Example: Using random User-Agent generation for Chrome and Edge on Windows**
            ```python
            from crawl4ai import BrowserConfig

            random_ua_config = BrowserConfig(
                user_agent_mode="random",
                user_agent_generator_config={
                    "os_names": ["windows"],
                    "browser_names": ["chrome", "edge"]
                }
            )
            # This will generate UAs like Chrome on Windows or Edge on Windows
            ```
    *   5.  **Proxy Configuration (`proxy_config` via `ProxyConfig` object):**
        *   **When and why to use proxies:**
            *   **IP Rotation:** To avoid IP-based blocking when making many requests to the same site.
            *   **Geo-targeting:** To access content as if you are browsing from a specific geographical location.
            *   **Anonymity/Privacy:** To mask your actual IP address.
        *   **Workflow: Setting up a single proxy:**
            1.  Create a `ProxyConfig` instance with `server`, and optionally `username` and `password`.
            2.  Assign this `ProxyConfig` instance to `BrowserConfig.proxy_config`.
        *   **Workflow: Integrating with proxy rotation strategies:**
            This is typically handled at the `CrawlerRunConfig` level for more dynamic rotation, but `BrowserConfig` can set a default proxy. For rotation, you'd use `CrawlerRunConfig(proxy_rotation_strategy=YourStrategy())`. See Section VII.A for details on rotation.
        *   **Code Example: Configuring a single HTTP/SOCKS proxy with authentication**
            ```python
            from crawl4ai import BrowserConfig, ProxyConfig

            # HTTP Proxy with authentication
            http_proxy_cfg = ProxyConfig(
                server="http://proxy.example.com:8080",
                username="proxy_user",
                password="proxy_password"
            )
            browser_with_http_proxy = BrowserConfig(proxy_config=http_proxy_cfg)

            # SOCKS5 Proxy (authentication handled in server string if supported by proxy)
            socks_proxy_cfg = ProxyConfig(
                server="socks5://another-proxy.example.com:1080"
            )
            browser_with_socks_proxy = BrowserConfig(proxy_config=socks_proxy_cfg)
            ```
        *   **Troubleshooting common proxy connection issues:**
            *   Incorrect server address or port.
            *   Wrong username/password.
            *   Proxy server is down or not reachable.
            *   Firewall blocking connection to the proxy.
            *   The website itself might be blocking the proxy's IP.
    *   6.  **Persistent Context and User Data (`use_persistent_context`, `user_data_dir`):**
        *   **Understanding the benefits of persistent contexts:**
            When `use_persistent_context=True` and a `user_data_dir` is specified, Playwright creates a persistent browser profile in that directory. This means cookies, localStorage, sessionStorage, and other browser data are saved between sessions. This is invaluable for:
            *   **Login Persistence:** Log in to a site once, and subsequent crawls using the same `user_data_dir` will likely remain logged in.
            *   **Maintaining Site Preferences:** If a site stores user preferences (e.g., dark mode, language) in local storage or cookies.
            *   **Reducing Repeated Setup:** Avoid re-doing consent banners or initial setup steps on every crawl.
        *   **Workflow: Creating and reusing a browser profile for login persistence:**
            1.  **First Run (Profile Creation & Login):**
                *   Set `headless=False` initially to manually perform the login.
                *   Specify a `user_data_dir` (e.g., `./my_browser_profile`).
                *   Set `use_persistent_context=True`.
                *   Run the crawler, navigate to the login page, and log in manually.
                *   Close the browser. The session data is now saved in `user_data_dir`.
            2.  **Subsequent Runs (Reusing Profile):**
                *   Use the *same* `user_data_dir`.
                *   Ensure `use_persistent_context=True`.
                *   You can now set `headless=True`.
                *   The crawler should start with the saved session, already logged in.
        *   **Best practices for managing `user_data_dir`:**
            *   Choose a unique, descriptive path for each distinct profile/session you want to maintain.
            *   Be aware of the disk space used, as profiles can grow.
            *   Ensure your script has write permissions to the specified directory.
        *   **Code Example: Setting up and using a persistent context for a login workflow**
            ```python
            import asyncio
            import os
            from pathlib import Path
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

            PROFILE_DIR = Path("./my_persistent_profile")

            async def initial_login_setup():
                # Create profile dir if it doesn't exist
                PROFILE_DIR.mkdir(parents=True, exist_ok=True)
                
                login_browser_cfg = BrowserConfig(
                    headless=False, # Run headed to manually log in
                    user_data_dir=str(PROFILE_DIR),
                    use_persistent_context=True
                )
                # For this setup run, we don't need a complex CrawlerRunConfig
                login_run_cfg = CrawlerRunConfig(url="https://example.com/login") # Replace with actual login URL

                print(f"Please log in manually in the browser window using profile: {PROFILE_DIR}")
                print("Close the browser window once logged in to save the session.")
                
                # We use a longer timeout to allow for manual login
                # And we don't need to process the result here, just establish the session
                async with AsyncWebCrawler(config=login_browser_cfg) as crawler:
                    # The page_timeout in BrowserConfig will apply here if not overridden in CrawlerRunConfig
                    # A simple goto is enough; the user handles the login in the browser
                    await crawler.arun(config=CrawlerRunConfig(url=login_run_cfg.url, page_timeout=300000)) # 5 min timeout

                print("Login session should be saved.")

            async def crawl_protected_page_with_session():
                if not PROFILE_DIR.exists() or not any(PROFILE_DIR.iterdir()):
                     print(f"Profile directory {PROFILE_DIR} is empty or does not exist. Run initial_login_setup() first.")
                     return

                persistent_browser_cfg = BrowserConfig(
                    headless=True, # Can now run headless
                    user_data_dir=str(PROFILE_DIR),
                    use_persistent_context=True
                )
                # Target a page that requires login
                protected_run_cfg = CrawlerRunConfig(url="https://example.com/dashboard") 

                async with AsyncWebCrawler(config=persistent_browser_cfg) as crawler:
                    result = await crawler.arun(config=protected_run_cfg)
                    if result.success and "Welcome User" in result.html: # Check for logged-in content
                        print(f"Successfully accessed protected page: {result.url}")
                        print(f"Content snippet: {result.markdown.raw_markdown[:200]}...")
                    elif result.status_code == 403 or "login" in result.url.lower():
                         print(f"Failed to access protected page. Still on login page or got 403. URL: {result.url}")
                    else:
                        print(f"Failed to crawl protected page: {result.error_message}, URL: {result.url}")
            
            async def main():
                # Run this once to log in and create the profile
                # await initial_login_setup() 
                
                # Then run this to use the saved session
                await crawl_protected_page_with_session()

            if __name__ == "__main__":
                # IMPORTANT: You'd typically run initial_login_setup() once manually,
                # then comment it out and run crawl_protected_page_with_session() for subsequent crawls.
                asyncio.run(main())
            ```
    *   7.  **Viewport Configuration (`viewport_width`, `viewport_height`, `viewport`):**
        *   **How viewport size can affect page rendering and element visibility:**
            Websites often use responsive design, meaning their layout and the visibility of certain elements change based on the viewport (browser window) size. If your target elements are only visible on larger screens (or mobile views), setting an appropriate viewport is crucial.
        *   **Strategies for choosing appropriate viewport dimensions:**
            *   Inspect the target website in a regular browser, resize the window, and see how content changes.
            *   Common desktop viewports: `1920x1080`, `1366x768`, `1280x720`.
            *   Common mobile viewports (emulated): `375x667` (iPhone 6/7/8), `414x896` (iPhone XR/11).
            *   If `viewport` dict is provided, it overrides `viewport_width` and `viewport_height`. E.g., `viewport={"width": 1920, "height": 1080}`.
    *   8.  **Advanced Browser Arguments (`extra_args`):**
        *   **When to use `extra_args`:**
            For passing command-line arguments directly to the browser executable. This is useful for enabling/disabling experimental features, or for very specific browser configurations not exposed by Playwright's high-level API.
        *   **Commonly used arguments and their effects (Chromium examples):**
            *   `--disable-gpu`: Can sometimes help in headless environments or reduce resource usage.
            *   `--no-sandbox`: Often required in Docker/CI environments, but use with caution as it reduces security. (Crawl4ai's `ManagedBrowser` often adds this automatically in headless Linux).
            *   `--lang=fr-FR`: Set browser UI language.
            *   `--disable-blink-features=AutomationControlled`: Attempt to make the browser appear less like an automated tool.
        *   **Caution:** Incorrect or incompatible arguments can prevent the browser from launching or cause instability. Refer to the browser's documentation (e.g., [Chromium Command Line Switches](https://peter.sh/experiments/chromium-command-line-switches/)) for a full list.
    *   9.  **Storage State Management (`storage_state`):**
        *   **What is storage state and why is it useful?**
            Storage state is a JSON object (or path to a JSON file) that captures cookies, `localStorage`, and `sessionStorage` from a browser context. It's a powerful way to persist and reuse login sessions or application states without relying on full user profiles (`user_data_dir`). It's generally more lightweight than managing entire profile directories.
        *   **Workflow: Capturing storage state:**
            1.  Launch a browser (often headed for manual interaction).
            2.  Perform the necessary actions (e.g., log in, accept cookies, set preferences).
            3.  Use Playwright's `context.storage_state(path="my_storage_state.json")` to save the state. Crawl4ai doesn't have a direct command to save state *during* a crawl, so this is typically done in a separate setup script.
                ```python
                # Example: Separate script to save storage state
                import asyncio
                from playwright.async_api import async_playwright

                async def save_state():
                    async with async_playwright() as p:
                        browser = await p.chromium.launch(headless=False)
                        context = await browser.new_context()
                        page = await context.new_page()
                        await page.goto("https://example.com/login") # Replace
                        # ... user manually logs in ...
                        input("Press Enter after logging in to save state...")
                        await context.storage_state(path="auth_state.json")
                        await browser.close()
                        print("Storage state saved to auth_state.json")

                # asyncio.run(save_state()) # Run this once
                ```
        *   **Workflow: Reusing storage state in Crawl4ai:**
            Pass the path to the saved JSON file (or the loaded JSON object itself) to `BrowserConfig(storage_state="auth_state.json")`.
        *   **Code Example: Illustrating how to load storage state in Crawl4ai**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

            async def crawl_with_saved_state():
                # Assumes "auth_state.json" was created by a previous login session
                browser_cfg = BrowserConfig(
                    headless=True,
                    storage_state="auth_state.json" # Load cookies, localStorage, etc.
                )
                # Target a page that requires the saved session
                run_cfg = CrawlerRunConfig(url="https://example.com/dashboard")

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    result = await crawler.arun(config=run_cfg)
                    if result.success and "Welcome User" in result.html: # Or other logged-in indicator
                        print("Successfully accessed dashboard using saved state.")
                    else:
                        print(f"Failed. Status: {result.status_code}, URL: {result.url}")
            
            if __name__ == "__main__":
                asyncio.run(crawl_with_saved_state())
            ```
*   C. **Best Practices for `BrowserConfig`:**
    *   **Start Simple:** Begin with minimal configuration (e.g., just `headless`) and add options as needed to address specific site requirements or improve stealth.
    *   **Prioritize Realistic Emulation:** For stealth, aim to make your crawler's browser fingerprint (User-Agent, viewport, headers, language settings) appear as close to a real user's browser as possible.
    *   **Manage User Data:** If using `user_data_dir` or `storage_state`, have a clear strategy for creating, updating, and cleaning up these profile/state files. Version control or separate directories for different tasks/sites can be helpful.
    *   **Test Proxy Configurations:** Thoroughly test proxies to ensure they are working and not blocked by target sites.
*   D. **Troubleshooting `BrowserConfig` Issues:**
    *   **Browser Not Launching:**
        *   Ensure Playwright and its browser drivers are correctly installed (`playwright install` or `crawl4ai-setup`).
        *   Check for conflicting `extra_args`.
        *   Permissions issues if `user_data_dir` is in a restricted location.
    *   **Proxy Connection Errors:**
        *   Verify proxy server address, port, username, and password.
        *   Test proxy connectivity outside of Crawl4ai.
        *   The proxy itself might be down or the target site might be blocking it.
    *   **User-Agent Related Detection:**
        *   If blocked, try different, common User-Agents or use `user_agent_mode="random"`.
        *   Ensure client hints (`sec-ch-ua`) sent by the browser (often managed by Playwright automatically based on UA) are consistent.
    *   **Persistent Context Not Working:**
        *   Double-check `user_data_dir` path and permissions.
        *   Ensure `use_persistent_context=True`.
        *   Some sites have aggressive session invalidation mechanisms.
        *   `storage_state` might be more reliable than `user_data_dir` for some sites as it's a more focused snapshot.

# IV. Customizing Each Crawl: `CrawlerRunConfig` In-Depth

While `BrowserConfig` sets the stage for the browser environment, `CrawlerRunConfig` provides the script for each act. It allows you to fine-tune how Crawl4ai interacts with and processes individual URLs or batches of URLs. This granular control is essential for adapting to diverse website structures and extracting the specific information you need.

*   A. **Purpose and Flexibility of `CrawlerRunConfig`:**
    *   **Explaining how `CrawlerRunConfig` allows fine-tuning behavior for each `arun()` or for individual URLs in `arun_many()`:**
        `CrawlerRunConfig` is passed directly to `crawler.arun()` or as a base configuration to `crawler.arun_many()`. This means you can have one `AsyncWebCrawler` instance (with its `BrowserConfig`) and then execute various types of crawls with different objectives by simply changing the `CrawlerRunConfig` for each call.
    *   **How it can override or augment settings from `BrowserConfig`:**
        Certain parameters in `CrawlerRunConfig` (like `user_agent` or `proxy_config`) can override those set in the `BrowserConfig` for that specific run. This is useful for targeted adjustments without reconfiguring the entire browser. For example, you might use a general pool of proxies in `BrowserConfig` but need a specific geo-targeted proxy for one particular `arun()` call.

*   B. **Key `CrawlerRunConfig` Decision Points and Workflows:**
    *   1.  **Caching Strategies (`cache_mode`):**
        *   **Understanding `CacheMode` options:**
            *   `CacheMode.BYPASS` (Default for `arun_many`, was `False` for `bypass_cache` in `arun`): Fetches fresh content every time. Use when up-to-date data is critical.
            *   `CacheMode.ENABLED`: Reads from cache if available and valid; writes to cache if not. This is the default for `arun()`. Good for development to speed up iterations.
            *   `CacheMode.READ_ONLY`: Only reads from the cache; never writes. Useful if you have a pre-populated cache and don't want to modify it.
            *   `CacheMode.WRITE_ONLY`: Always fetches fresh content and writes it to the cache, but never reads from the cache for serving the request. Useful for populating/updating a cache without using stale data.
            *   `CacheMode.DISABLED`: No caching involved at all. Similar to `BYPASS` but might have subtle differences in how the cache check is skipped entirely. `BYPASS` is generally clearer for "always fetch fresh".
        *   **Decision Guide: Choosing the right cache mode:**
            *   **Development/Testing:** `CacheMode.ENABLED` can save significant time by reusing previously fetched content. Use `CacheMode.BYPASS` when you explicitly need to test fresh fetching.
            *   **Production (Data Freshness Critical):** `CacheMode.BYPASS` or `CacheMode.WRITE_ONLY` (if you still want to update your cache).
            *   **Production (Speed/Cost Important, Some Staleness OK):** `CacheMode.ENABLED` with appropriate cache TTL settings (not directly in `CrawlerRunConfig`, but a general system consideration).
            *   **Using a Static Cache:** `CacheMode.READ_ONLY` if you have a pre-built dataset you want the crawler to use.
        *   **Impact on crawl speed and data freshness:** Caching improves speed for repeated requests to the same URL but can serve stale data if not managed. `BYPASS` ensures freshness but is slower for repeated crawls.
    *   2.  **Content Selection and Filtering (`css_selector`, `target_elements`, `excluded_tags`, `excluded_selector`):**
        These parameters help you narrow down the HTML content that Crawl4ai processes, leading to cleaner Markdown, more accurate extractions, and faster processing.
        *   **`css_selector` (string):**
            *   **Workflow:** If you only care about a specific section of a webpage (e.g., the main article content, a product description block), provide a CSS selector that uniquely identifies this container.
            *   **Impact:** Crawl4ai will take the HTML *within* this selected element and process only that for Markdown generation, link/media extraction *within that scope*, and structured data extraction.
            *   **When this is more efficient:** When the desired content is well-contained and you want to discard irrelevant surrounding HTML (headers, footers, sidebars, ads) *before* any further processing, saving computational resources.
        *   **`target_elements` (List[str]):**
            *   **Workflow:** If you want to generate Markdown or extract structured data from *multiple specific sections* of a page, provide a list of CSS selectors.
            *   **Impact:** Unlike `css_selector` which creates a single "scoped" document, `target_elements` tells the `MarkdownGenerationStrategy` and some `ExtractionStrategy` instances to focus their efforts only on the content within these elements. However, link and media extraction might still consider the whole page (or the scope defined by `css_selector` if also present).
            *   **Comparison with `css_selector`:** `css_selector` creates a new, smaller effective document for all downstream processing. `target_elements` guides specific downstream processes (like Markdown or schema-based extraction) to look only within those targets, but the "document" for other purposes (like general link extraction) might still be broader.
        *   **`excluded_tags` (List[str]) and `excluded_selector` (string):**
            *   **Strategies for removing unwanted content:** These are applied *after* `css_selector` (if used) but generally *before* `target_elements` are specifically focused on for Markdown/extraction.
            *   `excluded_tags`: Provide a list of HTML tag names (e.g., `["nav", "footer", "script", "style"]`) to remove entirely.
            *   `excluded_selector`: Provide a CSS selector for more complex elements to remove (e.g., `".ads-sidebar", "#cookie-banner"`).
            *   **Impact:** Cleaner input for Markdown generation and extraction, leading to more relevant and concise results.
        *   **Code Example: Using `css_selector` to focus on an article body**
            ```python
            # Assuming an article page where main content is in <article class="main-story">
            run_config_article = CrawlerRunConfig(
                url="https://example.com/news/my-article",
                css_selector="article.main-story" 
            )
            # result.cleaned_html and result.markdown will only contain content from within that article tag.
            ```
        *   **Code Example: Using `target_elements` to extract multiple sections for structured data**
            ```python
            # Assuming a product page with separate sections for description and reviews
            run_config_product = CrawlerRunConfig(
                url="https://example.com/product/123",
                target_elements=["#product-description", ".customer-reviews-section"],
                # Extraction strategy would then know to look within these.
            )
            ```
        *   **Code Example: Using `excluded_tags` to remove headers and footers**
            ```python
            run_config_clean = CrawlerRunConfig(
                url="https://example.com/some-page",
                excluded_tags=["header", "footer", "nav", "aside"]
            )
            ```
    *   3.  **JavaScript Execution and Interaction (`js_code`, `js_only`, `wait_for`, `wait_for_timeout`, `scan_full_page`, `scroll_delay`):**
        These parameters are vital for handling dynamic websites where content is loaded or modified by JavaScript.
        *   **Workflow: Executing simple JS snippets with `js_code` (str or List[str]):**
            *   Use `js_code` to run JavaScript after the initial page load. This can be for clicking buttons, expanding sections, scrolling, or any other client-side interaction needed to reveal content.
            *   If providing a list, scripts are executed sequentially.
        *   **Understanding `js_only` (bool):**
            *   Set to `True` for subsequent interactions on a page *within the same session* where you don't need a full page reload/navigation, but only want to execute new `js_code` and re-evaluate the page state.
            *   Requires `session_id` to be set to maintain the page context.
            *   Example: Clicking pagination buttons on a Single Page Application (SPA).
        *   **`wait_for` (Optional[str]):**
            *   **Strategies for effective waiting:**
                *   **CSS Selector:** `wait_for="css:.my-dynamic-content"` - Waits until an element matching the selector appears.
                *   **JavaScript Condition:** `wait_for="js:() => window.myAppDataLoaded === true"` - Waits until the JS expression evaluates to true.
                *   **Network Idle:** `wait_for="networkidle"` (Playwright specific event) - Waits until network activity subsides.
                *   **Timeout (number):** `wait_for=5000` - Simply waits for a fixed number of milliseconds.
            *   **Impact of `wait_for_timeout` (int, milliseconds):**
                If the `wait_for` condition isn't met within this timeout, the crawl proceeds or may fail depending on other settings. Defaults to `page_timeout`.
        *   **`scan_full_page` (bool) and `scroll_delay` (float, seconds):**
            *   **Handling lazy-loaded content and infinite scroll:**
                Set `scan_full_page=True` to make Crawl4ai attempt to scroll through the entire page, triggering lazy-loaded images or infinite scroll content.
            *   **Best practices for `scroll_delay`:**
                The `scroll_delay` is the pause between each scroll step. Adjust this based on how quickly the target site loads new content upon scrolling. Too short might miss content; too long will slow down the crawl. Start with `0.2` to `0.5` and adjust.
        *   **Code Example: Clicking a "Load More" button and waiting for new content**
            ```python
            # Assumes a button with id="load-more-btn" and new items get class ".new-item"
            run_config_load_more = CrawlerRunConfig(
                url="https://example.com/feed",
                js_code="document.getElementById('load-more-btn').click();",
                wait_for="css:.new-item" # Wait for at least one new item to appear
            )
            ```
        *   **Code Example: Scrolling to the bottom of a page to trigger lazy loading**
            ```python
            run_config_scroll = CrawlerRunConfig(
                url="https://example.com/gallery",
                scan_full_page=True,
                scroll_delay=0.75 # Give a bit more time for images to load
            )
            ```
    *   4.  **Media Capture (`screenshot`, `pdf`, `capture_mhtml`):**
        *   **When to capture screenshots (`screenshot=True`):**
            Useful for debugging (to see what the crawler "saw"), for visual verification of page state, or for archiving a visual snapshot. The result is a base64-encoded PNG string in `result.screenshot`.
        *   **When to generate PDFs (`pdf=True`):**
            Ideal for archiving web content in a stable, printable format. The result is raw PDF bytes in `result.pdf_data`.
        *   **Understanding MHTML (`capture_mhtml=True`):**
            MHTML (MIME HTML) saves a complete webpage (HTML, CSS, images, etc.) into a single `.mhtml` file. This is excellent for perfect archival as it can be opened in browsers like Chrome/Edge and displays the page as it was. Result is in `result.mhtml_data`.
        *   **Configuring related parameters like `screenshot_wait_for`:**
            If you need to wait for a specific element or condition *before* taking the screenshot (e.g., an animation to complete), you can use `screenshot_wait_for` with similar syntax to `wait_for`. (Note: check if `screenshot_wait_for` is a direct param or if you should use a general `wait_for` before setting `screenshot=True`). Typically, `wait_for` applies before the final HTML/media capture.
    *   5.  **Link and Domain Handling (`exclude_external_links`, `exclude_social_media_links`, `exclude_domains`):**
        *   **Strategies for controlling the scope of link extraction:**
            These flags help you refine the `result.links` collection.
            *   `exclude_external_links=True`: Only keeps links pointing to the same base domain.
            *   `exclude_social_media_links=True`: Removes links to common social media platforms (Facebook, Twitter, LinkedIn, etc.). It uses a predefined list which can be augmented via `exclude_social_media_domains`.
            *   `exclude_domains=["ads.example.com", "tracker.net"]`: Provide a list of specific domains whose links should be entirely ignored.
        *   **When and why to exclude certain types of links:**
            *   To focus on internal site structure.
            *   To avoid crawling irrelevant third-party sites.
            *   To clean up link data for analysis.
    *   6.  **Identity and Proxy Overrides (`user_agent`, `proxy_config` in `CrawlerRunConfig`):**
        *   **Scenarios where overriding `BrowserConfig` settings per-run is useful:**
            *   **A/B Testing:** Test how a site responds to different User-Agents.
            *   **Geo-Targeted Proxies:** Use a general proxy pool in `BrowserConfig` but switch to a specific country's proxy for a particular URL via `CrawlerRunConfig(proxy_config=...)`.
            *   **Site-Specific UAs:** If one site requires a very specific User-Agent not suitable for others.
    *   7.  **Session Management (`session_id`):**
        *   **Workflow: Maintaining a consistent browser state across multiple `arun()` calls:**
            1.  Choose a unique string for `session_id`.
            2.  On the first `arun()` call for this session, Crawl4ai creates a new browser page/tab associated with this ID.
            3.  On subsequent `arun()` calls with the *same* `session_id`, Crawl4ai reuses that existing page/tab, preserving its cookies, `localStorage`, and current URL (unless a new URL is provided in the `CrawlerRunConfig` for navigation).
            4.  Use `js_only=True` for these subsequent calls if you're just running JS on the *current* page of that session rather than navigating to a new URL.
            5.  When done with the session, explicitly kill it using `await crawler.browser_manager.kill_session(session_id)`.
        *   **How `session_id` interacts with `BrowserConfig.use_persistent_context`:**
            They are complementary. `use_persistent_context` (with `user_data_dir`) saves state *between crawler restarts*. `session_id` maintains state *within a single `AsyncWebCrawler` instance's lifetime* across multiple `arun` calls. You can use both: `use_persistent_context` to load an initial logged-in state, and then `session_id` to continue interacting with that state across several `arun` operations.
        *   **Best practices for managing session lifecycles:**
            *   Always kill sessions when they are no longer needed to free up browser resources.
            *   Use descriptive `session_id`s if managing multiple concurrent logical sessions.
        *   **Code Example: A multi-step form submission using the same `session_id`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

            async def multi_step_form():
                session_id = "form_submission_1"
                base_url = "https://example.com/multi-step-form" # Replace

                async with AsyncWebCrawler() as crawler:
                    # Step 1: Load first page of the form
                    config_step1 = CrawlerRunConfig(url=f"{base_url}?step=1", session_id=session_id)
                    result_step1 = await crawler.arun(config=config_step1)
                    print(f"Step 1 loaded: {result_step1.url}")

                    # Step 2: Fill first part, submit (JS might handle this, or a POST via strategy)
                    # For JS-driven submit on the same page:
                    js_fill_step1 = """
                        document.getElementById('field1').value = 'Value 1';
                        document.getElementById('nextButtonStep1').click();
                    """
                    config_step2_action = CrawlerRunConfig(
                        session_id=session_id, 
                        js_code=js_fill_step1,
                        js_only=True, # We are on the same page context
                        wait_for="css:#formStep2Indicator" # Wait for step 2 to appear
                    )
                    result_step2_load = await crawler.arun(config=config_step2_action) 
                    print(f"Step 2 loaded after JS action. Current URL: {result_step2_load.url}")
                    
                    # ... continue for other steps ...

                    # Finally, kill the session
                    await crawler.browser_manager.kill_session(session_id)
                    print(f"Session {session_id} killed.")

            if __name__ == "__main__":
                asyncio.run(multi_step_form())
            ```
    *   8.  **Robots.txt Handling (`check_robots_txt`):**
        *   **Understanding the importance of respecting `robots.txt`:**
            `robots.txt` is a standard used by websites to communicate with web crawlers about which parts of their site should not be accessed. Respecting these rules is crucial for ethical and legal crawling.
        *   **How Crawl4ai handles it:**
            If `check_robots_txt=True` (default is `False` in `CrawlerRunConfig` for flexibility, but often good practice to enable), Crawl4ai will:
            1.  Attempt to fetch `/robots.txt` for the domain.
            2.  Parse the rules relevant to its User-Agent (or a generic one if specific rules are absent).
            3.  If the target URL is disallowed, the crawl will be skipped, and the `CrawlResult` will indicate this (e.g., status code 403, error message).
            *   **When you might override it (with caution):**
                Disabling `check_robots_txt` (`False`) should only be done if you have explicit permission to crawl restricted areas or if you are certain `robots.txt` is misconfigured and blocking legitimate access. Always prioritize ethical crawling.

*   C. **Combining `CrawlerRunConfig` with `BrowserConfig`:**
    *   **Understanding precedence:** Settings in `CrawlerRunConfig` generally take precedence over those in `BrowserConfig` for the duration of that specific `arun()` call. For example, if `BrowserConfig` defines a default User-Agent, but `CrawlerRunConfig` also specifies a `user_agent`, the latter will be used for that run.
    *   **Examples of synergistic configurations:**
        *   `BrowserConfig` with a `user_data_dir` to load a logged-in profile.
        *   `CrawlerRunConfig` with `js_code` to navigate within the logged-in application and `screenshot=True` to capture the state of a specific dashboard.
        *   `BrowserConfig` with a default set of `extra_args` for browser hardening.
        *   `CrawlerRunConfig` with a specific `css_selector` to target only the main content of articles from various sites, using the same hardened browser setup.

*   D. **Best Practices for `CrawlerRunConfig`:**
    *   **Be Specific:** Tailor your `CrawlerRunConfig` to the specific requirements of the URL(s) you are crawling and the data you intend to extract.
    *   **Iterate and Test:** Start with a minimal `CrawlerRunConfig`. Test it. Add complexity (JS interactions, wait conditions, extraction strategies) step-by-step, testing at each stage.
    *   **Use `wait_for` Judiciously:** Effective `wait_for` conditions are key for dynamic content. Avoid overly long fixed delays; prefer waiting for specific elements or JS flags.
    *   **Manage `session_id` State:** If using `session_id`, ensure you have a clear understanding of the session's lifecycle and kill sessions when they are no longer needed.
    *   **Scope Content Wisely:** Use `css_selector` or `target_elements` to reduce processing overhead and improve the relevance of extracted data.

*   E. **Troubleshooting `CrawlerRunConfig` Issues:**
    *   **Content Not Being Extracted as Expected:**
        *   Verify your `css_selector` or `target_elements` are correct and unique using browser developer tools.
        *   Content might be loaded dynamically *after* your `js_code` or `wait_for` completes. Add more specific waits or delays.
        *   The structure of the page might be different from what your selectors expect.
    *   **Timeouts or Hangs During JS Execution or Waiting:**
        *   `js_code` might have an error or an infinite loop. Test it in the browser console.
        *   `wait_for` condition might never be met. Check the selector/JS expression and increase `wait_for_timeout` if necessary, or make the condition more robust.
        *   The page itself might be unresponsive or very slow.
    *   **Incorrect Media Capture:**
        *   Screenshots/PDFs might be taken too early. Use `wait_for` effectively before enabling capture.
        *   Content might be outside the configured viewport.
    *   **`CacheMode` Not Behaving as Expected:**
        *   Ensure you understand the differences between `ENABLED`, `BYPASS`, `READ_ONLY`, and `WRITE_ONLY`.
        *   Cache keys are based on the URL. If URL parameters change, it's a new cache entry.

# V. Interpreting Crawl Outputs: `CrawlResult` and Data Models

After a crawl operation completes, `AsyncWebCrawler.arun()` (or `arun_many()`) returns a `CrawlResult` object (or a `CrawlResultContainer` which wraps a list of `CrawlResult` objects). This object is a rich container holding all the data and metadata collected during the crawl.

*   A. **Understanding the `CrawlResult` Object:**
    *   **Navigating the key attributes:**
        *   `url` (str): The final URL crawled, after any redirects.
        *   `html` (str): The raw HTML content of the page as fetched.
        *   `cleaned_html` (Optional[str]): HTML after basic cleaning (scripts, styles removed) and potential scoping by `css_selector`. This is often the input for Markdown generation.
        *   `markdown` (Optional[MarkdownGenerationResult]): An object containing various Markdown outputs. See B.
        *   `extracted_content` (Optional[str]): JSON string (or other text) if an `ExtractionStrategy` was used and successfully extracted data.
        *   `media` (Dict[str, List[Dict]]): A dictionary containing lists of media items, keyed by type (e.g., "images", "videos", "audios", "tables").
        *   `links` (Dict[str, List[Dict]]): A dictionary containing lists of links, keyed by "internal" and "external".
        *   `metadata` (Optional[dict]): Page metadata (title, meta description, OpenGraph tags, etc.).
        *   `status_code` (Optional[int]): The HTTP status code of the final response.
        *   `success` (bool): `True` if the crawl and initial processing were successful.
        *   `error_message` (Optional[str]): Description of an error if `success` is `False`.
        *   `session_id` (Optional[str]): The session ID used for this crawl, if any.
        *   `response_headers` (Optional[dict]): HTTP response headers.
        *   `ssl_certificate` (Optional[SSLCertificate]): SSL certificate information if requested.
        *   `mhtml_data` (Optional[str]): MHTML snapshot if `capture_mhtml=True`.
        *   `screenshot` (Optional[str]): Base64 encoded screenshot if `screenshot=True`.
        *   `pdf_data` (Optional[bytes]): Raw PDF bytes if `pdf=True`.
        *   `downloaded_files` (Optional[List[str]]): List of paths to downloaded files if downloads occurred.
        *   `js_execution_result` (Optional[Dict[str, Any]]): Result of the last executed JS snippet if it returned a value.
    *   **How different `CrawlerRunConfig` settings populate these fields:**
        *   `css_selector`, `target_elements`, `excluded_tags`: Affect `cleaned_html` and subsequently `markdown`.
        *   `extraction_strategy`: Populates `extracted_content`.
        *   `screenshot`, `pdf`, `capture_mhtml`: Populate their respective fields.
        *   `fetch_ssl_certificate`: Populates `ssl_certificate`.

*   B. **Working with `MarkdownGenerationResult`:**
    The `result.markdown` attribute (if Markdown generation occurred) is an instance of `MarkdownGenerationResult`.
    *   `raw_markdown` (str): Markdown generated from `cleaned_html` *before* any `ContentFilter` (like `PruningContentFilter`) is applied.
    *   `markdown_with_citations` (str): `raw_markdown` but with inline links converted to citation style (e.g., `[text](1)`) if `citations=True` in the generator.
    *   `references_markdown` (str): The list of numbered references corresponding to the citations.
    *   `fit_markdown` (Optional[str]): Markdown generated from HTML that has been processed by a `ContentFilter`. This usually represents the most "relevant" content. If no filter was used, this might be empty or same as `raw_markdown`.
    *   `fit_html` (Optional[str]): The HTML content *after* a `ContentFilter` has been applied, which was then used to generate `fit_markdown`.
    *   **When to use `fit_markdown`:** This is often your desired output if you've configured a content filter to isolate the core article or relevant sections, as it's the most distilled version.
    *   **Code Example:**
        ```python
        if result.success and result.markdown:
            print("--- Raw Markdown (Snippet) ---")
            print(result.markdown.raw_markdown[:300])
            
            if result.markdown.fit_markdown:
                print("\n--- Fit Markdown (More Relevant Content - Snippet) ---")
                print(result.markdown.fit_markdown[:300])
            
            if result.markdown.references_markdown:
                print("\n--- References ---")
                print(result.markdown.references_markdown)
        ```

*   C. **Leveraging `ScrapingResult` (from `ContentScrapingStrategy`):**
    The `ScrapingResult` is an intermediate object produced by the `ContentScrapingStrategy` (e.g., `WebScrapingStrategy`). Its fields are then used to populate the main `CrawlResult`.
    *   `cleaned_html`: This becomes `CrawlResult.cleaned_html`.
    *   `media`: This dictionary populates `CrawlResult.media`.
    *   `links`: This dictionary populates `CrawlResult.links`.
    *   `metadata`: This populates `CrawlResult.metadata`.
    Understanding this helps if you're creating a custom scraping strategy, as you'll need to return a `ScrapingResult` object.

*   D. **Understanding `MediaItem` and `Link` Structures:**
    *   **Common fields in `MediaItem` (for images, videos, audio):**
        *   `src` (str): The source URL of the media.
        *   `alt` (Optional[str]): Alt text (primarily for images).
        *   `type` (str): "image", "video", or "audio".
        *   `score` (Optional[int]): Heuristic score indicating relevance (if scoring is applied).
        *   `desc` (Optional[str]): Text from nearby elements, potentially describing the image.
        *   `group_id` (Optional[int]): If part of a group of related images (e.g., variants from `srcset`).
        *   `format` (Optional[str]): Detected format (e.g., "jpeg", "png").
        *   `width` (Optional[int]): Detected width.
        *   `data` (Optional[str]): Base64 encoded data if it's a data URI.
    *   **Common fields in `Link`:**
        *   `href` (str): The target URL of the link.
        *   `text` (Optional[str]): The anchor text of the link.
        *   `title` (Optional[str]): The `title` attribute of the link.
        *   `base_domain` (Optional[str]): The base domain of the `href`.
    *   **Practical examples of iterating and processing these collections:**
        ```python
        if result.success:
            # Example: Filter high-score images or JPEGs
            high_quality_images = [
                img for img in result.media.get("images", [])
                if (img.get("score", 0) > 3 and img.get("format") == "jpeg") or \
                   (img.get("score", 0) > 3 and img.get("src", "").lower().endswith(".jpg"))
            ]
            print(f"\nFound {len(high_quality_images)} high-quality JPEGs:")
            for img in high_quality_images[:2]: # Show first 2
                print(f"  - Src: {img.get('src')}, Alt: {img.get('alt', 'N/A')}")

            # Example: Extract all PDF links
            pdf_links = [
                link.get("href") for link in result.links.get("internal", []) + result.links.get("external", [])
                if link.get("href", "").lower().endswith(".pdf")
            ]
            if pdf_links:
                print(f"\nFound {len(pdf_links)} PDF links:")
                for pdf_url in pdf_links[:2]:
                    print(f"  - {pdf_url}")
        ```

*   E. **Making Sense of `AsyncCrawlResponse` (from `AsyncCrawlerStrategy`):**
    This object is returned by the `AsyncCrawlerStrategy.crawl()` method and is more low-level than `CrawlResult`. It typically contains:
    *   `html` (str): The raw HTML.
    *   `response_headers` (Dict): HTTP headers from the server's response.
    *   `status_code` (int): HTTP status code.
    *   `screenshot` (Optional[str]): Base64 screenshot data.
    *   `pdf_data` (Optional[bytes]): PDF data.
    *   `mhtml_data` (Optional[str]): MHTML data.
    *   And potentially other strategy-specific fields.
    The `AsyncWebCrawler` uses the information from `AsyncCrawlResponse` to construct the more comprehensive `CrawlResult`. You'd typically interact with `AsyncCrawlResponse` only if you are developing a custom `AsyncCrawlerStrategy`.

*   F. **Tracking Costs with `TokenUsage` (for LLM-based strategies):**
    *   When using strategies that involve Large Language Models (e.g., `LLMExtractionStrategy`, `LLMContentFilter`), the `TokenUsage` object provides a breakdown of token consumption.
    *   **Where to find it:**
        *   For `LLMExtractionStrategy`: The `run()` method of the strategy itself often returns a list of results, and each result or an aggregated summary might contain `TokenUsage`. The exact location can depend on the specific LLM provider integration within the strategy. It's often part of the metadata returned by the LLM API call.
        *   For `LLMContentFilter`: Similarly, the filter might expose token usage details.
        *   It's not directly a field in `CrawlResult` unless a specific extraction strategy adds it to `extracted_content` or `metadata`. You'd typically access it from the strategy's direct output or by inspecting logs if the strategy logs token usage.
    *   **Importance:** Crucial for monitoring and managing costs associated with LLM API calls. `TokenUsage` usually includes `prompt_tokens`, `completion_tokens`, and `total_tokens`.

*   G. **Inspecting `SSLCertificate` Information:**
    *   **How to access:** If `CrawlerRunConfig(fetch_ssl_certificate=True)` is set, the `CrawlResult.ssl_certificate` attribute will be an instance of `SSLCertificate`.
    *   **Use cases:**
        *   **Security Audits:** Programmatically check certificate issuers, validity periods, or signature algorithms.
        *   **Verifying Certificate Validity:** Ensure the certificate is not expired and is issued by a trusted authority.
        *   **Data Collection:** Gather data on SSL/TLS deployment across a set of websites.
    *   The `SSLCertificate` object (as seen in `crawl4ai/ssl_certificate.py`) provides properties like `.issuer`, `.subject`, `.valid_from`, `.valid_until`, `.fingerprint`, and methods like `.to_pem()`, `.to_der()`, `.to_json()`.
        ```python
        if result.success and result.ssl_certificate:
            cert = result.ssl_certificate
            print(f"\nSSL Certificate for {result.url}:")
            print(f"  Issuer CN: {cert.issuer.get('CN', 'N/A')}")
            print(f"  Subject CN: {cert.subject.get('CN', 'N/A')}")
            print(f"  Valid until: {cert.valid_until}") # Already a decoded string
            # cert.to_pem(path="certificate.pem") # Example of saving
        ```

# VI. Core Crawler Strategies: `AsyncPlaywrightCrawlerStrategy` and `AsyncHTTPCrawlerStrategy`

Crawl4ai's power comes from its flexible strategy pattern. At the core of fetching web content are the "Crawler Strategies." You primarily interact with these through `AsyncWebCrawler`, but understanding their differences and capabilities helps in choosing the right tool for the job and customizing behavior.

*   A. **Choosing the Right Strategy:**
    The `AsyncWebCrawler` is initialized with a crawler strategy. If none is provided, `AsyncPlaywrightCrawlerStrategy` is the default.
    *   **`AsyncPlaywrightCrawlerStrategy` (Default):**
        *   **Pros:**
            *   **Full JavaScript Rendering:** Executes JavaScript on the page, just like a real browser. Essential for modern SPAs (Single Page Applications) and sites that load content dynamically.
            *   **Complex Interactions:** Can handle clicks, form submissions, scrolling, and other user-like interactions via `js_code` or hooks.
            *   **Robust:** Benefits from Playwright's mature browser automation capabilities.
            *   **Media Capture:** Natively supports screenshots, PDFs, and MHTML.
        *   **Cons:**
            *   **Higher Resource Usage:** Launching and managing a full browser instance is more memory and CPU intensive.
            *   **Slower than HTTP:** Browser rendering and JS execution take time.
        *   **When it's the best choice:**
            *   Most modern websites that rely heavily on JavaScript.
            *   When you need to interact with the page (e.g., click buttons, fill forms) to access content.
            *   When you need to capture screenshots or PDFs accurately reflecting the rendered page.
    *   **`AsyncHTTPCrawlerStrategy`:**
        *   **Pros:**
            *   **Lightweight & Very Fast:** Makes direct HTTP requests using libraries like `requests` or `httpx`. Skips browser rendering entirely.
            *   **Low Resource Usage:** Minimal memory and CPU footprint.
            *   Good for high-volume, simple fetches.
        *   **Cons:**
            *   **No JavaScript Execution:** Only retrieves the initial HTML response from the server. Dynamic content loaded by client-side JS will be missing.
            *   **Limited Interaction:** Cannot perform clicks, scrolls, or form submissions in the browser context.
            *   Screenshots/PDFs are not applicable as there's no rendering.
        *   **When it's a suitable alternative:**
            *   Fetching data from APIs that return JSON or XML.
            *   Crawling purely static HTML websites where all content is present in the initial server response.
            *   Parsing sitemaps or `robots.txt` files.
            *   When speed and resource efficiency are paramount, and JS rendering is not required.
    *   **Decision Guide: Playwright vs. HTTP:**
        1.  **Does the target content require JavaScript to load/render?**
            *   **Yes:** Use `AsyncPlaywrightCrawlerStrategy`.
            *   **No:** `AsyncHTTPCrawlerStrategy` is a good candidate.
        2.  **Do you need to interact with the page (clicks, forms, scrolls)?**
            *   **Yes:** Use `AsyncPlaywrightCrawlerStrategy`.
            *   **No:** `AsyncHTTPCrawlerStrategy` might be sufficient.
        3.  **Are you fetching from an API endpoint that returns structured data (JSON/XML)?**
            *   **Yes:** `AsyncHTTPCrawlerStrategy` is ideal.
        4.  **Is performance (requests/second) and low resource use a top priority for simple HTML pages?**
            *   **Yes:** `AsyncHTTPCrawlerStrategy`.
        5.  **Do you need screenshots or PDFs of the rendered page?**
            *   **Yes:** `AsyncPlaywrightCrawlerStrategy`.
        
        *If unsure, start with `AsyncPlaywrightCrawlerStrategy` as it's more versatile, then consider `AsyncHTTPCrawlerStrategy` if you find JS rendering is unnecessary for your target.*

*   B. **Deep Dive into `AsyncPlaywrightCrawlerStrategy` Hooks:**
    Hooks are a powerful feature of `AsyncPlaywrightCrawlerStrategy` that allow you to inject custom asynchronous code at various stages of the crawling lifecycle for a specific page context.
    *   1.  **Purpose and Power of Hooks:**
        *   **Customization:** Tailor browser behavior beyond what standard configuration options offer.
        *   **Interaction:** Perform complex page interactions that are difficult to express with simple `js_code`.
        *   **State Management:** Set up or clean up page/context state.
        *   **Conditional Logic:** Implement dynamic logic based on page content or state during the crawl.
    *   2.  **Available Hooks and Their Triggers:**
        *(Refer to `async_crawler_strategy.py` for exact signatures; parameters often include `page`, `context`, `url`, `response`, `config`, `shared_data` etc.)*
        *   `on_browser_created(browser, **kwargs)`: Called once after the Playwright `Browser` object is created but before any `BrowserContext` is made. Useful for global browser setup, but limited as there's no page yet.
        *   `on_page_context_created(page: Page, context: BrowserContext, **kwargs)`: Called right after a new `BrowserContext` and its initial `Page` are created. This is the **most common and recommended hook for setup tasks** like authentication, setting cookies, or configuring routes, as you have both `page` and `context` available.
        *   `before_goto(page: Page, context: BrowserContext, url: str, **kwargs)`: Called just before `page.goto(url)` is executed. Useful for setting request interception or last-minute header modifications specific to this navigation.
        *   `after_goto(page: Page, context: BrowserContext, url: str, response, **kwargs)`: Called after `page.goto(url)` completes successfully (i.e., navigation didn't fail). `response` is the Playwright `Response` object. Good for initial checks post-navigation.
        *   `on_user_agent_updated(page: Page, context: BrowserContext, user_agent: str, **kwargs)`: Triggered if the user agent is changed dynamically during the crawl lifecycle (less common for standard crawls).
        *   `on_execution_started(page: Page, context: BrowserContext, **kwargs)`: Called just before any `js_code` specified in `CrawlerRunConfig` is executed on the page.
        *   `before_retrieve_html(page: Page, context: BrowserContext, **kwargs)`: Called right before the final HTML content of the page is snapshot. This is your last chance to perform interactions that might alter the DOM before it's captured.
        *   `before_return_html(page: Page, context: BrowserContext, html: str, **kwargs)`: Called just before the `AsyncCrawlResponse` is constructed. You have access to the `html` string that's about to be returned. Useful for final HTML modifications if absolutely necessary, though generally, it's better to do this in a `ContentScrapingStrategy`.
    *   3.  **Common Workflows Using Hooks:**
        *   **Workflow: Implementing a login sequence using `on_page_context_created`:**
            This is the ideal hook for logins as it runs once per new context (or session).
            *   **Code Example:**
                ```python
                import asyncio
                from playwright.async_api import Page, BrowserContext
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, AsyncPlaywrightCrawlerStrategy

                async def login_hook(page: Page, context: BrowserContext, shared_data: dict, **kwargs):
                    print("[HOOK login_hook] Navigating to login page...")
                    await page.goto(shared_data["login_url"])
                    await page.fill("input[name='username']", shared_data["username"])
                    await page.fill("input[name='password']", shared_data["password"])
                    await page.click("button[type='submit']")
                    # Wait for a post-login element to ensure success
                    await page.wait_for_selector("#dashboard-welcome", timeout=10000) 
                    print("[HOOK login_hook] Login successful!")
                    # Cookies are now set in the context and will be used for subsequent requests

                async def main():
                    # Create a BrowserConfig that will use our hook
                    strategy = AsyncPlaywrightCrawlerStrategy()
                    strategy.set_hook("on_page_context_created", login_hook)
                    
                    # Use a persistent context to potentially save the login cookies across crawler restarts
                    # PROFILE_DIR_HOOK = Path("./hook_login_profile")
                    # PROFILE_DIR_HOOK.mkdir(parents=True, exist_ok=True)

                    browser_cfg = BrowserConfig(
                        headless=False, # Easier to debug login
                        # user_data_dir=str(PROFILE_DIR_HOOK), # Optional: for persistence
                        # use_persistent_context=True      # Optional: for persistence
                    )
                    
                    # Pass login credentials and URL via shared_data
                    run_cfg = CrawlerRunConfig(
                        url="https://example.com/protected-page", # Target page after login
                        shared_data={
                            "login_url": "https://example.com/login", # Replace
                            "username": "testuser",
                            "password": "testpassword"
                        }
                    )

                    async with AsyncWebCrawler(config=browser_cfg, crawler_strategy=strategy) as crawler:
                        result = await crawler.arun(config=run_cfg)
                        if result.success:
                            print(f"Crawled protected page content: {result.markdown.raw_markdown[:200]}...")
                        else:
                            print(f"Failed: {result.error_message}")
                
                if __name__ == "__main__":
                    asyncio.run(main())
                ```
        *   **Workflow: Setting custom cookies or headers for all requests in a context via `on_page_context_created`:**
            ```python
            async def set_custom_headers_hook(page: Page, context: BrowserContext, **kwargs):
                await context.set_extra_http_headers({"X-Custom-Auth": "mysecrettoken"})
                print("[HOOK set_custom_headers_hook] Custom headers set.")
            
            # strategy.set_hook("on_page_context_created", set_custom_headers_hook)
            ```
        *   **Workflow: Blocking specific resource types (e.g., fonts, images) using `page.route()` in `on_page_context_created`:**
            This can speed up page loads if you don't need these resources.
            ```python
            async def block_resources_hook(page: Page, context: BrowserContext, **kwargs):
                await context.route("**/*.{png,jpg,jpeg,woff,woff2}", lambda route: route.abort())
                print("[HOOK block_resources_hook] Aborting image and font requests.")

            # strategy.set_hook("on_page_context_created", block_resources_hook)
            ```
        *   **Workflow: Performing complex page interactions before HTML retrieval using `before_retrieve_html`:**
            Useful if content is revealed only after a sequence of actions not easily done with simple `js_code`.
            ```python
            async def reveal_content_hook(page: Page, context: BrowserContext, **kwargs):
                print("[HOOK reveal_content_hook] Clicking multiple expanders...")
                expanders = await page.query_selector_all(".accordion-header")
                for expander in expanders:
                    await expander.click()
                    await page.wait_for_timeout(100) # Small delay for content to render
            
            # strategy.set_hook("before_retrieve_html", reveal_content_hook)
            ```
    *   4.  **Best Practices for Writing Hook Functions:**
        *   **Keep them Focused:** Each hook should perform a specific, well-defined task.
        *   **Handle Errors Gracefully:** Use `try-except` blocks within hooks to catch potential errors (e.g., element not found, navigation failure) and prevent them from crashing the entire crawl. Log errors appropriately.
        *   **Async/Await:** Hooks are `async` functions. Ensure you `await` any Playwright calls or other asynchronous operations within them.
        *   **Idempotency (where applicable):** If a hook might be called multiple times for the same context (less common but possible in complex scenarios), design it to be idempotent if its actions are stateful (e.g., don't try to set the same route handler multiple times without removing the old one).
    *   5.  **Passing Shared Data to Hooks (`CrawlerRunConfig.shared_data`):**
        *   The `shared_data` dictionary in `CrawlerRunConfig` is passed as keyword arguments to your hook functions.
        *   This is the recommended way to pass dynamic information (like credentials, API keys, or state from previous steps) into your hooks without hardcoding them or relying on global variables.
        *   **Code Example:** (Shown in the login hook example above where `shared_data` passes `login_url`, `username`, `password`).

*   C. **Using `AsyncHTTPCrawlerStrategy`:**
    When you don't need JavaScript rendering, `AsyncHTTPCrawlerStrategy` offers a much faster and lighter alternative.
    *   **Simpler Configuration:** Many `BrowserConfig` options (like `headless`, `viewport`) are irrelevant. The core HTTP request details are controlled via `HTTPCrawlerConfig` passed within `CrawlerRunConfig`.
    *   **When to use `HTTPCrawlerConfig` for `method`, `headers`, `data`, `json`:**
        *   `method`: "GET" (default), "POST", "PUT", etc.
        *   `headers`: Custom HTTP headers for the request.
        *   `data`: For form-encoded POST data (`application/x-www-form-urlencoded`).
        *   `json`: For JSON POST data (`application/json`).
    *   **Code Example: Fetching JSON from an API using `AsyncHTTPCrawlerStrategy` and `HTTPCrawlerConfig`**
        ```python
        import asyncio
        import json
        from crawl4ai import (
            AsyncWebCrawler, CrawlerRunConfig, AsyncHTTPCrawlerStrategy, HTTPCrawlerConfig
        )

        async def fetch_api_data():
            # HTTPCrawlerConfig for a POST request with JSON payload
            http_cfg = HTTPCrawlerConfig(
                method="POST",
                headers={"Authorization": "Bearer myapitoken"},
                json_data={"query": "fetch_user_data", "user_id": 123}
            )

            # CrawlerRunConfig specific to this API call
            run_cfg = CrawlerRunConfig(
                url="https://api.example.com/data",
                http_crawler_config=http_cfg # Pass the HTTP-specific config
            )
            
            # Initialize crawler with AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                result = await crawler.arun(config=run_cfg)
                if result.success and result.status_code == 200:
                    try:
                        api_data = json.loads(result.html) # The 'html' field contains the raw response body
                        print("API Data:", api_data)
                    except json.JSONDecodeError:
                        print("Failed to parse JSON response:", result.html)
                else:
                    print(f"API call failed. Status: {result.status_code}, Error: {result.error_message}")
                    print(f"Response body: {result.html}")


        if __name__ == "__main__":
            asyncio.run(fetch_api_data())
        ```

# VII. Advanced Core Concepts and Workflows

Beyond basic crawling, Crawl4ai's core offers mechanisms for more sophisticated scenarios, including robust proxy management, fine-grained browser control, and configuration persistence.

*   A. **Proxy Management and Rotation:**
    While single proxies can be set in `BrowserConfig` or `CrawlerRunConfig`, robust applications often require rotating through a list of proxies to avoid IP bans and access geo-restricted content.
    *   1.  **Using `ProxyConfig` for Single Proxies:**
        (Recap from `BrowserConfig` and `CrawlerRunConfig` sections for completeness)
        -   **Purpose:** To route a specific `AsyncWebCrawler` instance or a particular `arun()` call through a designated proxy server.
        -   **How:**
            ```python
            from crawl4ai import BrowserConfig, CrawlerRunConfig, ProxyConfig
            
            # For all crawls by a crawler instance
            browser_cfg_proxy = BrowserConfig(
                proxy_config=ProxyConfig(server="http://user:pass@proxy1.example.com:8000")
            )
            # crawler = AsyncWebCrawler(config=browser_cfg_proxy)

            # For a single arun() call
            run_cfg_proxy = CrawlerRunConfig(
                url="https://example.com",
                proxy_config=ProxyConfig(server="http://user:pass@proxy2.example.com:8001")
            )
            # result = await crawler.arun(config=run_cfg_proxy)
            ```
        -   **Why:** Useful for testing with a specific proxy or when you have a stable, dedicated proxy for certain tasks.
    *   2.  **Implementing Proxy Rotation with `ProxyRotationStrategy`:**
        *   **Understanding the need for proxy rotation:**
            Websites often track and block IPs that make too many requests. Rotating proxies distributes your requests across multiple IPs, making your crawler appear less like a bot and reducing the likelihood of blocks.
        *   **How `RoundRobinProxyStrategy` works:**
            This is the primary built-in strategy. You provide it with a list of `ProxyConfig` objects. For each new crawl operation (or more granularly, depending on how the strategy is integrated), it cycles through this list, selecting the next proxy in a round-robin fashion.
        *   **Workflow: Setting up `RoundRobinProxyStrategy`:**
            1.  Define a list of `ProxyConfig` objects, each representing one proxy server.
            2.  Instantiate `RoundRobinProxyStrategy` with this list.
            3.  Assign this strategy instance to `CrawlerRunConfig.proxy_rotation_strategy`.
        *   **Code Example: Using `RoundRobinProxyStrategy` in `CrawlerRunConfig`**
            (Note: `ProxyRotationStrategy` is typically used with `arun_many` or in custom loops with `arun` where the `CrawlerRunConfig` is updated before each call).
            ```python
            import asyncio
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, ProxyConfig, 
                RoundRobinProxyStrategy, BrowserConfig
            )

            async def crawl_with_rotating_proxies():
                proxies = [
                    ProxyConfig(server="http://user1:pass1@proxy1.example.com:8000", ip="1.1.1.1"),
                    ProxyConfig(server="http://user2:pass2@proxy2.example.com:8001", ip="2.2.2.2"),
                    ProxyConfig(server="http://user3:pass3@proxy3.example.com:8002", ip="3.3.3.3"),
                ]
                
                proxy_strategy = RoundRobinProxyStrategy(proxies=proxies)

                # This run_config will now use the rotation strategy.
                # The proxy_config within it will be set dynamically by the strategy.
                run_cfg = CrawlerRunConfig(
                    url="https://api.ipify.org?format=json", # A site to check current IP
                    proxy_rotation_strategy=proxy_strategy
                )
                
                # Browser config remains simple or can have its own default proxy (which would be overridden)
                browser_cfg = BrowserConfig(headless=True)

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    for i in range(5): # Make 5 requests to see rotation
                        print(f"\nRequest {i+1}:")
                        # The strategy will pick the next proxy for each arun() call
                        # when proxy_rotation_strategy is set on the CrawlerRunConfig.
                        # The proxy_config field in run_cfg is effectively managed by the strategy.
                        
                        # It's more typical to see proxy_rotation_strategy used with arun_many,
                        # where the dispatcher would call get_next_proxy for each task.
                        # For a loop with arun(), you'd typically call get_next_proxy manually:
                        
                        next_proxy = await proxy_strategy.get_next_proxy()
                        current_run_cfg = run_cfg.clone(proxy_config=next_proxy) # Clone and set specific proxy

                        result = await crawler.arun(config=current_run_cfg)
                        if result.success:
                            print(f"  Crawled {result.url} via proxy {next_proxy.server if next_proxy else 'None'}")
                            print(f"  Response: {result.html[:100]}")
                        else:
                            print(f"  Failed to crawl {result.url}: {result.error_message}")
                        await asyncio.sleep(0.5) # Small delay between requests

            if __name__ == "__main__":
                asyncio.run(crawl_with_rotating_proxies())
            ```
            *Important Note:* The `ProxyRotationStrategy` itself usually doesn't automatically apply the proxy to each `arun` call if you're looping. It's designed to be queried (e.g., `await strategy.get_next_proxy()`) and then that specific `ProxyConfig` is passed to the `CrawlerRunConfig` for *that particular* `arun` call. `AsyncDispatcher` in `arun_many` handles this querying internally.

        *   **Considerations for managing proxy lists and health:**
            *   Real-world proxy lists can be large and proxies can become unavailable.
            *   You might need external logic to check proxy health and update the list passed to `RoundRobinProxyStrategy` periodically or implement a more advanced strategy that handles proxy failures (e.g., by temporarily removing a failing proxy from rotation).

*   B. **Browser and Session Management (`BrowserManager`, `ManagedBrowser`):**
    These classes are mostly for internal use by `AsyncPlaywrightCrawlerStrategy` but understanding their concepts can be beneficial.
    *   1.  **Understanding `BrowserManager`'s Role:**
        *   The `BrowserManager` (an internal component of `AsyncPlaywrightCrawlerStrategy`) is responsible for the lifecycle of Playwright `BrowserContext` objects.
        *   It handles creating new contexts based on the `BrowserConfig` and the specifics of a `CrawlerRunConfig` (like `session_id` or per-run proxy/UA overrides).
        *   **Context Signatures:** It generates a "signature" for each requested context configuration. If a subsequent request has the same signature (e.g., same `session_id`, same identity overrides), it can reuse an existing, compatible `BrowserContext` instead of creating a new one, which is efficient.
        *   It also manages the closing of contexts when a session is killed or the crawler is shut down.
    *   2.  **Leveraging `ManagedBrowser` (typically internal but concepts are useful):**
        *   `ManagedBrowser` is used when `BrowserConfig.use_managed_browser` is `True` (e.g., for `"builtin"` or `"docker"` modes, or when `use_persistent_context=True`).
        *   It's responsible for launching and managing the actual browser *process* (e.g., Chrome executable).
        *   **Persistent Profiles:** When `user_data_dir` is set, `ManagedBrowser` launches the browser with that profile directory, enabling persistence.
        *   **CDP Connection:** If `cdp_url` is provided in `BrowserConfig` (for `browser_mode="cdp"`), `ManagedBrowser` is bypassed, and Playwright connects directly to the existing browser endpoint. If `ManagedBrowser` launches the browser, it makes the CDP endpoint available (e.g., `ws://localhost:9222/...`).
    *   3.  **Explicit Session Termination with `crawler.browser_manager.kill_session(session_id)`:**
        *   **When and why:** If you've used a `session_id` in `CrawlerRunConfig` to maintain a specific page/tab across multiple `arun` calls, you should explicitly kill that session when it's no longer needed.
        *   **Impact:** This closes the associated browser page and potentially the `BrowserContext` (if no other pages are using it and it's not the default shared context), freeing up browser resources. If you don't kill sessions, they might linger until the entire `AsyncWebCrawler` is closed.
        *   Code Example:
            ```python
            # ... (inside an async with AsyncWebCrawler as crawler:)
            # await crawler.arun(config=CrawlerRunConfig(url="...", session_id="my_task_session"))
            # ...
            # await crawler.arun(config=CrawlerRunConfig(url="...", session_id="my_task_session", js_only=True, ...))
            # ...
            # Done with this logical task
            # await crawler.browser_manager.kill_session("my_task_session")
            ```

*   C. **Configuration Serialization and Deserialization (`to_serializable_dict`, `from_serializable_dict`):**
    `BrowserConfig`, `CrawlerRunConfig`, and their nested config objects (like `ProxyConfig`, `GeolocationConfig`) provide methods for easy serialization and deserialization. This is useful for:
    *   Storing complex configurations in files (e.g., JSON, YAML).
    *   Sharing configurations between different parts of an application or different scripts.
    *   Dynamically loading configurations at runtime.
    *   **Workflow: Saving and Loading Configurations:**
        1.  Create your config object (e.g., `my_run_config = CrawlerRunConfig(...)`).
        2.  Get a serializable dictionary: `serializable_dict = my_run_config.to_dict()` (or use `my_run_config.dump()` which leverages `to_serializable_dict` from `async_configs.py`).
        3.  Save this dictionary to a file (e.g., using `json.dump()`).
        4.  To load, read the dictionary from the file.
        5.  Recreate the config object: `loaded_run_config = CrawlerRunConfig.from_kwargs(loaded_dict)` (or use `CrawlerRunConfig.load(loaded_dict)`).
    *   **Code Example: Saving and loading a `CrawlerRunConfig` object**
        ```python
        import json
        import asyncio
        from crawl4ai import CrawlerRunConfig, CacheMode, GeolocationConfig, ProxyConfig
        from crawl4ai.async_configs import to_serializable_dict # For direct use if needed
        
        # Create a complex CrawlerRunConfig
        original_config = CrawlerRunConfig(
            url="https://example.com",
            cache_mode=CacheMode.ENABLED,
            js_code="console.log('hello');",
            screenshot=True,
            geolocation=GeolocationConfig(latitude=34.05, longitude=-118.24),
            proxy_config=ProxyConfig(server="http://myproxy.com:3128")
        )

        # 1. Serialize to a dictionary (using the .dump() method which calls to_serializable_dict internally)
        # config_dict = original_config.dump() 
        # Or, if using the utility function directly on an object not having .dump()
        config_dict = to_serializable_dict(original_config, ignore_default_value=True)


        # 2. Save to JSON file
        with open("my_crawler_run_config.json", "w") as f:
            json.dump(config_dict, f, indent=2)
        print("Config saved to my_crawler_run_config.json")

        # 3. Load from JSON file
        with open("my_crawler_run_config.json", "r") as f:
            loaded_dict = json.load(f)
        
        # 4. Recreate CrawlerRunConfig object (using .load() static method)
        # loaded_config = CrawlerRunConfig.load(loaded_dict)
        # Or using from_kwargs if .load() is not available or for more general objects
        from crawl4ai.async_configs import from_serializable_dict
        loaded_config_generic = from_serializable_dict(loaded_dict) # This will give back the CrawlerRunConfig instance

        # Verify (loaded_config_generic will be an instance of CrawlerRunConfig)
        assert isinstance(loaded_config_generic, CrawlerRunConfig)
        assert loaded_config_generic.url == original_config.url
        assert loaded_config_generic.cache_mode == original_config.cache_mode
        print(f"Loaded config successfully. Screenshot setting: {loaded_config_generic.screenshot}")
        print(f"Loaded proxy server: {loaded_config_generic.proxy_config.server if loaded_config_generic.proxy_config else 'None'}")

        # Example of using it
        # async with AsyncWebCrawler() as crawler:
        #     result = await crawler.arun(config=loaded_config_generic)
        #     # ...
        ```
    *   **Benefits:** This structured approach is more robust than pickling and ensures that only intended configuration parameters are serialized, making it easier to manage versions and share configurations.

*   D. **Logging and Debugging Core Operations:**
    *   **Leveraging `AsyncLogger`:**
        Crawl4ai uses an `AsyncLogger` instance (by default, or you can pass your own). This logger provides tagged and colored output, making it easier to follow the flow of operations.
        *   Configure verbosity via `BrowserConfig(verbose=True)` or `CrawlerRunConfig(verbose=True)`. The `CrawlerRunConfig` setting usually takes precedence for a specific run.
    *   **Interpreting verbose logs for troubleshooting:**
        Verbose logs will show:
        *   [INIT] tags for crawler and strategy initialization.
        *   [BROWSER] tags for browser launching, context creation, page events.
        *   [FETCH] tags for URL fetching attempts, cache hits/misses, status codes.
        *   [SCRAPE] tags for scraping strategy execution.
        *   [MARKDOWN] tags for markdown generation steps.
        *   [EXTRACTION] tags if an extraction strategy is active.
        *   [JS] tags for JavaScript execution.
        *   [HOOK] tags when custom hooks are executed.
        *   [ERROR] tags for any exceptions or failures.
        *   [CACHE] tags for cache operations.
    *   **Key log messages to watch for:**
        *   Browser launch arguments.
        *   CDP connection messages (if using managed/CDP modes).
        *   Cache hits/misses: `Cache HIT for <URL>` or `Cache MISS for <URL>`.
        *   Navigation events: `Navigating to <URL>...`, `Navigation to <URL> successful`.
        *   JavaScript execution: `Executing JS...`, `JS execution result...`.
        *   Hook execution: `Executing hook <hook_name>...`.
        *   Any messages tagged with [ERROR] or [WARNING].

# VIII. Integrating Core with Specialized Strategies

The `core` components of Crawl4ai (like `AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`) provide the foundation upon which more specialized strategies for PDF processing, Markdown generation, data extraction, chunking, and content filtering are built. Understanding how core configurations influence these specialized strategies is key to using them effectively.

*   A. **How Core Configurations Influence Specialized Crawlers/Processors:**
    *   **`PDFCrawlerStrategy` & `PDFContentScrapingStrategy`:**
        *   The `PDFCrawlerStrategy` itself often uses an underlying `AsyncHTTPCrawlerStrategy` to download the PDF file because PDFs are typically static assets not requiring browser rendering for the download itself.
        *   Thus, `BrowserConfig` settings like `headless` or `js_code` are usually *not directly relevant* to the PDF fetching part.
        *   However, `HTTPCrawlerConfig` (passed via `CrawlerRunConfig.http_crawler_config`) for setting custom headers (e.g., authentication) for the PDF download request can be important.
        *   The `PDFContentScrapingStrategy` (used by `PDFCrawlerStrategy` and can be used standalone with `AsyncWebCrawler` if you point it to a PDF URL with the right Content-Type) then processes the downloaded PDF bytes. Core configs like `word_count_threshold` or `css_selector` are *not applicable* here as it's not HTML. PDF-specific parameters for this strategy (like `save_images_locally`, `extract_images`) are passed during its initialization.
    *   **`MarkdownGenerationStrategy` (e.g., `DefaultMarkdownGenerator`):**
        *   This strategy consumes HTML, typically `CrawlResult.cleaned_html` or `CrawlResult.fit_html` (if a `content_filter` was used).
        *   Therefore, `CrawlerRunConfig` parameters that affect `cleaned_html` (like `css_selector`, `excluded_tags`, `target_elements`) directly influence the input to the Markdown generator. A well-scoped `cleaned_html` leads to more relevant Markdown.
        *   The `content_filter` *within* the `MarkdownGenerationStrategy` (e.g., `PruningContentFilter`) further refines the HTML before final Markdown conversion, producing `fit_markdown`.
    *   **`ExtractionStrategy` (e.g., `LLMExtractionStrategy`, `JsonCSSExtractionStrategy`, `RegexExtractionStrategy`):**
        *   This is typically set via `CrawlerRunConfig.extraction_strategy`.
        *   The `input_format` property of the chosen `ExtractionStrategy` determines what content it receives from `CrawlResult`. Common options are:
            *   `"markdown"`: Uses `CrawlResult.markdown.raw_markdown` (or `fit_markdown` if available and preferred by the strategy).
            *   `"html"`: Uses `CrawlResult.cleaned_html`.
            *   `"fit_markdown"`: Specifically uses `CrawlResult.markdown.fit_markdown`.
            *   `"fit_html"`: Specifically uses `CrawlResult.markdown.fit_html`.
        *   Core configurations that shape these inputs (e.g., `css_selector`, `target_elements`, `markdown_generator`'s `content_filter`) are thus critical for the success of the extraction.
        *   For `LLMExtractionStrategy`, the `LLMConfig` (set when initializing the strategy or passed via `CrawlerRunConfig.llm_config`) is paramount, controlling the LLM provider, model, API key, and prompting parameters.
        *   `CrawlerRunConfig.target_elements` can be particularly useful with extraction. If your extraction schema is designed to pull data from specific sections, `target_elements` ensures only those sections are considered by strategies that respect it (like an LLM strategy prompted to process focused HTML snippets).
    *   **`ChunkingStrategy` (e.g., `RegexChunking`):**
        *   Chunking strategies typically operate on plain text, often derived from `CrawlResult.markdown.raw_markdown` or `CrawlResult.markdown.fit_markdown`.
        *   Core configurations affecting the quality and scope of the source Markdown will, therefore, impact the chunks produced.
    *   **`ContentFilteringStrategy` (e.g., `PruningContentFilter`, `LLMContentFilter`):**
        *   These are usually part of a `MarkdownGenerationStrategy` configuration.
        *   They take `cleaned_html` (output of `ContentScrapingStrategy`) as input and produce a "fit" (more relevant) HTML, which is then converted to `fit_markdown`.
        *   Core settings like `css_selector` apply *before* these filters, pre-scoping the HTML they receive.

*   B. **Pointers to Dedicated Documentation for Specialized Strategies:**
    *   **PDF Processing:** For details on `PDFCrawlerStrategy` and `PDFContentScrapingStrategy`, including image extraction and text processing from PDFs, refer to:
        *   [PDF Processing Guide](../../processors/pdf-processing.md) *(Assuming this link structure)*
    *   **Markdown Generation:** To understand `DefaultMarkdownGenerator`, custom HTML-to-Markdown options, and content filtering within Markdown generation, see:
        *   [Markdown Generation Guide](../../strategies/markdown-generation.md)
    *   **Data Extraction:**
        *   For LLM-based extraction (`LLMExtractionStrategy`), schema definition, and prompting: [LLM Extraction Guide](../../strategies/llm-extraction.md)
        *   For CSS/XPath-based structured data extraction (`JsonCssExtractionStrategy`, `JsonXpathExtractionStrategy`): [Selector-Based Extraction Guide](../../strategies/selector-extraction.md)
        *   For Regex-based extraction (`RegexExtractionStrategy`): [Regex Extraction Guide](../../strategies/regex-extraction.md)
    *   **Content Chunking:** For details on `RegexChunking` and other chunking approaches for LLM context preparation:
        *   [Content Chunking Guide](../../strategies/chunking.md)
    *   **Content Filtering:** For `PruningContentFilter`, `LLMContentFilter`, and creating custom filters:
        *   [Content Filtering Guide](../../strategies/content-filtering.md)

# IX. Troubleshooting Common Core Issues

Even with robust configurations, crawling can sometimes hit snags. Here are common issues related to the core components and how to approach them:

*   A. **Browser Launch Failures:**
    *   **Symptom:** `AsyncWebCrawler.start()` or the `async with` block fails immediately, often with errors related to Playwright or browser executables.
    *   **Causes & Solutions:**
        *   **Missing Playwright Drivers:** Playwright needs browser-specific drivers.
            *   **Fix:** Run `playwright install` or `crawl4ai-setup` to install them. If in a restricted environment, you might need to specify driver download paths manually using Playwright environment variables.
        *   **Permissions Issues for `user_data_dir`:** If you've specified a `user_data_dir` in `BrowserConfig`, the crawler process needs write permissions to that directory.
            *   **Fix:** Ensure permissions are correct or choose a writable directory.
        *   **Conflicting Browser Processes:** An old or hung browser process might be using the same debugging port or profile directory.
            *   **Fix:** Manually kill lingering browser processes. `ManagedBrowser` attempts some cleanup, but it's not always foolproof.
        *   **Incompatible `extra_args`:** Custom browser arguments might be incorrect or conflict.
            *   **Fix:** Remove or verify `extra_args` one by one.
        *   **Unsupported OS/Architecture:** Ensure your Playwright version supports your OS and CPU architecture.

*   B. **Navigation Timeouts or Errors (`page.goto()` failures):**
    *   **Symptom:** The crawl hangs or fails during the page navigation step, often with `TimeoutError` or network-related errors in logs.
    *   **Causes & Solutions:**
        *   **Incorrect `wait_for` Conditions:** If `CrawlerRunConfig.wait_for` specifies a condition that's never met, the page will time out.
            *   **Fix:** Verify your CSS selector or JS condition. Use browser dev tools to test selectors. Increase `wait_for_timeout` if the condition legitimately takes longer to meet.
        *   **Network Issues or Site Blocking:** The target site might be down, slow, or actively blocking your IP/User-Agent.
            *   **Fix:** Check site accessibility manually. Try different User-Agents or proxies (`BrowserConfig` or `CrawlerRunConfig`). Implement robust rate limiting via a custom dispatcher if hitting site limits.
        *   **`page_timeout` Too Short:** The global `CrawlerRunConfig.page_timeout` (default 60s) might be too short for very slow-loading pages.
            *   **Fix:** Increase `page_timeout` for problematic sites.
        *   **SSL/TLS Errors:** The site might have an invalid SSL certificate.
            *   **Fix:** Set `BrowserConfig(ignore_https_errors=True)` if you trust the site and want to proceed (use with caution). Check `result.ssl_certificate` if `fetch_ssl_certificate=True` for details.

*   C. **Content Not Appearing as Expected:**
    *   **Symptom:** `result.markdown` is empty or missing key content; `result.extracted_content` is empty or incorrect.
    *   **Causes & Solutions:**
        *   **Lazy Loading:** Content is loaded via JavaScript as the user scrolls.
            *   **Fix:** Set `CrawlerRunConfig(scan_full_page=True, scroll_delay=0.5)` (adjust delay as needed). For more complex infinite scroll, you might need custom `js_code` to trigger loads and appropriate `wait_for` conditions.
        *   **JavaScript Errors on the Page:** Errors in the site's own JS can prevent content from rendering.
            *   **Fix:** Run with `BrowserConfig(headless=False)` to inspect the browser console. Enable `CrawlerRunConfig(log_console=True)` to capture console messages in Crawl4ai's logs.
        *   **Incorrect CSS Selectors or `target_elements`:** Your selectors might be wrong or too broad/narrow.
            *   **Fix:** Use browser developer tools to test and refine your CSS selectors.
        *   **Content Loaded After Crawler Finishes:** The crawler might be capturing HTML before all dynamic content has finished loading.
            *   **Fix:** Use more robust `wait_for` conditions (e.g., wait for a specific "data loaded" flag set by page JS, or wait for a specific network request to complete using Playwright's advanced routing/waiting features via hooks). Increase `delay_before_retrieve_html` as a simpler, less precise fix.
        *   **AJAX Content:** Content is loaded via AJAX calls. `wait_for="networkidle"` or waiting for specific XHR responses (via hooks) can help.

*   D. **Proxy-Related Problems:**
    *   **Symptom:** Connection errors, timeouts, or getting content from your own IP instead of the proxy.
    *   **Causes & Solutions:**
        *   **Invalid Proxy Credentials/Format:** Ensure `ProxyConfig.server` (e.g., `http://user:pass@host:port`) and auth details are correct.
        *   **Proxy Server Unresponsive:** The proxy server itself might be down or overloaded. Test it independently.
        *   **Website Blocking Proxy IP:** The target site may have blacklisted the proxy's IP. Rotate proxies.
        *   **Proxy Type Mismatch:** Ensure the proxy protocol (HTTP, HTTPS, SOCKS5) matches what the server expects.
        *   **Firewall Issues:** Local or network firewalls might be blocking outbound connections to the proxy port.

*   E. **Session State Not Persisting (`session_id`, `user_data_dir`, `storage_state`):**
    *   **Symptom:** Logins are lost between `arun()` calls despite using `session_id`, or site preferences aren't remembered when using `user_data_dir` or `storage_state`.
    *   **Causes & Solutions:**
        *   **Incorrect `session_id` Usage:** Ensure you're passing the *exact same* `session_id` string to subsequent `arun()` calls that need to share the state.
        *   **`js_only=False` on Subsequent Calls:** If you navigate to a new URL (even the same one) in a subsequent `arun` call for an existing session, it's a full navigation. If you only want to run JS on the *current page* of that session, set `js_only=True`.
        *   **`use_persistent_context=False` with `user_data_dir`:** Both must be set correctly in `BrowserConfig` for profile persistence to work.
        *   **Incorrect `user_data_dir` Path or Permissions:** Verify the path and ensure write access.
        *   **Invalid `storage_state` File/Format:** Ensure the JSON file is valid and was correctly saved by Playwright.
        *   **Site-Specific Session Handling:** Some sites have very aggressive session timeout mechanisms, use multiple cookies, or employ JavaScript-based session checks that might invalidate sessions even if cookies are present. Debugging these often requires running headed and observing network requests and storage.
        *   **Context Replaced by `BrowserManager`:** If you drastically change other context-affecting parameters in `CrawlerRunConfig` (like per-run proxy or UA) even with the same `session_id`, the `BrowserManager` might decide it needs a new `BrowserContext` due to a different "signature," potentially losing the state of the old one associated with that `session_id`. Keep context-affecting overrides consistent for a given session.

# X. Conclusion and Best Practices Summary

You've now explored the foundational `core` components of Crawl4ai, from launching browsers with `BrowserConfig` to executing fine-tuned crawl operations with `CrawlerRunConfig`, and interpreting the rich `CrawlResult`. The core is designed for flexibility and power, enabling you to tackle a wide array of web data extraction challenges.

**Recap of Key Core Concepts:**

*   **`AsyncWebCrawler`:** Your main interaction point, orchestrating the crawl.
*   **`BrowserConfig`:** Defines the global browser environment (engine, headless, proxy, UA). Set it up once for a crawler instance.
*   **`CrawlerRunConfig`:** Defines per-crawl behavior (URL, caching, JS, content selection, extraction). Pass it to `arun()` or `arun_many()`.
*   **Strategies:** Crawl4ai uses a strategy pattern for crawling (`AsyncPlaywrightCrawlerStrategy`, `AsyncHTTPCrawlerStrategy`), scraping (`WebScrapingStrategy`), Markdown generation (`DefaultMarkdownGenerator`), and data extraction.
*   **Hooks (in `AsyncPlaywrightCrawlerStrategy`):** Allow custom code injection at key lifecycle points for advanced control and interactions.
*   **`CrawlResult`:** The comprehensive output object containing HTML variants, Markdown, media, links, metadata, and more.

**Best Practices Summary:**

1.  **Plan Your Configuration:**
    *   Distinguish between global browser setup (`BrowserConfig`) and per-crawl instructions (`CrawlerRunConfig`).
    *   Start with simpler configurations and add complexity (like JS execution or specific selectors) iteratively.
2.  **Leverage Caching Wisely:**
    *   Use `CacheMode.ENABLED` during development to speed up iterations.
    *   Use `CacheMode.BYPASS` or `CacheMode.WRITE_ONLY` for production runs where fresh data is paramount.
3.  **Be Specific with Content Selection:**
    *   Use `css_selector` to scope processing to relevant page areas early.
    *   Employ `target_elements`, `excluded_tags`, and `excluded_selector` to refine the content fed into Markdown and extraction processes.
4.  **Handle Dynamic Content Effectively:**
    *   Use `js_code` for interactions.
    *   Rely on robust `wait_for` conditions (CSS selectors or JS expressions) rather than fixed delays.
    *   Utilize `scan_full_page` for lazy-loading and infinite scroll, adjusting `scroll_delay` as needed.
5.  **Manage Sessions and State:**
    *   For multi-step processes or login persistence within a crawler's lifetime, use `session_id` and `js_only=True` for subsequent on-page JS executions.
    *   For persistence across crawler restarts, use `BrowserConfig(use_persistent_context=True, user_data_dir="...")` or `BrowserConfig(storage_state="...")`.
    *   Always `kill_session()` when a `session_id`-based task is complete.
6.  **Prioritize Realistic Browser Emulation for Stealth:**
    *   Use common, valid User-Agents (consider `user_agent_mode="random"`).
    *   Set appropriate viewports.
    *   Use proxies, especially for larger crawls or geo-specific content.
7.  **Use Hooks for Complex Interactions:**
    *   For tasks like login sequences or intricate page manipulations, hooks (especially `on_page_context_created` and `before_retrieve_html`) offer more control than simple `js_code`.
    *   Pass dynamic data to hooks via `CrawlerRunConfig.shared_data`.
8.  **Choose the Right Crawler Strategy:**
    *   Default to `AsyncPlaywrightCrawlerStrategy` for most modern web pages.
    *   Opt for `AsyncHTTPCrawlerStrategy` for APIs, static HTML, or when JS rendering is not needed, to gain significant speed and reduce resource usage.
9.  **Log and Debug Systematically:**
    *   Enable `verbose=True` in `BrowserConfig` or `CrawlerRunConfig`.
    *   Run `headless=False` during development to visually inspect browser behavior.
    *   Check `result.error_message` and `result.status_code` for failures.
10. **Respect `robots.txt`:**
    *   Enable `check_robots_txt=True` in `CrawlerRunConfig` for ethical crawling, unless you have specific reasons and permissions to do otherwise.

By mastering these core components and applying these best practices, you can build powerful, efficient, and resilient web crawlers with Crawl4ai, capable of extracting valuable data from even the most challenging websites. Happy crawling!
```

---


## Core Functionality - Examples
Source: crawl4ai_core_examples_content.llm.md

```markdown
# Examples Outline for crawl4ai - core Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_core.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24 10:00:00
---

This document provides a collection of runnable code examples for the `core` component of the `crawl4ai` library. Each example is designed to showcase a specific feature or configuration.

## 1. Basic `AsyncWebCrawler` Usage

### 1.1. Example: Simplest crawl of a single URL with default `BrowserConfig` and `CrawlerRunConfig`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def simplest_crawl():
    # Uses default BrowserConfig and CrawlerRunConfig
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print("Crawl successful!")
            print(f"Markdown (first 300 chars):\n{result.markdown.raw_markdown[:300]}...")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(simplest_crawl())
```

---
### 1.2. Example: Using `AsyncWebCrawler` as an asynchronous context manager (`async with`).

This is the recommended way to manage the crawler's lifecycle.

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def context_manager_crawl():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print("Crawl successful using context manager!")
            print(f"Page title from metadata: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(context_manager_crawl())
```

---
### 1.3. Example: Explicitly starting and closing the `AsyncWebCrawler` using `start()` and `close()`.

Useful for scenarios where the crawler's lifecycle needs more manual control.

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def explicit_lifecycle_crawl():
    crawler = AsyncWebCrawler()
    await crawler.start()  # Explicitly start the crawler and browser
    try:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print("Crawl successful with explicit start/close!")
            print(f"Cleaned HTML (first 300 chars):\n{result.cleaned_html[:300]}...")
        else:
            print(f"Crawl failed: {result.error_message}")
    finally:
        await crawler.close()  # Ensure the crawler is closed

if __name__ == "__main__":
    asyncio.run(explicit_lifecycle_crawl())
```

---
### 1.4. Example: Handling a failed crawl (e.g., non-existent URL, network error) and checking `CrawlResult.success` and `CrawlResult.error_message`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def failed_crawl_handling():
    async with AsyncWebCrawler() as crawler:
        # Using a deliberately non-existent URL
        result = await crawler.arun(url="https://thissitedoesnotexist.crawl4ai")
        if not result.success:
            print(f"Crawl failed as expected for URL: {result.url}")
            print(f"Status Code: {result.status_code}")
            print(f"Error Message: {result.error_message}")
        else:
            print("Crawl unexpectedly succeeded!")

if __name__ == "__main__":
    asyncio.run(failed_crawl_handling())
```

---
### 1.5. Example: Processing raw HTML content directly using `crawler.aprocess_html()`.

This is useful if you already have HTML content and want to use Crawl4ai's processing capabilities.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def process_raw_html_directly():
    raw_html_content = """
    <html>
        <head><title>My Test Page</title></head>
        <body>
            <h1>Welcome!</h1>
            <p>This is a paragraph with a <a href="https://example.com">link</a>.</p>
            <script>console.log("This should be removed");</script>
        </body>
    </html>
    """
    # No need for BrowserConfig as we are not navigating
    async with AsyncWebCrawler() as crawler:
        # Use CrawlerRunConfig if you need specific processing options
        config = CrawlerRunConfig()
        result = await crawler.aprocess_html(
            url="raw://my_virtual_page", # Provide a conceptual URL
            html=raw_html_content,
            config=config
        )
        if result.success:
            print("Raw HTML processed successfully!")
            print(f"Markdown:\n{result.markdown.raw_markdown}")
            print(f"Cleaned HTML:\n{result.cleaned_html}")
        else:
            print(f"HTML processing failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(process_raw_html_directly())
```

---
### 1.6. Example: Crawling a local HTML file using the `file:///` prefix.

First, create a dummy HTML file named `local_test.html` in the same directory as your script.

```python
# local_test.html
# <!DOCTYPE html>
# <html>
# <head>
#     <title>Local Test File</title>
# </head>
# <body>
#     <h1>Hello from a local file!</h1>
#     <p>This content is loaded from the local filesystem.</p>
# </body>
# </html>
```

```python
import asyncio
import os
from pathlib import Path
from crawl4ai import AsyncWebCrawler

async def crawl_local_file():
    # Create a dummy local HTML file for the example
    script_dir = Path(__file__).parent
    local_file_path = script_dir / "local_test_for_crawl.html"
    with open(local_file_path, "w", encoding="utf-8") as f:
        f.write("<!DOCTYPE html><html><head><title>Local Test</title></head><body><h1>Local Content</h1></body></html>")

    file_url = f"file:///{local_file_path.resolve()}"
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=file_url)
        if result.success:
            print(f"Successfully crawled local file: {file_url}")
            print(f"Markdown (first 100 chars): {result.markdown.raw_markdown[:100]}...")
        else:
            print(f"Failed to crawl local file: {result.error_message}")
    
    # Clean up the dummy file
    if os.path.exists(local_file_path):
        os.remove(local_file_path)

if __name__ == "__main__":
    asyncio.run(crawl_local_file())
```

---
### 1.7. Example: Accessing basic fields from `CrawlResult` (e.g., `url`, `html`, `markdown.raw_markdown`, `status_code`, `response_headers`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def access_crawl_result_fields():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print(f"URL Crawled: {result.url}")
            print(f"Status Code: {result.status_code}")
            
            print("\n--- Response Headers (sample) ---")
            if result.response_headers:
                for key, value in list(result.response_headers.items())[:3]: # Print first 3 headers
                    print(f"{key}: {value}")
            
            print(f"\n--- Raw HTML (first 100 chars) ---\n{result.html[:100]}...")
            print(f"\n--- Cleaned HTML (first 100 chars) ---\n{result.cleaned_html[:100]}...")
            
            if result.markdown:
                 print(f"\n--- Raw Markdown (first 100 chars) ---\n{result.markdown.raw_markdown[:100]}...")
            
            print(f"\n--- Metadata (sample) ---")
            if result.metadata:
                 for key, value in list(result.metadata.items())[:3]: # Print first 3 metadata items
                    print(f"{key}: {value}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(access_crawl_result_fields())
```

---
## 2. Configuring the Browser (`BrowserConfig`)

### 2.1. Example: Initializing `AsyncWebCrawler` with a custom `BrowserConfig` object.

This example sets the browser to run in non-headless mode and uses Firefox.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def custom_browser_config_init():
    # Configure browser to be Firefox and visible
    browser_config = BrowserConfig(
        browser_type="firefox",
        headless=False  # Set to True to run without UI
    )
    
    # Pass the custom config to the crawler
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print(f"Crawl successful with custom BrowserConfig (Firefox, visible)!")
            print(f"Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    # This example might open a visible browser window.
    # Ensure Firefox is installed if you run this.
    # asyncio.run(custom_browser_config_init()) 
    print("Skipping custom_browser_config_init example in automated run to avoid GUI interaction.")
```

---
### 2.2. Browser Type and Headless Mode

#### 2.2.1. Example: Using Chromium browser (default).

This shows the default behavior if no `browser_type` is specified.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def chromium_default_crawl():
    # Chromium is the default, but we can explicitly set it
    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https://example.com")
        if result.success:
            print("Crawl successful with Chromium (default)!")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(chromium_default_crawl())
```

---
#### 2.2.2. Example: Using Firefox browser (`browser_type="firefox"`).

Ensure Firefox is installed on your system for this example to run.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def firefox_crawl():
    browser_config = BrowserConfig(browser_type="firefox", headless=True)
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url="https://example.com")
            if result.success:
                print("Crawl successful with Firefox!")
            else:
                print(f"Crawl failed with Firefox: {result.error_message}")
    except Exception as e:
        print(f"Error running Firefox example: {e}. Ensure Firefox is installed and Playwright browsers are set up (`crawl4ai-setup`).")


if __name__ == "__main__":
    # asyncio.run(firefox_crawl())
    print("Skipping Firefox example in automated run. Uncomment to run if Firefox is installed.")
```

---
#### 2.2.3. Example: Using WebKit browser (`browser_type="webkit"`).

Ensure WebKit (Safari's engine) is installed via Playwright.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def webkit_crawl():
    browser_config = BrowserConfig(browser_type="webkit", headless=True)
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url="https://example.com")
            if result.success:
                print("Crawl successful with WebKit!")
            else:
                print(f"Crawl failed with WebKit: {result.error_message}")
    except Exception as e:
         print(f"Error running WebKit example: {e}. Ensure WebKit is installed and Playwright browsers are set up (`crawl4ai-setup`).")


if __name__ == "__main__":
    # asyncio.run(webkit_crawl())
    print("Skipping WebKit example in automated run. Uncomment to run if WebKit is installed.")
```

---
#### 2.2.4. Example: Running the browser in non-headless mode (`headless=False`) for visual debugging.

This will open a visible browser window.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def non_headless_crawl():
    browser_config = BrowserConfig(headless=False)  # Browser window will be visible
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https
```

---


## Configuration Objects - Memory
Source: crawl4ai_config_objects_memory_content.llm.md

# Detailed Outline for crawl4ai - config_objects Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_config_objects.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Configuration Objects in Crawl4ai

*   **1.1. Purpose of Configuration Objects**
    *   Explanation: Configuration objects in `crawl4ai` serve to centralize and manage settings for various components and behaviors of the library. This includes browser setup, individual crawl run parameters, LLM provider interactions, proxy settings, and more.
    *   Benefit: This approach enhances code readability by grouping related settings, improves maintainability by providing a clear structure for configurations, and offers ease of customization for users to tailor the library's behavior to their specific needs.
*   **1.2. General Principles and Usage**
    *   **1.2.1. Immutability/Cloning:**
        *   Concept: Most configuration objects are designed with a `clone()` method, allowing users to create modified copies without altering the original configuration instance. This promotes safer state management, especially when reusing base configurations for multiple tasks.
        *   Method: `clone(**kwargs)` on most configuration objects.
    *   **1.2.2. Serialization and Deserialization:**
        *   Concept: `crawl4ai` configuration objects can be serialized to dictionary format (e.g., for saving to JSON) and deserialized back into their respective class instances.
        *   Methods:
            *   `dump() -> dict`: Serializes the object to a dictionary suitable for JSON, often using the internal `to_serializable_dict` helper.
            *   `load(data: dict) -> ConfigClass` (Static Method): Deserializes an object from a dictionary, often using the internal `from_serializable_dict` helper.
            *   `to_dict() -> dict`: Converts the object to a standard Python dictionary.
            *   `from_dict(data: dict) -> ConfigClass` (Static Method): Creates an instance from a standard Python dictionary.
        *   Helper Functions:
            *   `crawl4ai.async_configs.to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`: Recursively converts objects into a serializable dictionary format, handling complex types like enums and nested objects.
            *   `crawl4ai.async_configs.from_serializable_dict(data: Any) -> Any`: Reconstructs Python objects from the serializable dictionary format.
*   **1.3. Scope of this Document**
    *   Statement: This document provides a factual API reference for the primary configuration objects within the `crawl4ai` library, detailing their purpose, initialization parameters, attributes, and key methods.

## 2. Core Configuration Objects

### 2.1. `BrowserConfig`
Located in `crawl4ai.async_configs`.

*   **2.1.1. Purpose:**
    *   Description: The `BrowserConfig` class is used to configure the settings for a browser instance and its associated contexts when using browser-based crawler strategies like `AsyncPlaywrightCrawlerStrategy`. It centralizes all parameters that affect the creation and behavior of the browser.
*   **2.1.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class BrowserConfig:
            def __init__(
                self,
                browser_type: str = "chromium",
                headless: bool = True,
                browser_mode: str = "dedicated",
                use_managed_browser: bool = False,
                cdp_url: Optional[str] = None,
                use_persistent_context: bool = False,
                user_data_dir: Optional[str] = None,
                chrome_channel: Optional[str] = "chromium", # Note: 'channel' is preferred
                channel: Optional[str] = "chromium",
                proxy: Optional[str] = None,
                proxy_config: Optional[Union[ProxyConfig, dict]] = None,
                viewport_width: int = 1080,
                viewport_height: int = 600,
                viewport: Optional[dict] = None,
                accept_downloads: bool = False,
                downloads_path: Optional[str] = None,
                storage_state: Optional[Union[str, dict]] = None,
                ignore_https_errors: bool = True,
                java_script_enabled: bool = True,
                sleep_on_close: bool = False,
                verbose: bool = True,
                cookies: Optional[List[dict]] = None,
                headers: Optional[dict] = None,
                user_agent: Optional[str] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
                user_agent_mode: Optional[str] = "",
                user_agent_generator_config: Optional[dict] = None, # Default is {} in __init__
                text_mode: bool = False,
                light_mode: bool = False,
                extra_args: Optional[List[str]] = None,
                debugging_port: int = 9222,
                host: str = "localhost"
            ): ...
        ```
    *   Parameters:
        *   `browser_type (str, default: "chromium")`: Specifies the browser engine to use. Supported values: `"chromium"`, `"firefox"`, `"webkit"`.
        *   `headless (bool, default: True)`: If `True`, runs the browser without a visible GUI. Set to `False` for debugging or visual interaction.
        *   `browser_mode (str, default: "dedicated")`: Defines how the browser is initialized. Options: `"builtin"` (uses built-in CDP), `"dedicated"` (new instance each time), `"cdp"` (connects to an existing CDP endpoint specified by `cdp_url`), `"docker"` (runs browser in a Docker container).
        *   `use_managed_browser (bool, default: False)`: If `True`, launches the browser using a managed approach (e.g., via CDP or Docker), allowing for more advanced control. Automatically set to `True` if `browser_mode` is `"builtin"`, `"docker"`, or if `cdp_url` is provided, or if `use_persistent_context` is `True`.
        *   `cdp_url (Optional[str], default: None)`: The URL for the Chrome DevTools Protocol (CDP) endpoint. If not provided and `use_managed_browser` is active, it might be set by an internal browser manager.
        *   `use_persistent_context (bool, default: False)`: If `True`, uses a persistent browser context (profile), saving cookies, localStorage, etc., across sessions. Requires `user_data_dir`. Sets `use_managed_browser=True`.
        *   `user_data_dir (Optional[str], default: None)`: Path to a directory for storing user data for persistent sessions. If `None` and `use_persistent_context` is `True`, a temporary directory might be used.
        *   `chrome_channel (Optional[str], default: "chromium")`: Specifies the Chrome channel (e.g., "chrome", "msedge", "chromium-beta"). Only applicable if `browser_type` is "chromium".
        *   `channel (Optional[str], default: "chromium")`: Preferred alias for `chrome_channel`. Set to `""` for Firefox or WebKit.
        *   `proxy (Optional[str], default: None)`: A string representing the proxy server URL (e.g., "http://username:password@proxy.example.com:8080").
        *   `proxy_config (Optional[Union[ProxyConfig, dict]], default: None)`: A `ProxyConfig` object or a dictionary specifying detailed proxy settings. Overrides the `proxy` string if both are provided.
        *   `viewport_width (int, default: 1080)`: Default width of the browser viewport in pixels.
        *   `viewport_height (int, default: 600)`: Default height of the browser viewport in pixels.
        *   `viewport (Optional[dict], default: None)`: A dictionary specifying viewport dimensions, e.g., `{"width": 1920, "height": 1080}`. If set, overrides `viewport_width` and `viewport_height`.
        *   `accept_downloads (bool, default: False)`: If `True`, allows files to be downloaded by the browser.
        *   `downloads_path (Optional[str], default: None)`: Directory path where downloaded files will be stored. Required if `accept_downloads` is `True`.
        *   `storage_state (Optional[Union[str, dict]], default: None)`: Path to a JSON file or a dictionary containing the browser's storage state (cookies, localStorage, etc.) to load.
        *   `ignore_https_errors (bool, default: True)`: If `True`, HTTPS certificate errors will be ignored.
        *   `java_script_enabled (bool, default: True)`: If `True`, JavaScript execution is enabled on web pages.
        *   `sleep_on_close (bool, default: False)`: If `True`, introduces a small delay before the browser is closed.
        *   `verbose (bool, default: True)`: If `True`, enables verbose logging for browser operations.
        *   `cookies (Optional[List[dict]], default: None)`: A list of cookie dictionaries to be set in the browser context. Each dictionary should conform to Playwright's cookie format.
        *   `headers (Optional[dict], default: None)`: A dictionary of additional HTTP headers to be sent with every request made by the browser.
        *   `user_agent (Optional[str], default: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36")`: The User-Agent string the browser will use.
        *   `user_agent_mode (Optional[str], default: "")`: Mode for generating the User-Agent string. If set (e.g., to "random"), `user_agent_generator_config` can be used.
        *   `user_agent_generator_config (Optional[dict], default: {})`: Configuration dictionary for the User-Agent generator if `user_agent_mode` is active.
        *   `text_mode (bool, default: False)`: If `True`, attempts to disable images and other rich content to potentially speed up loading for text-focused crawls.
        *   `light_mode (bool, default: False)`: If `True`, disables certain background browser features for potential performance gains.
        *   `extra_args (Optional[List[str]], default: None)`: A list of additional command-line arguments to pass to the browser executable upon launch.
        *   `debugging_port (int, default: 9222)`: The port to use for the browser's remote debugging protocol (CDP).
        *   `host (str, default: "localhost")`: The host on which the browser's remote debugging protocol will listen.
*   **2.1.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
    *   `browser_hint (str)`: [Read-only] - A string representing client hints (Sec-CH-UA) generated based on the `user_agent` string. This is automatically set during initialization.
*   **2.1.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> BrowserConfig` (Static Method):
        *   Purpose: Creates a `BrowserConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `BrowserConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> BrowserConfig`:
        *   Purpose: Creates a deep copy of the current `BrowserConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `BrowserConfig` object into a dictionary format that is suitable for JSON storage or transmission, utilizing the `to_serializable_dict` helper.
    *   `load(cls, data: dict) -> BrowserConfig` (Static Method):
        *   Purpose: Deserializes a `BrowserConfig` object from a dictionary (typically one created by `dump()`), utilizing the `from_serializable_dict` helper.

### 2.2. `CrawlerRunConfig`
Located in `crawl4ai.async_configs`.

*   **2.2.1. Purpose:**
    *   Description: The `CrawlerRunConfig` class encapsulates all settings that control the behavior of a single crawl operation performed by `AsyncWebCrawler.arun()` or multiple operations within `AsyncWebCrawler.arun_many()`. This includes parameters for content processing, page interaction, caching, and media handling.
*   **2.2.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class CrawlerRunConfig:
            def __init__(
                self,
                url: Optional[str] = None,
                word_count_threshold: int = MIN_WORD_THRESHOLD,
                extraction_strategy: Optional[ExtractionStrategy] = None,
                chunking_strategy: Optional[ChunkingStrategy] = RegexChunking(),
                markdown_generator: Optional[MarkdownGenerationStrategy] = DefaultMarkdownGenerator(),
                only_text: bool = False,
                css_selector: Optional[str] = None,
                target_elements: Optional[List[str]] = None, # Default is [] in __init__
                excluded_tags: Optional[List[str]] = None, # Default is [] in __init__
                excluded_selector: Optional[str] = "", # Default is "" in __init__
                keep_data_attributes: bool = False,
                keep_attrs: Optional[List[str]] = None, # Default is [] in __init__
                remove_forms: bool = False,
                prettify: bool = False,
                parser_type: str = "lxml",
                scraping_strategy: Optional[ContentScrapingStrategy] = None, # Instantiated with WebScrapingStrategy() if None
                proxy_config: Optional[Union[ProxyConfig, dict]] = None,
                proxy_rotation_strategy: Optional[ProxyRotationStrategy] = None,
                locale: Optional[str] = None,
                timezone_id: Optional[str] = None,
                geolocation: Optional[GeolocationConfig] = None,
                fetch_ssl_certificate: bool = False,
                cache_mode: CacheMode = CacheMode.BYPASS,
                session_id: Optional[str] = None,
                shared_data: Optional[dict] = None,
                wait_until: str = "domcontentloaded",
                page_timeout: int = PAGE_TIMEOUT,
                wait_for: Optional[str] = None,
                wait_for_timeout: Optional[int] = None,
                wait_for_images: bool = False,
                delay_before_return_html: float = 0.1,
                mean_delay: float = 0.1,
                max_range: float = 0.3,
                semaphore_count: int = 5,
                js_code: Optional[Union[str, List[str]]] = None,
                js_only: bool = False,
                ignore_body_visibility: bool = True,
                scan_full_page: bool = False,
                scroll_delay: float = 0.2,
                process_iframes: bool = False,
                remove_overlay_elements: bool = False,
                simulate_user: bool = False,
                override_navigator: bool = False,
                magic: bool = False,
                adjust_viewport_to_content: bool = False,
                screenshot: bool = False,
                screenshot_wait_for: Optional[float] = None,
                screenshot_height_threshold: int = SCREENSHOT_HEIGHT_THRESHOLD,
                pdf: bool = False,
                capture_mhtml: bool = False,
                image_description_min_word_threshold: int = IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,
                image_score_threshold: int = IMAGE_SCORE_THRESHOLD,
                table_score_threshold: int = 7,
                exclude_external_images: bool = False,
                exclude_all_images: bool = False,
                exclude_social_media_domains: Optional[List[str]] = None, # Uses SOCIAL_MEDIA_DOMAINS if None
                exclude_external_links: bool = False,
                exclude_social_media_links: bool = False,
                exclude_domains: Optional[List[str]] = None, # Default is [] in __init__
                exclude_internal_links: bool = False,
                verbose: bool = True,
                log_console: bool = False,
                capture_network_requests: bool = False,
                capture_console_messages: bool = False,
                method: str = "GET",
                stream: bool = False,
                check_robots_txt: bool = False,
                user_agent: Optional[str] = None,
                user_agent_mode: Optional[str] = None,
                user_agent_generator_config: Optional[dict] = None, # Default is {} in __init__
                deep_crawl_strategy: Optional[DeepCrawlStrategy] = None,
                experimental: Optional[Dict[str, Any]] = None # Default is {} in __init__
            ): ...
        ```
    *   Parameters:
        *   `url (Optional[str], default: None)`: The target URL for this specific crawl run.
        *   `word_count_threshold (int, default: MIN_WORD_THRESHOLD)`: Minimum word count for a text block to be considered significant during content processing.
        *   `extraction_strategy (Optional[ExtractionStrategy], default: None)`: Strategy for extracting structured data from the page. If `None`, `NoExtractionStrategy` is used.
        *   `chunking_strategy (Optional[ChunkingStrategy], default: RegexChunking())`: Strategy to split content into chunks before extraction.
        *   `markdown_generator (Optional[MarkdownGenerationStrategy], default: DefaultMarkdownGenerator())`: Strategy for converting HTML to Markdown.
        *   `only_text (bool, default: False)`: If `True`, attempts to extract only textual content, potentially ignoring structural elements beneficial for rich Markdown.
        *   `css_selector (Optional[str], default: None)`: A CSS selector defining the primary region of the page to focus on for content extraction. The raw HTML is reduced to this region.
        *   `target_elements (Optional[List[str]], default: [])`: A list of CSS selectors. If provided, only the content within these elements will be considered for Markdown generation and structured data extraction. Unlike `css_selector`, this does not reduce the raw HTML but scopes the processing.
        *   `excluded_tags (Optional[List[str]], default: [])`: A list of HTML tag names (e.g., "nav", "footer") to be removed from the HTML before processing.
        *   `excluded_selector (Optional[str], default: "")`: A CSS selector specifying elements to be removed from the HTML before processing.
        *   `keep_data_attributes (bool, default: False)`: If `True`, `data-*` attributes on HTML elements are preserved during cleaning.
        *   `keep_attrs (Optional[List[str]], default: [])`: A list of specific HTML attribute names to preserve during HTML cleaning.
        *   `remove_forms (bool, default: False)`: If `True`, all `<form>` elements are removed from the HTML.
        *   `prettify (bool, default: False)`: If `True`, the cleaned HTML output is "prettified" for better readability.
        *   `parser_type (str, default: "lxml")`: The HTML parser to be used by the scraping strategy (e.g., "lxml", "html.parser").
        *   `scraping_strategy (Optional[ContentScrapingStrategy], default: WebScrapingStrategy())`: The strategy for scraping content from the HTML.
        *   `proxy_config (Optional[Union[ProxyConfig, dict]], default: None)`: Proxy configuration for this specific run. Overrides any proxy settings in `BrowserConfig`.
        *   `proxy_rotation_strategy (Optional[ProxyRotationStrategy], default: None)`: Strategy to use for rotating proxies if multiple are available.
        *   `locale (Optional[str], default: None)`: Locale to set for the browser context (e.g., "en-US", "fr-FR"). Affects `Accept-Language` header and JavaScript `navigator.language`.
        *   `timezone_id (Optional[str], default: None)`: Timezone ID to set for the browser context (e.g., "America/New_York", "Europe/Paris"). Affects JavaScript `Date` objects.
        *   `geolocation (Optional[GeolocationConfig], default: None)`: A `GeolocationConfig` object or dictionary to set the browser's mock geolocation.
        *   `fetch_ssl_certificate (bool, default: False)`: If `True`, the SSL certificate information for the main URL will be fetched and included in the `CrawlResult`.
        *   `cache_mode (CacheMode, default: CacheMode.BYPASS)`: Defines caching behavior for this run. See `CacheMode` enum for options.
        *   `session_id (Optional[str], default: None)`: An identifier for a browser session. If provided, `crawl4ai` will attempt to reuse an existing page/context associated with this ID, or create a new one and associate it.
        *   `shared_data (Optional[dict], default: None)`: A dictionary for passing custom data between hooks during the crawl lifecycle.
        *   `wait_until (str, default: "domcontentloaded")`: Playwright's page navigation wait condition (e.g., "load", "domcontentloaded", "networkidle", "commit").
        *   `page_timeout (int, default: PAGE_TIMEOUT)`: Maximum time in milliseconds for page navigation and other page operations.
        *   `wait_for (Optional[str], default: None)`: A CSS selector or a JavaScript expression (prefixed with "js:"). The crawler will wait until this condition is met before proceeding.
        *   `wait_for_timeout (Optional[int], default: None)`: Specific timeout in milliseconds for the `wait_for` condition. If `None`, `page_timeout` is used.
        *   `wait_for_images (bool, default: False)`: If `True`, attempts to wait for all images on the page to finish loading.
        *   `delay_before_return_html (float, default: 0.1)`: Delay in seconds to wait just before the final HTML content is retrieved from the page.
        *   `mean_delay (float, default: 0.1)`: Used with `arun_many`. The mean base delay in seconds between processing URLs.
        *   `max_range (float, default: 0.3)`: Used with `arun_many`. The maximum additional random delay (added to `mean_delay`) between processing URLs.
        *   `semaphore_count (int, default: 5)`: Used with `arun_many` and semaphore-based dispatchers. The maximum number of concurrent crawl operations.
        *   `js_code (Optional[Union[str, List[str]]], default: None)`: A string or list of strings containing JavaScript code to be executed on the page after it loads.
        *   `js_only (bool, default: False)`: If `True`, indicates that this `arun` call is primarily for JavaScript execution on an already loaded page (within a session) and a full page navigation might not be needed.
        *   `ignore_body_visibility (bool, default: True)`: If `True`, proceeds with content extraction even if the `<body>` element is not deemed visible by Playwright.
        *   `scan_full_page (bool, default: False)`: If `True`, the crawler will attempt to scroll through the entire page to trigger lazy-loaded content.
        *   `scroll_delay (float, default: 0.2)`: Delay in seconds between each scroll step when `scan_full_page` is `True`.
        *   `process_iframes (bool, default: False)`: If `True`, attempts to extract and inline content from `<iframe>` elements.
        *   `remove_overlay_elements (bool, default: False)`: If `True`, attempts to identify and remove common overlay elements (popups, cookie banners) before content extraction.
        *   `simulate_user (bool, default: False)`: If `True`, enables heuristics to simulate user interactions (like mouse movements) to potentially bypass some anti-bot measures.
        *   `override_navigator (bool, default: False)`: If `True`, overrides certain JavaScript `navigator` properties to appear more like a standard browser.
        *   `magic (bool, default: False)`: If `True`, enables a combination of techniques (like `remove_overlay_elements`, `simulate_user`) to try and handle dynamic/obfuscated sites.
        *   `adjust_viewport_to_content (bool, default: False)`: If `True`, attempts to adjust the browser viewport size to match the dimensions of the page content.
        *   `screenshot (bool, default: False)`: If `True`, a screenshot of the page will be taken and included in `CrawlResult.screenshot`.
        *   `screenshot_wait_for (Optional[float], default: None)`: Additional delay in seconds to wait before taking the screenshot.
        *   `screenshot_height_threshold (int, default: SCREENSHOT_HEIGHT_THRESHOLD)`: If page height exceeds this, a full-page screenshot strategy might be different.
        *   `pdf (bool, default: False)`: If `True`, a PDF version of the page will be generated and included in `CrawlResult.pdf`.
        *   `capture_mhtml (bool, default: False)`: If `True`, an MHTML archive of the page will be captured and included in `CrawlResult.mhtml`.
        *   `image_description_min_word_threshold (int, default: IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD)`: Minimum word count for surrounding text to be considered as an image description.
        *   `image_score_threshold (int, default: IMAGE_SCORE_THRESHOLD)`: Heuristic score threshold for an image to be included in `CrawlResult.media`.
        *   `table_score_threshold (int, default: 7)`: Heuristic score threshold for an HTML table to be considered a data table and included in `CrawlResult.media`.
        *   `exclude_external_images (bool, default: False)`: If `True`, images hosted on different domains than the main page URL are excluded.
        *   `exclude_all_images (bool, default: False)`: If `True`, all images are excluded from `CrawlResult.media`.
        *   `exclude_social_media_domains (Optional[List[str]], default: SOCIAL_MEDIA_DOMAINS from config)`: List of social media domains whose links should be excluded.
        *   `exclude_external_links (bool, default: False)`: If `True`, all links pointing to external domains are excluded from `CrawlResult.links`.
        *   `exclude_social_media_links (bool, default: False)`: If `True`, links to domains in `exclude_social_media_domains` are excluded.
        *   `exclude_domains (Optional[List[str]], default: [])`: A list of specific domains whose links should be excluded.
        *   `exclude_internal_links (bool, default: False)`: If `True`, all links pointing to the same domain are excluded.
        *   `verbose (bool, default: True)`: Enables verbose logging for this specific crawl run. Overrides `BrowserConfig.verbose`.
        *   `log_console (bool, default: False)`: If `True`, browser console messages are captured (requires `capture_console_messages=True` to be effective).
        *   `capture_network_requests (bool, default: False)`: If `True`, captures details of network requests and responses made by the page.
        *   `capture_console_messages (bool, default: False)`: If `True`, captures messages logged to the browser's console.
        *   `method (str, default: "GET")`: HTTP method to use, primarily for `AsyncHTTPCrawlerStrategy`.
        *   `stream (bool, default: False)`: If `True` when using `arun_many`, results are yielded as an async generator instead of returned as a list at the end.
        *   `check_robots_txt (bool, default: False)`: If `True`, `robots.txt` rules for the domain will be checked and respected.
        *   `user_agent (Optional[str], default: None)`: User-Agent string for this specific run. Overrides `BrowserConfig.user_agent`.
        *   `user_agent_mode (Optional[str], default: None)`: User-Agent generation mode for this specific run.
        *   `user_agent_generator_config (Optional[dict], default: {})`: Configuration for User-Agent generator for this run.
        *   `deep_crawl_strategy (Optional[DeepCrawlStrategy], default: None)`: Strategy to use for deep crawling beyond the initial URL.
        *   `experimental (Optional[Dict[str, Any]], default: {})`: A dictionary for passing experimental or beta parameters.
*   **2.2.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.2.4. Deprecated Property Handling (`__getattr__`, `_UNWANTED_PROPS`)**
    *   Behavior: Attempting to access a deprecated property (e.g., `bypass_cache`, `disable_cache`, `no_cache_read`, `no_cache_write`) raises an `AttributeError`. The error message directs the user to use the `cache_mode` parameter with the appropriate `CacheMode` enum member instead.
    *   List of Deprecated Properties and their `CacheMode` Equivalents:
        *   `bypass_cache`: Use `cache_mode=CacheMode.BYPASS`.
        *   `disable_cache`: Use `cache_mode=CacheMode.DISABLE`.
        *   `no_cache_read`: Use `cache_mode=CacheMode.WRITE_ONLY`.
        *   `no_cache_write`: Use `cache_mode=CacheMode.READ_ONLY`.
*   **2.2.5. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> CrawlerRunConfig` (Static Method):
        *   Purpose: Creates a `CrawlerRunConfig` instance from a dictionary of keyword arguments.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `CrawlerRunConfig` object to a dictionary suitable for JSON storage, handling complex nested objects using `to_serializable_dict`.
    *   `load(cls, data: dict) -> CrawlerRunConfig` (Static Method):
        *   Purpose: Deserializes a `CrawlerRunConfig` object from a dictionary (typically one created by `dump()`), using `from_serializable_dict`.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `CrawlerRunConfig` instance into a dictionary representation. Complex objects like strategies are typically represented by their class name or a simplified form.
    *   `clone(self, **kwargs) -> CrawlerRunConfig`:
        *   Purpose: Creates a deep copy of the current `CrawlerRunConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.

### 2.3. `LLMConfig`
Located in `crawl4ai.async_configs`.

*   **2.3.1. Purpose:**
    *   Description: The `LLMConfig` class provides configuration for interacting with Large Language Model (LLM) providers. It includes settings for the provider name, API token, base URL, and various model-specific parameters like temperature and max tokens.
*   **2.3.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class LLMConfig:
            def __init__(
                self,
                provider: str = DEFAULT_PROVIDER, # e.g., "openai/gpt-4o-mini"
                api_token: Optional[str] = None,
                base_url: Optional[str] = None,
                temperature: Optional[float] = None,
                max_tokens: Optional[int] = None,
                top_p: Optional[float] = None,
                frequency_penalty: Optional[float] = None,
                presence_penalty: Optional[float] = None,
                stop: Optional[List[str]] = None,
                n: Optional[int] = None,
            ): ...
        ```
    *   Parameters:
        *   `provider (str, default: DEFAULT_PROVIDER)`: The identifier for the LLM provider and model (e.g., "openai/gpt-4o-mini", "ollama/llama3.3", "gemini/gemini-1.5-pro").
        *   `api_token (Optional[str], default: None)`: The API token for authenticating with the LLM provider. If `None`, it attempts to load from environment variables based on the provider (e.g., `OPENAI_API_KEY` for OpenAI, `GEMINI_API_KEY` for Gemini). Can also be set as "env:YOUR_ENV_VAR_NAME".
        *   `base_url (Optional[str], default: None)`: A custom base URL for the LLM API endpoint, useful for self-hosted models or proxies.
        *   `temperature (Optional[float], default: None)`: Controls the randomness of the LLM's output. Higher values (e.g., 0.8) make output more random, lower values (e.g., 0.2) make it more deterministic.
        *   `max_tokens (Optional[int], default: None)`: The maximum number of tokens the LLM should generate in its response.
        *   `top_p (Optional[float], default: None)`: Nucleus sampling parameter. The model considers only tokens with cumulative probability mass up to `top_p`.
        *   `frequency_penalty (Optional[float], default: None)`: Penalizes new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        *   `presence_penalty (Optional[float], default: None)`: Penalizes new tokens based on whether they have appeared in the text so far, increasing the model's likelihood to talk about new topics.
        *   `stop (Optional[List[str]], default: None)`: A list of sequences where the API will stop generating further tokens.
        *   `n (Optional[int], default: None)`: The number of completions to generate for each prompt.
*   **2.3.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.3.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> LLMConfig` (Static Method):
        *   Purpose: Creates an `LLMConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `LLMConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> LLMConfig`:
        *   Purpose: Creates a deep copy of the current `LLMConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.

### 2.4. `GeolocationConfig`
Located in `crawl4ai.async_configs`.

*   **2.4.1. Purpose:**
    *   Description: The `GeolocationConfig` class stores settings for mocking the browser's geolocation, including latitude, longitude, and accuracy.
*   **2.4.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class GeolocationConfig:
            def __init__(
                self,
                latitude: float,
                longitude: float,
                accuracy: Optional[float] = 0.0
            ): ...
        ```
    *   Parameters:
        *   `latitude (float)`: The latitude coordinate (e.g., 37.7749 for San Francisco).
        *   `longitude (float)`: The longitude coordinate (e.g., -122.4194 for San Francisco).
        *   `accuracy (Optional[float], default: 0.0)`: The accuracy of the geolocation in meters.
*   **2.4.3. Key Public Attributes/Properties:**
    *   `latitude (float)`: Stores the latitude.
    *   `longitude (float)`: Stores the longitude.
    *   `accuracy (Optional[float])`: Stores the accuracy.
*   **2.4.4. Key Public Methods:**
    *   `from_dict(cls, geo_dict: dict) -> GeolocationConfig` (Static Method):
        *   Purpose: Creates a `GeolocationConfig` instance from a dictionary.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `GeolocationConfig` instance to a dictionary: `{"latitude": ..., "longitude": ..., "accuracy": ...}`.
    *   `clone(self, **kwargs) -> GeolocationConfig`:
        *   Purpose: Creates a copy of the `GeolocationConfig` instance, allowing for overriding specific attributes with `kwargs`.

### 2.5. `ProxyConfig`
Located in `crawl4ai.async_configs` (and `crawl4ai.proxy_strategy`).

*   **2.5.1. Purpose:**
    *   Description: The `ProxyConfig` class encapsulates the configuration for a single proxy server, including its address, authentication credentials (if any), and optionally its public IP address.
*   **2.5.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class ProxyConfig:
            def __init__(
                self,
                server: str,
                username: Optional[str] = None,
                password: Optional[str] = None,
                ip: Optional[str] = None,
            ): ...
        ```
    *   Parameters:
        *   `server (str)`: The proxy server URL, including protocol and port (e.g., "http://127.0.0.1:8080", "socks5://proxy.example.com:1080").
        *   `username (Optional[str], default: None)`: The username for proxy authentication, if required.
        *   `password (Optional[str], default: None)`: The password for proxy authentication, if required.
        *   `ip (Optional[str], default: None)`: The public IP address of the proxy server. If not provided, it will be automatically extracted from the `server` string if possible.
*   **2.5.3. Key Public Attributes/Properties:**
    *   `server (str)`: The proxy server URL.
    *   `username (Optional[str])`: The username for proxy authentication.
    *   `password (Optional[str])`: The password for proxy authentication.
    *   `ip (Optional[str])`: The public IP address of the proxy. This is either user-provided or automatically extracted from the `server` string during initialization via the internal `_extract_ip_from_server` method.
*   **2.5.4. Key Public Methods:**
    *   `_extract_ip_from_server(self) -> Optional[str]` (Internal method):
        *   Purpose: Extracts the IP address component from the `self.server` URL string.
    *   `from_string(cls, proxy_str: str) -> ProxyConfig` (Static Method):
        *   Purpose: Creates a `ProxyConfig` instance from a string.
        *   Formats:
            *   `'ip:port:username:password'`
            *   `'ip:port'` (no authentication)
    *   `from_dict(cls, proxy_dict: dict) -> ProxyConfig` (Static Method):
        *   Purpose: Creates a `ProxyConfig` instance from a dictionary with keys "server", "username", "password", and "ip".
    *   `from_env(cls, env_var: str = "PROXIES") -> List[ProxyConfig]` (Static Method):
        *   Purpose: Loads a list of `ProxyConfig` objects from a comma-separated environment variable. Each proxy string in the variable should conform to the format accepted by `from_string`.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `ProxyConfig` instance to a dictionary: `{"server": ..., "username": ..., "password": ..., "ip": ...}`.
    *   `clone(self, **kwargs) -> ProxyConfig`:
        *   Purpose: Creates a copy of the `ProxyConfig` instance, allowing for overriding specific attributes with `kwargs`.

### 2.6. `HTTPCrawlerConfig`
Located in `crawl4ai.async_configs`.

*   **2.6.1. Purpose:**
    *   Description: The `HTTPCrawlerConfig` class holds configuration settings specific to direct HTTP-based crawling strategies (e.g., `AsyncHTTPCrawlerStrategy`), which do not use a full browser environment.
*   **2.6.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class HTTPCrawlerConfig:
            def __init__(
                self,
                method: str = "GET",
                headers: Optional[Dict[str, str]] = None,
                data: Optional[Dict[str, Any]] = None,
                json: Optional[Dict[str, Any]] = None,
                follow_redirects: bool = True,
                verify_ssl: bool = True,
            ): ...
        ```
    *   Parameters:
        *   `method (str, default: "GET")`: The HTTP method to use for the request (e.g., "GET", "POST", "PUT").
        *   `headers (Optional[Dict[str, str]], default: None)`: A dictionary of custom HTTP headers to send with the request.
        *   `data (Optional[Dict[str, Any]], default: None)`: Data to be sent in the body of the request, typically for "POST" or "PUT" requests (e.g., form data).
        *   `json (Optional[Dict[str, Any]], default: None)`: JSON data to be sent in the body of the request. If provided, the `Content-Type` header is typically set to `application/json`.
        *   `follow_redirects (bool, default: True)`: If `True`, the crawler will automatically follow HTTP redirects.
        *   `verify_ssl (bool, default: True)`: If `True`, SSL certificates will be verified. Set to `False` to ignore SSL errors (use with caution).
*   **2.6.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.6.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> HTTPCrawlerConfig` (Static Method):
        *   Purpose: Creates an `HTTPCrawlerConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `HTTPCrawlerConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> HTTPCrawlerConfig`:
        *   Purpose: Creates a deep copy of the current `HTTPCrawlerConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `HTTPCrawlerConfig` object to a dictionary.
    *   `load(cls, data: dict) -> HTTPCrawlerConfig` (Static Method):
        *   Purpose: Deserializes an `HTTPCrawlerConfig` object from a dictionary.

## 3. Enumerations and Helper Constants

### 3.1. `CacheMode` (Enum)
Located in `crawl4ai.cache_context`.

*   **3.1.1. Purpose:**
    *   Description: The `CacheMode` enumeration defines the different caching behaviors that can be applied to a crawl operation. It is used in `CrawlerRunConfig` to control how results are read from and written to the cache.
*   **3.1.2. Enum Members:**
    *   `ENABLE (str)`: Value: "ENABLE". Description: Enables normal caching behavior. The crawler will attempt to read from the cache first, and if a result is not found or is stale, it will perform the crawl and write the new result to the cache.
    *   `DISABLE (str)`: Value: "DISABLE". Description: Disables all caching. The crawler will not read from or write to the cache. Every request will be a fresh crawl.
    *   `READ_ONLY (str)`: Value: "READ_ONLY". Description: The crawler will only attempt to read from the cache. If a result is found, it will be used. If not, the crawl will not proceed further for that URL, and no new data will be written to the cache.
    *   `WRITE_ONLY (str)`: Value: "WRITE_ONLY". Description: The crawler will not attempt to read from the cache. It will always perform a fresh crawl and then write the result to the cache.
    *   `BYPASS (str)`: Value: "BYPASS". Description: The crawler will skip reading from the cache for this specific operation and will perform a fresh crawl. The result of this crawl *will* be written to the cache. This is the default `cache_mode` for `CrawlerRunConfig`.
*   **3.1.3. Usage:**
    *   Example:
        ```python
        from crawl4ai import CrawlerRunConfig, CacheMode
        config = CrawlerRunConfig(cache_mode=CacheMode.ENABLE) # Use cache fully
        config_bypass = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Force fresh crawl, then cache
        ```

## 4. Serialization Helper Functions
Located in `crawl4ai.async_configs`.

### 4.1. `to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`

*   **4.1.1. Purpose:**
    *   Description: This utility function recursively converts various Python objects, including `crawl4ai` configuration objects, into a dictionary format that is suitable for JSON serialization. It uses a `{ "type": "ClassName", "params": { ... } }` structure for custom class instances to enable proper deserialization later.
*   **4.1.2. Parameters:**
    *   `obj (Any)`: The Python object to be serialized.
    *   `ignore_default_value (bool, default: False)`: If `True`, when serializing class instances, parameters whose current values match their `__init__` default values might be excluded from the "params" dictionary. (Note: The exact behavior depends on the availability of default values in the class signature and handling of empty/None values).
*   **4.1.3. Returns:**
    *   `Dict`: A dictionary representation of the input object, structured for easy serialization (e.g., to JSON) and later deserialization by `from_serializable_dict`.
*   **4.1.4. Key Behaviors:**
    *   **Basic Types:** `str`, `int`, `float`, `bool`, `None` are returned as is.
    *   **Enums:** Serialized as `{"type": "EnumClassName", "params": enum_member.value}`.
    *   **Datetime Objects:** Serialized to their ISO 8601 string representation.
    *   **Lists, Tuples, Sets, Frozensets:** Serialized by recursively calling `to_serializable_dict` on each of their elements, returning a list.
    *   **Plain Dictionaries:** Serialized as `{"type": "dict", "value": {key: serialized_value, ...}}`.
    *   **Class Instances (e.g., Config Objects):**
        *   The object's class name is stored in the "type" field.
        *   Parameters from the `__init__` signature and attributes from `__slots__` (if defined) are collected.
        *   Their current values are recursively serialized and stored in the "params" dictionary.
        *   The structure is `{"type": "ClassName", "params": {"param_name": serialized_param_value, ...}}`.

### 4.2. `from_serializable_dict(data: Any) -> Any`

*   **4.2.1. Purpose:**
    *   Description: This utility function reconstructs Python objects, including `crawl4ai` configuration objects, from the serializable dictionary format previously created by `to_serializable_dict`.
*   **4.2.2. Parameters:**
    *   `data (Any)`: The dictionary (or basic data type) to be deserialized. This is typically the output of `to_serializable_dict` after being, for example, loaded from a JSON string.
*   **4.2.3. Returns:**
    *   `Any`: The reconstructed Python object (e.g., an instance of `BrowserConfig`, `LLMConfig`, a list, a plain dictionary, etc.).
*   **4.2.4. Key Behaviors:**
    *   **Basic Types:** `str`, `int`, `float`, `bool`, `None` are returned as is.
    *   **Typed Dictionaries (from `to_serializable_dict`):**
        *   If `data` is a dictionary and contains a "type" key:
            *   If `data["type"] == "dict"`, it reconstructs a plain Python dictionary from `data["value"]` by recursively deserializing its items.
            *   Otherwise, it attempts to locate the class specified by `data["type"]` within the `crawl4ai` module.
                *   If the class is an `Enum`, it instantiates the enum member using `data["params"]` (the enum value).
                *   If it's a regular class, it recursively deserializes the items in `data["params"]` and uses them as keyword arguments (`**kwargs`) to instantiate the class.
    *   **Lists:** If `data` is a list, it reconstructs a list by recursively calling `from_serializable_dict` on each of its elements.
    *   **Legacy Dictionaries:** If `data` is a dictionary but does not conform to the "type" key structure (for backward compatibility), it attempts to deserialize its values.

## 5. Cross-References and Relationships

*   **5.1. `BrowserConfig` Usage:**
    *   Typically instantiated once and passed to the `AsyncWebCrawler` constructor via its `config` parameter.
    *   `browser_config = BrowserConfig(headless=False)`
    *   `crawler = AsyncWebCrawler(config=browser_config)`
    *   It defines the global browser settings that will be used for all subsequent crawl operations unless overridden by `CrawlerRunConfig` on a per-run basis.
*   **5.2. `CrawlerRunConfig` Usage:**
    *   Passed to the `arun()` or `arun_many()` methods of `AsyncWebCrawler`.
    *   `run_config = CrawlerRunConfig(screenshot=True, cache_mode=CacheMode.BYPASS)`
    *   `result = await crawler.arun(url="https://example.com", config=run_config)`
    *   Allows for fine-grained control over individual crawl requests, overriding global settings from `BrowserConfig` or `AsyncWebCrawler`'s defaults where applicable (e.g., `user_agent`, `proxy_config`, `cache_mode`).
*   **5.3. `LLMConfig` Usage:**
    *   Instantiated and passed to LLM-based extraction strategies (e.g., `LLMExtractionStrategy`) or content filters (`LLMContentFilter`) during their initialization.
    *   `llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token="sk-...")`
    *   `extraction_strategy = LLMExtractionStrategy(llm_config=llm_conf, schema=my_schema)`
*   **5.4. `GeolocationConfig` and `ProxyConfig` Usage:**
    *   `GeolocationConfig` is typically instantiated and assigned to the `geolocation` parameter of `CrawlerRunConfig`.
        *   `geo_conf = GeolocationConfig(latitude=34.0522, longitude=-118.2437)`
        *   `run_config = CrawlerRunConfig(geolocation=geo_conf)`
    *   `ProxyConfig` can be assigned to the `proxy_config` parameter of `BrowserConfig` (for a global proxy applied to all contexts) or `CrawlerRunConfig` (for a proxy specific to a single crawl run).
        *   `proxy_conf = ProxyConfig(server="http://myproxy:8080")`
        *   `browser_config = BrowserConfig(proxy_config=proxy_conf)` (global)
        *   `run_config = CrawlerRunConfig(proxy_config=proxy_conf)` (per-run)
*   **5.5. `HTTPCrawlerConfig` Usage:**
    *   Used when the `crawler_strategy` for `AsyncWebCrawler` is set to `AsyncHTTPCrawlerStrategy` (for non-browser-based HTTP requests).
    *   `http_conf = HTTPCrawlerConfig(method="POST", json={"key": "value"})`
    *   `http_strategy = AsyncHTTPCrawlerStrategy(http_crawler_config=http_conf)`
    *   `crawler = AsyncWebCrawler(crawler_strategy=http_strategy)`
    *   Alternatively, parameters like `method`, `data`, `json` can be passed directly to `arun()` when using `AsyncHTTPCrawlerStrategy` if they are part of the `CrawlerRunConfig`.

---


## Configuration Objects - Reasoning
Source: crawl4ai_config_objects_reasoning_content.llm.md

Okay, I'm ready to generate the detailed "Reasoning & Problem-Solving Framework" document for the `config_objects` component based on the provided outline and information.

```markdown
# Detailed Outline for crawl4ai - config_objects Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_config_objects.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Configuration in Crawl4ai

*   1.1. **The "Why": The Importance of Configuration**
    *   1.1.1. **Explaining how configuration objects provide granular control over crawling.**
        Crawl4ai is designed to tackle a wide array of web crawling and scraping tasks, from simple page fetches to complex interactions with dynamic websites and data extraction using LLMs. To manage this complexity effectively, Crawl4ai employs a system of dedicated configuration objects. These objects allow you to precisely define how the crawler behaves at different stages: how the browser is set up, how individual web pages are processed, and how interactions with Large Language Models (LLMs) are handled.
        Without a robust configuration system, you'd be forced to pass numerous, often-conflicting parameters to a single function, making your code hard to read, maintain, and debug. Configuration objects provide a structured, organized, and explicit way to tell Crawl4ai exactly what you want it to do.

    *   1.1.2. **Discussing the benefits of separating browser setup (`BrowserConfig`) from individual crawl behavior (`CrawlerRunConfig`) and LLM settings (`LLMConfig`).**
        The separation of concerns is a key design principle in Crawl4ai's configuration system:
        *   **`BrowserConfig`:** This object dictates the *environment* in which your crawls will run. It handles aspects like which browser to use (Chrome, Firefox), whether to run in headless mode, proxy settings, and browser identity (user-agent). This setup is typically done once per `AsyncWebCrawler` instance or per logical group of crawling tasks that require the same browser environment.
        *   **`CrawlerRunConfig`:** This object controls the specifics of *each individual crawl operation* (e.g., a single call to `arun()`). It defines how a particular URL is fetched, what content to extract, which JavaScript to execute on the page, caching behavior for that specific URL, and any media capture settings (screenshots, PDFs). This allows you to use the same browser setup to crawl different URLs with vastly different processing requirements.
        *   **`LLMConfig`:** When leveraging LLMs for tasks like content summarization or structured data extraction, `LLMConfig` centralizes all settings related to the LLM provider, model choice, API keys, and generation parameters (like temperature or max tokens). This keeps LLM-specific details separate from the core crawling and browser logic.

        This separation offers significant advantages:
        *   **Modularity:** You can define a browser setup once and reuse it for many different crawl tasks, each with its own `CrawlerRunConfig`.
        *   **Clarity:** It's easier to understand which settings affect which part of the crawling process.
        *   **Maintainability:** Changes to browser setup don't require modifying every crawl task's configuration, and vice-versa.
        *   **Flexibility:** You can easily swap out different LLM providers or models without altering your core crawling logic.

    *   1.1.3. **Overview of how these objects work together to achieve complex crawling scenarios.**
        Imagine you need to crawl a series of product pages.
        1.  You'd first instantiate an `AsyncWebCrawler` with a `BrowserConfig` that sets up a browser with, perhaps, a common desktop user-agent and no proxy.
        2.  Then, for each product page URL, you'd call `crawler.arun()` with a `CrawlerRunConfig`. This `CrawlerRunConfig` might specify:
            *   A `css_selector` to target only the main product information block.
            *   An `extraction_strategy` (like `JsonCssExtractionStrategy` or `LLMExtractionStrategy` with an `LLMConfig`) to pull out the product name, price, and description.
            *   `screenshot=True` to capture an image of the product page.
        3.  If another part of your task involves crawling blog posts from the same site, you could reuse the same `AsyncWebCrawler` (and thus the same `BrowserConfig`) but pass a *different* `CrawlerRunConfig` to `arun()` tailored for blog posts (e.g., different selectors, a different extraction strategy focused on article text).

        This layered approach allows you to build sophisticated crawlers by combining these configuration objects in a logical and manageable way.

*   1.2. **Core Philosophy: Flexibility and Reusability**
    *   1.2.1. **How the design promotes creating base configurations and specializing them.**
        A common and highly recommended pattern is to define "base" configuration objects that capture common settings for your project or for a specific type of task. Then, for individual crawls or variations, you can use the `clone()` method to create a new instance of the configuration object and override only the specific parameters you need to change. This significantly reduces code duplication and makes your configurations easier to manage.

        For example, you might have a `base_browser_config` for all your crawls and a `base_ecommerce_run_config` for scraping e-commerce sites. When scraping a specific e-commerce site, you'd clone `base_ecommerce_run_config` and only adjust, say, the `css_selector` or `extraction_strategy`.

    *   1.2.2. **The role of `clone()`, `dump()`, and `load()` in managing configuration lifecycle.**
        Crawl4ai's configuration objects come with built-in methods to streamline their management:
        *   **`clone(**kwargs)`:** Creates a deep copy of the configuration object, allowing you to override specific parameters for the new instance without affecting the original. This is perfect for creating specialized versions from a base configuration.
        *   **`dump()`:** Serializes the configuration object into a Python dictionary. This dictionary can then be easily saved to a JSON or YAML file, stored in a database, or transmitted over a network.
        *   **`load(data: dict)`:** A static method on each configuration class that reconstructs a configuration object from a dictionary (typically one produced by `dump()`). This allows you to load configurations from external sources, making your crawling setup more dynamic and shareable.

        These methods facilitate:
        *   **Versioning:** Store different configuration versions in files.
        *   **Sharing:** Easily share configurations between different parts of your application or with team members.
        *   **Dynamic Setup:** Load configurations based on runtime parameters or external inputs.

*   1.3. **Scope of This Guide**
    *   1.3.1. **What this guide will cover (deep dive into reasoning for `BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, `GeolocationConfig`, `ProxyConfig`, `HTTPCrawlerConfig`).**
        This guide focuses on the *reasoning* behind using various configuration objects and their parameters. We'll explore *how* to make effective choices, *why* certain features are designed the way they are, and *when* to use specific settings to solve common crawling challenges. We will perform a deep dive into:
        *   `BrowserConfig`: For setting up the browser's environment and identity.
        *   `CrawlerRunConfig`: For tailoring individual crawl operations.
        *   `LLMConfig`: For configuring interactions with Large Language Models.
        *   And touch upon specialized configs like `GeolocationConfig`, `ProxyConfig`, and `HTTPCrawlerConfig` for specific use cases.
    *   1.3.2. **Briefly mentioning where to find exhaustive API parameter lists (referencing a "memory" document or API docs).**
        While this guide provides practical examples and discusses many key parameters, it is not an exhaustive API reference. For a complete list of all available parameters, their types, default values, and concise descriptions, please refer to the official API documentation or the "Foundational Memory" document for `config_objects` if available. This guide aims to complement that factual information by providing the "how-to" and "why."

## 2. Mastering `BrowserConfig`: Setting Up Your Crawler's Identity and Environment

*   2.1. **Understanding `BrowserConfig`: Beyond Default Behavior**
    *   2.1.1. **When is the default `BrowserConfig` sufficient?**
        If you're performing simple crawls of public, static websites that don't have strong anti-bot measures, the default `BrowserConfig` (which you get by simply instantiating `AsyncWebCrawler()` without a custom config) might work perfectly fine. It typically launches a headless Chromium browser with a generic user-agent. For quick tests or very straightforward tasks, this is often all you need.

    *   2.1.2. **Key scenarios demanding `BrowserConfig` customization:**
        You'll need to customize `BrowserConfig` when your crawling tasks become more complex or when you encounter challenges like:
        *   **Evading Bot Detection:** Many websites employ techniques to identify and block automated crawlers. Customizing user-agents, browser hints, and even browser behavior can help your crawler appear more like a regular human user.
        *   **Testing Geo-Specific Content:** If a website serves different content based on the user's geographic location, you'll need to configure the browser to simulate originating from that specific region (using `GeolocationConfig` within `CrawlerRunConfig`, but also ensuring your browser's IP via a proxy in `BrowserConfig` aligns).
        *   **Using Proxies:** To rotate IP addresses, mask your origin, or access geo-restricted content, configuring proxies is essential.
        *   **Managing Browser Resources and Performance:** For large-scale crawls, controlling browser features (like disabling images or JavaScript) or using different browser modes (like Docker) can significantly impact performance and resource consumption.
        *   **Persistent Sessions and Authenticated Crawling:** If you need to log into a website and maintain that session across multiple crawl operations, `BrowserConfig` provides options for persistent contexts.

*   2.2. **Strategic `BrowserConfig` Customizations**
    *   2.2.1. **Crafting a Believable Browser Identity**
        *   **`user_agent` and `user_agent_mode`:**
            *   **Why faking User-Agents can be crucial:** The User-Agent string is one of the first pieces of information a web server receives. Many sites use it to tailor content or, more critically for crawlers, to identify and block non-standard or known bot User-Agents. Using a common, legitimate browser User-Agent makes your crawler less conspicuous.
            *   **Choosing between a static `user_agent` and `user_agent_mode="random"`:**
                *   **Static `user_agent`:** Use this if you want to consistently mimic a specific browser and OS combination. This can be useful for targeting mobile-specific views or ensuring consistent rendering.
                *   **`user_agent_mode="random"`:** Crawl4ai will use its built-in `ValidUAGenerator` to pick a common, valid User-Agent for each new browser context (or potentially page, depending on strategy details). This can help avoid patterns if a site tracks User-Agents over time. The `user_agent_generator_config` parameter can be used to further customize the random generation if needed, for example, to only generate User-Agents for a specific OS or device type.
            *   **Trade-offs and when to use each:**
                *   Static: More predictable, good for specific targeting.
                *   Random: Better for avoiding simple User-Agent-based blocking over many requests, but ensure the randomness still aligns with common browser profiles.
            *   **Code Example: Setting a specific User-Agent vs. using random generation.**
                ```python
                from crawl4ai import BrowserConfig

                # Specific User-Agent
                config_specific_ua = BrowserConfig(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                print(f"Specific UA Config: {config_specific_ua.user_agent}")

                # Random User-Agent (default behavior when user_agent_mode="random" or just not set with a static UA)
                config_random_ua = BrowserConfig(user_agent_mode="random")
                # Note: The actual UA is generated when the browser context is created by AsyncWebCrawler
                # We can inspect the generated UA through the browser_hint which is derived from it.
                print(f"Random UA Config (Hint): {config_random_ua.browser_hint}")
                # Example for generating one manually:
                from crawl4ai.user_agent_generator import ValidUAGenerator
                ua_gen = ValidUAGenerator()
                random_ua_example = ua_gen.generate()
                print(f"Example Random UA: {random_ua_example}")
                ```

        *   **`browser_hint` and `sec-ch-ua` headers:**
            *   **How these contribute to a more convincing browser profile:** Modern browsers send Client Hints (like `Sec-CH-UA`, `Sec-CH-UA-Mobile`, `Sec-CH-UA-Platform`) that provide more granular information about the browser than the traditional User-Agent string. Crawl4ai automatically generates a plausible `browser_hint` (which populates `Sec-CH-UA`) based on the `user_agent` to enhance authenticity.
            *   **Ensuring consistency:** It's vital that your Client Hints are consistent with your main User-Agent string. Crawl4ai aims to do this automatically. If you manually set headers, ensure they don't contradict your chosen `user_agent`.

    *   2.2.2. **Headless vs. Headful: The Visibility Trade-off (`headless`)**
        *   **Why use headless mode (`headless=True`, default):**
            *   **Servers & Automation:** Ideal for running crawlers on servers or in automated CI/CD pipelines where no graphical interface is available or needed.
            *   **Speed & Resources:** Generally consumes fewer resources than a full GUI browser, leading to faster crawls, especially at scale.
        *   **When headful mode (`headless=False`) is essential:**
            *   **Debugging:** Visually inspecting what the browser sees is invaluable for debugging issues with page rendering, element selection, or unexpected site behavior.
            *   **Anti-Bot Measures:** Some sophisticated websites can detect headless browsers (e.g., by checking for specific JavaScript properties or rendering inconsistencies). Running in headful mode can sometimes bypass these checks.
        *   **Impact on performance and detectability:** Headless is faster but potentially more detectable. Headful is slower, uses more resources, but can appear more like a real user.
        *   **Decision Guide: Choosing the right mode for your task.**
            *   Start with `headless=True` for production and automated runs.
            *   Switch to `headless=False` when:
                *   Debugging selectors or interactions.
                *   You suspect the site is blocking headless browsers.
                *   You need to manually perform actions like solving a CAPTCHA during a setup phase.
            ```python
            from crawl4ai import BrowserConfig

            # Default: Headless
            config_headless = BrowserConfig() # headless=True is the default
            print(f"Headless mode: {config_headless.headless}")

            # Explicitly Headful for debugging
            config_headful = BrowserConfig(headless=False)
            print(f"Headful mode: {config_headful.headless}")
            ```

    *   2.2.3. **Controlling the Browser's Lifecycle and Environment**
        *   **`browser_mode` ("builtin", "dedicated", "cdp", "docker"):**
            *   **Explaining each mode and its typical use case:**
                *   `"dedicated"` (Default): Launches a fresh, isolated browser instance for the `AsyncWebCrawler`. This is good for most use cases, ensuring no state leaks between different crawler instances if you were to run multiple in the same script (though typically you'd use one `AsyncWebCrawler` and multiple `arun` calls).
                *   `"builtin"`: (More advanced) Intended for scenarios where Crawl4ai manages a long-lived browser process in the background, potentially shared across different crawler objects or Python processes. This can be more resource-efficient for very frequent, short-lived crawl tasks. It leverages `use_managed_browser=True` and a CDP connection to this managed browser.
                *   `"cdp"` (or `use_managed_browser=True` with a `cdp_url`): Allows you to connect Crawl4ai to an *existing* Chrome/Chromium browser instance that has been launched with a remote debugging port. Useful if you want to control a browser you've launched manually or one managed by another tool.
                *   `"docker"`: Facilitates running the browser inside a Docker container. Crawl4ai can manage launching a browser in a container and connecting to it. This is excellent for consistent environments and isolating dependencies. (Requires Docker setup and relevant browser images).
            *   **"dedicated":**
                *   Pros: Simple to understand, good isolation for typical `AsyncWebCrawler` usage.
                *   Cons: Can be resource-intensive if you're instantiating many `AsyncWebCrawler` objects each with its own dedicated browser, instead of reusing one `AsyncWebCrawler` for multiple `arun` calls.
            *   **"cdp" / `use_managed_browser=True`:** This implies that Crawl4ai will try to connect to a browser via the Chrome DevTools Protocol (CDP).
                *   If `cdp_url` is provided in `BrowserConfig`, it uses that.
                *   If `browser_mode` is "builtin" or "docker", Crawl4ai's internal `ManagedBrowser` (or a Docker strategy) would start a browser and provide the `cdp_url` internally.
        *   **`use_persistent_context` and `user_data_dir`:**
            *   **The power of persistent sessions:** When `use_persistent_context=True`, Playwright (the underlying browser automation library) attempts to save and reuse browser state (cookies, local storage, etc.) across sessions, using the directory specified by `user_data_dir`. This is invaluable for:
                *   **Authenticated Crawls:** Log in once (manually or scripted), and subsequent crawls with the same `user_data_dir` can often bypass the login process.
                *   **Maintaining Preferences:** Site preferences, "accept cookies" banners, etc., can be remembered.
            *   **Workflow for authenticated crawling:**
                1.  **Initial Setup Run:**
                    ```python
                    # First run: Login and save session
                    login_browser_config = BrowserConfig(
                        headless=False,  # Often easier to do initial login with a visible browser
                        use_persistent_context=True,
                        user_data_dir="./my_browser_profile" # Choose a path
                    )
                    # ... (code to navigate to login page, fill credentials, submit using crawler.arun() with appropriate js_code)
                    # After successful login, close the crawler. The session is saved in "./my_browser_profile".
                    ```
                2.  **Subsequent Runs:**
                    ```python
                    # Subsequent runs: Reuse the saved profile
                    reuse_browser_config = BrowserConfig(
                        headless=True, # Can now run headless
                        use_persistent_context=True,
                        user_data_dir="./my_browser_profile" # Must be the same path
                    )
                    # ... (crawler.arun() calls to access protected pages will now use the saved session)
                    ```
            *   **Best Practice:** Use distinct `user_data_dir` paths for different websites or different user accounts to keep sessions isolated.
            *   **Note:** `use_persistent_context=True` automatically implies `use_managed_browser=True` because persistent contexts are a feature of Playwright's browser contexts launched via CDP.

    *   2.2.4. **Navigating Networks: Proxies and SSL (`proxy_config`, `ignore_https_errors`)**
        *   **Integrating Proxies with `proxy_config` (referencing `ProxyConfig` object):**
            *   **Why use proxies:**
                *   **IP Rotation:** Avoid rate limits or blocks by distributing requests across multiple IP addresses.
                *   **Geo-Targeting:** Access content specific to a certain geographic region by using a proxy located in that region.
                *   **Anonymity/Privacy:** Mask your crawler's true origin IP (though be mindful of the proxy provider's logging policies).
            *   **How to structure the `proxy_config` dictionary:**
                The `proxy_config` parameter in `BrowserConfig` expects a dictionary compatible with Playwright's proxy settings. Typically, this includes:
                *   `server`: The proxy server address (e.g., `"http://proxy.example.com:8080"` or `"socks5://proxy.example.com:1080"`).
                *   `username` (optional): Username for proxy authentication.
                *   `password` (optional): Password for proxy authentication.
                A `ProxyConfig` object from `crawl4ai.async_configs` can also be used here by converting it to a dictionary with `my_proxy_config.to_dict()`.
            *   **Workflow: Implementing a basic proxy rotation:**
                While Crawl4ai has a more advanced `ProxyRotationStrategy` (covered elsewhere), a simple rotation can be achieved by dynamically creating `BrowserConfig` instances:
                ```python
                # Conceptual: Basic proxy rotation
                proxies = [
                    {"server": "http://proxy1.example.com:8080", "username": "user1", "password": "p1"},
                    {"server": "http://proxy2.example.com:8080", "username": "user2", "password": "p2"},
                ]
                current_proxy_index = 0

                def get_next_proxy_config_dict():
                    nonlocal current_proxy_index
                    proxy_details = proxies[current_proxy_index % len(proxies)]
                    current_proxy_index += 1
                    return proxy_details

                # In your loop or arun_many setup:
                # proxy_dict = get_next_proxy_config_dict()
                # browser_cfg = BrowserConfig(proxy_config=proxy_dict)
                # crawler = AsyncWebCrawler(config=browser_cfg)
                # await crawler.arun(...)
                ```
            *   **Code Example: Configuring a single authenticated proxy.**
                ```python
                from crawl4ai import BrowserConfig

                proxy_settings = {
                    "server": "http://myproxy.service.com:3128",
                    "username": "proxy_user",
                    "password": "proxy_password"
                }
                config_with_proxy = BrowserConfig(proxy_config=proxy_settings)

                # To use with AsyncWebCrawler:
                # async with AsyncWebCrawler(config=config_with_proxy) as crawler:
                #     result = await crawler.arun(url="https://api.ipify.org?format=json") # Check your IP
                #     print(result.html)
                ```
        *   **`ignore_https_errors`:**
            *   **When this might be needed:** Primarily for development or testing environments where you might encounter self-signed SSL certificates or other non-production SSL configurations.
            *   **Warning:** Setting `ignore_https_errors=True` in a production environment or when accessing sensitive sites is **highly discouraged** as it bypasses crucial security checks, making your crawler vulnerable to man-in-the-middle attacks. Use with extreme caution.

    *   2.2.5. **Fine-tuning for Performance (`text_mode`, `light_mode`, `extra_args`)**
        *   **`text_mode=True`:**
            *   **Benefits:** This mode attempts to disable the loading of images, CSS, and fonts, and may also disable JavaScript depending on the underlying strategy implementation. This can significantly speed up page loads and reduce bandwidth consumption, especially for sites where you are primarily interested in textual content.
        *   **`light_mode=True`:**
            *   **How it differs:** `light_mode` is a more aggressive optimization. It not only includes `text_mode` behaviors but also enables a set of browser launch arguments (`BROWSER_DISABLE_OPTIONS` in `browser_manager.py`) designed to disable various background features, rendering optimizations, and GPU acceleration. This is aimed at achieving maximum performance gains, especially in resource-constrained environments or for very large-scale crawls where every millisecond counts.
        *   **`extra_args`:**
            *   **Unlocking advanced browser capabilities and optimizations:** This parameter allows you to pass a list of custom command-line arguments directly to the browser when it's launched. This is a powerful way to enable or disable specific browser features not covered by other `BrowserConfig` options.
            *   **Common and useful flags:**
                *   `"--disable-gpu"`: Can resolve issues on systems without proper GPU drivers or in headless environments.
                *   `"--no-sandbox"`: Often required when running Chrome/Chromium inside Docker containers, especially as root.
                *   `"--disable-extensions"`: Prevents any installed browser extensions from interfering with the crawl.
                *   `"--disable-dev-shm-usage"`: Can prevent crashes in Docker due to limited shared memory.
            *   **Where to find lists of available browser arguments:** Search for "Chromium command line switches" or "Firefox command line options" for comprehensive lists.
            *   **Code Example:**
                ```python
                from crawl4ai import BrowserConfig

                performance_config = BrowserConfig(
                    light_mode=True, # Includes text_mode and other optimizations
                    extra_args=["--disable-blink-features=AutomationControlled"] # Example: Hiding automation flags
                )
                # Use this config with AsyncWebCrawler
                ```

*   2.3. **Best Practices for `BrowserConfig`**
    *   2.3.1. **Start simple, add complexity as needed:** Don't over-configure from the outset. Begin with defaults and only add customizations as specific needs or problems arise.
    *   2.3.2. **Prioritize realistic browser profiles for stealth:** If evading bot detection is a goal, ensure your `user_agent`, `browser_hint` (implicitly handled by `user_agent`), and other settings present a common and consistent browser profile.
    *   2.3.3. **Use persistent contexts for authenticated sessions:** Leverage `use_persistent_context=True` and `user_data_dir` for sites requiring login, to avoid re-authenticating on every run.
    *   2.3.4. **Be mindful of resource consumption:** Headful mode, multiple "dedicated" browser instances, and not using `light_mode` or `text_mode` can consume more resources. Optimize for your environment and scale.

*   2.4. **Troubleshooting Common `BrowserConfig` Issues**
    *   2.4.1. **Browser not launching or crashing:**
        *   Check Playwright installation: Run `playwright install` or `crawl4ai-setup`.
        *   Missing system dependencies: Especially on Linux, ensure all required libraries for the browser (e.g., Chromium dependencies) are installed. `crawl4ai-doctor` might help.
        *   `extra_args` conflicts: Some launch arguments might conflict or be invalid.
        *   Resource limits: Particularly in Docker or VMs, ensure sufficient CPU/memory. Consider `--disable-dev-shm-usage` if using Docker.
    *   2.4.2. **Pages not rendering correctly (potential `user_agent` or JS issues):**
        *   Try `headless=False` to visually inspect.
        *   Ensure `javascript_enabled=True` in `CrawlerRunConfig` (default) if the site relies heavily on JS.
        *   Experiment with different `user_agent` strings; some sites serve different content or block based on UA.
    *   2.4.3. **Proxy connection failures:**
        *   Verify proxy server address, port, username, and password.
        *   Test the proxy outside of Crawl4ai (e.g., with `curl` or in a browser) to ensure it's working.
        *   Check for firewall issues blocking connections to the proxy.
    *   2.4.4. **Debugging Tip: Always try `headless=False` first.** This is the single most useful step for diagnosing many browser-related issues, as it lets you see exactly what the browser is doing (or not doing).

## 3. Tailoring Crawls with `CrawlerRunConfig`: Precision in Every Operation

*   3.1. **The Purpose of `CrawlerRunConfig`: Granular Control per Crawl**
    *   3.1.1. **Why it's distinct from `BrowserConfig`:**
        While `BrowserConfig` sets up the *global environment* for the browser (how it launches, its identity, network settings), `CrawlerRunConfig` dictates the *specifics for a single `arun()` operation*. This separation is crucial because you might use the same browser instance (configured once with `BrowserConfig`) to crawl multiple URLs, each requiring different processing steps. For example, one URL might need a screenshot, another might need JavaScript execution, and a third might target a specific CSS selector for content extraction.

    *   3.1.2. **How it empowers you to customize each `arun()` or tasks within `arun_many()`:**
        By passing a `CrawlerRunConfig` object to `crawler.arun()` (or as part of the task definition in `crawler.arun_many()`), you gain fine-grained control over:
        *   What part of the page to focus on (`css_selector`, `target_elements`).
        *   What content to exclude (`excluded_tags`, `excluded_selector`).
        *   How content is extracted and transformed (`extraction_strategy`, `markdown_generator`).
        *   Page interactions (`js_code`, `wait_for`).
        *   Media capture (`screenshot`, `pdf`).
        *   Link and media filtering.
        *   Caching behavior for that specific URL.
        *   And much more.
        This allows for highly tailored and efficient crawling workflows.

*   3.2. **Strategies for Effective Content Extraction**
    *   3.2.1. **Scoping Your Extraction (`css_selector`, `target_elements`)**
        *   **`css_selector`:**
            *   **Impact:** This parameter is powerful. When set, Crawl4ai attempts to isolate the HTML content to *only the element(s) matching this CSS selector* **before** most other processing (like cleaning, Markdown generation, or structured extraction) occurs. This means the `cleaned_html` and subsequently the `markdown` output will be derived *only* from this selected portion.
            *   **Use Case:** You want to extract only the main article body from a news website, ignoring headers, footers, sidebars, and ads. Setting `css_selector=".article-content"` would achieve this.
            *   **Benefit:** Significantly reduces noise and focuses all downstream processing on the relevant content, which can improve the quality of Markdown and structured data, and also speed up LLM-based extractions by providing less context.
        *   **`target_elements`:**
            *   **How it differs:** Unlike `css_selector` which pre-filters the raw HTML, `target_elements` (a list of CSS selectors) primarily influences *downstream processing*, particularly Markdown generation and structured data extraction strategies like `JsonCssExtractionStrategy`. The initial `cleaned_html` (if `css_selector` is not also used) will still represent the broader page content. However, when generating Markdown or extracting structured fields, only the content within these `target_elements` will be considered.
            *   **Use Case:** You want to generate Markdown primarily from the main article body (`<article>`) but also need to extract the author's name from a `<div class="author-bio">` and the publication date from a `<time>` element, which might be outside the main article. You could set `target_elements=["article", ".author-bio", "time"]`.
            *   **Benefit:** Allows for more nuanced content selection for different purposes. You can get a broad `cleaned_html` (useful for general context) while focusing Markdown generation and specific data extraction on distinct parts of the page.
        *   **Decision Guide: `css_selector` for pre-filtering raw HTML vs. `target_elements` for post-cleaning focus.**
            *   Use `css_selector` when you are confident that *all* relevant information for *all* downstream tasks (Markdown, structured extraction, etc.) is contained within a single, selectable region of the page. This is the most aggressive filtering.
            *   Use `target_elements` when you need to generate Markdown or extract data from *multiple, potentially disparate sections* of the page, or when your `extraction_strategy` needs to "see" more of the page structure to correctly identify fields that might be outside the main content block.
            *   You *can* use them together: `css_selector` would first limit the HTML, and then `target_elements` would further refine which parts of that limited HTML are used for specific downstream tasks.
        *   **Code Example: Illustrating the difference in output.**
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, NoExtractionStrategy, DefaultMarkdownGenerator

            sample_html = """
            <html><body>
                <header><h1>Site Title</h1><nav><a>Home</a></nav></header>
                <main id='content'>
                    <article class='main-story'><h2>Article Heading</h2><p>Main article text.</p></article>
                    <aside class='sidebar'><p>Sidebar content.</p></aside>
                </main>
                <footer><p>Copyright info</p></footer>
            </body></html>
            """

            async def run_example():
                async with AsyncWebCrawler() as crawler:
                    # Scenario 1: Using css_selector
                    config_css = CrawlerRunConfig(css_selector="article.main-story")
                    result_css = await crawler.arun(url=f"raw://{sample_html}", config=config_css)
                    print(f"--- With css_selector='article.main-story' ---")
                    print(f"Cleaned HTML (snippet):\n{result_css.cleaned_html[:200]}\n") # Will be only the article
                    print(f"Markdown:\n{result_css.markdown.raw_markdown}\n")

                    # Scenario 2: Using target_elements
                    # Note: DefaultMarkdownGenerator implicitly uses target_elements if set.
                    # If no target_elements, it uses the whole cleaned_html (or content from css_selector if that's set).
                    config_target = CrawlerRunConfig(
                        target_elements=["article.main-story", "aside.sidebar"],
                        # To make the effect clear, let's use a custom Markdown generator
                        # that explicitly respects target_elements for its input.
                        # The default one would also work similarly.
                        markdown_generator=DefaultMarkdownGenerator()
                    )
                    result_target = await crawler.arun(url=f"raw://{sample_html}", config=config_target)
                    print(f"--- With target_elements=['article.main-story', 'aside.sidebar'] ---")
                    print(f"Cleaned HTML (snippet):\n{result_target.cleaned_html[:200]}\n") # Will be the whole page
                    print(f"Markdown (focused on targets):\n{result_target.markdown.raw_markdown}\n")
                    # The markdown here will primarily be from the article and sidebar combined.

                    # Scenario 3: Using both css_selector and target_elements
                    config_both = CrawlerRunConfig(
                        css_selector="main#content", # First, limit to main
                        target_elements=["article.main-story"] # Then, for markdown, only the article within main
                    )
                    result_both = await crawler.arun(url=f"raw://{sample_html}", config=config_both)
                    print(f"--- With css_selector='main#content' AND target_elements=['article.main-story'] ---")
                    print(f"Cleaned HTML (snippet):\n{result_both.cleaned_html[:200]}\n") # Will be main#content
                    print(f"Markdown (focused on article within main):\n{result_both.markdown.raw_markdown}\n")


            await run_example()
            ```

    *   3.2.2. **Refining Content by Exclusion (`excluded_tags`, `excluded_selector`)**
        *   **How `excluded_tags` globally removes unwanted tag types:** This parameter takes a list of HTML tag names (e.g., `['script', 'style', 'nav', 'footer', 'header', 'form', 'button', 'input', 'textarea', 'select', 'option']`). Before any other processing, Crawl4ai will remove all occurrences of these tags and their content from the HTML. This is a blunt but effective way to strip common non-content elements.
        *   **Using `excluded_selector` for more specific CSS-based exclusions:** If you need to remove elements based on their class, ID, or other attributes (e.g., ad banners with class `.ad-banner`, comment sections in `<div id="comments">`), provide a CSS selector string. All matching elements will be removed. This is more targeted than `excluded_tags`.
        *   **Impact on `cleaned_html` and subsequent Markdown/extraction:** Both `excluded_tags` and `excluded_selector` modify the HTML *before* it becomes the `cleaned_html` and before Markdown generation or structured data extraction. This means the excluded content will not appear in any downstream outputs.
        *   **Code Example: Removing navigation and footer before Markdown generation.**
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

            sample_html_nav_footer = """
            <html><body>
                <nav><a>Home</a> <a>About</a></nav>
                <article><p>Main content here.</p></article>
                <div class="advertisement"><p>Buy now!</p></div>
                <footer><p>&copy; 2024</p></footer>
            </body></html>
            """

            async def run_exclusion_example():
                config_exclusions = CrawlerRunConfig(
                    excluded_tags=['nav', 'footer'],
                    excluded_selector=".advertisement"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_html_nav_footer}", config=config_exclusions)
                    print("--- HTML after exclusions ---")
                    print(result.cleaned_html)
                    print("\n--- Markdown after exclusions ---")
                    print(result.markdown.raw_markdown)
            
            await run_exclusion_example()
            # Expected output will not contain <nav>, <footer>, or <div class="advertisement">
            ```

    *   3.2.3. **Choosing Your Extraction Toolkit (`extraction_strategy`, `chunking_strategy`, `markdown_generator`, `only_text`)**
        *   **The default pipeline:** If you don't specify these, Crawl4ai uses:
            *   `WebScrapingStrategy` (which handles basic HTML cleaning, link/media extraction).
            *   `DefaultMarkdownGenerator` (which converts the `cleaned_html` to Markdown).
            *   `NoExtractionStrategy` (meaning `result.extracted_content` will be `None`).
        *   **When to use `only_text=True`:** If your sole goal is to get a plain text representation of the page's main content, and you don't need Markdown, HTML structure, or structured data, setting `only_text=True` can be a quick and efficient option. It typically tries to extract the "body" text and may perform some basic cleaning. The result will be in `result.markdown.raw_markdown` (despite the name, it will be plain text).
        *   **Plugging in `LLMExtractionStrategy`:**
            *   **Why:** This strategy is powerful when:
                *   The data you want is not easily selectable with CSS or XPath (e.g., it's embedded in prose).
                *   The website structure is inconsistent across pages.
                *   You need to infer or transform data based on context.
            *   **Workflow:**
                1.  Define a Pydantic model representing the schema of the data you want to extract.
                2.  Instantiate an `LLMConfig` with your LLM provider details.
                3.  Instantiate `LLMExtractionStrategy(schema=YourPydanticModel.model_json_schema(), llm_config=your_llm_config, instruction="Your specific extraction instructions...")`.
                4.  Pass this strategy to `CrawlerRunConfig(extraction_strategy=your_llm_extraction_strategy)`.
                The extracted data will be available as a JSON string in `result.extracted_content`.
            *   (Cross-reference to `LLMConfig` section for LLM-specific settings like `provider`, `api_token`, `temperature`).
        *   **Custom `chunking_strategy`:**
            *   By default, `LLMExtractionStrategy` might send the entire relevant HTML (or Markdown, depending on its `input_format`) to the LLM. If this content is too large for the LLM's context window, you can provide a `chunking_strategy` (e.g., `RegexChunking`) to `LLMExtractionStrategy`. This strategy will break the input into smaller, manageable chunks before sending them to the LLM.
            *   When to use: For very long documents where you still want to apply LLM extraction across the entire content.
        *   **Custom `markdown_generator`:**
            *   If the `DefaultMarkdownGenerator` doesn't produce Markdown in the exact style or with the specific conversions you need, you can implement your own class inheriting from `MarkdownGenerationStrategy` and pass an instance to `CrawlerRunConfig(markdown_generator=YourCustomMarkdownGenerator())`.
        *   **Code Example: Using `CrawlerRunConfig` with `LLMExtractionStrategy` for structured data from an article.**
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, LLMConfig, 
                LLMExtractionStrategy, NoExtractionStrategy
            )
            from pydantic import BaseModel, Field
            import json
            import os

            # Define Pydantic schema for extraction
            class ArticleInfo(BaseModel):
                headline: str = Field(..., description="The main headline of the article")
                author: str = Field(None, description="The author of the article, if available")
                publication_date: str = Field(None, description="The publication date, if available")

            sample_article_html = """
            <html><body>
                <article>
                    <h1>Amazing Discovery in AI</h1>
                    <p class='byline'>By Dr. AI Expert on 2024-05-24</p>
                    <p>Scientists today announced a breakthrough...</p>
                </article>
            </body></html>
            """

            async def run_llm_extraction():
                # Configure LLM (using OpenAI for this example)
                # Ensure OPENAI_API_KEY is set in your environment
                llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))
                
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_conf,
                    schema=ArticleInfo.model_json_schema(),
                    instruction="Extract the headline, author, and publication date from the article content."
                )

                config_llm_extract = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    # LLMExtractionStrategy defaults to "markdown" input, so no need to change input_format
                    # unless you want to feed it raw HTML, then set extraction_strategy.input_format = "html"
                )

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_article_html}", config=config_llm_extract)
                    if result.success and result.extracted_content:
                        extracted_data = json.loads(result.extracted_content)
                        # LLMExtractionStrategy often returns a list of extracted items
                        if isinstance(extracted_data, list) and extracted_data:
                             article_info = ArticleInfo(**extracted_data[0]) # Assuming one main article
                             print(f"Headline: {article_info.headline}")
                             print(f"Author: {article_info.author}")
                             print(f"Date: {article_info.publication_date}")
                        elif isinstance(extracted_data, dict) : # Sometimes it might be a single object
                             article_info = ArticleInfo(**extracted_data)
                             print(f"Headline: {article_info.headline}")
                             print(f"Author: {article_info.author}")
                             print(f"Date: {article_info.publication_date}")
                    else:
                        print(f"Extraction failed or no content: {result.error_message}")
            
            # await run_llm_extraction() # Uncomment to run, requires OPENAI_API_KEY
            ```

    *   3.2.4. **Attribute Handling (`keep_data_attributes`, `keep_attrs`)**
        *   **Why `keep_data_attributes=True` can be useful:** HTML `data-*` attributes are often used by JavaScript frameworks to store state or custom metadata. By default, many cleaning processes might strip these. If this data is important for your extraction or understanding of the page, set `keep_data_attributes=True`.
        *   **Using `keep_attrs` to preserve specific essential attributes:** `keep_attrs` takes a list of attribute names (e.g., `['href', 'src', 'id', 'class', 'title']`). During the HTML cleaning process, only these specified attributes (and `data-*` attributes if `keep_data_attributes` is true) will be retained on tags. All other attributes will be removed. This helps in producing cleaner, more focused HTML for downstream tasks.
            *   Default important attributes like `href` for `<a>` tags and `src` for `<img>` tags are usually kept by the default scraping strategy (`WebScrapingStrategy`) logic, but `keep_attrs` provides explicit control.
            ```python
            from crawl4ai import CrawlerRunConfig

            # Keep only 'id' and 'data-custom' attributes
            config_attrs = CrawlerRunConfig(
                keep_attrs=['id'], 
                keep_data_attributes=True # This would keep 'data-custom'
            )
            # Example of how it affects cleaned_html:
            # HTML: <div id="main" class="container" data-custom="value" style="color:red">Content</div>
            # Cleaned (conceptual): <div id="main" data-custom="value">Content</div>
            ```

*   3.3. **Managing Page Dynamics and Interactions**
    *   3.3.1. **Interacting with Dynamic Pages (`js_code`, `wait_for`, `scan_full_page`, `scroll_delay`)**
        *   **`js_code`:**
            *   **Executing arbitrary JavaScript:** This is your primary tool for interacting with page elements like clicking buttons, filling forms, expanding sections, or triggering custom JavaScript functions defined on the page.
            *   **Single strings vs. lists of JS commands:**
                *   A single string: For a simple, one-off action.
                *   A list of strings: For a sequence of actions. Crawl4ai will execute them in order.
            *   **Code Example: Clicking a "Load More" button multiple times (conceptual).**
                ```python
                # Conceptual - actual selector depends on the target site
                js_load_more_multiple = [
                    "document.querySelector('.load-more-button').click();",
                    "await new Promise(r => setTimeout(r, 2000));", # Wait 2s for content
                    "document.querySelector('.load-more-button').click();",
                    "await new Promise(r => setTimeout(r, 2000));", # Wait again
                    "document.querySelector('.load-more-button').click();"
                ]
                config_load_more = CrawlerRunConfig(js_code=js_load_more_multiple)
                # result = await crawler.arun(url="some-infinite-scroll-page.com", config=config_load_more)
                ```
        *   **`wait_for`:**
            *   **Ensuring critical content is present:** Many dynamic pages load content asynchronously. `wait_for` tells Crawl4ai to pause and wait until a specific condition is met before proceeding with content extraction or further `js_code` execution.
            *   **CSS selectors vs. JS expressions:**
                *   `wait_for="css:.my-element"`: Waits until an element matching the CSS selector `.my-element` appears in the DOM.
                *   `wait_for="js:() => window.myAppDataLoaded === true"`: Waits until the provided JavaScript expression evaluates to `true`. This is powerful for waiting on custom application states.
            *   **Impact on reliability:** Using `wait_for` dramatically increases the reliability of crawls on dynamic sites by preventing premature content extraction before necessary elements are loaded.
            *   **Code Example: Waiting for a specific `div` with ID `#results-container` to appear.**
                ```python
                config_wait_for_div = CrawlerRunConfig(
                    js_code="document.querySelector('#search-button').click();", # Perform a search
                    wait_for="css:#results-container" # Wait for results to load
                )
                # result = await crawler.arun(url="search-page.com", config=config_wait_for_div)
                ```
        *   **`scan_full_page` and `scroll_delay`:**
            *   **How this combination helps:**
                *   `scan_full_page=True`: Instructs Crawl4ai to attempt to scroll through the entire page, from top to bottom. This is designed to trigger lazy-loaded images or content that only appears as the user scrolls.
                *   `scroll_delay` (float, seconds): Specifies the pause duration between each scroll step during `scan_full_page`. A small delay (e.g., 0.2 to 0.5 seconds) gives the browser time to load newly visible content.
            *   **Tuning `scroll_delay`:** If images or content are still missing, try increasing `scroll_delay`. If the page loads quickly, a smaller delay might suffice.

    *   3.3.2. **Controlling Time (`page_timeout`, `wait_for_timeout`, `delay_before_return_html`, `mean_delay`, `max_range`)**
        *   **`page_timeout` and `wait_for_timeout`:**
            *   `page_timeout` (milliseconds, default from `config.PAGE_TIMEOUT` e.g., 60000): The maximum time allowed for the initial page navigation (the `page.goto()` call) to complete.
            *   `wait_for_timeout` (milliseconds): If `wait_for` is specified, this is the maximum time to wait for that condition to be met. If not set, it often defaults to `page_timeout`.
            *   **Purpose:** These prevent your crawler from hanging indefinitely on slow-loading pages or if a `wait_for` condition is never satisfied.
        *   **`delay_before_return_html` (float, seconds, default 0.1):**
            *   Sometimes, even after a page signals "load" or a `wait_for` condition is met, there might be final JavaScript rendering updates. This parameter introduces a small, fixed delay just before the HTML content is grabbed, potentially capturing these last-moment changes.
        *   **`mean_delay` & `max_range` (for `arun_many`):**
            *   These parameters are primarily used by dispatchers like `MemoryAdaptiveDispatcher` when you call `crawler.arun_many()`.
            *   `mean_delay` (seconds, default 0.1): The average base delay between consecutive requests to the *same domain*.
            *   `max_range` (seconds, default 0.3): A random amount of additional delay (between 0 and `max_range`) is added to `mean_delay`.
            *   **Purpose:** This introduces jitter and helps in polite crawling, making your requests less predictable and reducing the load on the target server.

    *   3.3.3. **Handling Embedded Content (`process_iframes`)**
        *   **When to set `process_iframes=True`:** If the content you need to extract is located inside an `<iframe>` on the page, setting this to `True` will instruct Crawl4ai to attempt to locate, access, and extract content from within iframes.
        *   **Limitations and complexities:**
            *   **Cross-Origin Restrictions:** Browsers enforce security policies that can prevent access to the content of iframes from a different domain unless specific CORS headers are set.
            *   **Nested Iframes:** Deeply nested iframes can be challenging to navigate.
            *   **Performance:** Processing iframes adds overhead and can slow down crawls.
            *   Currently, Crawl4ai's default iframe processing is basic and might merge content. For highly specific iframe interactions, you might need custom `js_code` targeting the iframe's content document.

*   3.4. **Media and Link Management Strategies**
    *   3.4.1. **Capturing Visuals and Documents (`screenshot`, `pdf`, `capture_mhtml`)**
        *   **Use cases:**
            *   `screenshot=True`: Captures a PNG image of the viewport (or full page if configured). Useful for visual verification, archiving page appearance, or when image-based analysis is needed. Result in `result.screenshot` (base64 string).
            *   `pdf=True`: Generates a PDF representation of the page. Good for archiving articles or creating printable versions. Result in `result.pdf` (bytes).
            *   `capture_mhtml=True`: Saves the page as an MHTML (.mht) archive. This format bundles all page resources (HTML, CSS, images, JS) into a single file, allowing for offline viewing with near-perfect fidelity. Result in `result.mhtml` (string).
        *   **How `scan_full_page` and `wait_for_images` can improve capture quality:**
            *   `scan_full_page=True`: Ensures lazy-loaded content is visible before capture.
            *   `wait_for_images=True`: Attempts to wait for images to fully load before taking a screenshot or PDF, leading to more complete visuals.

    *   3.4.2. **Curating Media (`image_score_threshold`, `exclude_external_images`, `exclude_all_images`)**
        *   **`image_score_threshold` (int, default from `config.IMAGE_SCORE_THRESHOLD` e.g., 3):**
            *   Crawl4ai internally scores images based on heuristics (size, alt text, proximity to content). This threshold filters out images with scores below the specified value. Higher values mean more stringent filtering (fewer, more "important" images).
        *   **`exclude_external_images=True`:** If set, images hosted on domains different from the crawled page's domain will be excluded from `result.media["images"]`. Useful for focusing on first-party content.
        *   **`exclude_all_images=True`:** If you don't need any image data at all, setting this to `True` will skip all image processing and `result.media["images"]` will be empty. This can improve performance.

    *   3.4.3. **Managing Links (`exclude_external_links`, `exclude_social_media_links`, `exclude_domains`, `exclude_internal_links`)**
        *   **Strategies for cleaning up the `links` output:**
            *   `exclude_external_links=True`: Only internal links (links to the same base domain) will be included in `result.links["internal"]`. `result.links["external"]` will be empty.
            *   `exclude_social_media_links=True`: Removes links pointing to common social media domains (Facebook, Twitter, LinkedIn, etc., defined in `config.SOCIAL_MEDIA_DOMAINS`) from both internal and external link lists.
            *   `exclude_domains=['ads.example.com', 'tracker.net']`: Provide a list of specific domains. Any link pointing to these domains will be excluded.
            *   `exclude_internal_links=True`: Only external links will be included in `result.links["external"]`. `result.links["internal"]` will be empty. Useful if you're only interested in outgoing links.

*   3.5. **Caching and Session Persistence (`cache_mode`, `session_id`)**
    *   3.5.1. **`cache_mode`: Optimizing for Speed and Freshness**
        *   This enum (`from crawl4ai import CacheMode`) controls how Crawl4ai interacts with its local cache for a given `arun()` call.
        *   `CacheMode.ENABLED` (Default if not set explicitly, but `CrawlerRunConfig` defaults to `BYPASS` if no `cache_mode` is passed in `__init__`):
            *   Reads from cache if a fresh entry exists for the URL.
            *   If not, fetches from the network and writes the result to the cache.
            *   **Use When:** Good for development to iterate quickly on parsing/extraction logic without re-fetching, or for crawling relatively static content.
        *   `CacheMode.BYPASS`:
            *   Ignores the cache completely. Always fetches the URL from the network.
            *   Does *not* write the result to the cache.
            *   **Use When:** You always need the absolute latest version of a page, or when debugging fetching/rendering issues.
        *   `CacheMode.READ_ONLY`:
            *   Only reads from the cache if an entry exists.
            *   Does *not* fetch from the network if the URL is not in the cache.
            *   Does *not* write to the cache.
            *   **Use When:** You want to run your processing logic strictly against a pre-existing cached dataset without making any network requests.
        *   `CacheMode.WRITE_ONLY`:
            *   Always fetches the URL from the network.
            *   Always writes (or overwrites) the result to the cache.
            *   Does *not* read from the cache before fetching.
            *   **Use When:** You want to populate or refresh your cache with the latest content.
        *   `CacheMode.DISABLED`:
            *   Completely disables any interaction with the cache system for this run. No reads, no writes.
            *   This is stronger than `BYPASS` as `BYPASS` might still involve some cache system overhead (e.g., checking if it should bypass).
            *   **Use When:** You want to ensure the cache system is not touched at all, perhaps for performance testing of raw fetching.
        *   **Decision Guide: Choosing the right cache mode.**
            *   **Development/Iteration:** `ENABLED` (to speed up repeated runs while changing extraction logic).
            *   **Production (Dynamic Content):** `BYPASS` or `ENABLED` with appropriate cache expiry (not directly settable via `CacheMode` but by cache implementation).
            *   **Production (Static/Archival Content):** `ENABLED` or `WRITE_ONLY` (for initial population) followed by `READ_ONLY` or `ENABLED`.
            *   **Testing against fixed data:** `READ_ONLY`.
            *   **Cache warming:** `WRITE_ONLY`.

    *   3.5.2. **`session_id`: Orchestrating Multi-Step Crawls**
        *   **How `session_id` allows sequential `arun()` calls to reuse the same browser page and context:**
            When you provide the same `session_id` string to multiple `arun()` calls within the *same* `AsyncWebCrawler` instance, Crawl4ai will reuse the existing browser page and its context (cookies, local storage, current URL, DOM state) for those calls, instead of opening a new page/tab for each.
        *   **Workflow: Simulating a login and subsequent data fetch.**
            1.  **First `arun()` (Establish Session & Login):**
                ```python
                # login_config = CrawlerRunConfig(
                #     url="https://example.com/login",
                #     session_id="my_secure_session",
                #     js_code=[
                #         "document.querySelector('#username').value = 'user';",
                #         "document.querySelector('#password').value = 'pass';",
                #         "document.querySelector('button[type=submit]').click();"
                #     ],
                #     wait_for="css:.user-dashboard" # Wait for a post-login element
                # )
                # login_result = await crawler.arun(config=login_config)
                ```
            2.  **Second `arun()` (Access Protected Page - using same `session_id`):**
                ```python
                # dashboard_config = CrawlerRunConfig(
                #     url="https://example.com/dashboard", # Navigate to a new page in the same session
                #     session_id="my_secure_session", # Crucial: same session_id
                #     # No js_code needed if already logged in, or add JS for dashboard interactions
                # )
                # dashboard_result = await crawler.arun(config=dashboard_config)
                ```
            3.  **Third `arun()` (Perform further actions - using same `session_id` and `js_only=True`):**
                If you just want to execute more JavaScript on the *current page* of the session without navigating:
                ```python
                # click_button_config = CrawlerRunConfig(
                #     session_id="my_secure_session",
                #     js_code="document.querySelector('#load-user-data-button').click();",
                #     wait_for="css:.user-data-loaded",
                #     js_only=True # Tells Crawl4ai not to navigate, just run JS on the current page
                # )
                # data_result = await crawler.arun(config=click_button_config)
                ```
        *   **Important: `js_only=True`**
            *   When `js_only=True` is set in `CrawlerRunConfig`, Crawl4ai will *not* perform a `page.goto(url)` operation. Instead, it will execute the provided `js_code` (if any) on the *current page* associated with the `session_id`.
            *   The `url` parameter in `CrawlerRunConfig` is effectively ignored when `js_only=True`.
            *   This is very useful for multi-step interactions on the same page (e.g., clicking multiple "load more" buttons, filling out different parts of a form sequentially).
        * **Cleaning Up:** Remember to kill the session when done to free up browser resources:
            ```python
            # await crawler.kill_session("my_secure_session")
            ```

*   3.6. **Best Practices for `CrawlerRunConfig`**
    *   3.6.1. **Test selectors and JS snippets in your browser's developer console first:** This saves a lot of time and helps ensure your selectors are correct and your JS code behaves as expected before integrating it into Crawl4ai.
    *   3.6.2. **Start with broader selectors and refine if necessary:** It's often easier to start with a more general `css_selector` or `target_elements` and then narrow it down if you're getting too much noise, rather than starting too specific and missing content.
    *   3.6.3. **Use `cache_mode=CacheMode.BYPASS` when testing changes** to selectors, JS code, or extraction strategies to ensure you're always working with fresh page content.
    *   3.6.4. **Combine `js_code` with appropriate `wait_for` conditions for reliability:** Don't assume JS actions complete instantly. Always wait for a clear indicator (an element appearing, a JS variable changing) that the action has had its desired effect.

*   3.7. **Troubleshooting Common `CrawlerRunConfig` Issues**
    *   3.7.1. **Content not being extracted as expected:**
        *   **Selector Issues:** Double-check your `css_selector` or selectors within your `extraction_strategy`. Test them in the browser devtools.
        *   **Dynamic Content Not Loaded:** The content might be loaded by JavaScript after the initial page load. Use `wait_for`, `js_code` to trigger loading, or `scan_full_page`. Try with `headless=False` in `BrowserConfig` to see what the browser is actually rendering.
    *   3.7.2. **Timeouts:**
        *   **Page taking too long:** Increase `page_timeout`.
        *   **`wait_for` condition never met:** Your selector might be wrong, the JS condition might never become true, or the element simply doesn't appear within the `wait_for_timeout`. Debug with `headless=False`.
    *   3.7.3. **JavaScript errors:**
        *   Set `log_console=True` in `BrowserConfig` (or the `arun` call directly if supported) to see browser console messages, which can reveal JS errors.
        *   Test your `js_code` snippets in the browser console.
    *   3.7.4. **`extraction_strategy` not yielding desired output:**
        *   **For `JsonCssExtractionStrategy`:** Verify your schema selectors.
        *   **For `LLMExtractionStrategy`:** Refine your Pydantic schema, improve your `instruction`, adjust `LLMConfig` parameters (like `temperature`), or provide better/more context if using `chunking_strategy`. Ensure the `input_format` for the strategy ("markdown" or "html") matches the type of content that will yield the best results from the LLM.

## 4. Configuring LLM Interactions with `LLMConfig`

*   4.1. **Purpose: Centralized LLM Settings**
    *   4.1.1. **Why `LLMConfig` is essential when using `LLMExtractionStrategy`, `LLMContentFilter`, or other LLM-powered components.**
        When your crawling workflow involves interacting with Large Language Models (e.g., for extracting structured data from unstructured text using `LLMExtractionStrategy`, or for filtering relevant content using `LLMContentFilter`), `LLMConfig` provides a dedicated and centralized place to manage all settings related to these interactions. This includes specifying which LLM provider and model to use, API keys, and parameters that control the LLM's generation behavior (like temperature, max tokens, etc.).

    *   4.1.2. **How it promotes consistency in LLM calls.**
        By encapsulating LLM settings in a separate object, you ensure that:
        *   All LLM-powered components in your Crawl4ai setup can share the same configuration if desired, leading to consistent behavior.
        *   You can easily switch LLM providers or models by changing the `LLMConfig` in one place, without needing to modify every strategy that uses an LLM.
        *   LLM-specific details are kept separate from the core browser and crawl run configurations, improving code organization.

*   4.2. **Core `LLMConfig` Parameters and Their Impact**
    *   4.2.1. **Provider Setup (`provider`, `api_token`, `base_url`)**
        *   **Choosing the right `provider` (e.g., "openai/gpt-4o-mini", "ollama/llama3", "groq/llama3-70b-8192"):**
            *   Crawl4ai leverages the [LiteLLM](https://litellm.ai/) library, which supports a vast range of LLM providers (OpenAI, Azure OpenAI, Anthropic, Cohere, Google Gemini, Ollama, Groq, and many more). The `provider` string typically follows the format `"provider_name/model_name"`.
            *   **Considerations for choosing a provider/model:**
                *   **Cost:** Different models and providers have varying pricing structures.
                *   **Model Capabilities:** Some models excel at specific tasks (e.g., instruction following, summarization, code generation).
                *   **Context Window Size:** The maximum amount of text the model can process at once.
                *   **Speed/Latency:** How quickly the model responds.
                *   **Availability & Rate Limits:** Ensure the provider can handle your expected load.
                *   **Open vs. Closed Source:** Ollama allows running open-source models locally, while others are API-based.
        *   **`api_token`: How to securely provide API keys (direct string vs. `env:YOUR_ENV_VAR`).**
            *   **Direct String:** You can pass the API key directly: `api_token="sk-..."`. **Not recommended for production code.**
            *   **Environment Variable (Recommended):** Use the `env:` prefix to tell Crawl4ai to read the key from an environment variable: `api_token="env:OPENAI_API_KEY"`. This is much more secure as it keeps secrets out of your codebase. Crawl4ai automatically looks for common environment variables like `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GEMINI_API_KEY`, etc., based on the `provider` if `api_token` is not explicitly set.
        *   **`base_url`: When to use this for self-hosted models (like local Ollama) or custom API gateways.**
            *   If you are running an LLM locally (e.g., using Ollama, which defaults to `http://localhost:11434`), or if you are routing API calls through a custom gateway or proxy, you'll need to set the `base_url` to point to the correct endpoint.
            *   For many cloud providers, LiteLLM knows the default `base_url`, so you often don't need to set it.
        *   **Code Example: Configuring for OpenAI vs. a local Ollama instance.**
            ```python
            from crawl4ai import LLMConfig
            import os

            # OpenAI Configuration (assumes OPENAI_API_KEY is set in environment)
            openai_config = LLMConfig(
                provider="openai/gpt-4o-mini",
                # api_token=os.getenv("OPENAI_API_KEY") # Or let Crawl4ai find it
            )
            print(f"OpenAI Provider: {openai_config.provider}")

            # Local Ollama Configuration (Llama3 running via Ollama)
            ollama_config = LLMConfig(
                provider="ollama/llama3", 
                base_url="http://localhost:11434", # Default Ollama endpoint
                api_token="ollama" # Standard token for Ollama if no specific auth
            )
            print(f"Ollama Provider: {ollama_config.provider}, Base URL: {ollama_config.base_url}")
            
            # Groq Configuration (Llama3-70b via Groq, fast inference)
            groq_config = LLMConfig(
                provider="groq/llama3-70b-8192",
                api_token=os.getenv("GROQ_API_KEY") # Needs GROQ_API_KEY env var
            )
            print(f"Groq Provider: {groq_config.provider}")
            ```

    *   4.2.2. **Fine-tuning LLM Generation (`temperature`, `max_tokens`, `top_p`, etc.)**
        These parameters control the behavior of the LLM when it generates text.
        *   **`temperature` (float, typically 0.0 to 2.0):**
            *   Controls the randomness of the output.
            *   Lower values (e.g., 0.0 - 0.3): More deterministic, focused, and factual. Good for precise data extraction or when you want predictable output based on a strict schema.
            *   Higher values (e.g., 0.7 - 1.0+): More creative, diverse, and potentially surprising. Better for tasks like summarization, brainstorming, or generating varied text.
        *   **`max_tokens` (int):**
            *   The maximum number of tokens (words/sub-words) the LLM should generate in its response.
            *   Crucial for managing costs (as most APIs charge per token) and ensuring the output doesn't become excessively long.
            *   Set it based on the expected length of your desired output (e.g., for a short summary vs. a detailed extraction).
        *   **`top_p` (float, typically 0.0 to 1.0):**
            *   An alternative to `temperature` for controlling randomness, known as nucleus sampling. The model considers only the tokens whose cumulative probability mass exceeds `top_p`.
            *   A common value is 0.9. Lower values make the output more focused.
            *   Usually, you'd use either `temperature` or `top_p`, not both simultaneously (or set one to its neutral default, e.g., `top_p=1.0` if using `temperature`).
        *   **Other parameters (`frequency_penalty`, `presence_penalty`, `stop`, `n`):**
            *   `frequency_penalty` (float): Penalizes tokens that have already appeared frequently, encouraging the model to use different words.
            *   `presence_penalty` (float): Penalizes tokens that have appeared at all, encouraging novelty.
            *   `stop` (string or list of strings): Sequences where the API will stop generating further tokens.
            *   `n` (int): How many completions to generate for each prompt.
            *   **When to use:** These are more advanced and used for specific fine-tuning, e.g., reducing repetition or generating multiple candidate outputs. Consult your LLM provider's documentation for details on how they interpret these.
        *   **Use Case: Adjusting parameters for extracting a strict JSON schema vs. generating a summary.**
            *   **Strict JSON Schema Extraction:** `temperature=0.1`, `top_p=1.0` (or not set), `max_tokens` appropriate for the schema size.
            *   **Creative Summary Generation:** `temperature=0.7`, `top_p=0.9`, `max_tokens` set to desired summary length.

*   4.3. **Workflow: Integrating `LLMConfig` in Your Crawl**
    *   4.3.1. **Step 1: Instantiate `LLMConfig` with your desired settings.**
        ```python
        from crawl4ai import LLMConfig
        import os
        
        my_llm_config = LLMConfig(
            provider="openai/gpt-4o-mini",
            api_token=os.getenv("OPENAI_API_KEY"),
            temperature=0.2,
            max_tokens=1024
        )
        ```
    *   4.3.2. **Step 2: Pass the `LLMConfig` instance to an LLM-dependent strategy.**
        For example, if using `LLMExtractionStrategy`:
        ```python
        from crawl4ai.extraction_strategy import LLMExtractionStrategy
        from pydantic import BaseModel

        class MyData(BaseModel):
            name: str
            value: int

        llm_extraction_strategy = LLMExtractionStrategy(
            llm_config=my_llm_config,
            schema=MyData.model_json_schema(),
            instruction="Extract name and value."
        )
        ```
    *   4.3.3. **Step 3: Include that strategy in your `CrawlerRunConfig`.**
        ```python
        from crawl4ai import CrawlerRunConfig

        my_run_config = CrawlerRunConfig(
            extraction_strategy=llm_extraction_strategy
            # ... other run config settings
        )
        ```
    *   **Code Example: A complete flow showing `LLMConfig` -> `LLMExtractionStrategy` -> `CrawlerRunConfig` -> `arun()`.**
        ```python
        from crawl4ai import AsyncWebCrawler, LLMConfig, LLMExtractionStrategy, CrawlerRunConfig
        from pydantic import BaseModel, Field
        import json
        import os

        class Product(BaseModel):
            product_name: str = Field(description="The name of the product")
            price: float = Field(description="The price of the product")

        sample_product_page_html = """
        <html><body>
            <div class='product-details'>
                <h2>Awesome Gadget X1000</h2>
                <p class='price-tag'>Price: $99.99</p>
                <p>This gadget does amazing things...</p>
            </div>
        </body></html>
        """

        async def run_full_llm_flow():
            # 1. LLMConfig
            llm_conf = LLMConfig(
                provider="openai/gpt-4o-mini", 
                api_token=os.getenv("OPENAI_API_KEY"), # Ensure this is set
                temperature=0.1
            )

            # 2. LLMExtractionStrategy
            product_extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_conf,
                schema=Product.model_json_schema(),
                instruction="From the provided HTML, extract the product name and its price."
            )

            # 3. CrawlerRunConfig
            product_run_config = CrawlerRunConfig(
                extraction_strategy=product_extraction_strategy,
                # LLMExtractionStrategy expects HTML input by default if input_format is not changed
                input_format="html" # Explicitly telling the strategy to use HTML
            )

            # 4. AsyncWebCrawler and arun()
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(
                    url=f"raw://{sample_product_page_html}", 
                    config=product_run_config
                )

                if result.success and result.extracted_content:
                    try:
                        extracted_data_list = json.loads(result.extracted_content)
                        if extracted_data_list: # LLMExtractionStrategy often returns a list
                            product_info = Product(**extracted_data_list[0])
                            print(f"Product: {product_info.product_name}, Price: ${product_info.price}")
                        else:
                            print("LLM returned no data.")
                    except json.JSONDecodeError:
                        print(f"Failed to parse LLM JSON output: {result.extracted_content}")
                    except Exception as e:
                        print(f"Error processing extracted data: {e}")
                else:
                    print(f"Crawl or extraction failed: {result.error_message}")
        
        # if os.getenv("OPENAI_API_KEY"):
        #     await run_full_llm_flow()
        # else:
        #     print("OPENAI_API_KEY not set. Skipping LLMConfig example.")
        ```

*   4.4. **Best Practices for `LLMConfig`**
    *   4.4.1. **Use environment variables for API keys:** Never hardcode API keys in your scripts. Use `api_token="env:YOUR_KEY_NAME"`.
    *   4.4.2. **Start with conservative `max_tokens`:** This helps manage costs, especially during testing. Increase it only if necessary for the desired output length.
    *   4.4.3. **Test prompts and parameters iteratively:** LLM behavior can be sensitive to prompting and parameters. Start with simple prompts and gradually refine them. Test with low `temperature` for predictability first.
    *   4.4.4. **Be aware of rate limits:** Different LLM providers have different rate limits. If you're making many calls, implement appropriate delays or use a queueing system to avoid hitting these limits. Crawl4ai's built-in backoff in `perform_completion_with_backoff` helps, but sustained high volume might still be an issue.

*   4.5. **Troubleshooting `LLMConfig` and LLM Interactions**
    *   4.5.1. **Authentication errors (invalid API key, incorrect provider string):**
        *   Double-check your `api_token` and ensure the environment variable is correctly set and accessible.
        *   Verify the `provider` string matches one supported by LiteLLM and that you have the necessary access/credits for that provider.
        *   If using `base_url`, ensure it's correct and the local LLM server (like Ollama) is running.
    *   4.5.2. **LLM not following instructions or schema (if `extraction_type="schema"`):**
        *   **Prompt Engineering:** This is key. Your `instruction` needs to be very clear, specific, and unambiguous. Provide examples within the prompt if necessary.
        *   **Parameter Tuning:** Adjust `temperature`. For schema extraction, very low (e.g., 0.0 or 0.1) is usually best.
        *   **Model Choice:** Some models are better at instruction-following or JSON generation than others. Experiment if one model isn't working.
        *   **Schema Complexity:** If your Pydantic schema is very complex, the LLM might struggle. Try simplifying it or breaking down the extraction into multiple steps/prompts.
        *   **Input Content:** Ensure the `input_format` for your `LLMExtractionStrategy` ("markdown" or "html") provides the LLM with the most useful version of the content. Sometimes, clean Markdown is better; other times, the raw HTML structure helps.
    *   4.5.3. **Rate limit errors from the LLM provider:**
        *   The `perform_completion_with_backoff` utility in Crawl4ai attempts to handle transient rate limits with exponential backoff.
        *   If you consistently hit rate limits, you may need to reduce the concurrency of your LLM calls (e.g., process fewer chunks in parallel) or request a higher rate limit from your provider.
    *   4.5.4. **Unexpectedly high costs (monitor token usage):**
        *   Keep `max_tokens` as low as feasible for your task.
        *   Be mindful of input token count, especially if using `LLMExtractionStrategy` on large chunks of text. Optimize `chunk_size` in your `chunking_strategy`.
        *   Monitor your LLM provider's billing dashboard regularly.

## 5. Specialized Configuration Objects: `GeolocationConfig`, `ProxyConfig`, `HTTPCrawlerConfig`

These objects provide targeted configuration for specific advanced crawling needs.

*   5.1. **Simulating Location with `GeolocationConfig`**
    *   5.1.1. **Purpose: Why you might need to make the browser appear from a specific geographic location.**
        Websites can serve different content, prices, or even different site versions based on the user's perceived geographic location (often determined by IP address, but also potentially by browser geolocation APIs). `GeolocationConfig` allows you to override the browser's reported GPS coordinates.
    *   5.1.2. **Use Cases:**
        *   **Accessing Geo-Restricted Websites or Content:** Some sites block access or show limited content to users outside specific regions.
        *   **Testing Localization and Internationalization:** Verify that your website correctly displays language, currency, and content for different locales.
        *   **Scraping Geo-Specific Data:** Collect data that varies by location, like local search results, store availability, or regional pricing.
    *   5.1.3. **How to use:**
        1.  Instantiate `GeolocationConfig` with the desired `latitude`, `longitude`, and optionally `accuracy` (in meters).
            ```python
            from crawl4ai.async_configs import GeolocationConfig
            paris_location = GeolocationConfig(latitude=48.8566, longitude=2.3522, accuracy=50.0)
            ```
        2.  Pass this object to the `geolocation` parameter of `CrawlerRunConfig`.
            ```python
            from crawl4ai import CrawlerRunConfig
            run_config_paris = CrawlerRunConfig(geolocation=paris_location)
            ```
        *   **Important Note:** For `GeolocationConfig` to be truly effective in making a website *believe* you are in that location, you usually also need to route your traffic through a **proxy server located in that same geographic region**. Setting GPS coordinates alone might not be enough if your IP address still points to your actual location.
    *   5.1.4. **Interaction with browser permissions (Playwright handles this implicitly when geolocation is set).**
        When you set geolocation via Playwright (which Crawl4ai uses under the hood), it typically also grants the necessary browser permission for the page to access this spoofed location information, mimicking a user clicking "Allow" on a location access prompt.
    *   **Code Example: Crawling a site as if from Paris, France (assuming a Paris proxy is also configured in `BrowserConfig`).**
        ```python
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, GeolocationConfig

        async def crawl_from_paris():
            # Assume proxy_for_paris is configured in BrowserConfig
            # For this example, we'll just show GeolocationConfig
            paris_browser_config = BrowserConfig(
                # proxy_config={"server": "http://paris-proxy.example.com:8080"} # Illustrative
            )
            
            paris_location = GeolocationConfig(latitude=48.8566, longitude=2.3522, accuracy=100.0)
            
            # Also good to set locale and timezone to match
            paris_run_config = CrawlerRunConfig(
                geolocation=paris_location,
                locale="fr-FR",
                timezone_id="Europe/Paris"
            )

            async with AsyncWebCrawler(config=paris_browser_config) as crawler:
                # A site that shows location-based info
                result = await crawler.arun(url="https://www.iplocation.net/", config=paris_run_config)
                if result.success:
                    print("--- Page content (should reflect Paris if proxy and geo are working) ---")
                    print(result.markdown.raw_markdown[:500]) 
                else:
                    print(f"Crawl failed: {result.error_message}")
        
        # await crawl_from_paris()
        ```

*   5.2. **Detailed Proxy Setup with `ProxyConfig`**
    *   5.2.1. **When to use `ProxyConfig` object vs. the simpler `proxy` string in `BrowserConfig`.**
        *   The `proxy` parameter directly in `BrowserConfig` (e.g., `BrowserConfig(proxy="http://user:pass@host:port")`) is a simpler way to set a proxy for Playwright, but it's a Playwright-level string.
        *   The `proxy_config` parameter in `BrowserConfig` expects a dictionary like `{"server": "...", "username": "...", ...}` which Playwright also accepts.
        *   The `crawl4ai.async_configs.ProxyConfig` object is a Pydantic model that helps structure these details, especially useful if you are:
            *   Programmatically constructing proxy configurations.
            *   Building a custom `ProxyRotationStrategy` that needs to manage a list of `ProxyConfig` objects.
            *   Needing to store or pass around proxy details in a typed way.
            *   It also includes an `ip` field, which can be useful for internal tracking or verification, though it's not directly used by Playwright's connection mechanism.
        When passing to `BrowserConfig(proxy_config=...)`, you'd typically use `my_proxy_config_object.to_dict()`.
    *   5.2.2. **Key parameters of `ProxyConfig` object: `server`, `username`, `password`, `ip`.**
        *   `server` (str): The proxy server URL (e.g., `"http://127.0.0.1:8080"`, `"socks5://myproxy.com:1080"`).
        *   `username` (Optional[str]): Username for proxy authentication.
        *   `password` (Optional[str]): Password for proxy authentication.
        *   `ip` (Optional[str]): The IP address of the proxy. This is more for your internal tracking or if your proxy provider gives you an outbound IP to verify against; Playwright itself primarily uses the `server` field for connection.
    *   5.2.3. **How `ProxyConfig` instances are typically managed by a `ProxyRotationStrategy`.**
        If you're using a `ProxyRotationStrategy` (detailed in its own documentation section), that strategy would typically hold a list of `ProxyConfig` objects. Its `get_next_proxy()` method would return one of these `ProxyConfig` objects, which would then be used to configure the `proxy_config` (via its dictionary representation) for a `BrowserConfig` or directly within a `CrawlerRunConfig` if the strategy involves per-run proxy changes.
    *   **Code Example: Creating `ProxyConfig` objects.**
        ```python
        from crawl4ai.async_configs import ProxyConfig, BrowserConfig

        # Create ProxyConfig objects
        proxy1 = ProxyConfig(
            server="http://proxy1.example.com:8000", 
            username="user1", 
            password="password1",
            ip="1.2.3.4" # For your reference
        )
        proxy2 = ProxyConfig(
            server="socks5://proxy2.example.com:1080",
            ip="5.6.7.8"
        )

        print(f"Proxy 1 Server: {proxy1.server}")
        
        # To use with BrowserConfig:
        # browser_cfg = BrowserConfig(proxy_config=proxy1.to_dict())
        # Or if you have a list and a rotation strategy:
        # rotation_strategy = RoundRobinProxyStrategy(proxies=[proxy1, proxy2])
        # next_proxy_obj = await rotation_strategy.get_next_proxy()
        # if next_proxy_obj:
        #     browser_cfg = BrowserConfig(proxy_config=next_proxy_obj.to_dict())
        ```

*   5.3. **Lightweight Crawling with `HTTPCrawlerConfig`**
    *   5.3.1. **Understanding the `AsyncHTTPCrawlerStrategy`:**
        *   **When it's a better choice:** The default `AsyncPlaywrightCrawlerStrategy` uses a full browser (Playwright), which is powerful but resource-intensive. For tasks that don't require JavaScript execution, complex DOM interactions, or browser rendering, the `AsyncHTTPCrawlerStrategy` is a much lighter and faster alternative. It makes direct HTTP requests using the `requests` library (via `httpx` for async).
        *   Ideal for:
            *   Scraping static HTML sites.
            *   Accessing APIs that return JSON, XML, or other text-based data.
            *   Downloading files directly.
        *   **Trade-offs:**
            *   Cannot execute JavaScript. Content rendered by client-side JS will be missed.
            *   No DOM interaction capabilities (like clicking buttons).
            *   Doesn't handle complex browser features like cookies or sessions automatically in the same way Playwright does (though you can manage headers manually).
    *   5.3.2. **Purpose of `HTTPCrawlerConfig`: Tailoring direct HTTP requests.**
        When you use `AsyncHTTPCrawlerStrategy`, the `HTTPCrawlerConfig` object allows you to specify details for the HTTP request itself, such as the method, headers, and body data.
    *   5.3.3. **Key Parameters of `HTTPCrawlerConfig`:**
        *   `method` (str, default "GET"): The HTTP method (e.g., "GET", "POST", "PUT", "DELETE").
        *   `headers` (Optional[Dict[str, str]]): Custom HTTP headers to send with the request.
        *   `data` (Optional[Dict[str, Any]]): Dictionary of data to be form-urlencoded and sent in the request body (typically for "POST" requests with `Content-Type: application/x-www-form-urlencoded`).
        *   `json` (Optional[Dict[str, Any]]): Dictionary of data to be JSON-encoded and sent in the request body (typically for "POST" or "PUT" requests with `Content-Type: application/json`).
        *   `follow_redirects` (bool, default True): Whether `httpx` should automatically follow HTTP redirects (3xx status codes).
        *   `verify_ssl` (bool, default True): Whether to verify SSL certificates. Set to `False` with caution, similar to `ignore_https_errors` in `BrowserConfig`.
    *   5.3.4. **Workflow:**
        1.  Instantiate `AsyncHTTPCrawlerStrategy`.
            ```python
            from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            ```
        2.  Create an `AsyncWebCrawler` instance, passing this strategy.
            ```python
            # crawler = AsyncWebCrawler(crawler_strategy=http_strategy)
            ```
        3.  When calling `crawler.arun()`, if you need to customize the HTTP request (e.g., for a POST), create an `HTTPCrawlerConfig` and pass it via `CrawlerRunConfig`.
            ```python
            from crawl4ai.async_configs import HTTPCrawlerConfig, CrawlerRunConfig
            
            # http_post_config = HTTPCrawlerConfig(
            #     method="POST",
            #     json={"key": "value"},
            #     headers={"X-Custom-Header": "MyValue"}
            # )
            # run_config_http = CrawlerRunConfig(
            #     # Note: When using AsyncHTTPCrawlerStrategy, its specific config
            #     # is often passed directly to arun or its strategy methods,
            #     # rather than through CrawlerRunConfig's generic 'experimental' field.
            #     # However, let's assume for consistency or future enhancement
            #     # it could be passed like this:
            #     experimental={"http_crawler_config": http_post_config.to_dict()}
            # )
            # For current direct use with arun():
            # result = await crawler.arun(
            #     url="https://api.example.com/submit",
            #     method="POST", # Pass directly to arun when using AsyncHTTPCrawlerStrategy
            #     json_data={"key": "value"}, # Pass directly
            #     headers={"X-Custom-Header": "MyValue"} # Pass directly
            # )
            ```
            **Correction/Clarification:** `AsyncHTTPCrawlerStrategy.crawl()` directly accepts `method`, `headers`, `data`, `json_data`, etc. as keyword arguments. `HTTPCrawlerConfig` is more of a Pydantic model to structure these, but they are passed directly to `arun` when the active strategy is `AsyncHTTPCrawlerStrategy`. `CrawlerRunConfig` is less relevant for these HTTP-specific parameters when *not* using a browser-based strategy.

    *   **Code Example: Fetching data from a JSON API using `AsyncHTTPCrawlerStrategy`.**
        ```python
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
        from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
        import json

        async def fetch_json_api():
            # 1. Use AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            
            # 2. Create Crawler with this strategy
            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                # 3. Call arun, passing HTTP-specific params directly
                result = await crawler.arun(
                    url="https://jsonplaceholder.typicode.com/todos/1",
                    method="GET" # Default, but explicit here
                )

                if result.success:
                    print(f"Status Code: {result.status_code}")
                    try:
                        # The 'html' field will contain the raw response body
                        todo_data = json.loads(result.html) 
                        print("Fetched TODO Data:")
                        print(todo_data)
                    except json.JSONDecodeError:
                        print(f"Failed to parse JSON response: {result.html[:200]}")
                else:
                    print(f"API call failed: {result.error_message}")

        # await fetch_json_api()
        ```

## 6. Efficiently Managing Configurations: `clone()`, `dump()`, and `load()`

*   6.1. **The Rationale: Why Manage Configurations Programmatically?**
    Manually creating and managing numerous configuration objects with slight variations can quickly become tedious, error-prone, and lead to code duplication. Crawl4ai provides `clone()`, `dump()`, and `load()` methods on its configuration objects (`BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, etc.) to address these challenges. Programmatic management offers:
    *   **Reduced Repetition:** Define base configurations once and create variations easily.
    *   **Modularity and Reusability:** Store and load common configurations, promoting a "don't repeat yourself" (DRY) approach.
    *   **Persistence:** Save configurations to files (JSON, YAML) for later use, version control, or sharing across different scripts or team members.
    *   **Dynamic Configuration:** Load or modify configurations at runtime based on external inputs or application logic.
    *   **Improved Readability:** Complex setups can be broken down into smaller, named configurations, making the overall code easier to understand.

*   6.2. **`clone(**kwargs)`: Creating Variations with Ease**
    *   6.2.1. **How it works:** The `clone()` method, available on configuration objects like `BrowserConfig` and `CrawlerRunConfig`, performs a *deep copy* of the original configuration object. You can then pass keyword arguments to `clone()` to override specific attributes in the newly created copy. The original object remains unchanged.
    *   6.2.2. **Use Cases:**
        *   **Creating slightly different `CrawlerRunConfig` objects:**
            *   For different sections of a website (e.g., product pages vs. blog posts) that share most crawl settings but require different `extraction_strategy` or `css_selector`.
            *   For A/B testing different `wait_for` conditions or `js_code` snippets.
        *   **Generating multiple `BrowserConfig` instances:**
            *   For testing with different user agents, proxy settings, or headless modes while keeping other browser settings consistent.
    *   6.2.3. **Code Example:**
        ```python
        from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode
        # from crawl4ai.extraction_strategy import SomeExtractionStrategy # Placeholder

        # --- BrowserConfig Cloning ---
        base_browser_config = BrowserConfig(
            headless=True,
            user_agent="MyDefaultAgent/1.0"
        )

        # Clone for debugging (headful)
        debug_browser_config = base_browser_config.clone(headless=False, verbosity=True)
        print(f"Base headless: {base_browser_config.headless}, Debug headless: {debug_browser_config.headless}")

        # Clone for a specific mobile UA
        mobile_browser_config = base_browser_config.clone(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 13_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1"
        )
        print(f"Mobile UA: {mobile_browser_config.user_agent}")

        # --- CrawlerRunConfig Cloning ---
        base_run_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            word_count_threshold=50,
            screenshot=False
        )

        # Config for scraping articles, needs specific extraction and screenshot
        # Assuming ArticleExtractionStrategy is a defined class
        # article_strategy = SomeExtractionStrategy(type="article") 
        article_run_config = base_run_config.clone(
            # extraction_strategy=article_strategy, 
            screenshot=True,
            css_selector="main.article-body"
        )
        print(f"Article config screenshot: {article_run_config.screenshot}, CSS: {article_run_config.css_selector}")

        # Config for scraping product listings, different strategy, no screenshot
        # Assuming ProductListExtractionStrategy is a defined class
        # product_list_strategy = SomeExtractionStrategy(type="product_list")
        product_list_run_config = base_run_config.clone(
            # extraction_strategy=product_list_strategy,
            css_selector="ul.product-grid"
        )
        print(f"Product list screenshot: {product_list_run_config.screenshot}, CSS: {product_list_run_config.css_selector}")
        ```

*   6.3. **`dump()` and `load(data: dict)`: Persistence and Portability**
    *   6.3.1. **`dump()`:**
        *   **How it serializes:** The `dump()` method converts the configuration object's state into a Python dictionary. This dictionary is designed to be JSON-serializable, meaning it contains only basic Python types (strings, numbers, booleans, lists, dictionaries) and representations of nested configuration objects.
        *   **What can be serialized:**
            *   Basic attributes (strings, ints, bools).
            *   Nested Crawl4ai configuration objects (e.g., a `GeolocationConfig` within a `CrawlerRunConfig` will also be `dump`ed).
            *   Enum members are typically serialized to their string values.
        *   **Limitations:** `dump()` primarily serializes the *configurable parameters* of the object. It generally cannot serialize:
            *   Arbitrary Python objects assigned to attributes (e.g., custom, non-Crawl4ai class instances like a complex `extraction_strategy` instance that isn't just a basic Crawl4ai strategy). If you need to persist such complex objects, you'd typically handle their serialization and deserialization separately (e.g., using `pickle` with caution, or by re-instantiating them based on some stored identifier).
            *   Runtime state that isn't part of the initial configuration.
    *   6.3.2. **`load(data: dict)`:**
        *   **How it reconstructs:** This is a *static method* on the configuration class (e.g., `BrowserConfig.load(my_dict)`). It takes a dictionary (usually one produced by `dump()`) and creates a new instance of the configuration object, populating it with the values from the dictionary.
        *   **Ensuring dictionary structure:** The input dictionary should have keys that correspond to the parameters of the configuration object's `__init__` method or its settable attributes. Nested config objects in the dictionary will also be reconstructed using their respective `load()` methods.
    *   6.3.3. **Workflow: Saving and Loading Configurations**
        1.  **Create and Configure:** Instantiate and set up your config object.
            ```python
            # my_browser_config = BrowserConfig(user_agent="TestAgent/1.0", headless=False)
            ```
        2.  **Dump to Dictionary:**
            ```python
            # config_dict = my_browser_config.dump()
            ```
        3.  **Save to File (e.g., JSON):**
            ```python
            import json
            # with open("browser_settings.json", "w") as f:
            #     json.dump(config_dict, f, indent=4)
            ```
        4.  **Later, Load from File:**
            ```python
            # with open("browser_settings.json", "r") as f:
            #     loaded_dict_from_file = json.load(f)
            ```
        5.  **Reconstruct Object using `load()`:**
            ```python
            # loaded_browser_config = BrowserConfig.load(loaded_dict_from_file)
            # print(f"Loaded User-Agent: {loaded_browser_config.user_agent}")
            ```
    *   **Code Example: Saving a `BrowserConfig` to JSON and then loading it back.**
        ```python
        from crawl4ai import BrowserConfig
        import json
        import os

        # 1. Create and configure
        original_browser_config = BrowserConfig(
            user_agent="MyPersistentAgent/2.0", 
            headless=True,
            extra_args=["--incognito"],
            proxy_config={"server": "http://testproxy.com:1234"}
        )
        print(f"Original Config: {original_browser_config.user_agent}, Headless: {original_browser_config.headless}")

        # 2. Dump to dictionary
        config_as_dict = original_browser_config.dump()
        print(f"\nDumped Dictionary:\n{json.dumps(config_as_dict, indent=2)}")

        # 3. Save to JSON file
        file_path = "my_saved_browser_config.json"
        with open(file_path, "w") as f:
            json.dump(config_as_dict, f, indent=2)
        print(f"\nSaved config to {file_path}")

        # 4. Load from JSON file
        with open(file_path, "r") as f:
            loaded_dict = json.load(f)
        
        # 5. Reconstruct object using load()
        loaded_config = BrowserConfig.load(loaded_dict)
        print(f"\nLoaded Config from file: {loaded_config.user_agent}, Headless: {loaded_config.headless}")
        print(f"Loaded Proxy Server: {loaded_config.proxy_config.get('server') if loaded_config.proxy_config else 'None'}")

        # Clean up
        os.remove(file_path)
        ```

*   6.4. **Best Practices for Configuration Management**
    *   6.4.1. **Define base configurations:** For settings that are common across many crawls (e.g., a standard `BrowserConfig` for your organization, or a default `CrawlerRunConfig` for a type of website), define them once.
    *   6.4.2. **Use `clone()` for variations:** When you need slight modifications for specific tasks, use `base_config.clone(param_to_override=new_value)`. This keeps your code DRY and makes it clear what's changing.
    *   6.4.3. **Store complex/reused configurations externally:** For configurations that are elaborate or used across multiple scripts/projects, save them as JSON or YAML files and load them using `ConfigClass.load()`. This decouples configuration from code.
    *   6.4.4. **Consider versioning your configuration files:** If your external configuration files evolve, use a version control system (like Git) to track changes, just as you would with your code. This helps in managing different setups or rolling back if needed.

## 7. Advanced Scenarios: Combining Configuration Objects for Powerful Workflows

*   7.1. **Introduction: The Synergy of Configuration Objects**
    The true power of Crawl4ai's configuration system shines when you combine different configuration objects (`BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, `GeolocationConfig`, etc.) to tackle complex, real-world crawling challenges. Each object controls a specific aspect of the crawl, and their interplay allows for highly tailored and sophisticated behavior. This section explores several scenarios to illustrate this synergy.

*   7.2. **Scenario 1: Geo-Targeted Content Extraction with Specific Browser Identity and Proxies**
    *   **Objective:** Crawl a news website that serves different content based on the user's country, appearing as a mobile user from Germany, and routing traffic through a German proxy server.
    *   **`BrowserConfig` Elements:**
        *   `user_agent`: A User-Agent string for a common mobile browser in Germany (e.g., Chrome on Android).
            *   *Why:* To make the server believe the request is from a mobile device.
        *   `proxy_config`: Details of a proxy server located in Germany.
            *   *Why:* The IP address is a primary way websites determine location.
        *   `channel` (if Chromium-based): Could be set to "chrome" to ensure Chrome-specific behavior if the UA is Chrome.
    *   **`CrawlerRunConfig` Elements:**
        *   `geolocation`: An instance of `GeolocationConfig` with latitude/longitude for a city in Germany (e.g., Berlin).
            *   *Why:* To provide GPS coordinates that match the desired location, for sites using browser geolocation APIs.
        *   `locale`: Set to "de-DE".
            *   *Why:* To set the `Accept-Language` header and JavaScript `navigator.language` to German, further reinforcing the German user profile.
        *   `timezone_id`: Set to "Europe/Berlin".
            *   *Why:* To make the browser's reported timezone consistent with Germany.
        *   `extraction_strategy`: An appropriate strategy to extract news headlines and summaries.
    *   **Workflow Explanation:**
        1.  The `BrowserConfig` launches a browser that routes its traffic through the German proxy, making all network requests appear to originate from Germany. Its User-Agent string identifies it as a German mobile user.
        2.  The `CrawlerRunConfig` then instructs this browser context to report German GPS coordinates, set its language to German, and use a German timezone.
        3.  When `arun()` navigates to the news URL, the website should (if it performs geo-targeting) serve the German version of its content.
        4.  The specified `extraction_strategy` then processes this German-specific content.
    *   **Code Example: Setting up this combined configuration.**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, 
            GeolocationConfig, CacheMode
        )
        # Assume an appropriate extraction strategy, e.g., for news articles
        # from crawl4ai.extraction_strategy import SomeArticleExtractionStrategy 

        async def crawl_german_news():
            german_proxy = {
                "server": "http://your-german-proxy.com:port", # Replace with actual proxy
                # "username": "proxy_user", # If authenticated
                # "password": "proxy_pass"  # If authenticated
            }

            browser_cfg_german = BrowserConfig(
                user_agent="Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Mobile Safari/537.36", # Example Android Chrome
                proxy_config=german_proxy,
                headless=True 
            )

            geo_config_berlin = GeolocationConfig(latitude=52.5200, longitude=13.4050, accuracy=100.0)
            
            # article_strategy = SomeArticleExtractionStrategy() # Replace with actual strategy

            run_cfg_german_news = CrawlerRunConfig(
                geolocation=geo_config_berlin,
                locale="de-DE",
                timezone_id="Europe/Berlin",
                # extraction_strategy=article_strategy,
                cache_mode=CacheMode.BYPASS # Ensure fresh content for geo-testing
            )

            async with AsyncWebCrawler(config=browser_cfg_german) as crawler:
                # Use a site that shows location or IP for testing, e.g., ipinfo.io
                result = await crawler.arun(url="https://ipinfo.io/json", config=run_cfg_german_news)
                
                if result.success:
                    print("--- Geo-Targeted Crawl Result (ipinfo.io) ---")
                    print(result.html) # Should show German IP and location details
                    # For a real news site, you'd inspect result.markdown or result.extracted_content
                else:
                    print(f"Crawl failed: {result.error_message}")

        # await crawl_german_news() # Uncomment to run with a real proxy
        ```

*   7.3. **Scenario 2: High-Volume Data Extraction from API-like Endpoints (No JS) with Rate Limiting**
    *   **Objective:** Efficiently scrape data from a list of 1000 product API endpoints (e.g., `api.example.com/product/{id}`) that return JSON and are known to be static (no JavaScript rendering needed). Ensure polite crawling to avoid overwhelming the server.
    *   **Strategy Choice:** `AsyncHTTPCrawlerStrategy` is ideal here for speed and low overhead.
    *   **`HTTPCrawlerConfig` Elements (if needed per request, often passed directly to `arun` with HTTP strategy):**
        *   `headers`: If the API requires specific headers like an `Authorization` token or `Accept: application/json`.
        *   `method`: Likely "GET" for fetching product data.
    *   **`CrawlerRunConfig` Elements:**
        *   `extraction_strategy`: `NoExtractionStrategy` if the API returns clean JSON directly in `result.html`. If it returns HTML containing JSON (e.g., in a `<script>` tag), you might need a custom extractor or a simple regex in post-processing.
        *   `cache_mode`: `CacheMode.ENABLED` might be good if product data doesn't change extremely frequently, or `CacheMode.BYPASS` if always fresh data is paramount.
    *   **Dispatcher & Rate Limiting:**
        *   Use `crawler.arun_many()` with its default `MemoryAdaptiveDispatcher`.
        *   Configure the `CrawlerRunConfig` (passed to `arun_many`) with `mean_delay` and `max_range` to introduce delays between requests to the *same domain*.
        *   The `MemoryAdaptiveDispatcher` itself can also be configured with a `RateLimiter` instance for more global control if needed, but per-domain delays via `CrawlerRunConfig` are often sufficient for politeness.
    *   **Workflow Explanation:**
        1.  Instantiate `AsyncWebCrawler` with `AsyncHTTPCrawlerStrategy`.
        2.  Prepare a list of product API URLs.
        3.  Create a `CrawlerRunConfig` that includes `mean_delay` and `max_range` for polite crawling.
        4.  Call `crawler.arun_many(urls=product_urls, config=run_config_with_delay)`.
        5.  The dispatcher will manage concurrency (based on memory by default) and inter-request delays.
        6.  Each result's `html` attribute will contain the raw JSON response from the API.
    *   **Code Example: Fetching data from a list of URLs using `AsyncHTTPCrawlerStrategy`.**
        ```python
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
        from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
        from crawl4ai.async_dispatchers import MemoryAdaptiveDispatcher, RateLimiter # For more advanced control
        import json
        import asyncio

        # Sample product IDs
        product_ids = list(range(1, 21)) # Let's do 20 for a quick demo
        api_urls = [f"https://jsonplaceholder.typicode.com/todos/{pid}" for pid in product_ids]

        async def fetch_product_apis():
            http_strategy = AsyncHTTPCrawlerStrategy()
            
            # Configure run config for politeness
            # This will apply per-domain delays managed by the dispatcher
            run_config_polite = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                mean_delay=0.5,  # Average 0.5s delay between requests to jsonplaceholder.typicode.com
                max_range=0.3,   # Add random 0-0.3s to that
                # No specific extraction_strategy needed as API returns JSON directly in result.html
            )
            
            # Optional: Configure the dispatcher itself if more control than CrawlerRunConfig's delay offers
            # custom_dispatcher = MemoryAdaptiveDispatcher(
            #     rate_limiter=RateLimiter(base_delay=(0.5, 1.0)) # Global rate limiting
            # )

            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                print(f"Fetching {len(api_urls)} URLs...")
                results_stream = await crawler.arun_many(
                    urls=api_urls, 
                    config=run_config_polite,
                    # dispatcher=custom_dispatcher # If using custom dispatcher
                )
                
                all_product_data = []
                async for result_container in results_stream: # Assuming stream=True in run_config_polite
                    result = result_container.result # Access the CrawlResult
                    if result.success:
                        try:
                            product_data = json.loads(result.html)
                            all_product_data.append(product_data)
                            print(f"Fetched: {product_data.get('title', 'N/A')[:30]}...")
                        except json.JSONDecodeError:
                            print(f"Error parsing JSON for {result.url}: {result.html[:100]}")
                    else:
                        print(f"Failed {result.url}: {result.error_message}")
                
                print(f"\nSuccessfully fetched {len(all_product_data)} product details.")
                # print("Sample of first product:", all_product_data[0] if all_product_data else "None")

        # await fetch_product_apis()
        ```
        *Note: For `arun_many`, `CrawlerRunConfig`'s `mean_delay` and `max_range` are hints for the dispatcher's internal per-domain rate limiting. The `RateLimiter` object passed to the dispatcher provides more explicit global control.*

*   7.4. **Scenario 3: Multi-Step Authenticated Crawl with LLM-based Data Summarization**
    *   **Objective:**
        1.  Log into a website.
        2.  Navigate to a user-specific dashboard page.
        3.  Extract structured data (e.g., a list of recent orders) from the dashboard.
        4.  Use an LLM to generate a brief summary of these orders.
    *   **`BrowserConfig` Elements:**
        *   `use_persistent_context=True`, `user_data_dir="my_site_profile"`: To save and reuse login cookies/session.
        *   `headless=False` (recommended for initial login script development).
    *   **`CrawlerRunConfig` (Step 1: Login):**
        *   `url`: Login page URL.
        *   `session_id`: A unique ID, e.g., "my_dashboard_session".
        *   `js_code`: JavaScript to fill username, password, and click submit.
        *   `wait_for`: CSS selector or JS condition confirming successful login (e.g., visibility of a dashboard element or URL change).
        *   `cache_mode=CacheMode.BYPASS` (to ensure login is attempted).
    *   **`CrawlerRunConfig` (Step 2: Navigate & Extract Data - using same `session_id`):**
        *   `url`: Dashboard page URL.
        *   `session_id`: Must be "my_dashboard_session".
        *   `extraction_strategy`: An instance of `JsonCssExtractionStrategy` (or `LLMExtractionStrategy`) configured to extract order details.
        *   `cache_mode=CacheMode.BYPASS` (to get fresh dashboard data).
    *   **`LLMConfig` (for summarization, if using an LLM strategy for it):**
        *   `provider`, `api_token`.
        *   `temperature`, `max_tokens` suitable for summarization.
    *   **Post-processing or an `LLMSummarizationStrategy`:**
        *   If summarization is a separate step: After getting `extracted_content` (list of orders), manually call an LLM with this data.
        *   If using a hypothetical `LLMSummarizationStrategy`: This strategy would take the extracted order data (perhaps from a previous `extraction_strategy` or directly from the page content if simple enough) and use the LLM to summarize it. This would be part of the `CrawlerRunConfig` for Step 2.
    *   **Workflow Explanation:**
        1.  The first `arun()` call uses `js_code` to log in. The session (cookies) is stored due to `use_persistent_context`.
        2.  The second `arun()` call reuses the `session_id`. Playwright/Crawl4ai uses the stored cookies, allowing access to the dashboard. The `extraction_strategy` then pulls the order data.
        3.  The extracted order data (JSON string from `result.extracted_content`) is parsed.
        4.  This data is then passed to an LLM for summarization (either via another `LLMExtractionStrategy` configured for summarization or a direct API call).
    *   **Code Example: Focusing on the `CrawlerRunConfig` aspects.**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig,
            LLMExtractionStrategy, CacheMode
        )
        from pydantic import BaseModel, Field
        import json
        import os

        # --- Schemas ---
        class OrderItem(BaseModel):
            item_name: str
            quantity: int
            price: float

        class DashboardData(BaseModel):
            user_name: str
            recent_orders: list[OrderItem]

        # --- Mock HTML ---
        LOGIN_PAGE_HTML = "<html><body><form><input name='user'><input name='pass' type='password'><button type='submit'>Login</button></form></body></html>"
        DASHBOARD_HTML_TEMPLATE = """
        <html><body><div id='dashboard'>
            Welcome, {user_name}!
            <h2>Recent Orders</h2>
            <ul id='order-list'>
                <li><span>Order 1: Widget A (2) @ $10.00</span></li>
                <li><span>Order 2: Gadget B (1) @ $25.50</span></li>
            </ul>
        </div></body></html>
        """

        async def run_authenticated_llm_summary():
            session_id = "auth_crawl_session"
            user_data_dir = "./auth_browser_profile" # For session persistence
            
            # For real use, ensure OPENAI_API_KEY is set
            if not os.getenv("OPENAI_API_KEY"):
                print("OPENAI_API_KEY not set. Skipping authenticated LLM summary example.")
                return

            # Browser config with persistence
            browser_cfg = BrowserConfig(
                use_persistent_context=True, 
                user_data_dir=user_data_dir,
                headless=True # Set to False to observe login if needed
            )

            # LLM Config for extraction & summarization
            llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"), temperature=0.2)

            # Strategy to extract orders from dashboard
            order_extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_conf,
                schema=DashboardData.model_json_schema(),
                instruction="Extract the username and all recent orders from the dashboard HTML. For each order, get item name, quantity, and price.",
                input_format="html" # Feed raw HTML for LLM to parse structure
            )

            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                # Step 1: Simulate Login (replace with actual login logic for a real site)
                # For this example, we'll just navigate to a mock "login successful" page
                # In a real scenario, js_code would fill and submit the login form.
                print("Simulating login...")
                login_config = CrawlerRunConfig(
                    url=f"raw://{LOGIN_PAGE_HTML.replace('{user_name}', 'TestUser')}", # Mock successful login state
                    session_id=session_id,
                    wait_for="css:body" # Just wait for body to exist on this mock page
                )
                login_result = await crawler.arun(config=login_config)
                if not login_result.success:
                    print(f"Login step failed: {login_result.error_message}")
                    return
                print("Login step simulated/completed.")

                # Step 2: Navigate to dashboard and extract orders
                print("Navigating to dashboard and extracting orders...")
                dashboard_html = DASHBOARD_HTML_TEMPLATE.replace("{user_name}", "TestUser") # Mock dashboard
                dashboard_config = CrawlerRunConfig(
                    url=f"raw://{dashboard_html}", # Use mock dashboard HTML
                    session_id=session_id,
                    extraction_strategy=order_extraction_strategy,
                    cache_mode=CacheMode.BYPASS
                )
                dashboard_result = await crawler.arun(config=dashboard_config)

                if not dashboard_result.success or not dashboard_result.extracted_content:
                    print(f"Dashboard data extraction failed: {dashboard_result.error_message}")
                    await crawler.kill_session(session_id)
                    return
                
                print("Orders extracted successfully.")
                extracted_data = json.loads(dashboard_result.extracted_content)
                
                # LLMExtractionStrategy might return a list, take the first element.
                dashboard_info = DashboardData(**(extracted_data[0] if isinstance(extracted_data, list) else extracted_data))
                print(f"Welcome, {dashboard_info.user_name}!")
                for order in dashboard_info.recent_orders:
                    print(f" - {order.item_name} (x{order.quantity}) at ${order.price}")

                # Step 3: Summarize orders using another LLM call (can be part of a more complex strategy or separate)
                if dashboard_info.recent_orders:
                    print("Summarizing orders...")
                    orders_text = "\n".join([f"- {o.item_name} (x{o.quantity}) for ${o.price}" for o in dashboard_info.recent_orders])
                    
                    summarization_prompt = f"Summarize these orders for {dashboard_info.user_name}:\n{orders_text}\n\nSummary:"
                    
                    # Using a generic completion method for simplicity, could also be another LLMExtractionStrategy
                    from crawl4ai.utils import perform_completion_with_backoff # Assuming direct LiteLLM call
                    summary_response = await perform_completion_with_backoff(
                        provider=llm_conf.provider,
                        prompt=summarization_prompt, # Note: LiteLLM uses 'messages' array usually
                        messages=[{"role": "user", "content": summarization_prompt}],
                        api_key=llm_conf.api_token,
                        base_url=llm_conf.base_url,
                        max_tokens=100
                    )
                    summary_text = summary_response.choices[0].message.content
                    print(f"\nOrder Summary:\n{summary_text}")

                # Clean up session
                await crawler.kill_session(session_id)
                # And remove profile dir if it was for temp use
                # import shutil; shutil.rmtree(user_data_dir, ignore_errors=True)
        
        # await run_authenticated_llm_summary() # Uncomment to run
        ```

*   7.5. **Scenario 4: Dynamic Content Scraping with Robust Error Handling and Fallbacks**
    *   **Objective:** Scrape product details from an e-commerce site where some product attributes (e.g., "discounted price," "stock level") might load dynamically or not be present for all items. The goal is to get as much data as possible and handle missing pieces gracefully.
    *   **`BrowserConfig` Elements:**
        *   Standard setup, potentially with `headless=False` during development for observation.
    *   **`CrawlerRunConfig` Elements (and Python control flow):**
        *   **Initial Load & Wait:**
            *   `url`: The product page URL.
            *   `wait_for`: A selector for a core element that *must* be present (e.g., product title or main image).
        *   **Attempting to Trigger Dynamic Content (if applicable):**
            *   `js_code`: May include clicks on tabs (e.g., "Specifications," "Reviews") or scrolls if certain data is lazy-loaded upon such interactions.
            *   Further `wait_for` calls after each interaction to allow content to load.
        *   **Extraction Strategy (e.g., `JsonCssExtractionStrategy` or `LLMExtractionStrategy`):**
            *   The schema should define fields as `Optional` where data might be missing (e.g., `discounted_price: Optional[float] = None`).
            *   For CSS-based extraction, selectors for optional fields should be robust enough not to break if the element isn't found (the strategy should handle this by returning `None` for that field).
        *   **Python-Level Fallbacks/Retries (Conceptual):**
            While `CrawlerRunConfig` itself doesn't have direct retry logic for parts of an extraction, you can structure your Python code around `arun()`:
            ```python
            # Conceptual Python-level retry for an optional element
            # result = await crawler.arun(config=initial_config)
            # extracted_data = json.loads(result.extracted_content)[0]
            # if not extracted_data.get("stock_level"):
            #     print("Stock level not found, trying to click 'Check Stock' button...")
            #     retry_config = initial_config.clone(
            #         js_code="document.querySelector('#check-stock-btn')?.click();",
            #         wait_for="css:.stock-info-loaded", # Wait for stock info to appear
            #         js_only=True, # Operate on the same page
            #         session_id="product_page_session" # Ensure same page
            #     )
            #     stock_result = await crawler.arun(config=retry_config)
            #     # Re-extract or merge results
            ```
    *   **Workflow Explanation:**
        1.  Load the main page and wait for essential static elements.
        2.  If certain data is known to be dynamic (e.g., loaded on a tab click), use `js_code` to trigger that interaction, followed by another `wait_for`.
        3.  Use an extraction strategy with an optional schema.
        4.  If key optional data is missing, and there's a known interaction to reveal it (like clicking a button), you can make a subsequent `arun()` call (with `js_only=True` and the same `session_id`) to perform that action and then attempt to re-extract or extract just that missing piece.
    *   **Code Example (Conceptual - focusing on the idea of layered attempts):**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig,
            JsonCssExtractionStrategy, CacheMode # Example using CSS strategy
        )
        import json

        # Define a schema where some fields are optional
        PRODUCT_SCHEMA = {
            "name": "Product Info",
            "baseSelector": "div.product-main", # Assuming a main product container
            "fields": [
                {"name": "title", "selector": "h1.product-title", "type": "text"},
                {"name": "price", "selector": ".price-current", "type": "text"},
                # Optional field: discount might not always be there
                {"name": "discounted_price", "selector": ".price-discounted", "type": "text", "default": None},
                # Optional field: stock might load after a click
                {"name": "stock_status", "selector": ".stock-status-display", "type": "text", "default": "Unknown"}
            ]
        }
        
        # Mock HTMLs
        INITIAL_HTML = """
        <div class='product-main'>
            <h1 class='product-title'>Super Widget</h1>
            <span class='price-current'>$100</span>
            <!-- Discounted price and stock are not initially visible -->
            <button id='show-details-btn'>Show More Details</button>
            <div id='extra-details' style='display:none;'>
                 <span class='price-discounted'>$80</span>
                 <span class='stock-status-display'>In Stock</span>
            </div>
        </div>
        """
        HTML_AFTER_CLICK = INITIAL_HTML.replace("style='display:none;'", "style='display:block;'")


        async def crawl_dynamic_product():
            session_id = "dynamic_product_session"
            extraction_strategy = JsonCssExtractionStrategy(PRODUCT_SCHEMA)

            async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
                # --- Attempt 1: Initial Load ---
                print("--- Attempt 1: Initial Load ---")
                config_attempt1 = CrawlerRunConfig(
                    url=f"raw://{INITIAL_HTML}",
                    session_id=session_id,
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS
                )
                result1 = await crawler.arun(config=config_attempt1)
                data1 = {}
                if result1.success and result1.extracted_content:
                    data1_list = json.loads(result1.extracted_content)
                    if data1_list: data1 = data1_list[0]
                print(f"Initial Data: {data1}")

                # --- Attempt 2: Click button and re-evaluate (or re-extract if strategy supports it) ---
                # If some data is missing (e.g., stock_status is 'Unknown' or discounted_price is None)
                # and we know an action can reveal it.
                if data1.get("stock_status") == "Unknown" or not data1.get("discounted_price"):
                    print("\n--- Attempt 2: Clicking 'Show More Details' ---")
                    
                    # For this raw HTML example, we'll just "navigate" to the state after click
                    # In a real scenario, js_code would click the button.
                    config_attempt2 = CrawlerRunConfig(
                        url=f"raw://{HTML_AFTER_CLICK}", # Simulating state after click
                        session_id=session_id, # Maintain session
                        # js_code="document.getElementById('show-details-btn')?.click();", # Real interaction
                        # wait_for="css:#extra-details[style*='display:block']", # Wait for it to be visible
                        js_only=False, # Set to True if js_code is used on existing page
                        extraction_strategy=extraction_strategy, # Re-extract
                        cache_mode=CacheMode.BYPASS
                    )
                    result2 = await crawler.arun(config=config_attempt2)
                    data2 = {}
                    if result2.success and result2.extracted_content:
                        data2_list = json.loads(result2.extracted_content)
                        if data2_list: data2 = data2_list[0]
                    print(f"Data after interaction: {data2}")
                    # In a real app, you'd merge data1 and data2 intelligently
                
                await crawler.kill_session(session_id)

        # await crawl_dynamic_product()
        ```
        This conceptual example shows how you might chain `arun` calls with different `CrawlerRunConfig`s (sharing a `session_id`) to handle dynamic content revealing steps. More robust solutions might involve custom retry logic in Python or more sophisticated `wait_for` JS expressions.


## 8. Conclusion and Further Exploration

*   8.1. **Recap of the power and flexibility offered by Crawl4ai's configuration objects.**
    Throughout this guide, we've explored how `BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, and other specialized configuration objects in Crawl4ai provide a powerful and flexible framework for tailoring your web crawling and scraping tasks. From defining browser identity and environment to controlling per-page interactions, content extraction, media handling, and LLM integration, these objects give you granular control over every aspect of the crawl. The separation of concerns and methods like `clone()`, `dump()`, and `load()` further enhance reusability and manageability of your configurations.

*   8.2. **Encouragement to experiment with different combinations.**
    The true strength of Crawl4ai's configuration system lies in the ability to combine these objects and their parameters in creative ways to solve unique challenges. Don't hesitate to experiment:
    *   Try different `user_agent` strings with varying `headless` modes.
    *   Combine `css_selector` with `target_elements` for precise content focus.
    *   Use `js_code` and `wait_for` to navigate complex SPAs.
    *   Integrate `LLMExtractionStrategy` with fine-tuned `LLMConfig` settings for difficult extractions.
    *   Leverage `session_id` for multi-step workflows.
    The more you experiment, the better you'll understand how to harness the full potential of Crawl4ai for your specific needs.

*   8.3. **Pointers to other relevant documentation sections.**
    This guide has focused on the "how" and "why" of using configuration objects. For more details on specific areas, please refer to:
    *   **API Reference / "Foundational Memory" Document for `config_objects`:** For an exhaustive list of all parameters, their types, and default values.
    *   **Documentation on Specific Strategies:** Deep dives into `LLMExtractionStrategy`, `JsonCssExtractionStrategy`, `AsyncHTTPCrawlerStrategy`, various `MarkdownGenerationStrategy` and `ContentFilterStrategy` options.
    *   **Advanced Browser Management:** Detailed guides on `use_persistent_context`, `user_data_dir`, Docker integration, and managing browser profiles.
    *   **`arun_many()` and Dispatchers:** For understanding how to efficiently crawl multiple URLs in parallel and customize dispatch behavior with `MemoryAdaptiveDispatcher`, `SemaphoreDispatcher`, and `RateLimiter`.
    *   **Hooks and Custom Callbacks:** For advanced customization of the crawling lifecycle.

By mastering these configuration objects, you can build robust, efficient, and highly customized web crawlers with Crawl4ai. Happy crawling!
```

---


## Configuration Objects - Examples
Source: crawl4ai_config_objects_examples_content.llm.md

```markdown
# Examples Document for crawl4ai - config_objects Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_config_objects.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

This document provides a collection of runnable code examples demonstrating how to use the various configuration objects within the `crawl4ai` library. These examples are designed to showcase diverse usage patterns and help users quickly understand how to configure the crawler for their specific needs.

## 1. Introduction to Configuration Objects

### 1.1. Overview: Purpose and interaction of `BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, `ProxyConfig`, and `GeolocationConfig`.

The `crawl4ai` library utilizes several key configuration objects to manage different aspects of the crawling and data extraction process:

*   **`BrowserConfig`**: Controls the browser's launch and behavior, such as its type (Chromium, Firefox, WebKit), headless mode, proxy settings, user agent, viewport size, and persistent contexts. This configuration is typically passed when initializing the `AsyncWebCrawler`.
*   **`CrawlerRunConfig`**: Dictates the behavior for each specific `arun()` call. This includes URL-specific settings like caching modes, JavaScript execution, waiting conditions, screenshot/PDF generation, content processing strategies (extraction, chunking, markdown), and run-specific overrides for browser settings like locale or geolocation.
*   **`LLMConfig`**: Configures the Large Language Model (LLM) provider and its parameters when using LLM-based extraction strategies (e.g., `LLMExtractionStrategy`). It includes provider name, API token, base URL, and generation parameters like temperature and max tokens.
*   **`ProxyConfig`**: A dedicated object for defining proxy server details, including server URL, authentication credentials (username/password), and IP. It can be used within `BrowserConfig` (for browser-wide proxy) or `CrawlerRunConfig` (for run-specific proxy, especially with HTTP-based strategies or proxy rotation).
*   **`GeolocationConfig`**: Specifies geolocation data (latitude, longitude, accuracy) to simulate a specific geographic location for the browser context, often used within `CrawlerRunConfig`.

These objects interact to provide fine-grained control. For instance, `BrowserConfig` sets up the general browser environment, while `CrawlerRunConfig` can override or augment these settings for individual crawl tasks. `LLMConfig`, `ProxyConfig`, and `GeolocationConfig` are often nested within `BrowserConfig` or `CrawlerRunConfig` to provide specialized configurations.

---
### 1.2. Example: Basic workflow - Creating default instances of each config object and passing them to `AsyncWebCrawler` or relevant strategies.

This example shows the most basic initialization of config objects and how they might be passed to the crawler.

```python
import asyncio
import os
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    LLMConfig,
    ProxyConfig,
    GeolocationConfig,
    CacheMode
)

async def basic_config_workflow():
    # 1. BrowserConfig: Default settings (headless chromium)
    browser_config = BrowserConfig()
    print(f"Default BrowserConfig: {browser_config.to_dict()}")

    # 2. GeolocationConfig (Optional, for specific location simulation)
    geolocation_config = GeolocationConfig(latitude=37.7749, longitude=-122.4194) # San Francisco
    print(f"\nGeolocationConfig: {geolocation_config.to_dict()}")

    # 3. ProxyConfig (Optional, if a proxy is needed)
    # For this example, we'll assume no proxy is needed for the default BrowserConfig.
    # If one were, it could be:
    # proxy_config = ProxyConfig(server="http://myproxy.com:8080")
    # browser_config_with_proxy = BrowserConfig(proxy_config=proxy_config)

    # 4. LLMConfig (Optional, only if using LLM-based extraction)
    # Requires an API key, e.g., from environment
    llm_config = LLMConfig(
        provider="openai/gpt-4o-mini",
        api_token=os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE") # Replace or set env var
    )
    print(f"\nLLMConfig (provider only shown if token is placeholder): {llm_config.provider}")

    # 5. CrawlerRunConfig: Default settings, but can integrate other configs
    # For a specific run, you might pass geolocation or an LLM strategy
    crawler_run_config_basic = CrawlerRunConfig(
        url="https://example.com",
        cache_mode=CacheMode.BYPASS # For this demo, bypass cache
    )
    print(f"\nBasic CrawlerRunConfig: {crawler_run_config_basic.to_dict(exclude_none=True)}")

    crawler_run_config_with_geo = CrawlerRunConfig(
        url="https://example.com/geo-specific-content",
        geolocation=geolocation_config,
        cache_mode=CacheMode.BYPASS
    )
    print(f"\nCrawlerRunConfig with Geolocation: {crawler_run_config_with_geo.to_dict(exclude_none=True)}")

    # To use LLMConfig, you'd typically pass it to an LLMExtractionStrategy,
    # which then goes into CrawlerRunConfig.extraction_strategy.
    # from crawl4ai.extraction_strategy import LLMExtractionStrategy
    # llm_extraction_strategy = LLMExtractionStrategy(llm_config=llm_config, schema={"title": "Page title"})
    # crawler_run_config_with_llm = CrawlerRunConfig(
    #     url="https://example.com/article",
    #     extraction_strategy=llm_extraction_strategy,
    #     cache_mode=CacheMode.BYPASS
    # )
    # print(f"\nCrawlerRunConfig with LLM Strategy (conceptual): {crawler_run_config_with_llm.to_dict(exclude_none=True)}")


    # Using AsyncWebCrawler with the basic browser config
    async with AsyncWebCrawler(config=browser_config) as crawler:
        print("\n--- Running basic crawl ---")
        result = await crawler.arun(config=crawler_run_config_basic)
        if result.success:
            print(f"Successfully crawled {result.url}. Markdown length: {len(result.markdown.raw_markdown)}")
        else:
            print(f"Failed to crawl {result.url}: {result.error_message}")

        # Example with geolocation (on a site that might use it)
        # For a real test, use a site like https://mylocation.org/
        print("\n--- Running crawl with geolocation (conceptual) ---")
        result_geo = await crawler.arun(config=crawler_run_config_with_geo)
        if result_geo.success:
            print(f"Successfully crawled {result_geo.url} (geo). Markdown length: {len(result_geo.markdown.raw_markdown)}")
        else:
            print(f"Failed to crawl {result_geo.url} (geo): {result_geo.error_message}")


if __name__ == "__main__":
    asyncio.run(basic_config_workflow())
```

---
## 2. `GeolocationConfig` Examples

### 2.1. Example: Basic initialization of `GeolocationConfig` for a specific location (e.g., San Francisco).

This example shows how to initialize `GeolocationConfig` with latitude and longitude, defaulting accuracy to 0.0.

```python
import asyncio
from crawl4ai import GeolocationConfig, CrawlerRunConfig, AsyncWebCrawler

async def basic_geolocation_config():
    # San Francisco coordinates
    sf_geo_config = GeolocationConfig(latitude=37.7749, longitude=-122.4194)
    print(f"San Francisco GeolocationConfig: {sf_geo_config.to_dict()}")

    # To see its effect, you need a site that uses geolocation.
    # We'll use a placeholder for demonstration.
    # A good test site could be https://mylocation.org/ or https://www.gps-coordinates.net/
    run_config = CrawlerRunConfig(
        url="https://www.whatismybrowser.com/detect/what-is-my-user-agent", # Shows some geo info
        geolocation=sf_geo_config,
        verbose=True
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"\nCrawled {result.url} with geolocation. HTML snippet (first 500 chars):")
            # You would typically parse the HTML to find location-specific content
            # For this example, we just print part of the HTML.
            # print(result.html[:500]) # The actual page might not show the spoofed geo directly
            print("Geolocation spoofing applied. Check a geo-sensitive site for full effect.")
        else:
            print(f"Failed to crawl: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(basic_geolocation_config())
```

---
### 2.2. Example: Initializing `GeolocationConfig` with latitude, longitude, and custom accuracy.

This example demonstrates setting a specific accuracy for the geolocation.

```python
import asyncio
from crawl4ai import GeolocationConfig

async def geolocation_with_accuracy():
    # Paris coordinates with 100 meters accuracy
    paris_geo_config = GeolocationConfig(
        latitude=48.8566,
        longitude=2.3522,
        accuracy=100.0  # Accuracy in meters
    )
    print(f"Paris GeolocationConfig with accuracy: {paris_geo_config.to_dict()}")
    # You would then use this in CrawlerRunConfig like the previous example.

if __name__ == "__main__":
    asyncio.run(geolocation_with_accuracy())
```

---
### 2.3. Example: Creating `GeolocationConfig` from a dictionary using `GeolocationConfig.from_dict()`.

This shows how to instantiate `GeolocationConfig` from a dictionary, useful for dynamic configurations.

```python
import asyncio
from crawl4ai import GeolocationConfig

async def geolocation_from_dict_example():
    geo_data_dict = {
        "latitude": 51.5074,  # London
        "longitude": 0.1278,
        "accuracy": 50.0
    }
    london_geo_config = GeolocationConfig.from_dict(geo_data_dict)
    print(f"London GeolocationConfig from_dict: {london_geo_config.to_dict()}")
    assert london_geo_config.latitude == 51.5074
    assert london_geo_config.accuracy == 50.0

if __name__ == "__main__":
    asyncio.run(geolocation_from_dict_example())
```

---
### 2.4. Example: Converting `GeolocationConfig` to a dictionary using `to_dict()`.

Demonstrates serializing a `GeolocationConfig` instance back into a dictionary.

```python
import asyncio
from crawl4ai import GeolocationConfig

async def geolocation_to_dict_example():
    tokyo_geo_config = GeolocationConfig(latitude=35.6895, longitude=139.6917, accuracy=75.0)
    config_dict = tokyo_geo_config.to_dict()
    print(f"Tokyo GeolocationConfig as dict: {config_dict}")
    assert config_dict["latitude"] == 35.6895
    assert config_dict["accuracy"] == 75.0

if __name__ == "__main__":
    asyncio.run(geolocation_to_dict_example())
```

---
### 2.5. Example: Cloning `GeolocationConfig` and modifying accuracy using `clone()`.

Shows how to create a copy of a `GeolocationConfig` instance and modify specific attributes.

```python
import asyncio
from crawl4ai import GeolocationConfig

async def geolocation_clone_example():
    original_geo_config = GeolocationConfig(latitude=40.7128, longitude=-74.0060) # New York
    print(f"Original NY GeolocationConfig: {original_geo_config.to_dict()}")

    # Clone and modify accuracy
    cloned_geo_config = original_geo_config.clone(accuracy=25.0)
    print(f"Cloned NY GeolocationConfig with new accuracy: {cloned_geo_config.to_dict()}")

    assert cloned_geo_config.latitude == original_geo_config.latitude
    assert cloned_geo_config.accuracy == 25.0
    assert original_geo_config.accuracy == 0.0 # Original remains unchanged

if __name__ == "__main__":
    asyncio.run(geolocation_clone_example())
```

---
### 2.6. Example: Integrating `GeolocationConfig` into `CrawlerRunConfig` to simulate location for a crawl.

This example explicitly shows passing `GeolocationConfig` to `CrawlerRunConfig` for a simulated crawl.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, GeolocationConfig, CacheMode

async def integrate_geolocation_in_crawler_run():
    # Berlin coordinates
    berlin_geo = GeolocationConfig(latitude=52.5200, longitude=13.4050, accuracy=10.0)

    # Configure the specific run to use this geolocation
    # Use a site that might reflect geolocation, e.g., a weather site or Google search
    run_config_with_geo = CrawlerRunConfig(
        url="https://www.google.com/search?q=weather", # Google search might show location-based results
        geolocation=berlin_geo,
        cache_mode=CacheMode.BYPASS, # Ensure fresh fetch for demo
        verbose=True
    )

    async with AsyncWebCrawler() as crawler:
        print(f"Crawling with geolocation for Berlin: {berlin_geo.to_dict()}")
        result = await crawler.arun(config=run_config_with_geo)
        if result.success:
            print(f"Successfully crawled {result.url} with simulated Berlin location.")
            # Inspect result.html or result.markdown for signs of location-based content
            # For instance, search for "Berlin" in the content
            if "Berlin" in result.html[:2000]: # Check first 2000 chars of raw HTML
                 print("Berlin might be reflected in the page content.")
            else:
                 print("Berlin not obviously reflected in initial HTML content (site might not use precise geo or requires more interaction).")
        else:
            print(f"Failed to crawl: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(integrate_geolocation_in_crawler_run())
```

---
## 3. `ProxyConfig` Examples

### 3.1. Example: Basic initialization of `ProxyConfig` with a server URL.

Shows how to set up a simple proxy without authentication.

```python
import asyncio
from crawl4ai import ProxyConfig

async def basic_proxy_config():
    proxy_config = ProxyConfig(server="http://myproxy.example.com:8080")
    print(f"Basic ProxyConfig: {proxy_config.to_dict()}")
    assert proxy_config.server == "http://myproxy.example.com:8080"
    assert proxy_config.ip == "myproxy.example.com" # IP is extracted

if __name__ == "__main__":
    asyncio.run(basic_proxy_config())
```

---
### 3.2. Example: Initializing `ProxyConfig` with server, username, and password for an authenticated proxy.

Demonstrates setting up an authenticated proxy.

```python
import asyncio
from crawl4ai import ProxyConfig

async def authenticated_proxy_config():
    auth_proxy_config = ProxyConfig(
        server="http://authproxy.example.com:3128",
        username="proxyuser",
        password="proxypassword123"
    )
    print(f"Authenticated ProxyConfig: {auth_proxy_config.to_dict()}")
    assert auth_proxy_config.username == "proxyuser"

if __name__ == "__main__":
    asyncio.run(authenticated_proxy_config())
```

---
### 3.3. Example: Initializing `ProxyConfig` with an explicit IP address.

This example shows specifying the IP address directly, which can be useful if the server URL is complex or resolution is tricky.

```python
import asyncio
from crawl4ai import ProxyConfig

async def explicit_ip_proxy_config():
    proxy_config_with_ip = ProxyConfig(
        server="http://hostname.that.is.not.ip:9999",
        ip="192.168.1.100" # Explicitly set the IP
    )
    print(f"ProxyConfig with explicit IP: {proxy_config_with_ip.to_dict()}")
    assert proxy_config_with_ip.ip == "192.168.1.100"

if __name__ == "__main__":
    asyncio.run(explicit_ip_proxy_config())
```

---
### 3.4. Example: Demonstrating automatic IP extraction from the server URL in `ProxyConfig`.

`ProxyConfig` attempts to extract the IP/hostname from the server URL if `ip` is not provided.

```python
import asyncio
from crawl4ai import ProxyConfig

async def auto_ip_extraction_proxy_config():
    # IP extraction from http://ip:port
    proxy1 = ProxyConfig(server="http://123.45.67.89:8000")
    print(f"Proxy 1 (IP from http): {proxy1.to_dict()}")
    assert proxy1.ip == "123.45.67.89"

    # IP extraction from https://hostname:port
    proxy2 = ProxyConfig(server="https://secureproxy.example.net:8443")
    print(f"Proxy 2 (Hostname from https): {proxy2.to_dict()}")
    assert proxy2.ip == "secureproxy.example.net"

    # IP extraction from socks5://hostname:port
    proxy3 = ProxyConfig(server="socks5://socksp.example.org:1080")
    print(f"Proxy 3 (Hostname from socks5): {proxy3.to_dict()}")
    assert proxy3.ip == "socksp.example.org"
    
    # IP extraction from just hostname:port (assumes http)
    proxy4 = ProxyConfig(server="anotherproxy.com:7070")
    print(f"Proxy 4 (Hostname from hostname:port): {proxy4.to_dict()}")
    assert proxy4.ip == "anotherproxy.com"


if __name__ == "__main__":
    asyncio.run(auto_ip_extraction_proxy_config())
```

---
### 3.5. Example: Creating `ProxyConfig` from a string in 'ip:port' format using `ProxyConfig.from_string()`.

Illustrates creating a `ProxyConfig` object from a simple 'ip:port' string.

```python
import asyncio
from crawl4ai import ProxyConfig

async def proxy_from_simple_string():
    proxy_str = "192.168.1.50:8888"
    proxy_config = ProxyConfig.from_string(proxy_str)
    print(f"ProxyConfig from '{proxy_str}': {proxy_config.to_dict()}")
    assert proxy_config.server == "http://192.168.1.50:8888"
    assert proxy_config.ip == "192.168.1.50"
    assert proxy_config.username is None

if __name__ == "__main__":
    asyncio.run(proxy_from_simple_string())
```

---
### 3.6. Example: Creating `ProxyConfig` from a string in 'ip:port:username:password' format using `ProxyConfig.from_string()`.

Shows creating an authenticated `ProxyConfig` object from a formatted string.

```python
import asyncio
from crawl4ai import ProxyConfig

async def proxy_from_auth_string():
    proxy_auth_str = "proxy.example.net:3128:user123:pass456"
    proxy_config = ProxyConfig.from_string(proxy_auth_str)
    print(f"ProxyConfig from '{proxy_auth_str}': {proxy_config.to_dict()}")
    assert proxy_config.server == "http://proxy.example.net:3128"
    assert proxy_config.ip == "proxy.example.net"
    assert proxy_config.username == "user123"
    assert proxy_config.password == "pass456"

if __name__ == "__main__":
    asyncio.run(proxy_from_auth_string())
```

---
### 3.7. Example: Creating `ProxyConfig` from a dictionary using `ProxyConfig.from_dict()`.

Demonstrates instantiating `ProxyConfig` from a dictionary.

```python
import asyncio
from crawl4ai import ProxyConfig

async def proxy_from_dict_example():
    proxy_data_dict = {
        "server": "socks5://secure.proxy.com:1080",
        "username": "sockuser",
        "password": "sockpassword"
    }
    proxy_config = ProxyConfig.from_dict(proxy_data_dict)
    print(f"ProxyConfig from_dict: {proxy_config.to_dict()}")
    assert proxy_config.server == "socks5://secure.proxy.com:1080"
    assert proxy_config.username == "sockuser"

if __name__ == "__main__":
    asyncio.run(proxy_from_dict_example())
```

---
### 3.8. Example: Loading a single `ProxyConfig` from the `PROXIES` environment variable using `ProxyConfig.from_env()`.

Shows how to load proxy settings from an environment variable.

```python
import asyncio
import os
from crawl4ai import ProxyConfig

async def proxy_from_env_single():
    env_var_name = "MY_CRAWLER_PROXIES" # Custom env var name
    proxy_string_in_env = "10.0.0.1:8000:envuser:envpass"

    # Temporarily set the environment variable for this example
    os.environ[env_var_name] = proxy_string_in_env
    
    try:
        proxy_configs = ProxyConfig.from_env(env_var=env_var_name)
        assert len(proxy_configs) == 1
        proxy_config = proxy_configs[0]
        
        print(f"Loaded ProxyConfig from env var '{env_var_name}': {proxy_config.to_dict()}")
        assert proxy_config.server == "http://10.0.0.1:8000"
        assert proxy_config.username == "envuser"
        assert proxy_config.password == "envpass"
    finally:
        del os.environ[env_var_name] # Clean up

if __name__ == "__main__":
    asyncio.run(proxy_from_env_single())
```

---
### 3.9. Example: Loading multiple `ProxyConfig` instances from a comma-separated `PROXIES` environment variable.

Demonstrates loading a list of proxies if the environment variable contains multiple comma-separated proxy strings.

```python
import asyncio
import os
from crawl4ai import ProxyConfig

async def proxy_from_env_multiple():
    env_var_name = "PROXIES" # Default env var name
    multiple_proxy_strings = "10.0.0.1:8000,10.0.0.2:8001:user2:pass2"

    os.environ[env_var_name] = multiple_proxy_strings
    
    try:
        proxy_configs = ProxyConfig.from_env() # Uses "PROXIES" by default
        print(f"Loaded {len(proxy_configs)} ProxyConfigs from env var '{env_var_name}':")
        for i, pc in enumerate(proxy_configs):
            print(f"  Proxy {i+1}: {pc.to_dict()}")
        
        assert len(proxy_configs) == 2
        assert proxy_configs[0].server == "http://10.0.0.1:8000"
        assert proxy_configs[1].server == "http://10.0.0.2:8001"
        assert proxy_configs[1].username == "user2"
    finally:
        del os.environ[env_var_name]

if __name__ == "__main__":
    asyncio.run(proxy_from_env_multiple())
```

---
### 3.10. Example: Converting `ProxyConfig` to a dictionary using `to_dict()`.

Shows serializing a `ProxyConfig` instance back to a dictionary.

```python
import asyncio
from crawl4ai import ProxyConfig

async def proxy_to_dict_example():
    proxy_config = ProxyConfig(
        server="https_proxy.example.com:443", 
        username="user", 
        password="secure"
    )
    config_dict = proxy_config.to_dict()
    print(f"ProxyConfig as dict: {config_dict}")
    assert config_dict["server"] == "https_proxy.example.com:443" # Note: schema is not added by default by constructor if not present
    assert config_dict["username"] == "user"

if __name__ == "__main__":
    asyncio.run(proxy_to_dict_example())
```

---
### 3.11. Example: Cloning `ProxyConfig` and changing server details using `clone()`.

Demonstrates creating a modified copy of a `ProxyConfig` instance.

```python
import asyncio
from crawl4ai import ProxyConfig

async def proxy_clone_example():
    original_proxy = ProxyConfig(server="http://original.proxy.com:8000", username="orig_user")
    print(f"Original ProxyConfig: {original_proxy.to_dict()}")

    cloned_proxy = original_proxy.clone(server="http://new.proxy.com:8080", username="new_user", password="new_password")
    print(f"Cloned ProxyConfig with new details: {cloned_proxy.to_dict()}")

    assert cloned_proxy.server == "http://new.proxy.com:8080"
    assert cloned_proxy.username == "new_user"
    assert original_proxy.server == "http://original.proxy.com:8000" # Original is unchanged

if __name__ == "__main__":
    asyncio.run(proxy_clone_example())
```

---
### 3.12. Example: Using `ProxyConfig` within `BrowserConfig` to set a browser-level proxy.

This shows how `ProxyConfig` is used to configure the proxy for all browser activity.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, ProxyConfig, CrawlerRunConfig

# This example requires a running proxy server to test against.
# For demonstration, we'll use httpbin.org/ip which returns the requester's IP.
# If run through a proxy, it should show the proxy's IP.

async def browser_level_proxy():
    # Replace with your actual proxy details
    # For this example, let's assume a (non-functional) placeholder
    proxy_details = ProxyConfig(server="http://your-proxy-server.com:8080") 
    # If your proxy requires auth:
    # proxy_details = ProxyConfig(server="http://your-proxy-server.com:8080", username="user", password="pass")


    browser_config = BrowserConfig(
        proxy_config=proxy_details,
        headless=True, # Usually True for automated tasks
        verbose=True
    )
    
    # A URL that shows your IP address
    test_url = "https://httpbin.org/ip"
    run_config = CrawlerRunConfig(url=test_url)

    print(f"Attempting to crawl {test_url} via proxy: {proxy_details.server}")
    print("NOTE: This example will likely fail or show your direct IP if the placeholder proxy is not replaced with a real, working proxy.")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Successfully crawled {result.url}.")
            # The response content should ideally show the proxy's IP
            print(f"Response content (first 200 chars): {result.html[:200]}")
            # If using a real proxy, you'd parse result.html (which is JSON from httpbin.org/ip)
            # and check if 'origin' IP matches your proxy's IP.
        else:
            print(f"Failed to crawl via proxy: {result.error_message}")
            print("This could be due to the proxy server not being reachable, incorrect credentials, or other network issues.")

if __name__ == "__main__":
    # asyncio.run(browser_level_proxy())
    print("Skipping browser_level_proxy example as it requires a live proxy server.")
    print("To run, replace placeholder proxy details and uncomment asyncio.run().")
```

---
### 3.13. Example: Using `ProxyConfig` within `CrawlerRunConfig` for HTTP-specific crawler strategies.

When using `AsyncHTTPCrawlerStrategy` (not browser-based), `ProxyConfig` can be passed via `CrawlerRunConfig`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, ProxyConfig
from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy # For non-browser crawling

async def http_crawler_with_proxy():
    # Replace with your actual proxy details
    proxy_details = ProxyConfig(server="http://your-http-proxy.com:8000")

    # This config applies only to this specific run, not the browser (if one were used elsewhere)
    run_config_with_proxy = CrawlerRunConfig(
        url="https://api.ipify.org?format=json", # A simple API to get IP
        proxy_config=proxy_details,
        verbose=True
    )

    # Initialize crawler with an HTTP-based strategy
    http_strategy = AsyncHTTPCrawlerStrategy()
    
    print(f"Attempting HTTP GET for {run_config_with_proxy.url} via proxy: {proxy_details.server}")
    print("NOTE: This example will likely fail or show your direct IP if the placeholder proxy is not replaced.")

    async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
        result = await crawler.arun(config=run_config_with_proxy)
        if result.success:
            print(f"Successfully fetched {result.url}.")
            print(f"Response content: {result.html}")
            # Parse result.html (JSON) to check if 'ip' matches your proxy's IP.
        else:
            print(f"Failed to fetch via proxy: {result.error_message}")

if __name__ == "__main__":
    # asyncio.run(http_crawler_with_proxy())
    print("Skipping http_crawler_with_proxy example as it requires a live proxy server.")
    print("To run, replace placeholder proxy details and uncomment asyncio.run().")

```

---
## 4. `BrowserConfig` Examples

### 4.1. Basic Initialization

#### 4.1.1. Example: Default initialization of `BrowserConfig`.
This uses default settings: Chromium, headless mode, standard viewport.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def default_browser_config_init():
    browser_cfg = BrowserConfig()
    print(f"Default BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")

    # Demonstrate its use
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with default browser config. Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(default_browser_config_init())
```

---
#### 4.1.2. Example: Specifying `browser_type` as "firefox".
Shows how to launch Firefox instead of the default Chromium.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def firefox_browser_config():
    browser_cfg = BrowserConfig(browser_type="firefox", headless=True) # Ensure headless for automation
    print(f"Firefox BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.browser_type == "firefox"

    # Demonstrate its use
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with Firefox. Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed with Firefox: {result.error_message}")
            print("Ensure Firefox is installed and accessible by Playwright.")

if __name__ == "__main__":
    asyncio.run(firefox_browser_config())
```

---
#### 4.1.3. Example: Specifying `browser_type` as "webkit".
Shows how to launch WebKit (Safari's engine).

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def webkit_browser_config():
    browser_cfg = BrowserConfig(browser_type="webkit", headless=True) # Ensure headless
    print(f"WebKit BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.browser_type == "webkit"

    # Demonstrate its use
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with WebKit. Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed with WebKit: {result.error_message}")
            print("Ensure WebKit is installed and accessible by Playwright.")

if __name__ == "__main__":
    asyncio.run(webkit_browser_config())
```

---
#### 4.1.4. Example: Running `BrowserConfig` in headed mode (`headless=False`) for visual debugging.
Launches a visible browser window. Useful for observing the crawl.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def headed_browser_config():
    # Note: In some environments (like CI/CD), headed mode might not be possible or may require Xvfb.
    try:
        browser_cfg = BrowserConfig(headless=False) # Visible browser window
        print(f"Headed BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
        assert not browser_cfg.headless

        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            print("Attempting to launch a visible browser to crawl example.com...")
            print("The browser window should appear briefly.")
            result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig(page_timeout=10000)) # 10s timeout
            if result.success:
                print(f"Crawled successfully with a visible browser. Page title: {result.metadata.get('title')}")
            else:
                print(f"Crawl failed with visible browser: {result.error_message}")
    except Exception as e:
        print(f"Could not run headed browser example (this is common in restricted environments): {e}")
        print("Skipping headed browser test.")


if __name__ == "__main__":
    # This example might require a display server.
    # asyncio.run(headed_browser_config())
    print("Skipping headed_browser_config example. Uncomment to run if you have a display server.")
```

---
### 4.2. Browser Mode and Management

#### 4.2.1. Example: Using `browser_mode="builtin"` for Playwright's managed CDP.
This mode is for connecting to Playwright's own managed browser instance via CDP, typically for advanced control or specific scenarios.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

# Note: 'builtin' mode often implies a more complex setup where the browser is
# launched and managed by Playwright in a specific way, and Crawl4ai connects to it.
# For a simple demonstration, it might behave similarly to 'dedicated' if not carefully orchestrated.
# The key is `use_managed_browser=True` and letting the BrowserManager handle CDP details.

async def builtin_browser_mode_config():
    # 'builtin' mode primarily signals that the browser lifecycle is managed
    # internally, often implying connection to a persistent browser instance.
    # `use_managed_browser` is True implicitly.
    # For this test, the cdp_url will be set by the internal ManagedBrowser.
    
    browser_cfg = BrowserConfig(
        browser_mode="builtin", 
        headless=True,
        verbose=True
    )
    print(f"Builtin mode BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.browser_mode == "builtin"
    assert browser_cfg.use_managed_browser # Should be True for builtin

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # The crawler will internally manage the browser and connect via CDP
        print("Crawler will use its managed browser instance via CDP (builtin mode).")
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with 'builtin' browser mode. Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed with 'builtin' browser mode: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(builtin_browser_mode_config())
```

---
#### 4.2.2. Example: Using `browser_mode="docker"` (conceptual outline, actual Docker setup is external).
This mode implies Crawl4ai connecting to a browser running inside a Docker container. Actual Docker setup is beyond this example's scope.

```python
import asyncio
from crawl4ai import BrowserConfig # AsyncWebCrawler, CrawlerRunConfig (not directly run here)

# This is a conceptual outline. Running this requires:
# 1. A Docker image with a browser and Playwright server (or just browser with CDP enabled).
# 2. The Docker container running and exposing the CDP port.
# 3. The `cdp_url` correctly pointing to the Docker container's exposed port.

async def docker_browser_mode_conceptual():
    # Assume Docker container exposes CDP on localhost:9222
    # The cdp_url would be set by the (not-yet-implemented) DockerBrowserStrategy.
    # For now, browser_mode="docker" signals intent; actual connection uses cdp_url.
    
    docker_cdp_url = "ws://localhost:9222/devtools/browser/some-id" # Example CDP URL from Docker
    
    browser_cfg_docker_intent = BrowserConfig(
        browser_mode="docker",
        headless=True # Usually true for Docker
    )
    # When a DockerBrowserStrategy is implemented, it would handle launching/connecting
    # and setting the cdp_url. For now, this mode serves as a placeholder.
    # To actually connect to a Dockerized browser, you'd use 'custom' mode with a cdp_url.
    print(f"Conceptual Docker mode BrowserConfig (intent): {browser_cfg_docker_intent.to_dict(exclude_none=True)}")
    assert browser_cfg_docker_intent.browser_mode == "docker"
    assert browser_cfg_docker_intent.use_managed_browser # Docker mode implies managed connection

    print("\nTo actually connect to a Dockerized browser, you'd typically use:")
    browser_cfg_docker_connect = BrowserConfig(
        browser_mode="custom", # Or 'dedicated' if Crawl4ai itself starts the Docker container
        cdp_url=docker_cdp_url, # The actual CDP endpoint of the browser in Docker
        headless=True 
    )
    print(f"Actual connection to Dockerized browser would use: {browser_cfg_docker_connect.to_dict(exclude_none=True)}")

if __name__ == "__main__":
    asyncio.run(docker_browser_mode_conceptual())
    print("\nNote: 'docker' mode is currently more of an intent. For actual connection to a pre-existing Dockerized browser, use 'custom' mode with its cdp_url.")
```

---
#### 4.2.3. Example: Connecting to an externally managed browser via CDP URL using `browser_mode="custom"` and `cdp_url`.
If you have a browser already running (e.g., manually, or by another tool) and its Chrome DevTools Protocol endpoint is known.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

# This example requires an external Chrome/Chromium instance started with remote debugging enabled.
# E.g., /path/to/chrome --remote-debugging-port=9222 --headless --user-data-dir=/tmp/mychromedata
# Then find the browser's CDP endpoint (e.g., from http://localhost:9222/json/version -> webSocketDebuggerUrl)

async def custom_cdp_connection():
    # Replace with your actual CDP URL. This is a common placeholder.
    # If no browser is running at this CDP, the example will fail.
    external_cdp_url = "ws://localhost:9222/devtools/browser/some-unique-id" 
    
    # For a real test, you must get this from an actual running browser instance.
    # e.g. by navigating to http://localhost:9222/json in another browser
    # and copying the webSocketDebuggerUrl for a "page" type target.
    # Or, if connecting to the browser endpoint, it would be like:
    # external_cdp_url = "ws://localhost:9222/devtools/browser/...." (from /json/version)


    # Using a known CDP URL implies 'custom' management.
    browser_cfg = BrowserConfig(
        cdp_url=external_cdp_url, # This signals to connect to an existing browser
        browser_mode="custom"     # Explicitly set custom mode
    )
    print(f"Custom CDP BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.cdp_url == external_cdp_url
    assert browser_cfg.use_managed_browser # Connecting via CDP means it's "managed" in this context

    print(f"\nAttempting to connect to external browser via CDP: {external_cdp_url}")
    print("Ensure a browser is running with remote debugging enabled on the specified port and path.")
    
    try:
        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
            if result.success:
                print(f"Successfully crawled using external browser. Page title: {result.metadata.get('title')}")
            else:
                print(f"Crawl failed using external browser: {result.error_message}")
    except Exception as e:
        print(f"Failed to connect or crawl with external browser: {e}")
        print("Common reasons: No browser at CDP URL, incorrect CDP URL, network issues.")

if __name__ == "__main__":
    # asyncio.run(custom_cdp_connection())
    print("Skipping custom_cdp_connection example as it requires a pre-configured external browser with CDP.")
    print("To run, start a browser with remote debugging and update 'external_cdp_url'.")
```

---
#### 4.2.4. Example: Enabling a persistent browser context with `use_persistent_context=True` and specifying `user_data_dir`.
Saves browser state (cookies, localStorage, etc.) to a directory, allowing sessions to persist across crawler restarts.

```python
import asyncio
import shutil
from pathlib import Path
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def persistent_context_example():
    # Create a temporary directory for user data
    user_data_path = Path("./temp_crawl4ai_user_data_persistent")
    if user_data_path.exists():
        shutil.rmtree(user_data_path) # Clean up from previous runs
    user_data_path.mkdir(parents=True, exist_ok=True)

    print(f"Using persistent user data directory: {user_data_path.resolve()}")

    browser_cfg_persistent = BrowserConfig(
        use_persistent_context=True,
        user_data_dir=str(user_data_path.resolve()), # Must be an absolute path for Playwright
        headless=True, # Can be False for debugging the persistent state
        verbose=True
    )
    # `use_persistent_context=True` automatically implies `use_managed_browser=True`.
    print(f"Persistent Context BrowserConfig: {browser_cfg_persistent.to_dict(exclude_none=True)}")
    assert browser_cfg_persistent.use_persistent_context
    assert browser_cfg_persistent.user_data_dir == str(user_data_path.resolve())

    # First run: crawl a page, maybe set a cookie (implicitly or explicitly)
    run_config1 = CrawlerRunConfig(url="https://httpbin.org/cookies/set?mycookie=myvalue")
    
    async with AsyncWebCrawler(config=browser_cfg_persistent) as crawler:
        print("\n--- First run: Setting a cookie ---")
        result1 = await crawler.arun(config=run_config1)
        if result1.success:
            print(f"First run to {result1.url} successful.")
            # httpbin.org/cookies/set redirects to /cookies, which shows current cookies
            if "mycookie" in result1.html and "myvalue" in result1.html:
                 print("Cookie 'mycookie=myvalue' likely set and visible in response.")
            else:
                 print("Cookie might not be immediately visible in this response, check next run.")
        else:
            print(f"First run failed: {result1.error_message}")

    # Second run: with the same BrowserConfig (and thus same user_data_dir)
    # The cookie "mycookie" should persist.
    run_config2 = CrawlerRunConfig(url="https://httpbin.org/cookies") # This page shows received cookies

    async with AsyncWebCrawler(config=browser_cfg_persistent) as crawler: # Reuses the same persistent context
        print("\n--- Second run: Checking for persisted cookie ---")
        result2 = await crawler.arun(config=run_config2)
        if result2.success:
            print(f"Second run to {result2.url} successful.")
            print(f"Response content (cookies): {result2.html}")
            if "mycookie" in result2.html and "myvalue" in result2.html:
                print("SUCCESS: Cookie 'mycookie=myvalue' persisted across runs!")
            else:
                print("FAILURE: Cookie did not persist or was not found.")
        else:
            print(f"Second run failed: {result2.error_message}")

    # Clean up the temporary directory
    if user_data_path.exists():
        shutil.rmtree(user_data_path)
    print(f"\nCleaned up user data directory: {user_data_path.resolve()}")

if __name__ == "__main__":
    asyncio.run(persistent_context_example())
```

---
#### 4.2.5. Example: Specifying Chrome browser channel using `channel="chrome"`.
Launches the stable Chrome browser if installed, instead of Chromium.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def chrome_channel_config():
    # This requires Google Chrome (stable channel) to be installed.
    # Playwright will attempt to find it.
    browser_cfg = BrowserConfig(
        browser_type="chromium", # Still 'chromium' as base type for Playwright
        channel="chrome",        # Specify 'chrome' channel
        headless=True
    )
    print(f"Chrome Channel BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.channel == "chrome"

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print("Attempting to crawl with Google Chrome (stable channel)...")
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with Chrome channel. Page title: {result.metadata.get('title')}")
            # To truly verify, one might check specific Chrome-only features or detailed UA string,
            # but that's beyond simple config demonstration.
        else:
            print(f"Crawl failed with Chrome channel: {result.error_message}")
            print("Ensure Google Chrome (stable) is installed and accessible by Playwright.")

if __name__ == "__main__":
    asyncio.run(chrome_channel_config())
```

---
#### 4.2.6. Example: Specifying Microsoft Edge browser channel using `channel="msedge"`.
Launches Microsoft Edge browser if installed.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def msedge_channel_config():
    # This requires Microsoft Edge (stable channel) to be installed.
    browser_cfg = BrowserConfig(
        browser_type="chromium", # Base type for Playwright
        channel="msedge",        # Specify 'msedge' channel
        headless=True
    )
    print(f"MS Edge Channel BrowserConfig: {browser_cfg.to_dict(exclude_none=True)}")
    assert browser_cfg.channel == "msedge"

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print("Attempting to crawl with Microsoft Edge (stable channel)...")
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Crawled successfully with MS Edge channel. Page title: {result.metadata.get('title')}")
        else:
            print(f"Crawl failed with MS Edge channel: {result.error_message}")
            print("Ensure Microsoft Edge (stable) is installed and accessible by Playwright.")

if __name__ == "__main__":
    asyncio.run(msedge_channel_config())
```

---
### 4.3. Proxy Configuration in `BrowserConfig`

#### 4.3.1. Example: Setting a simple proxy string using the `proxy` parameter.
This is a shorthand for providing proxy details directly as a string.

```python
import asyncio
from crawl4ai import BrowserConfig #, AsyncWebCrawler, CrawlerRunConfig

async def simple_proxy_string_in_browserconfig():
    # Format: "scheme://user:pass@host:port" or "scheme://host:port"
    # This is a non-functional placeholder.
    proxy_server_string = "http://user:password@proxy.example.com:8080" 
    
    browser_cfg = BrowserConfig(proxy=proxy_server_string, headless=True)
    print(f"BrowserConfig with simple proxy string: {browser_cfg.proxy}")
    
    # The `proxy` string is parsed internally into Playwright's proxy format.
    # To demonstrate its effect, one would typically use it with AsyncWebCrawler:
    # async with AsyncWebCrawler(config=browser_cfg) as crawler:
    #     result = await crawler.arun(url="https://httpbin.org/ip")
    #     print(result.html) # Should show proxy's IP
    
    print(f"Proxy server string set to: {browser_cfg.proxy}")
    # Note: The ProxyConfig object is not explicitly created or exposed when using the 'proxy' string directly.
    # If you need to access ProxyConfig attributes, use the `proxy_config` parameter with a ProxyConfig object.

if __name__ == "__main__":
    asyncio.run(simple_proxy_string_in_browserconfig())
```

---
#### 4.3.2. Example: Using a detailed `ProxyConfig` object via the `proxy_config` parameter.
Provides more structured control over proxy settings.

```python
import asyncio
from crawl4ai import BrowserConfig, ProxyConfig #, AsyncWebCrawler, CrawlerRunConfig

async def detailed_proxy_object_in_browserconfig():
    # This is a non-functional placeholder.
    proxy_obj = ProxyConfig(
        server="http://anotherproxy.example.com:3128",
        username="proxy_user_obj",
        password="proxy_password_obj"
    )
    
    browser_cfg = BrowserConfig(proxy_config=proxy_obj, headless=True)
    print(f"BrowserConfig with ProxyConfig object: {browser_cfg.proxy_config.to_dict()}") # type: ignore
    assert browser_cfg.proxy_config.server == "http://anotherproxy.example.com:3128" # type: ignore

    # To demonstrate its effect:
    # async with AsyncWebCrawler(config=browser_cfg) as crawler:
    #     result = await crawler.arun(url="https://httpbin.org/ip")
    #     print(result.html) # Should show proxy's IP
    print("ProxyConfig object set. For a live test, replace placeholder with a real proxy.")

if __name__ == "__main__":
    asyncio.run(detailed_proxy_object_in_browserconfig())
```

---
### 4.4. Viewport and Display Settings

#### 4.4.1. Example: Setting custom viewport dimensions using `viewport_width` and `viewport_height`.
Controls the initial size of the browser window/viewport.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def custom_viewport_dimensions():
    browser_cfg = BrowserConfig(
        viewport_width=1920,
        viewport_height=1080,
        headless=True
    )
    print(f"BrowserConfig with custom viewport: Width={browser_cfg.viewport_width}, Height={browser_cfg.viewport_height}")
    assert browser_cfg.viewport_width == 1920
    assert browser_cfg.viewport_height == 1080

    # Demonstrate by checking JavaScript window dimensions
    run_config = CrawlerRunConfig(
        js_code="JSON.stringify({width: window.innerWidth, height: window.innerHeight})"
    )
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url="https://example.com", config=run_config)
        if result.success and result.js_execution_result:
            dims = result.js_execution_result
            print(f"JS reported dimensions: {dims}")
            # Note: Playwright viewport might differ slightly from window.innerWidth/Height due to scrollbars etc.
            # This example primarily shows the config is passed.
            assert dims.get("width") is not None # Check if JS executed
        else:
            print(f"Crawl or JS execution failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(custom_viewport_dimensions())
```

---
#### 4.4.2. Example: Setting viewport dimensions using the `viewport` dictionary parameter.
An alternative way to set viewport size, overriding individual width/height parameters if both are set.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig
import json

async def viewport_dict_parameter():
    viewport_dimensions = {"width": 800, "height": 600}
    browser_cfg = BrowserConfig(
        viewport=viewport_dimensions,
        headless=True,
        # If viewport_width/height were also set, 'viewport' dict takes precedence
        viewport_width=1200 # This will be overridden by the viewport dict
    )
    print(f"BrowserConfig with viewport dict: {browser_cfg.viewport}")
    print(f"Effective viewport_width: {browser_cfg.viewport_width}") # Should be 800
    print(f"Effective viewport_height: {browser_cfg.viewport_height}") # Should be 600

    assert browser_cfg.viewport_width == 800
    assert browser_cfg.viewport_height == 600

    # Demonstrate by checking JavaScript window dimensions
    run_config = CrawlerRunConfig(
        js_code="JSON.stringify({width: window.innerWidth, height: window.innerHeight})"
    )
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url="https://example.com", config=run_config)
        if result.success and result.js_execution_result:
            dims = result.js_execution_result
            print(f"JS reported dimensions: {dims}")
            # Similar to above, exact match might vary due to browser chrome
            assert dims.get("width") is not None
        else:
            print(f"Crawl or JS execution failed: {result.error_message}")
            
if __name__ == "__main__":
    asyncio.run(viewport_dict_parameter())
```

---
### 4.5. Downloads and Storage State

#### 4.5.1. Example: Enabling file downloads with `accept_downloads=True` and specifying `downloads_path`.
Allows the browser to download files triggered by page interactions or navigations.

```python
import asyncio
import os
import shutil
from pathlib import Path
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def enable_file_downloads():
    # Create a temporary directory for downloads
    temp_downloads_dir = Path("./temp_crawl4ai_downloads")
    if temp_downloads_dir.exists():
        shutil.rmtree(temp_downloads_dir)
    temp_downloads_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Downloads will be saved to: {temp_downloads_dir.resolve()}")

    browser_cfg = BrowserConfig(
        accept_downloads=True,
        downloads_path=str(temp_downloads_dir.resolve()),
        headless=True
    )
    print(f"BrowserConfig for downloads: accept_downloads={browser_cfg.accept_downloads}, path={browser_cfg.downloads_path}")
    assert browser_cfg.accept_downloads
    assert browser_cfg.downloads_path == str(temp_downloads_dir.resolve())
    
    # A small, publicly downloadable file (e.g., a sample text file or small image)
    # This URL directly triggers a download for a sample PDF from an educational site
    download_trigger_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

    run_config = CrawlerRunConfig(
        url=download_trigger_url,
        # For direct downloads, Playwright often handles it without JS click.
        # If it was a button: js_code="document.querySelector('#downloadButton').click();"
        # We also need to give it time for the download to complete.
        page_timeout=30000 # 30 seconds for download
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to download from: {download_trigger_url}")
        result = await crawler.arun(config=run_config)
        
        if result.success:
            print("Crawl part successful. Checking for downloaded files...")
            if result.downloaded_files:
                print(f"Files downloaded to {browser_cfg.downloads_path}:")
                for file_path in result.downloaded_files:
                    print(f"  - {file_path} (Size: {Path(file_path).stat().st_size} bytes)")
                assert len(result.downloaded_files) > 0
            else:
                print("No files reported as downloaded by the crawler. The page might not have triggered a download as expected, or the download event was missed.")
        else:
            print(f"Crawl failed: {result.error_message}")
            
    # Clean up
    if temp_downloads_dir.exists():
        shutil.rmtree(temp_downloads_dir)
    print(f"Cleaned up downloads directory: {temp_downloads_dir.resolve()}")

if __name__ == "__main__":
    asyncio.run(enable_file_downloads())
```

---
#### 4.5.2. Example: Loading browser state (cookies, localStorage) from a file path using `storage_state`.
Restores a previously saved browser session.

```python
import asyncio
import json
import shutil
from pathlib import Path
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def load_storage_state_from_file():
    # Create a dummy storage state file for this example
    storage_state_path = Path("./temp_crawl4ai_storage_state.json")
    dummy_storage_state = {
        "cookies": [{
            "name": "persistent_cookie", "value": "loaded_from_file",
            "domain": "httpbin.org", "path": "/", "expires": -1,
            "httpOnly": False, "secure": False, "sameSite": "Lax"
        }],
        "origins": [{
            "origin": "https://httpbin.org",
            "localStorage": [{"name": "persistent_ls_item", "value": "loaded_from_file_ls"}]
        }]
    }
    with open(storage_state_path, 'w') as f:
        json.dump(dummy_storage_state, f)

    print(f"Using storage state from file: {storage_state_path.resolve()}")

    browser_cfg = BrowserConfig(
        storage_state=str(storage_state_path.resolve()),
        headless=True,
        verbose=True
    )
    print(f"BrowserConfig with storage_state file: {browser_cfg.storage_state}")
    
    # URL to check cookies and localStorage
    check_url = "https://httpbin.org/anything" 
    # JS to retrieve localStorage (httpbin doesn't show it directly in /anything)
    js_to_get_ls = "JSON.stringify(localStorage.getItem('persistent_ls_item'))"

    run_config = CrawlerRunConfig(url=check_url, js_code=js_to_get_ls)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with loaded storage state.")
            response_data = json.loads(result.html) # httpbin.org/anything returns JSON
            
            # Check for cookie
            if "persistent_cookie=loaded_from_file" in response_data.get("headers", {}).get("Cookie", ""):
                print("SUCCESS: Cookie 'persistent_cookie' was loaded and sent!")
            else:
                print(f"Cookie not found in request headers. Cookies: {response_data.get('headers', {}).get('Cookie')}")

            # Check for localStorage item (via JS execution result)
            if result.js_execution_result == '"loaded_from_file_ls"': # JS returns JSON string
                print("SUCCESS: localStorage item 'persistent_ls_item' was loaded!")
            else:
                print(f"localStorage item not found or incorrect. JS result: {result.js_execution_result}")
        else:
            print(f"Crawl failed: {result.error_message}")

    # Clean up
    if storage_state_path.exists():
        storage_state_path.unlink()
    print(f"\nCleaned up storage state file: {storage_state_path.resolve()}")

if __name__ == "__main__":
    asyncio.run(load_storage_state_from_file())
```

---
#### 4.5.3. Example: Loading browser state from an in-memory dictionary using `storage_state`.
Allows providing cookies and localStorage directly as a Python dictionary.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def load_storage_state_from_dict():
    in_memory_storage_state = {
        "cookies": [{
            "name": "mem_cookie", "value": "loaded_from_dict",
            "url": "https://httpbin.org" # More robust to use 'url' or 'domain'/'path'
        }],
        "origins": [{
            "origin": "https://httpbin.org",
            "localStorage": [{"name": "mem_ls_item", "value": "loaded_from_dict_ls"}]
        }]
    }
    print(f"Using in-memory storage state: {in_memory_storage_state}")

    browser_cfg = BrowserConfig(
        storage_state=in_memory_storage_state,
        headless=True,
        verbose=True
    )
    
    check_url = "https://httpbin.org/anything"
    js_to_get_ls = "JSON.stringify(localStorage.getItem('mem_ls_item'))"
    run_config = CrawlerRunConfig(url=check_url, js_code=js_to_get_ls)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with in-memory storage state.")
            response_data = json.loads(result.html)
            
            if "mem_cookie=loaded_from_dict" in response_data.get("headers", {}).get("Cookie", ""):
                print("SUCCESS: Cookie 'mem_cookie' was loaded and sent!")
            else:
                print(f"Cookie not found in request headers. Cookies: {response_data.get('headers', {}).get('Cookie')}")

            if result.js_execution_result == '"loaded_from_dict_ls"':
                print("SUCCESS: localStorage item 'mem_ls_item' was loaded!")
            else:
                print(f"localStorage item not found or incorrect. JS result: {result.js_execution_result}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(load_storage_state_from_dict())
```

---
### 4.6. Security and Scripting Control

#### 4.6.1. Example: Enforcing HTTPS error checks by setting `ignore_https_errors=False`.
By default, Crawl4ai (via Playwright) ignores HTTPS errors. This shows how to enforce them.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def enforce_https_errors():
    browser_cfg = BrowserConfig(
        ignore_https_errors=False, # Default is True
        headless=True
    )
    print(f"BrowserConfig with ignore_https_errors={browser_cfg.ignore_https_errors}")
    assert not browser_cfg.ignore_https_errors
    
    # Use a site with a known SSL issue (e.g., expired, self-signed)
    # expired.badssl.com is a good test site for this
    # For safety in automated tests, we won't hit a live "bad" SSL site by default.
    # test_url_with_ssl_issue = "https://expired.badssl.com/"
    test_url_good_ssl = "https://example.com"


    print(f"Attempting to crawl {test_url_good_ssl} (should succeed).")
    run_config = CrawlerRunConfig(url=test_url_good_ssl)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Successfully crawled {test_url_good_ssl} with HTTPS errors NOT ignored.")
        else:
            print(f"Crawl failed for {test_url_good_ssl}: {result.error_message}")

    # To test the error case:
    # print(f"\nAttempting to crawl {test_url_with_ssl_issue} (should fail due to SSL error).")
    # run_config_bad_ssl = CrawlerRunConfig(url=test_url_with_ssl_issue)
    # async with AsyncWebCrawler(config=browser_cfg) as crawler:
    #     result_bad = await crawler.arun(config=run_config_bad_ssl)
    #     if not result_bad.success and "net::ERR_CERT_DATE_INVALID" in result_bad.error_message: # Or similar error
    #         print(f"SUCCESS: Crawl failed as expected for {test_url_with_ssl_issue} due to SSL error: {result_bad.error_message[:100]}...")
    #     elif result_bad.success:
    #         print(f"UNEXPECTED: Crawl succeeded for {test_url_with_ssl_issue}, SSL error might not have been caught.")
    #     else:
    #         print(f"Crawl failed for {test_url_with_ssl_issue} for other reasons: {result_bad.error_message}")
    print("\nNote: Actual test for HTTPS error enforcement requires a site like expired.badssl.com.")


if __name__ == "__main__":
    asyncio.run(enforce_https_errors())
```

---
#### 4.6.2. Example: Disabling JavaScript execution in pages by setting `java_script_enabled=False`.
Prevents JavaScript from running, which can speed up crawls but might break sites reliant on JS.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def disable_javascript():
    browser_cfg = BrowserConfig(
        java_script_enabled=False, # Default is True
        headless=True
    )
    print(f"BrowserConfig with java_script_enabled={browser_cfg.java_script_enabled}")
    assert not browser_cfg.java_script_enabled
    
    # A page that uses JS to modify content
    # httpbin.org/html includes a simple script
    test_url = "https://httpbin.org/html" 
    run_config = CrawlerRunConfig(url=test_url)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to crawl {test_url} with JavaScript disabled.")
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Successfully crawled {test_url}.")
            # Look for signs that JS did NOT run.
            # The sample JS on httpbin.org/html adds "Hello, world!" to an h1.
            # If JS is disabled, this text should be absent or different.
            # The original h1 on httpbin.org/html is "Herman Melville - Moby-Dick"
            if "Moby-Dick" in result.html and "Hello, world!" not in result.html:
                print("SUCCESS: JavaScript seems to have been disabled (JS-added content not found).")
            else:
                print("JavaScript execution state is inconclusive from this page's content.")
                # print(result.html) # For debugging
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(disable_javascript())
```

---
### 4.7. Headers, Cookies, and User Agent Customization

#### 4.7.1. Example: Adding a list of custom `cookies` to be set in the browser context.
These cookies will be sent with all requests made by this browser context.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def custom_cookies_browser_config():
    custom_cookies_list = [
        {"name": "my_custom_cookie", "value": "cookie_value_123", "url": "https://httpbin.org"},
        {"name": "another_cookie", "value": "more_data", "domain": ".httpbin.org", "path": "/"}
    ]
    browser_cfg = BrowserConfig(cookies=custom_cookies_list, headless=True)
    print(f"BrowserConfig with custom cookies: {browser_cfg.cookies}")
    
    # httpbin.org/cookies shows cookies sent by the client
    run_config = CrawlerRunConfig(url="https://httpbin.org/cookies")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with custom cookies.")
            response_data = json.loads(result.html) # httpbin returns JSON
            print(f"Cookies received by server: {response_data.get('cookies')}")
            assert "my_custom_cookie" in response_data.get("cookies", {})
            assert response_data.get("cookies", {}).get("my_custom_cookie") == "cookie_value_123"
            assert "another_cookie" in response_data.get("cookies", {})
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(custom_cookies_browser_config())
```

---
#### 4.7.2. Example: Setting default `headers` for all requests made within the browser context.
These headers will be added to every HTTP request.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def default_headers_browser_config():
    custom_headers_dict = {
        "X-Custom-Header": "Crawl4AI-Test",
        "Accept-Language": "de-DE,de;q=0.9" # Example: German language preference
    }
    browser_cfg = BrowserConfig(headers=custom_headers_dict, headless=True)
    print(f"BrowserConfig with default headers: {browser_cfg.headers}")
    
    # httpbin.org/headers shows headers received by the server
    run_config = CrawlerRunConfig(url="https://httpbin.org/headers")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with custom default headers.")
            response_data = json.loads(result.html) # httpbin returns JSON
            print(f"Headers received by server (excerpt):")
            received_headers = response_data.get("headers", {})
            for key, value in custom_headers_dict.items():
                print(f"  {key}: {received_headers.get(key)}")
                assert received_headers.get(key) == value
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(default_headers_browser_config())
```

---
#### 4.7.3. Example: Setting a specific `user_agent` string.
Overrides the default Playwright user agent.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def specific_user_agent_config():
    my_ua_string = "MyCustomCrawler/1.0 (compatible; MyBot/0.1; +http://mybot.example.com)"
    browser_cfg = BrowserConfig(user_agent=my_ua_string, headless=True)
    print(f"BrowserConfig with specific User-Agent: {browser_cfg.user_agent}")
    assert browser_cfg.user_agent == my_ua_string
    
    # httpbin.org/user-agent shows the User-Agent header received by the server
    run_config = CrawlerRunConfig(url="https://httpbin.org/user-agent")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with specific User-Agent.")
            response_data = json.loads(result.html)
            print(f"User-Agent received by server: {response_data.get('user-agent')}")
            assert response_data.get('user-agent') == my_ua_string
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(specific_user_agent_config())
```

---
#### 4.7.4. Example: Generating a random `user_agent` by setting `user_agent_mode="random"`.
Uses the built-in user agent generator to pick a random, valid user agent.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def random_user_agent_config():
    # user_agent_mode="random" will use ValidUAGenerator by default
    browser_cfg = BrowserConfig(user_agent_mode="random", headless=True, verbose=True)
    # The actual user_agent string is generated upon BrowserConfig initialization if mode is random.
    print(f"BrowserConfig with random User-Agent mode. Generated UA: {browser_cfg.user_agent}")
    assert browser_cfg.user_agent is not None 
    assert browser_cfg.user_agent != BrowserConfig().user_agent # Should be different from default
    
    run_config = CrawlerRunConfig(url="https://httpbin.org/user-agent")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with a random User-Agent.")
            response_data = json.loads(result.html)
            print(f"User-Agent received by server: {response_data.get('user-agent')}")
            assert response_data.get('user-agent') == browser_cfg.user_agent # Check if it matches the one set
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(random_user_agent_config())
```

---
#### 4.7.5. Example: Customizing random user agent generation using `user_agent_generator_config` (e.g., specifying device type or OS).
Allows fine-tuning the type of random user agent generated.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def custom_random_user_agent_config():
    # Example: Generate a random user agent for a Linux Desktop
    ua_gen_config = {"device_type": "desktop", "os_name": "linux"}
    
    browser_cfg = BrowserConfig(
        user_agent_mode="random",
        user_agent_generator_config=ua_gen_config,
        headless=True,
        verbose=True
    )
    generated_ua = browser_cfg.user_agent
    print(f"BrowserConfig with custom random UA generation. Config: {ua_gen_config}")
    print(f"Generated UA: {generated_ua}")
    
    # Basic check if the UA string seems plausible for Linux
    assert "Linux" in generated_ua or "X11" in generated_ua
    
    run_config = CrawlerRunConfig(url="https://httpbin.org/user-agent")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url} with custom random User-Agent.")
            response_data = json.loads(result.html)
            print(f"User-Agent received by server: {response_data.get('user-agent')}")
            assert response_data.get('user-agent') == generated_ua
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(custom_random_user_agent_config())
```

---
#### 4.7.6. Example: Demonstrating `BrowserConfig` automatically setting `sec-ch-ua` client hint headers.
`BrowserConfig` automatically derives and sets appropriate `Sec-CH-UA` client hint headers based on the `user_agent`.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def sec_ch_ua_header_demonstration():
    # Using a specific User-Agent that would imply certain client hints
    # This UA is for Chrome 116 on Linux
    ua_string = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
    
    browser_cfg = BrowserConfig(user_agent=ua_string, headless=True, verbose=True)
    print(f"BrowserConfig User-Agent: {browser_cfg.user_agent}")
    print(f"Automatically generated Sec-CH-UA client hint: {browser_cfg.browser_hint}")
    
    # Expected client hint might look something like:
    # '"Chromium";v="116", "Not)A;Brand";v="24", "Google Chrome";v="116"'
    # The exact value depends on the UAGen library's parsing of the UA.
    assert "Chromium" in browser_cfg.browser_hint or "Google Chrome" in browser_cfg.browser_hint
    assert "116" in browser_cfg.browser_hint

    run_config = CrawlerRunConfig(url="https://httpbin.org/headers")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawled {result.url}. Checking received headers by server...")
            response_data = json.loads(result.html)
            received_headers = response_data.get("headers", {})
            
            print(f"  User-Agent: {received_headers.get('User-Agent')}")
            print(f"  Sec-Ch-Ua: {received_headers.get('Sec-Ch-Ua')}") # Case-insensitive matching might be needed for real server
            
            # Check if the Sec-CH-UA header set by BrowserConfig was received
            # Note: httpbin.org might not show all client hints perfectly, or Playwright might override some.
            # This primarily tests that BrowserConfig correctly *sets* it for Playwright.
            # The actual sent header can depend on Playwright's behavior with that browser version.
            if browser_cfg.browser_hint.strip('"') in received_headers.get('Sec-Ch-Ua', '').strip('"'):
                 print("SUCCESS: Sec-CH-UA client hint seems to be correctly passed through.")
            else:
                 print("NOTE: Sec-CH-UA might differ slightly due to Playwright/browser behavior or httpbin.org limitations.")
                 print(f"   Expected hint from BrowserConfig: {browser_cfg.browser_hint}")

        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(sec_ch_ua_header_demonstration())
```

---
### 4.8. Performance and Other Browser Settings

#### 4.8.1. Example: Using `text_mode=True` to attempt disabling images and rich content for faster text-focused crawls.
This mode aims to block resources like images, fonts, and potentially some scripts to speed up loading for text extraction.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def text_mode_config():
    browser_cfg = BrowserConfig(text_mode=True, headless=True, verbose=True)
    print(f"BrowserConfig with text_mode: {browser_cfg.text_mode}")
    assert browser_cfg.text_mode
    
    # A page with images
    run_config = CrawlerRunConfig(url="https://example.com") # example.com has an IANA logo

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to crawl {run_config.url} in text_mode.")
        # We'll also capture network requests to see if image requests are blocked.
        result = await crawler.arun(config=run_config.clone(capture_network_requests=True))
        
        if result.success:
            print(f"Successfully crawled {run_config.url} in text_mode.")
            
            image_requests_found = False
            if result.network_requests:
                for req in result.network_requests:
                    if req.get("event_type") == "request" and req.get("resource_type") == "image":
                        image_requests_found = True
                        print(f"Found image request (unexpected in text_mode): {req.get('url')}")
                        break
            
            if not image_requests_found:
                print("SUCCESS: No image requests were detected, as expected in text_mode.")
            else:
                print("WARNING: Image requests were detected. Text_mode might not have fully blocked them for this site/browser.")
            
            # Check if images are absent in the rendered HTML (might be harder to verify reliably)
            # print(f"HTML (first 500 chars):\n{result.html[:500]}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(text_mode_config())
```

---
#### 4.8.2. Example: Using `light_mode=True` to disable certain background browser features for performance.
`light_mode` applies a set of browser flags (defined in `BROWSER_DISABLE_OPTIONS` in `browser_manager.py`) to reduce resource usage.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def light_mode_config():
    browser_cfg = BrowserConfig(light_mode=True, headless=True, verbose=True)
    print(f"BrowserConfig with light_mode: {browser_cfg.light_mode}")
    print(f"Extra args applied by light_mode (subset shown): {browser_cfg.extra_args[:5]}...")
    assert browser_cfg.light_mode
    # Check if some known light_mode flags are present in extra_args
    assert "--disable-background-networking" in browser_cfg.extra_args 
    
    run_config = CrawlerRunConfig(url="https://example.com")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to crawl {run_config.url} in light_mode.")
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Successfully crawled {run_config.url} in light_mode.")
            # Effect of light_mode is on browser resource usage, not directly visible in content typically.
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(light_mode_config())
```

---
#### 4.8.3. Example: Passing additional command-line `extra_args` to the browser launcher.
Allows fine-grained control over browser behavior by passing Chromium/Firefox specific flags.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def extra_args_config():
    # Example: Disable GPU (often useful in headless environments)
    # and set a custom window size (though viewport is preferred for content area)
    custom_args = [
        "--disable-gpu", 
        "--window-size=800,600" # Note: viewport_width/height is preferred for content area
    ]
    browser_cfg = BrowserConfig(extra_args=custom_args, headless=True) # Usually headless with these args
    print(f"BrowserConfig with extra_args: {browser_cfg.extra_args}")
    assert "--disable-gpu" in browser_cfg.extra_args
    
    run_config = CrawlerRunConfig(url="https://example.com")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to crawl {run_config.url} with extra browser arguments.")
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Successfully crawled {run_config.url} with extra arguments.")
            # Verifying effect of these args often requires deeper inspection or specific test pages.
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(extra_args_config())
```

---
#### 4.8.4. Example: Setting a custom `debugging_port` and `host` for the browser's remote debugging.
Useful if the default port (9222) is in use or for specific network configurations.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

# Note: This example primarily shows config. It might be hard to verify the port
# change without external tools unless use_managed_browser is also True
# and we try to connect to the new CDP URL.

async def custom_debugging_port_host():
    custom_port = 9333
    custom_host = "127.0.0.1" # Often 'localhost' or '127.0.0.1'

    # This is most relevant for use_managed_browser=True or browser_mode='builtin'
    # as it tells the ManagedBrowser which port to launch on.
    browser_cfg = BrowserConfig(
        debugging_port=custom_port,
        host=custom_host,
        headless=True,
        use_managed_browser=True, # To see the effect of debugging_port
        verbose=True
    )
    print(f"BrowserConfig with custom debugging_port={browser_cfg.debugging_port} and host={browser_cfg.host}")
    assert browser_cfg.debugging_port == custom_port
    assert browser_cfg.host == custom_host

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to launch managed browser on {custom_host}:{custom_port} and crawl.")
        # The ManagedBrowser instance within AsyncWebCrawler will use these settings.
        # If successful, it means the browser launched on the custom port and CDP connection was made.
        result = await crawler.arun(url="https://example.com", config=CrawlerRunConfig())
        if result.success:
            print(f"Successfully crawled. Managed browser likely used {custom_host}:{custom_port}.")
        else:
            print(f"Crawl failed: {result.error_message}. This could be due to port conflict or other launch issues.")
            print("Ensure the custom port is available.")
            
if __name__ == "__main__":
    asyncio.run(custom_debugging_port_host())
```

---
#### 4.8.5. Example: Enabling verbose logging for browser operations via `verbose=True`.
Provides more detailed output from the `AsyncWebCrawler` and underlying strategies.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def verbose_browser_logging():
    # Set verbose=True in BrowserConfig
    # This will be picked up by the AsyncLogger if not overridden by CrawlerRunConfig
    browser_cfg = BrowserConfig(verbose=True, headless=True)
    print(f"BrowserConfig verbose setting: {browser_cfg.verbose}")
    
    # CrawlerRunConfig can also have a verbose setting, which might take precedence for that run.
    # Here, we don't set it in CrawlerRunConfig, so BrowserConfig's verbose should apply.
    run_config = CrawlerRunConfig(url="https://example.com")

    print("\nRunning crawl with verbose=True in BrowserConfig. Expect more detailed logs.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # The logger used by the crawler will inherit verbosity from browser_cfg
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"\nCrawl successful. Verbose logs should have been printed during the process.")
        else:
            print(f"\nCrawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(verbose_browser_logging())
```

---
#### 4.8.6. Example: Using `sleep_on_close=True` to pause before the browser fully closes (useful for debugging).
This is useful when `headless=False` to inspect the final state of the browser before it closes.

```python
import asyncio
from crawl4ai import BrowserConfig, AsyncWebCrawler, CrawlerRunConfig

async def sleep_on_close_example():
    # This is most effective with headless=False
    # In a script, the pause might not be very noticeable if headless=True
    browser_cfg = BrowserConfig(
        sleep_on_close=True, 
        headless=False, # Set to False to see the browser window pause
        verbose=True
    )
    print(f"BrowserConfig with sleep_on_close={browser_cfg.sleep_on_close}")
    
    run_config = CrawlerRunConfig(url="https://example.com")

    print("\nRunning crawl. Browser window should appear and pause for a moment before closing if headless=False.")
    print("(If headless=True, the effect is a slight delay before script termination).")
    try:
        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            result = await crawler.arun(config=run_config)
            if result.success:
                print(f"Crawl successful. Browser should have paused before closing (if visible).")
            else:
                print(f"Crawl failed: {result.error_message}")
    except Exception as e:
        print(f"Could not run headed browser example for sleep_on_close (common in restricted environments): {e}")
        print("Skipping sleep_on_close headed test.")


if __name__ == "__main__":
    # asyncio.run(sleep_on_close_example())
    print("Skipping sleep_on_close_example. Uncomment to run, preferably with headless=False if you have a display.")
```

---
### 4.9. `BrowserConfig` Utility Methods

#### 4.9.1. Example: Creating `BrowserConfig` from a dictionary of keyword arguments using `BrowserConfig.from_kwargs()`.
Useful for creating config objects dynamically.

```python
import asyncio
from crawl4ai import BrowserConfig

async def browserconfig_from_kwargs():
    kwargs = {
        "browser_type": "firefox",
        "headless": False,
        "viewport_width": 1200,
        "user_agent": "MyTestAgent/1.0"
    }
    browser_cfg = BrowserConfig.from_kwargs(kwargs)
    
    print("BrowserConfig created from_kwargs:")
    print(f"  Browser Type: {browser_cfg.browser_type}")
    print(f"  Headless: {browser_cfg.headless}")
    print(f"  Viewport Width: {browser_cfg.viewport_width}")
    print(f"  User Agent: {browser_cfg.user_agent}")

    assert browser_cfg.browser_type == "firefox"
    assert not browser_cfg.headless
    assert browser_cfg.viewport_width == 1200
    assert browser_cfg.user_agent == "MyTestAgent/1.0"

if __name__ == "__main__":
    asyncio.run(browserconfig_from_kwargs())
```

---
#### 4.9.2. Example: Converting `BrowserConfig` instance to a dictionary using `to_dict()`.
Serializes the config object's attributes to a Python dictionary.

```python
import asyncio
from crawl4ai import BrowserConfig

async def browserconfig_to_dict():
    browser_cfg = BrowserConfig(
        browser_type="webkit",
        headless=True,
        proxy="http://proxy.internal:3128",
        verbose=False
    )
    config_dict = browser_cfg.to_dict()

    print("BrowserConfig instance:")
    print(f"  Original object browser_type: {browser_cfg.browser_type}")
    print(f"  Original object proxy: {browser_cfg.proxy}")
    
    print("\nConverted to dictionary:")
    for key, value in config_dict.items():
        # Only print a few for brevity if it's too long
        if key in ["browser_type", "headless", "proxy", "verbose", "user_agent"]:
             print(f"  {key}: {value}")
    
    assert config_dict["browser_type"] == "webkit"
    assert config_dict["headless"] is True
    assert config_dict["proxy"] == "http://proxy.internal:3128"
    assert config_dict["verbose"] is False # Check a default that was changed or not set

if __name__ == "__main__":
    asyncio.run(browserconfig_to_dict())
```

---
#### 4.9.3. Example: Cloning a `BrowserConfig` instance and modifying its `headless` mode using `clone()`.
Creates a new instance with optionally overridden attributes.

```python
import asyncio
from crawl4ai import BrowserConfig

async def browserconfig_clone_modify():
    original_cfg = BrowserConfig(browser_type="chromium", headless=True, viewport_width=1024)
    print(f"Original Config: headless={original_cfg.headless}, viewport_width={original_cfg.viewport_width}")

    # Clone and change headless mode, keep other settings
    cloned_cfg_headed = original_cfg.clone(headless=False)
    print(f"Cloned Config (headed): headless={cloned_cfg_headed.headless}, viewport_width={cloned_cfg_headed.viewport_width}")

    assert original_cfg.headless is True # Original unchanged
    assert cloned_cfg_headed.headless is False # Cloned is modified
    assert cloned_cfg_headed.browser_type == original_cfg.browser_type # Unspecified attributes are copied
    assert cloned_cfg_headed.viewport_width == original_cfg.viewport_width

if __name__ == "__main__":
    asyncio.run(browserconfig_clone_modify())
```

---
#### 4.9.4. Example: Serializing `BrowserConfig` to a JSON-compatible dictionary using `dump()`.
The `dump()` method leverages `to_serializable_dict` for JSON compatibility.

```python
import asyncio
import json
from crawl4ai import BrowserConfig, ProxyConfig

async def browserconfig_dump_json_compatible():
    proxy_obj = ProxyConfig(server="http://jsondump.proxy:1234")
    browser_cfg = BrowserConfig(
        browser_type="firefox",
        headless=False,
        proxy_config=proxy_obj, # Nested object
        extra_args=["--some-flag"]
    )
    
    # dump() calls to_serializable_dict() internally
    dumped_dict = browser_cfg.dump() 

    print("Dumped BrowserConfig (JSON compatible):")
    print(json.dumps(dumped_dict, indent=2))

    # Check if nested ProxyConfig was serialized correctly
    assert dumped_dict["params"]["proxy_config"]["type"] == "ProxyConfig"
    assert dumped_dict["params"]["proxy_config"]["params"]["server"] == "http://jsondump.proxy:1234"
    assert dumped_dict["params"]["browser_type"] == "firefox"
    
    # Verify it can be loaded back (see next example)

if __name__ == "__main__":
    asyncio.run(browserconfig_dump_json_compatible())
```

---
#### 4.9.5. Example: Deserializing `BrowserConfig` from a dictionary (potentially read from JSON) using `load()`.
The `load()` method reconstructs a `BrowserConfig` instance from its serialized form.

```python
import asyncio
from crawl4ai import BrowserConfig, ProxyConfig

async def browserconfig_load_from_dict():
    serialized_data = {
        "type": "BrowserConfig",
        "params": {
            "browser_type": "webkit",
            "headless": True,
            "proxy_config": {
                "type": "ProxyConfig",
                "params": {"server": "http://jsonload.proxy:5678"}
            },
            "user_agent": "LoadedAgent/2.0",
            "extra_args": ["--another-flag"]
        }
    }
    
    loaded_cfg = BrowserConfig.load(serialized_data)
    
    print("Loaded BrowserConfig from dictionary:")
    print(f"  Browser Type: {loaded_cfg.browser_type}")
    print(f"  Headless: {loaded_cfg.headless}")
    print(f"  User Agent: {loaded_cfg.user_agent}")
    print(f"  Extra Args: {loaded_cfg.extra_args}")
    if loaded_cfg.proxy_config:
        print(f"  Proxy Server: {loaded_cfg.proxy_config.server}")

    assert isinstance(loaded_cfg, BrowserConfig)
    assert loaded_cfg.browser_type == "webkit"
    assert loaded_cfg.headless is True
    assert isinstance(loaded_cfg.proxy_config, ProxyConfig)
    assert loaded_cfg.proxy_config.server == "http://jsonload.proxy:5678" # type: ignore
    assert loaded_cfg.user_agent == "LoadedAgent/2.0"
    assert "--another-flag" in loaded_cfg.extra_args

if __name__ == "__main__":
    asyncio.run(browserconfig_load_from_dict())
```

---
## 5. `HTTPCrawlerConfig` Examples (For non-browser HTTP crawling)
`HTTPCrawlerConfig` is used with strategies like `AsyncHTTPCrawlerStrategy` that make direct HTTP requests without a full browser.

### 5.1. Example: Basic `HTTPCrawlerConfig` for a GET request (default).

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig
# from crawl4ai import AsyncWebCrawler, AsyncHTTPCrawlerStrategy (for full example)

async def basic_httpcrawler_config():
    http_cfg = HTTPCrawlerConfig() # Defaults to method="GET"
    print(f"Default HTTPCrawlerConfig: {http_cfg.to_dict()}")
    assert http_cfg.method == "GET"
    
    # To use it:
    # strategy = AsyncHTTPCrawlerStrategy(http_config=http_cfg)
    # async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
    #     result = await crawler.arun(url="https://httpbin.org/get")
    #     print(result.html)

if __name__ == "__main__":
    asyncio.run(basic_httpcrawler_config())
```

---
### 5.2. Example: Configuring a POST request with form `data` using `HTTPCrawlerConfig`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig
# from crawl4ai import AsyncWebCrawler, AsyncHTTPCrawlerStrategy (for full example)
# import json

async def post_form_data_httpcrawler_config():
    form_payload = {"key1": "value1", "key2": "value2"}
    http_cfg = HTTPCrawlerConfig(method="POST", data=form_payload)
    
    print(f"HTTPCrawlerConfig for POST with form data: {http_cfg.to_dict()}")
    assert http_cfg.method == "POST"
    assert http_cfg.data == form_payload
    
    # To use it:
    # strategy = AsyncHTTPCrawlerStrategy() # Default config is fine, override in arun
    # async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
    #     run_cfg = CrawlerRunConfig(url="https://httpbin.org/post", http_config=http_cfg)
    #     result = await crawler.arun(config=run_cfg)
    #     if result.success:
    #         response_data = json.loads(result.html)
    #         print(f"Server received form data: {response_data.get('form')}")
    #         assert response_data.get('form') == form_payload

if __name__ == "__main__":
    asyncio.run(post_form_data_httpcrawler_config())
    print("Note: For a live test, you'd use this with AsyncHTTPCrawlerStrategy.")
```

---
### 5.3. Example: Configuring a POST request with a `json` payload using `HTTPCrawlerConfig`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig
# from crawl4ai import AsyncWebCrawler, AsyncHTTPCrawlerStrategy (for full example)
# import json

async def post_json_payload_httpcrawler_config():
    json_payload = {"name": "Crawl4AI", "version": "0.6.3"}
    http_cfg = HTTPCrawlerConfig(method="POST", json=json_payload) # Use 'json' parameter
    
    print(f"HTTPCrawlerConfig for POST with JSON payload: {http_cfg.to_dict()}")
    assert http_cfg.method == "POST"
    assert http_cfg.json == json_payload
    
    # To use it:
    # strategy = AsyncHTTPCrawlerStrategy()
    # async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
    #     run_cfg = CrawlerRunConfig(url="https://httpbin.org/post", http_config=http_cfg)
    #     result = await crawler.arun(config=run_cfg)
    #     if result.success:
    #         response_data = json.loads(result.html)
    #         print(f"Server received JSON data: {response_data.get('json')}")
    #         assert response_data.get('json') == json_payload

if __name__ == "__main__":
    asyncio.run(post_json_payload_httpcrawler_config())
    print("Note: For a live test, you'd use this with AsyncHTTPCrawlerStrategy.")
```

---
### 5.4. Example: Setting custom `headers` for an HTTP request via `HTTPCrawlerConfig`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig
# from crawl4ai import AsyncWebCrawler, AsyncHTTPCrawlerStrategy (for full example)
# import json

async def custom_headers_httpcrawler_config():
    custom_http_headers = {
        "X-API-Key": "mysecretapikey",
        "Content-Type": "application/json" # Though httpx might set this for json payload
    }
    http_cfg = HTTPCrawlerConfig(headers=custom_http_headers, method="GET")
    
    print(f"HTTPCrawlerConfig with custom headers: {http_cfg.headers}")
    assert http_cfg.headers["X-API-Key"] == "mysecretapikey"
    
    # To use it:
    # strategy = AsyncHTTPCrawlerStrategy()
    # async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
    #     run_cfg = CrawlerRunConfig(url="https://httpbin.org/headers", http_config=http_cfg)
    #     result = await crawler.arun(config=run_cfg)
    #     if result.success:
    #         response_data = json.loads(result.html)
    #         print(f"Server received headers (excerpt):")
    #         received_headers = response_data.get("headers", {})
    #         assert received_headers.get("X-Api-Key") == "mysecretapikey" # HTTP headers are case-insensitive

if __name__ == "__main__":
    asyncio.run(custom_headers_httpcrawler_config())
    print("Note: For a live test, you'd use this with AsyncHTTPCrawlerStrategy.")
```

---
### 5.5. Example: Disabling automatic redirect following (`follow_redirects=False`) in `HTTPCrawlerConfig`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig, AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy

async def no_redirect_httpcrawler_config():
    http_cfg_no_redirect = HTTPCrawlerConfig(follow_redirects=False)
    print(f"HTTPCrawlerConfig with follow_redirects={http_cfg_no_redirect.follow_redirects}")
    assert not http_cfg_no_redirect.follow_redirects
    
    # httpbin.org/redirect/1 will redirect once
    redirect_url = "https://httpbin.org/redirect/1"
    run_config = CrawlerRunConfig(url=redirect_url)
    
    strategy = AsyncHTTPCrawlerStrategy(http_config=http_cfg_no_redirect)
    async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
        print(f"Attempting to crawl {redirect_url} with redirects disabled.")
        result = await crawler.arun(config=run_config)
        if result.success:
            print(f"Crawl to {result.url} status: {result.status_code}")
            # Expecting a 302 status code since redirects are off
            assert result.status_code in [301, 302, 303, 307, 308] 
            print(f"SUCCESS: Received redirect status {result.status_code} as expected.")
            print(f"Location header: {result.response_headers.get('Location')}")
        else:
            # This might happen if the test URL itself changes or there's an issue
            print(f"Crawl failed or did not behave as expected: {result.error_message}")
            print(f"Status code: {result.status_code}")


if __name__ == "__main__":
    asyncio.run(no_redirect_httpcrawler_config())
```

---
### 5.6. Example: Disabling SSL certificate verification (`verify_ssl=False`) in `HTTPCrawlerConfig`.
**Warning:** Disabling SSL verification is a security risk and should only be used in trusted environments or for specific testing purposes.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig, AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy

async def no_ssl_verify_httpcrawler_config():
    http_cfg_no_ssl = HTTPCrawlerConfig(verify_ssl=False)
    print(f"HTTPCrawlerConfig with verify_ssl={http_cfg_no_ssl.verify_ssl}")
    assert not http_cfg_no_ssl.verify_ssl
    
    # Use a site with a self-signed or expired certificate for testing
    # For example: "https://self-signed.badssl.com/" or "https://expired.badssl.com/"
    # For this example, we'll use a regular site, as hitting badssl might cause other issues in CI.
    # The real test is that it *doesn't* fail on a bad SSL site.
    test_url_bad_ssl = "https://expired.badssl.com/" 
    test_url_good_ssl = "https://example.com"

    print(f"Attempting to crawl {test_url_bad_ssl} with SSL verification disabled.")
    print("NOTE: This test is more meaningful if tested against a site with actual SSL issues.")
    
    strategy = AsyncHTTPCrawlerStrategy(http_config=http_cfg_no_ssl)
    async with AsyncWebCrawler(crawler_strategy=strategy) as crawler:
        run_config_bad = CrawlerRunConfig(url=test_url_bad_ssl)
        result_bad = await crawler.arun(config=run_config_bad)
        if result_bad.success:
            print(f"Successfully crawled {test_url_bad_ssl} (SSL errors were ignored).")
        else:
            # It might still fail for other reasons (e.g., site down)
            print(f"Crawl to {test_url_bad_ssl} failed, but not necessarily due to SSL: {result_bad.error_message}")

        # Verify it still works for good SSL sites
        run_config_good = CrawlerRunConfig(url=test_url_good_ssl)
        result_good = await crawler.arun(config=run_config_good)
        if result_good.success:
             print(f"Successfully crawled {test_url_good_ssl} as well.")

if __name__ == "__main__":
    asyncio.run(no_ssl_verify_httpcrawler_config())
```

---
### 5.7. Example: Creating `HTTPCrawlerConfig` using `HTTPCrawlerConfig.from_kwargs()`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig

async def httpcrawlerconfig_from_kwargs():
    kwargs = {
        "method": "PUT",
        "headers": {"Authorization": "Bearer mytoken"},
        "json_payload": {"update_key": "new_value"} # Note: constructor uses 'json'
    }
    # The from_kwargs will map json_payload to json if the class expects 'json'
    # Let's check the actual constructor signature for 'json' vs 'json_payload'
    # The HTTPCrawlerConfig constructor takes 'json' not 'json_payload'.
    # So for from_kwargs, we should use the correct parameter names.
    kwargs_correct = {
        "method": "PUT",
        "headers": {"Authorization": "Bearer mytoken"},
        "json": {"update_key": "new_value"} 
    }

    http_cfg = HTTPCrawlerConfig.from_kwargs(kwargs_correct)
    
    print("HTTPCrawlerConfig created from_kwargs:")
    print(f"  Method: {http_cfg.method}")
    print(f"  Headers: {http_cfg.headers}")
    print(f"  JSON Payload: {http_cfg.json}")

    assert http_cfg.method == "PUT"
    assert http_cfg.headers["Authorization"] == "Bearer mytoken"
    assert http_cfg.json["update_key"] == "new_value"

if __name__ == "__main__":
    asyncio.run(httpcrawlerconfig_from_kwargs())
```

---
### 5.8. Example: Converting `HTTPCrawlerConfig` to a dictionary using `to_dict()`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig

async def httpcrawlerconfig_to_dict():
    http_cfg = HTTPCrawlerConfig(
        method="DELETE",
        headers={"X-Request-ID": "123xyz"},
        follow_redirects=False
    )
    config_dict = http_cfg.to_dict()

    print("HTTPCrawlerConfig instance:")
    print(f"  Original method: {http_cfg.method}")
    
    print("\nConverted to dictionary:")
    print(json.dumps(config_dict, indent=2))
    
    assert config_dict["method"] == "DELETE"
    assert config_dict["headers"]["X-Request-ID"] == "123xyz"
    assert config_dict["follow_redirects"] is False

if __name__ == "__main__":
    asyncio.run(httpcrawlerconfig_to_dict())
```

---
### 5.9. Example: Cloning `HTTPCrawlerConfig` and changing the HTTP method using `clone()`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig

async def httpcrawlerconfig_clone_modify():
    original_cfg = HTTPCrawlerConfig(method="GET", verify_ssl=True)
    print(f"Original Config: method={original_cfg.method}, verify_ssl={original_cfg.verify_ssl}")

    cloned_cfg = original_cfg.clone(method="PATCH", verify_ssl=False)
    print(f"Cloned Config: method={cloned_cfg.method}, verify_ssl={cloned_cfg.verify_ssl}")

    assert original_cfg.method == "GET"
    assert cloned_cfg.method == "PATCH"
    assert cloned_cfg.verify_ssl is False
    assert original_cfg.verify_ssl is True # Original unchanged

if __name__ == "__main__":
    asyncio.run(httpcrawlerconfig_clone_modify())
```

---
### 5.10. Example: Serializing `HTTPCrawlerConfig` using `dump()`.

```python
import asyncio
import json
from crawl4ai import HTTPCrawlerConfig

async def httpcrawlerconfig_dump():
    http_cfg = HTTPCrawlerConfig(
        method="OPTIONS",
        headers={"Origin": "https://example.com"},
        data={"ping": "true"}
    )
    
    dumped_dict = http_cfg.dump()

    print("Dumped HTTPCrawlerConfig (JSON compatible):")
    print(json.dumps(dumped_dict, indent=2))
    
    assert dumped_dict["type"] == "HTTPCrawlerConfig"
    assert dumped_dict["params"]["method"] == "OPTIONS"
    assert dumped_dict["params"]["data"]["ping"] == "true"

if __name__ == "__main__":
    asyncio.run(httpcrawlerconfig_dump())
```

---
### 5.11. Example: Deserializing `HTTPCrawlerConfig` using `load()`.

```python
import asyncio
from crawl4ai import HTTPCrawlerConfig

async def httpcrawlerconfig_load():
    serialized_data = {
        "type": "HTTPCrawlerConfig",
        "params": {
            "method": "HEAD",
            "headers": {"Cache-Control": "no-cache"},
            "follow_redirects": False
        }
    }
    
    loaded_cfg = HTTPCrawlerConfig.load(serialized_data)
    
    print("Loaded HTTPCrawlerConfig from dictionary:")
    print(f"  Method: {loaded_cfg.method}")
    print(f"  Headers: {loaded_cfg.headers}")
    print(f"  Follow Redirects: {loaded_cfg.follow_redirects}")

    assert isinstance(loaded_cfg, HTTPCrawlerConfig)
    assert loaded_cfg.method == "HEAD"
    assert loaded_cfg.headers["Cache-Control"] == "no-cache"
    assert loaded_cfg.follow_redirects is False

if __name__ == "__main__":
    asyncio.run(httpcrawlerconfig_load())
```

---
## 6. `CrawlerRunConfig` Examples

### 6.1. Basic Initialization

#### 6.1.1. Example: Default initialization of `CrawlerRunConfig`.
Creates a `CrawlerRunConfig` with all default values. The `url` will need to be provided when calling `arun`.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler

async def default_crawler_run_config():
    run_cfg = CrawlerRunConfig() # No URL specified here
    print(f"Default CrawlerRunConfig: {run_cfg.to_dict(exclude_none=True)}")
    assert run_cfg.url is None # URL must be passed to arun if not set here

    # Example of using it (requires URL in arun)
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=run_cfg)
        if result.success:
            print(f"Crawled example.com with default run config. Markdown length: {len(result.markdown.raw_markdown)}")

if __name__ == "__main__":
    asyncio.run(default_crawler_run_config())
```

---
#### 6.1.2. Example: Specifying the target `url` directly within `CrawlerRunConfig`.
The `url` can be part of the config object itself.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler

async def url_in_crawler_run_config():
    run_cfg_with_url = CrawlerRunConfig(url="https://example.org")
    print(f"CrawlerRunConfig with URL: {run_cfg_with_url.to_dict(exclude_none=True)}")
    assert run_cfg_with_url.url == "https://example.org"

    # Example of using it (arun will use the URL from config)
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg_with_url) # No URL needed here
        if result.success:
            print(f"Crawled {result.url} using URL from config. Markdown length: {len(result.markdown.raw_markdown)}")

if __name__ == "__main__":
    asyncio.run(url_in_crawler_run_config())
```

---
### 6.2. Content Processing & Extraction

#### 6.2.1. Example: Adjusting `word_count_threshold` to control content block filtering.
Lower threshold means more (potentially shorter) blocks are kept.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def word_count_threshold_example():
    # Default is 200, let's try a much lower one
    run_cfg_low_wct = CrawlerRunConfig(
        url="https://news.ycombinator.com", 
        word_count_threshold=10, # Keep blocks with at least 10 words
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result_low = await crawler.arun(config=run_cfg_low_wct)
        if result_low.success:
            print(f"Markdown length with WCT=10: {len(result_low.markdown.raw_markdown)}")

            run_cfg_high_wct = CrawlerRunConfig(
                url="https://news.ycombinator.com",
                word_count_threshold=100, # Keep blocks with at least 100 words
                cache_mode=CacheMode.BYPASS
            )
            result_high = await crawler.arun(config=run_cfg_high_wct)
            if result_high.success:
                print(f"Markdown length with WCT=100: {len(result_high.markdown.raw_markdown)}")
                # Expect result_low.markdown to be longer or include more diverse small blocks
                assert len(result_low.markdown.raw_markdown) >= len(result_high.markdown.raw_markdown)
            else:
                print(f"Crawl with high WCT failed: {result_high.error_message}")
        else:
            print(f"Crawl with low WCT failed: {result_low.error_message}")

if __name__ == "__main__":
    asyncio.run(word_count_threshold_example())
```

---
#### 6.2.2. Example: Integrating a custom `extraction_strategy` (e.g., `NoExtractionStrategy`).
`NoExtractionStrategy` skips structured data extraction, only providing HTML/Markdown.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.extraction_strategy import NoExtractionStrategy

async def no_extraction_strategy_example():
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        extraction_strategy=NoExtractionStrategy(),
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} with NoExtractionStrategy.")
            print(f"Extracted content: {result.extracted_content}") # Should be None or empty
            assert result.extracted_content is None or result.extracted_content == "[]" # Default is "[]" from NoExtraction
            print(f"Markdown content (first 100 chars): {result.markdown.raw_markdown[:100]}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(no_extraction_strategy_example())
```

---
#### 6.2.3. Example: Using `RegExChunking` as the `chunking_strategy`.
Splits content based on regular expressions, useful before passing to some extraction strategies.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.chunking_strategy import RegExChunking
# For LLMExtractionStrategy to show effect of chunking
from crawl4ai.extraction_strategy import LLMExtractionStrategy 
from crawl4ai import LLMConfig
import os

async def regex_chunking_example():
    # This example is more meaningful if combined with an extraction strategy
    # that processes chunks, like LLMExtractionStrategy.

    # Define a schema for LLM extraction
    schema = {"key_topics": "List the main topics discussed in this text."}
    llm_config_test = LLMConfig(
        provider="openai/gpt-4o-mini", 
        api_token=os.getenv("OPENAI_API_KEY_PLACEHOLDER", "YOUR_API_KEY")
    )
    
    # Chunk by paragraphs (approximate regex)
    regex_chunker = RegExChunking(patterns=[r"\n\s*\n"]) # Split on blank lines

    # If OPENAI_API_KEY_PLACEHOLDER is set to a real key, this test will make an API call.
    if llm_config_test.api_token == "YOUR_API_KEY":
        print("Skipping RegExChunking with LLM example due to missing OpenAI API key.")
        extraction_strat = None
    else:
        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_config_test,
            schema=schema,
            # Note: The chunking_strategy is applied *before* LLM extraction if the LLM strategy expects chunks.
            # However, LLMExtractionStrategy itself handles chunking internally if content is too long.
            # To demonstrate RegExChunking independently, one might process its output directly.
            # For this example, we'll let LLMExtractionStrategy use it.
        )


    run_cfg = CrawlerRunConfig(
        url="https://en.wikipedia.org/wiki/Python_(programming_language)",
        chunking_strategy=regex_chunker, # This will be used by LLMExtractionStrategy if it's configured to take pre-chunked input
        extraction_strategy=extraction_strat,
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=50 # Get more content for chunking
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url}.")
            if result.extracted_content:
                print(f"Extracted content (potentially from chunks): {result.extracted_content[:500]}...")
            else:
                print("No structured content extracted (or API key missing). Markdown was still generated from chunked/full content.")
            print(f"Markdown (first 300 chars): {result.markdown.raw_markdown[:300]}")
            # To truly verify RegExChunking effect, one would need to inspect how LLMExtractionStrategy uses it,
            # or use a custom strategy that explicitly consumes `crawler_run_config.chunking_strategy.chunk(content)`.
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(regex_chunking_example())
```

---
#### 6.2.4. Example: Configuring `DefaultMarkdownGenerator` for `markdown_generator`.
Allows customization of how Markdown is generated (e.g., with/without citations, different content source).

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def custom_markdown_generator_example():
    # Generate Markdown without citations
    md_gen_no_citations = DefaultMarkdownGenerator(
        options={"citations": False} # html2text option
    )
    run_cfg_no_cite = CrawlerRunConfig(
        url="https://example.com",
        markdown_generator=md_gen_no_citations,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result_no_cite = await crawler.arun(config=run_cfg_no_cite)
        if result_no_cite.success:
            print("--- Markdown without Citations (first 300 chars) ---")
            print(result_no_cite.markdown.raw_markdown[:300])
            # DefaultMarkdownGenerator produces raw_markdown and markdown_with_citations
            # We expect raw_markdown to be the one without reference style links.
            assert "]: http" not in result_no_cite.markdown.raw_markdown # Heuristic check
        
        # Generate Markdown from raw HTML instead of cleaned HTML
        md_gen_from_raw = DefaultMarkdownGenerator(content_source="raw_html")
        run_cfg_raw_html = CrawlerRunConfig(
             url="https://example.com",
             markdown_generator=md_gen_from_raw,
             cache_mode=CacheMode.BYPASS
        )
        result_raw = await crawler.arun(config=run_cfg_raw_html)
        if result_raw.success:
            print("\n--- Markdown from Raw HTML (first 300 chars) ---")
            print(result_raw.markdown.raw_markdown[:300])
            # This might be much noisier than from cleaned_html
            
if __name__ == "__main__":
    asyncio.run(custom_markdown_generator_example())
```

---
#### 6.2.5. Example: Enabling `only_text=True` for text-only extraction from HTML.
Strips all HTML tags, leaving only the text content.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def only_text_example():
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        only_text=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} with only_text=True.")
            print(f"Cleaned HTML (should be just text): {result.cleaned_html[:300]}")
            assert "<" not in result.cleaned_html[:10] # Heuristic: no HTML tags at start
            print(f"Markdown (should be similar to cleaned_html): {result.markdown.raw_markdown[:300]}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(only_text_example())
```

---
#### 6.2.6. Example: Using `css_selector` to focus HTML processing on a specific part of the page.
The `cleaned_html` (and thus default Markdown) will only contain content from the matched selector.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def css_selector_focus_example():
    # On example.com, let's try to get only the content within the first <p> tag inside <div>
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        css_selector="div > p:first-of-type", # Selects the first paragraph in the main div
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url}, focusing on 'div > p:first-of-type'.")
            print(f"Cleaned HTML: {result.cleaned_html}")
            # Expected: "This domain is for use in illustrative examples in documents..."
            assert "illustrative examples" in result.cleaned_html
            assert "More information..." not in result.cleaned_html # Content from other <p>
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(css_selector_focus_example())
```

---
#### 6.2.7. Example: Specifying multiple `target_elements` for focused Markdown generation.
Markdown and structured extraction will focus on these elements, but other page data (links, media) is still gathered from the whole page.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def target_elements_example():
    # On example.com, target the <h1> and the link <a>
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        target_elements=["h1", "div > p > a"], # Target heading and the "More information" link
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} with target_elements=['h1', 'div > p > a'].")
            print(f"Generated Markdown (focused on targets):\n{result.markdown.raw_markdown}")
            assert "Example Domain" in result.markdown.raw_markdown
            assert "More information..." in result.markdown.raw_markdown
            # The first paragraph's text should NOT be in the markdown if not targeted
            assert "illustrative examples" not in result.markdown.raw_markdown

            # Check if all links from the page are still collected (they should be)
            print(f"\nTotal internal links found on page: {len(result.links.get('internal', []))}")
            assert len(result.links.get("internal", [])) > 0 
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(target_elements_example())
```

---
#### 6.2.8. Example: Excluding specific HTML tags (e.g., `nav`, `footer`) using `excluded_tags`.
These tags and their content will be removed from `cleaned_html`.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def excluded_tags_example():
    # Let's try to exclude <p> tags from example.com
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        excluded_tags=["p"],
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} excluding <p> tags.")
            print(f"Cleaned HTML (should not contain <p>):\n{result.cleaned_html}")
            assert "<p>" not in result.cleaned_html.lower()
            assert "illustrative examples" not in result.cleaned_html # This text is in a <p>
            assert "Example Domain" in result.cleaned_html # The <h1> should remain
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(excluded_tags_example())
```

---
#### 6.2.9. Example: Excluding elements based on a CSS selector using `excluded_selector`.
Removes elements matching the CSS selector from `cleaned_html`.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def excluded_selector_example():
    # Exclude the link "More information..." on example.com which is in a <p> inside a <div>
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        excluded_selector="div > p > a", # CSS selector for the link
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} excluding 'div > p > a'.")
            print(f"Cleaned HTML:\n{result.cleaned_html}")
            assert "More information..." not in result.cleaned_html
            assert "illustrative examples" in result.cleaned_html # The rest of the <p> should be there
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(excluded_selector_example())
```

---
#### 6.2.10. Example: Preserving `data-*` attributes during HTML cleaning with `keep_data_attributes=True`.
By default, many attributes are stripped. This keeps `data-*` attributes.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def keep_data_attributes_example():
    sample_html_with_data_attr = """
    <html><body>
        <div data-testid="main-content" data-analytics-id="section1">
            <p>Some important text.</p>
            <span data-custom-info="extra detail">More text.</span>
        </div>
    </body></html>
    """
    run_cfg = CrawlerRunConfig(
        url=f"raw://{sample_html_with_data_attr}",
        keep_data_attributes=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled content with keep_data_attributes=True.")
            print(f"Cleaned HTML:\n{result.cleaned_html}")
            assert 'data-testid="main-content"' in result.cleaned_html
            assert 'data-analytics-id="section1"' in result.cleaned_html
            assert 'data-custom-info="extra detail"' in result.cleaned_html
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(keep_data_attributes_example())
```

---
#### 6.2.11. Example: Specifying a custom list of HTML attributes to preserve using `keep_attrs`.
Allows fine-grained control over which attributes are kept.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def keep_specific_attributes_example():
    sample_html_with_attrs = """
    <html><body>
        <article id="article-123" class="blog-post important" style="color:blue;" title="My Article">
            <h1>Title</h1>
            <a href="/path" data-linkid="789">Link</a>
        </article>
    </body></html>
    """
    # We want to keep 'id', 'class' from the article, and 'href' from the link
    # and also data-linkid
    run_cfg = CrawlerRunConfig(
        url=f"raw://{sample_html_with_attrs}",
        keep_attrs=["id", "class", "href", "data-linkid"], 
        # keep_data_attributes=False (default), so only "data-linkid" if explicitly listed here.
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled content with keep_attrs=['id', 'class', 'href', 'data-linkid'].")
            cleaned_html = result.cleaned_html
            print(f"Cleaned HTML:\n{cleaned_html}")
            assert 'id="article-123"' in cleaned_html
            assert 'class="blog-post important"' in cleaned_html # Classes are usually kept by default cleaner for semantic reasons
            assert 'href="/path"' in cleaned_html
            assert 'data-linkid="789"' in cleaned_html
            assert 'style="color:blue;"' not in cleaned_html # style should be removed
            assert 'title="My Article"' not in cleaned_html # title should be removed
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(keep_specific_attributes_example())
```

---
#### 6.2.12. Example: Removing all `<form>` elements from HTML using `remove_forms=True`.
Useful for cleaning up pages before text extraction if forms are noisy.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def remove_forms_example():
    html_with_form = """
    <html><body>
        <p>Some text before form.</p>
        <form action="/submit">
            <label for="name">Name:</label>
            <input type="text" id="name" name="name"><br>
            <input type="submit" value="Submit">
        </form>
        <p>Some text after form.</p>
    </body></html>
    """
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_form}",
        remove_forms=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled content with remove_forms=True.")
            cleaned_html = result.cleaned_html
            print(f"Cleaned HTML (should not contain <form>):\n{cleaned_html}")
            assert "<form" not in cleaned_html.lower()
            assert "Some text before form." in cleaned_html
            assert "Some text after form." in cleaned_html
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(remove_forms_example())
```

---
#### 6.2.13. Example: Enabling HTML prettifying of the cleaned HTML output with `prettify=True`.
Formats the `cleaned_html` to be more human-readable.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def prettify_html_example():
    # A simple, unformatted HTML string
    raw_html = "<html><body><div><p>Hello</p><p>World</p></div></body></html>"
    run_cfg = CrawlerRunConfig(
        url=f"raw://{raw_html}",
        prettify=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled content with prettify=True.")
            cleaned_html = result.cleaned_html
            print(f"Prettified Cleaned HTML:\n{cleaned_html}")
            # Prettified HTML usually has more newlines and indentation
            assert cleaned_html.count('\n') > 3 # Heuristic check for newlines
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(prettify_html_example())
```

---
#### 6.2.14. Example: Changing the HTML parser to "html.parser" using `parser_type`.
The default is "lxml". "html.parser" is Python's built-in parser.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode

async def change_parser_type_example():
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        parser_type="html.parser", # Use Python's built-in parser
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Using parser_type: {run_cfg.parser_type}")
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} using 'html.parser'.")
            # Behavior should be largely similar for well-formed HTML
            assert "Example Domain" in result.cleaned_html
        else:
            print(f"Crawl failed with 'html.parser': {result.error_message}")

if __name__ == "__main__":
    asyncio.run(change_parser_type_example())
```

---
#### 6.2.15. Example: Specifying a custom `scraping_strategy` (e.g., `PDFScrapingStrategy` - if applicable).
Demonstrates using a different strategy for content scraping, here for PDF files.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.content_scraping_strategy import WebScrapingStrategy # Default
# For PDF, we'd import PDF相关的strategies
from crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFScrapingStrategy
# Note: PDFScrapingStrategy is typically used by PDFCrawlerStrategy, not directly by AsyncWebCrawler for a URL.
# This example will show how to set it conceptually if a strategy took it.
# A more realistic PDF example uses PDFCrawlerStrategy directly.

async def custom_scraping_strategy_conceptual():
    # This is conceptual for CrawlerRunConfig.scraping_strategy if a strategy used it.
    # In practice, for PDF, you'd use PDFCrawlerStrategy.
    
    # Conceptual: if AsyncWebCrawler could directly use PDFScrapingStrategy via config
    # (It doesn't by default for generic URLs; PDF processing has its own flow)
    # pdf_scraping_strat = PDFScrapingStrategy() 
    # run_cfg_pdf = CrawlerRunConfig(
    #     url="file:///path/to/your/document.pdf", # Example local PDF
    #     scraping_strategy=pdf_scraping_strat,
    #     cache_mode=CacheMode.BYPASS
    # )
    # print(f"Conceptual CrawlerRunConfig with PDFScrapingStrategy: {run_cfg_pdf.scraping_strategy}")


    # More realistic usage for PDF:
    # Use PDFCrawlerStrategy directly if you know it's a PDF
    pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf" # A sample PDF
    
    # We need a PDF-specific crawler strategy, not just scraping_strategy in CrawlerRunConfig
    # This example demonstrates where scraping_strategy *would* go if a generic URL crawler used it.
    # The PDF files provided show PDFCrawlerStrategy and PDFContentScrapingStrategy.
    # PDFContentScrapingStrategy is the equivalent scraper for PDFs.
    
    # Correct way for PDF:
    pdf_crawler_strategy = PDFCrawlerStrategy()
    pdf_scraping_config = PDFScrapingStrategy() # This is the content scraper for PDFs
    
    run_config_for_pdf = CrawlerRunConfig(
        url=pdf_url,
        scraping_strategy=pdf_scraping_config, # This is what the PDFCrawlerStrategy would use internally
        cache_mode=CacheMode.BYPASS
    )

    print(f"Using PDFScrapingStrategy conceptually in CrawlerRunConfig for a PDF URL: {pdf_url}")
    
    # The AsyncWebCrawler's default Playwright strategy won't use run_config.scraping_strategy.
    # We need to pass the PDFCrawlerStrategy to AsyncWebCrawler itself.
    async with AsyncWebCrawler(crawler_strategy=pdf_crawler_strategy) as crawler:
        # The run_config's scraping_strategy will be passed to pdf_crawler_strategy.crawl()
        # if pdf_crawler_strategy.crawl() is designed to accept it via **kwargs
        # and then pass it to its internal scraper.
        # The provided PDF __init__.py passes the 'save_images_locally', 'extract_images' etc.
        # directly from CrawlerRunConfig to the PDFContentScrapingStrategy.
        
        # For this example, let's adjust CrawlerRunConfig to pass params PDFContentScrapingStrategy expects:
        pdf_specific_run_config = CrawlerRunConfig(
            url=pdf_url,
            extract_images=True, # A param PDFContentScrapingStrategy uses
            cache_mode=CacheMode.BYPASS
        )

        result = await crawler.arun(config=pdf_specific_run_config)
        if result.success:
            print(f"Successfully processed PDF {result.url}.")
            print(f"Markdown content (first 300 chars):\n{result.markdown.raw_markdown[:300]}")
            if result.media and result.media.get("images"):
                print(f"Extracted {len(result.media['images'])} images from PDF.")
        else:
            print(f"Failed to process PDF: {result.error_message}")


if __name__ == "__main__":
    asyncio.run(custom_scraping_strategy_conceptual())
```

---
### 6.3. Proxy Configuration for a Specific Run

#### 6.3.1. Example: Providing a `ProxyConfig` object to `CrawlerRunConfig.proxy_config`.
This overrides any proxy set in `BrowserConfig` for this specific `arun` call.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, ProxyConfig

async def run_specific_proxy():
    # Browser might have a default proxy or no proxy
    browser_cfg = BrowserConfig(headless=True, verbose=True) 
    
    # Proxy for this specific run (non-functional placeholder)
    run_proxy = ProxyConfig(server="http://runspecificproxy.example.com:8888")
    
    run_cfg_with_proxy = CrawlerRunConfig(
        url="https://httpbin.org/ip", # Shows requester IP
        proxy_config=run_proxy
    )
    print(f"CrawlerRunConfig with specific proxy: {run_cfg_with_proxy.proxy_config.to_dict()}") # type: ignore

    print("NOTE: This example will likely fail or show your direct IP if the placeholder proxy is not replaced.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_with_proxy)
        if result.success:
            print(f"Crawled {result.url} using run-specific proxy.")
            print(f"Response (should show proxy IP): {result.html}")
        else:
            print(f"Crawl with run-specific proxy failed: {result.error_message}")

if __name__ == "__main__":
    # asyncio.run(run_specific_proxy())
    print("Skipping run_specific_proxy example as it requires a live proxy server.")
```

---
#### 6.3.2. Example: Using `RoundRobinProxyStrategy` for `proxy_rotation_strategy`.
Demonstrates setting up a proxy rotation strategy for `arun_many` or multiple `arun` calls.

```python
import asyncio
from crawl4ai import (
    AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, 
    ProxyConfig, RoundRobinProxyStrategy
)

async def proxy_rotation_example():
    # List of proxies (non-functional placeholders)
    proxies = [
        ProxyConfig(server="http://proxy1.example.com:8000"),
        ProxyConfig(server="http://proxy2.example.com:8001", username="user2", password="p2"),
        ProxyConfig(server="http://proxy3.example.com:8002"),
    ]
    
    proxy_rotator = RoundRobinProxyStrategy(proxies=proxies)
    
    # Browser config (no proxy set here, it will be set per run)
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    urls_to_crawl = [
        "https://httpbin.org/ip?site=1",
        "https://httpbin.org/ip?site=2",
        "https://httpbin.org/ip?site=3",
        "https://httpbin.org/ip?site=4" # Will cycle back to proxy1
    ]

    print("NOTE: This example will likely fail or show direct IPs if placeholder proxies are not replaced.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for i, url in enumerate(urls_to_crawl):
            # Get next proxy from strategy and create a run_config with it
            # The proxy_rotation_strategy itself is passed to CrawlerRunConfig
            run_cfg = CrawlerRunConfig(
                url=url,
                proxy_rotation_strategy=proxy_rotator # The crawler will call get_next_proxy()
            )
            
            # The `arun` method will internally use proxy_rotation_strategy.get_next_proxy()
            # to set the proxy_config for the actual call if proxy_rotation_strategy is present.
            # The proxy_config will be set inside the AsyncPlaywrightCrawlerStrategy.
            
            print(f"\n--- Crawling {url} (Attempt {i+1}) ---")
            # current_proxy_for_run = await proxy_rotator.get_next_proxy() # This is what arun would do
            # print(f"Expected proxy for this run: {current_proxy_for_run.server if current_proxy_for_run else 'None'}")

            result = await crawler.arun(config=run_cfg)
            
            if result.success:
                print(f"Crawled {result.url} successfully.")
                print(f"Response (should show IP of proxy {i % len(proxies) + 1}): {result.html}")
            else:
                print(f"Crawl for {result.url} failed: {result.error_message}")
            
            await asyncio.sleep(0.5) # Small delay between requests

if __name__ == "__main__":
    # asyncio.run(proxy_rotation_example())
    print("Skipping proxy_rotation_example as it requires live proxy servers.")
```

---
### 6.4. Localization and Geolocation for a Specific Run

#### 6.4.1. Example: Setting browser `locale` (e.g., "es-ES") for the crawl.
This can affect language of the page, date/number formats.

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def set_browser_locale_for_run():
    browser_cfg = BrowserConfig(headless=True, verbose=True) # Default locale (usually en-US)
    
    run_cfg_spanish = CrawlerRunConfig(
        url="https://httpbin.org/headers", # This endpoint shows request headers
        locale="es-ES" # Spanish (Spain)
    )
    print(f"CrawlerRunConfig with locale: {run_cfg_spanish.locale}")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_spanish)
        if result.success:
            print(f"Crawled {result.url} with locale 'es-ES'.")
            response_data = json.loads(result.html)
            accept_language_header = response_data.get("headers", {}).get("Accept-Language")
            print(f"Accept-Language header sent: {accept_language_header}")
            # Playwright typically sets Accept-Language based on locale
            assert accept_language_header and "es-ES" in accept_language_header.split(',')[0]
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(set_browser_locale_for_run())
```

---
#### 6.4.2. Example: Setting browser `timezone_id` (e.g., "Europe/Madrid") for the crawl.
Affects JavaScript `Date` objects and potentially server-side logic based on timezone.

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def set_browser_timezone_for_run():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    run_cfg_madrid_tz = CrawlerRunConfig(
        url="https://httpbin.org/anything", # We'll execute JS to get timezone
        timezone_id="Europe/Madrid",
        js_code="JSON.stringify({offset: new Date().getTimezoneOffset(), localeString: new Date().toLocaleString('en-US', {timeZoneName:'short'})})"
    )
    print(f"CrawlerRunConfig with timezone_id: {run_cfg_madrid_tz.timezone_id}")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_madrid_tz)
        if result.success and result.js_execution_result:
            print(f"Crawled {result.url} with timezone 'Europe/Madrid'.")
            js_result = result.js_execution_result
            print(f"JS Date info: {js_result}")
            # For Madrid (CET/CEST), offset is -60 (CET) or -120 (CEST) from UTC.
            # localeString might show 'CET' or 'CEST'.
            # This is a heuristic check.
            assert js_result.get("offset") in [-60, -120] or "CET" in js_result.get("localeString", "") or "CEST" in js_result.get("localeString", "")
        elif not result.js_execution_result:
            print(f"JS execution did not return a result for timezone check.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(set_browser_timezone_for_run())
```

---
#### 6.4.3. Example: Providing a `GeolocationConfig` object for the crawl.
Simulates a specific GPS location for the browser.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, GeolocationConfig

async def set_geolocation_for_run():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # Sydney, Australia
    sydney_geo = GeolocationConfig(latitude=-33.8688, longitude=151.2093, accuracy=50.0)
    
    run_cfg_sydney = CrawlerRunConfig(
        url="https://www.gps-coordinates.net/my-location", # This site shows your detected location
        geolocation=sydney_geo,
        wait_for="css=#address", # Wait for the address to be populated
        page_timeout=20000, # Give it time
        verbose=True
    )
    print(f"CrawlerRunConfig with geolocation: {run_cfg_sydney.geolocation.to_dict()}")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Attempting to crawl {run_cfg_sydney.url} simulating Sydney location.")
        result = await crawler.arun(config=run_cfg_sydney)
        if result.success:
            print(f"Successfully crawled {result.url} with simulated Sydney location.")
            # Check if the page content reflects Sydney
            # print(f"Page HTML (first 1000 chars):\n{result.html[:1000]}")
            if "Sydney" in result.html or "Australia" in result.html:
                 print("SUCCESS: Sydney or Australia mentioned in page, geolocation likely worked.")
            else:
                 print("Geolocation effect not immediately obvious in HTML. Manual check of site behavior needed.")
        else:
            print(f"Crawl failed: {result.error_message}")
            print("This might be due to the test site's behavior or if permissions for geolocation were not granted by the browser context setup.")

if __name__ == "__main__":
    asyncio.run(set_geolocation_for_run())
```

---
### 6.5. Caching Behavior with `CacheMode`

#### 6.5.1. Example: Enabling full read/write caching with `cache_mode=CacheMode.ENABLED`.
Reads from cache if available, otherwise fetches and writes to cache. (Default if no mode is set by user and cache is available).

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
from crawl4ai.utils import get_cache_dir # Helper to find cache location

async def cache_enabled_example():
    # Ensure cache dir exists for this test
    cache_dir = get_cache_dir()
    print(f"Using cache directory: {cache_dir}")

    run_cfg = CrawlerRunConfig(
        url="https://example.com", # A static page, good for caching demo
        cache_mode=CacheMode.ENABLED
    )
    
    browser_cfg = BrowserConfig(headless=True, verbose=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # First run: Should fetch and cache
        print("\n--- First run (CacheMode.ENABLED) ---")
        start_time1 = time.perf_counter()
        result1 = await crawler.arun(config=run_cfg)
        duration1 = time.perf_counter() - start_time1
        if result1.success:
            print(f"First run successful. Duration: {duration1:.2f}s. Cached: {result1.metadata.get('cached', False)}")
            assert not result1.metadata.get('cached', False) # Should not be cached on first run (unless already in DB)
        else:
            print(f"First run failed: {result1.error_message}")
            return

        # Second run: Should read from cache
        print("\n--- Second run (CacheMode.ENABLED) ---")
        start_time2 = time.perf_counter()
        result2 = await crawler.arun(config=run_cfg) # Same URL and config
        duration2 = time.perf_counter() - start_time2
        if result2.success:
            print(f"Second run successful. Duration: {duration2:.2f}s. Cached: {result2.metadata.get('cached', False)}")
            assert result2.metadata.get('cached', True) # Should be cached now
            assert duration2 < duration1 # Cached read should be faster
            assert result1.markdown.raw_markdown == result2.markdown.raw_markdown
        else:
            print(f"Second run failed: {result2.error_message}")

if __name__ == "__main__":
    asyncio.run(cache_enabled_example())
```

---
#### 6.5.2. Example: Bypassing the cache for a single run with `cache_mode=CacheMode.BYPASS`.
Ignores existing cache and fetches fresh data, but still writes the new data to cache.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig

async def cache_bypass_example():
    run_cfg_cache_first = CrawlerRunConfig(
        url="https://example.com/cache_test_page_bypass", 
        cache_mode=CacheMode.ENABLED # First, ensure it's cached
    )
    run_cfg_bypass = CrawlerRunConfig(
        url="https://example.com/cache_test_page_bypass", # Same URL
        cache_mode=CacheMode.BYPASS # This run will bypass read but still write
    )
    browser_cfg = BrowserConfig(headless=True, verbose=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # First run: Populate cache
        print("\n--- First run (CacheMode.ENABLED to populate cache) ---")
        await crawler.arun(config=run_cfg_cache_first)
        print("Cache potentially populated.")

        # Second run: Bypass cache (should fetch fresh, then write to cache)
        print("\n--- Second run (CacheMode.BYPASS) ---")
        start_time_bypass = time.perf_counter()
        result_bypass = await crawler.arun(config=run_cfg_bypass)
        duration_bypass = time.perf_counter() - start_time_bypass
        if result_bypass.success:
            print(f"Bypass run successful. Duration: {duration_bypass:.2f}s. Cached flag: {result_bypass.metadata.get('cached', False)}")
            # 'cached' flag is True if data was retrieved from cache. For BYPASS, it should be False for the read part.
            assert not result_bypass.metadata.get('cached', False) 
        else:
            print(f"Bypass run failed: {result_bypass.error_message}")
            return

        # Third run: Normal enabled mode, should now read the data written by BYPASS run
        print("\n--- Third run (CacheMode.ENABLED, after BYPASS) ---")
        start_time_cached = time.perf_counter()
        result_cached_after_bypass = await crawler.arun(config=run_cfg_cache_first) # Use ENABLED config
        duration_cached = time.perf_counter() - start_time_cached
        if result_cached_after_bypass.success:
            print(f"Cached run successful. Duration: {duration_cached:.2f}s. Cached flag: {result_cached_after_bypass.metadata.get('cached', False)}")
            assert result_cached_after_bypass.metadata.get('cached', True)
            assert duration_cached < duration_bypass
        else:
            print(f"Cached run after bypass failed: {result_cached_after_bypass.error_message}")


if __name__ == "__main__":
    asyncio.run(cache_bypass_example())
```

---
#### 6.5.3. Example: Disabling all caching operations with `cache_mode=CacheMode.DISABLED`.
Neither reads from nor writes to the cache.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig

async def cache_disabled_example():
    run_cfg = CrawlerRunConfig(
        url="https://example.com/cache_test_page_disabled",
        cache_mode=CacheMode.DISABLED
    )
    browser_cfg = BrowserConfig(headless=True, verbose=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # First run
        print("\n--- First run (CacheMode.DISABLED) ---")
        start_time1 = time.perf_counter()
        result1 = await crawler.arun(config=run_cfg)
        duration1 = time.perf_counter() - start_time1
        if result1.success:
            print(f"First run successful. Duration: {duration1:.2f}s. Cached: {result1.metadata.get('cached', False)}")
            assert not result1.metadata.get('cached', False) # Should not be cached
        else:
            print(f"First run failed: {result1.error_message}")
            return

        # Second run: Should also fetch fresh as cache is disabled
        print("\n--- Second run (CacheMode.DISABLED) ---")
        start_time2 = time.perf_counter()
        result2 = await crawler.arun(config=run_cfg) # Same URL and config
        duration2 = time.perf_counter() - start_time2
        if result2.success:
            print(f"Second run successful. Duration: {duration2:.2f}s. Cached: {result2.metadata.get('cached', False)}")
            assert not result2.metadata.get('cached', False) # Still not cached
            # Durations might be similar as both are fresh fetches
            print(f"Durations: Run1={duration1:.2f}s, Run2={duration2:.2f}s")
        else:
            print(f"Second run failed: {result2.error_message}")

if __name__ == "__main__":
    asyncio.run(cache_disabled_example())
```

---
#### 6.5.4. Example: Configuring cache for read-only access with `cache_mode=CacheMode.READ_ONLY`.
Only reads from cache if data exists; does not write new data if fetched fresh.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig, AsyncDBManager

async def cache_readonly_example():
    url_readonly = "https://example.com/cache_test_page_readonly"
    
    # Ensure the item is NOT in cache initially by using a unique URL or clearing
    # For this test, we'll first try to read (should fail to find in cache), then try to write (should not write)
    
    run_cfg_readonly = CrawlerRunConfig(url=url_readonly, cache_mode=CacheMode.READ_ONLY)
    run_cfg_enabled = CrawlerRunConfig(url=url_readonly, cache_mode=CacheMode.ENABLED) # To check if it was written
    
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    db_manager = AsyncDBManager() # For direct cache check

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Ensure item is not in cache
        await db_manager.adelete_url_cache(url_readonly)
        print(f"Ensured {url_readonly} is not in cache.")

        # First run: READ_ONLY mode, item not in cache. Should fetch fresh, NOT write.
        print("\n--- First run (CacheMode.READ_ONLY, item not in cache) ---")
        result_fetch = await crawler.arun(config=run_cfg_readonly)
        if result_fetch.success:
            print(f"READ_ONLY (fetch): Successful. Cached: {result_fetch.metadata.get('cached', False)}")
            assert not result_fetch.metadata.get('cached', False)
        else:
            print(f"READ_ONLY (fetch) failed: {result_fetch.error_message}")
            return

        # Check if it was written to cache (it shouldn't have been)
        cached_item_after_readonly = await db_manager.aget_cached_url(url_readonly)
        if cached_item_after_readonly is None:
            print("SUCCESS: Item was NOT written to cache after READ_ONLY fetch, as expected.")
        else:
            print("FAILURE: Item was unexpectedly written to cache after READ_ONLY fetch.")
            await db_manager.adelete_url_cache(url_readonly) # Clean for next step

        # Now, let's populate the cache using ENABLED mode
        print("\n--- Populating cache (CacheMode.ENABLED) ---")
        await crawler.arun(config=run_cfg_enabled)
        print("Cache populated.")

        # Second run: READ_ONLY mode, item IS in cache. Should read from cache.
        print("\n--- Second run (CacheMode.READ_ONLY, item IS in cache) ---")
        start_time_read = time.perf_counter()
        result_read_cache = await crawler.arun(config=run_cfg_readonly)
        duration_read = time.perf_counter() - start_time_read
        if result_read_cache.success:
            print(f"READ_ONLY (read cache): Successful. Duration: {duration_read:.2f}s. Cached: {result_read_cache.metadata.get('cached', False)}")
            assert result_read_cache.metadata.get('cached', True)
        else:
            print(f"READ_ONLY (read cache) failed: {result_read_cache.error_message}")
        
        # Clean up
        await db_manager.adelete_url_cache(url_readonly)


if __name__ == "__main__":
    asyncio.run(cache_readonly_example())
```

---
#### 6.5.5. Example: Configuring cache for write-only access with `cache_mode=CacheMode.WRITE_ONLY`.
Always fetches fresh data, but writes it to the cache. Does not read from cache even if data exists.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig, AsyncDBManager

async def cache_writeonly_example():
    url_writeonly = "https://example.com/cache_test_page_writeonly"
    run_cfg_writeonly = CrawlerRunConfig(url=url_writeonly, cache_mode=CacheMode.WRITE_ONLY)
    run_cfg_enabled = CrawlerRunConfig(url=url_writeonly, cache_mode=CacheMode.ENABLED)
    
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    db_manager = AsyncDBManager()

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Ensure item is not in cache
        await db_manager.adelete_url_cache(url_writeonly)
        print(f"Ensured {url_writeonly} is not in cache.")

        # First run: WRITE_ONLY. Should fetch fresh AND write to cache.
        print("\n--- First run (CacheMode.WRITE_ONLY) ---")
        result_write1 = await crawler.arun(config=run_cfg_writeonly)
        if result_write1.success:
            print(f"WRITE_ONLY run 1: Successful. Cached flag: {result_write1.metadata.get('cached', False)}")
            assert not result_write1.metadata.get('cached', False) # Read part is skipped
        else:
            print(f"WRITE_ONLY run 1 failed: {result_write1.error_message}")
            return

        # Check if it was written
        cached_item_after_write1 = await db_manager.aget_cached_url(url_writeonly)
        if cached_item_after_write1:
            print("SUCCESS: Item was written to cache after first WRITE_ONLY run.")
        else:
            print("FAILURE: Item was NOT written to cache after first WRITE_ONLY run.")
            return

        # Second run: WRITE_ONLY again. Should still fetch fresh (not read cache), then overwrite cache.
        print("\n--- Second run (CacheMode.WRITE_ONLY, item is in cache) ---")
        # To verify it fetches fresh, we'd need a page that changes content, or compare timestamps.
        # For simplicity, we'll just confirm the behavior.
        start_time_write2 = time.perf_counter()
        result_write2 = await crawler.arun(config=run_cfg_writeonly)
        duration_write2 = time.perf_counter() - start_time_write2
        if result_write2.success:
            print(f"WRITE_ONLY run 2: Successful. Duration: {duration_write2:.2f}s. Cached flag: {result_write2.metadata.get('cached', False)}")
            assert not result_write2.metadata.get('cached', False)
        else:
            print(f"WRITE_ONLY run 2 failed: {result_write2.error_message}")
        
        # Clean up
        await db_manager.adelete_url_cache(url_writeonly)

if __name__ == "__main__":
    asyncio.run(cache_writeonly_example())
```

---
#### 6.5.6. Example: Showing modern equivalent for deprecated `bypass_cache=True` (uses `CacheMode.BYPASS`).
The `bypass_cache=True` flag in older versions is now `cache_mode=CacheMode.BYPASS`.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, CacheMode

async def deprecated_bypass_cache_equivalent():
    # Old way (would raise warning or error in newer versions if directly used as param)
    # run_cfg_old = CrawlerRunConfig(bypass_cache=True) 

    # New way
    run_cfg_new = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
    
    print("Demonstrating CacheMode.BYPASS (equivalent to old bypass_cache=True):")
    print(f"  cache_mode: {run_cfg_new.cache_mode}")
    
    # Verify behavior (conceptual, actual test in CacheMode.BYPASS example)
    # - It will not read from cache.
    # - It will fetch fresh data.
    # - It will write the fetched data to cache.
    assert run_cfg_new.cache_mode == CacheMode.BYPASS
    print("This configuration means the crawler will ignore existing cache entries for reading but will update the cache with the new data.")

if __name__ == "__main__":
    asyncio.run(deprecated_bypass_cache_equivalent())
```

---
#### 6.5.7. Example: Showing modern equivalent for deprecated `disable_cache=True` (uses `CacheMode.DISABLED`).
The `disable_cache=True` flag in older versions is now `cache_mode=CacheMode.DISABLED`.

```python
import asyncio
from crawl4ai import CrawlerRunConfig, CacheMode

async def deprecated_disable_cache_equivalent():
    # Old way (would raise warning or error in newer versions if directly used as param)
    # run_cfg_old = CrawlerRunConfig(disable_cache=True)

    # New way
    run_cfg_new = CrawlerRunConfig(cache_mode=CacheMode.DISABLED)

    print("Demonstrating CacheMode.DISABLED (equivalent to old disable_cache=True):")
    print(f"  cache_mode: {run_cfg_new.cache_mode}")
    
    # Verify behavior (conceptual, actual test in CacheMode.DISABLED example)
    # - It will not read from cache.
    # - It will fetch fresh data.
    # - It will NOT write the fetched data to cache.
    assert run_cfg_new.cache_mode == CacheMode.DISABLED
    print("This configuration means the crawler will neither read from nor write to the cache.")

if __name__ == "__main__":
    asyncio.run(deprecated_disable_cache_equivalent())
```

---
### 6.6. Session Management and Shared Data

#### 6.6.1. Example: Using `session_id` to maintain a persistent browser context across multiple `arun` calls.
Allows subsequent `arun` calls to reuse the same browser page/tab and its state (cookies, localStorage, JS context).

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def session_id_example():
    my_session_id = "my_persistent_web_session_123"
    browser_cfg = BrowserConfig(headless=True, verbose=True) # Keep browser open across arun calls if session_id is used

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # First call: Visit a page, set a cookie via JS
        js_set_cookie = "document.cookie = 'session_test_cookie=hello_crawl4ai; path=/';"
        run_cfg1 = CrawlerRunConfig(
            url="https://httpbin.org/anything", # Initial page
            session_id=my_session_id,
            js_code=js_set_cookie
        )
        print(f"\n--- First call with session_id='{my_session_id}', setting cookie ---")
        result1 = await crawler.arun(config=run_cfg1)
        if result1.success:
            print(f"First call to {result1.url} successful.")
        else:
            print(f"First call failed: {result1.error_message}")
            return

        # Second call: Visit another page on the same domain, cookie should be sent
        run_cfg2 = CrawlerRunConfig(
            url="https://httpbin.org/cookies", # This page shows cookies
            session_id=my_session_id # Crucial: use the same session_id
        )
        print(f"\n--- Second call with session_id='{my_session_id}', checking cookie ---")
        result2 = await crawler.arun(config=run_cfg2)
        if result2.success:
            print(f"Second call to {result2.url} successful.")
            response_data = json.loads(result2.html)
            print(f"Cookies received by server: {response_data.get('cookies')}")
            assert "session_test_cookie" in response_data.get("cookies", {})
            assert response_data.get("cookies", {}).get("session_test_cookie") == "hello_crawl4ai"
            print("SUCCESS: Cookie persisted across calls within the same session_id!")
        else:
            print(f"Second call failed: {result2.error_message}")
        
        # Important: Clean up the session if you're done with it and it's not managed by use_persistent_context
        await crawler.crawler_strategy.kill_session(my_session_id)
        print(f"\nSession '{my_session_id}' explicitly killed.")


if __name__ == "__main__":
    asyncio.run(session_id_example())
```

---
#### 6.6.2. Example: Passing `shared_data` dictionary for use in custom hooks (hooks themselves are external to this component).
`shared_data` is a way to pass arbitrary data to and between custom hook functions if you've implemented them.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy # To attach hooks

async def shared_data_example():
    # Define a simple hook that accesses shared_data
    async def my_custom_before_goto_hook(page, context, url, shared_data, **kwargs):
        print(f"[HOOK - before_goto] Accessing shared_data: {shared_data}")
        shared_data["hook_modified_value"] = "value_set_by_hook"
        # This hook could, for example, conditionally set headers based on shared_data
        return page

    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # The strategy where hooks are attached
    crawler_strategy = AsyncPlaywrightCrawlerStrategy(config=browser_cfg)
    crawler_strategy.set_hook("before_goto", my_custom_before_goto_hook)
    
    initial_shared_data = {"user_id": 123, "task_type": "data_collection"}
    
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        shared_data=initial_shared_data.copy() # Pass a copy to avoid modification issues if reused
    )
    print(f"CrawlerRunConfig with initial shared_data: {run_cfg.shared_data}")

    async with AsyncWebCrawler(crawler_strategy=crawler_strategy) as crawler:
        print("\n--- Running crawl with shared_data and custom hook ---")
        result = await crawler.arun(config=run_cfg)
        
        if result.success:
            print(f"Crawl to {result.url} successful.")
            # Check if the hook modified the shared_data (if the hook logic does so)
            # The hook we defined modifies the dictionary passed to it.
            # The `run_cfg.shared_data` is the one passed, so it should be modified.
            print(f"Shared data after crawl (modified by hook): {run_cfg.shared_data}")
            assert run_cfg.shared_data["hook_modified_value"] == "value_set_by_hook"
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(shared_data_example())
```

---
### 6.7. Page Navigation, Waits, and Timing

#### 6.7.1. Example: Changing `wait_until` to "load" or "networkidle".
Controls when Playwright considers navigation complete. Default is "domcontentloaded".

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def wait_until_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    url_to_crawl = "https://example.com" # A simple, fast-loading page

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Test with "domcontentloaded" (default)
        run_cfg_dom = CrawlerRunConfig(url=url_to_crawl, wait_until="domcontentloaded")
        start_time_dom = time.perf_counter()
        await crawler.arun(config=run_cfg_dom)
        duration_dom = time.perf_counter() - start_time_dom
        print(f"Wait_until 'domcontentloaded' duration: {duration_dom:.2f}s")

        # Test with "load" (waits for all resources like images)
        run_cfg_load = CrawlerRunConfig(url=url_to_crawl, wait_until="load")
        start_time_load = time.perf_counter()
        await crawler.arun(config=run_cfg_load)
        duration_load = time.perf_counter() - start_time_load
        print(f"Wait_until 'load' duration: {duration_load:.2f}s")
        # For a simple page, 'load' might not be much longer than 'domcontentloaded'
        # but for image-heavy pages, it would be.

        # Test with "networkidle" (waits for network to be idle for 500ms)
        run_cfg_idle = CrawlerRunConfig(url=url_to_crawl, wait_until="networkidle")
        start_time_idle = time.perf_counter()
        await crawler.arun(config=run_cfg_idle)
        duration_idle = time.perf_counter() - start_time_idle
        print(f"Wait_until 'networkidle' duration: {duration_idle:.2f}s")
        # 'networkidle' can be significantly longer if there are background network activities

if __name__ == "__main__":
    asyncio.run(wait_until_example())
```

---
#### 6.7.2. Example: Setting a custom `page_timeout` for page operations.
Maximum time (in ms) for operations like page navigation. Default is 60000ms (60s).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def page_timeout_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # A URL that is intentionally slow or non-existent to trigger timeout
    # httpbin.org/delay/X can simulate a slow response
    slow_url = "https://httpbin.org/delay/5" # Delays for 5 seconds

    # Set a short timeout to demonstrate
    run_cfg_short_timeout = CrawlerRunConfig(url=slow_url, page_timeout=2000) # 2 seconds
    
    print(f"Attempting to crawl {slow_url} with page_timeout=2000ms.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_short_timeout)
        if not result.success:
            print(f"Crawl failed as expected due to timeout: {result.error_message}")
            assert "Timeout" in result.error_message or "timeout" in result.error_message.lower()
        else:
            print("Crawl succeeded unexpectedly (timeout might not have triggered).")

if __name__ == "__main__":
    asyncio.run(page_timeout_example())
```

---
#### 6.7.3. Example: Using `wait_for` with a CSS selector before proceeding.
Waits for a specific element to appear in the DOM.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def wait_for_css_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # Simulate a page where content appears after a delay
    # We'll use JS to add an element after 2 seconds
    html_content_delayed = """
    <html><body>
        <div id='loading'>Loading...</div>
        <script>
            setTimeout(() => {
                const newElem = document.createElement('p');
                newElem.id = 'late-content';
                newElem.textContent = 'Content has arrived!';
                document.body.appendChild(newElem);
                document.getElementById('loading').remove();
            }, 2000);
        </script>
    </body></html>
    """
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_content_delayed}",
        wait_for="css:#late-content", # Wait for the element with id 'late-content'
        page_timeout=5000 # Overall timeout for the operation
    )
    
    print("Attempting to crawl page with delayed content, waiting for '#late-content'.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful. Cleaned HTML: {result.cleaned_html}")
            assert "Content has arrived!" in result.cleaned_html
            assert "Loading..." not in result.cleaned_html # Should be removed by the script
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(wait_for_css_example())
```

---
#### 6.7.4. Example: Using `wait_for` with a JavaScript predicate.
Waits for a JavaScript expression to evaluate to true.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def wait_for_js_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # Simulate a page where a JS variable changes after a delay
    html_js_var_delayed = """
    <html><body>
        <p id="status">Initializing...</p>
        <script>
            window.myAppStatus = 'pending';
            setTimeout(() => {
                window.myAppStatus = 'ready';
                document.getElementById('status').textContent = 'Application is Ready!';
            }, 2500);
        </script>
    </body></html>
    """
    
    js_predicate = "window.myAppStatus === 'ready'"
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_js_var_delayed}",
        wait_for=f"js:{js_predicate}",
        page_timeout=5000
    )
    
    print(f"Attempting to crawl page, waiting for JS: `{js_predicate}`.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful after JS condition met.")
            print(f"Cleaned HTML: {result.cleaned_html}")
            assert "Application is Ready!" in result.cleaned_html
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(wait_for_js_example())
```

---
#### 6.7.5. Example: Setting a specific `wait_for_timeout` for the `wait_for` condition.
If `wait_for` condition is not met within this timeout (in ms), the crawl proceeds or fails based on overall `page_timeout`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def wait_for_timeout_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    html_never_appears = "<html><body><p>Content</p></body></html>"
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_never_appears}",
        wait_for="css:#nonexistent-element",
        wait_for_timeout=1000, # Wait only 1 second for this specific element
        page_timeout=5000      # Overall page timeout
    )
    
    print("Attempting to crawl, waiting for a nonexistent element with a short wait_for_timeout.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        start_time = asyncio.get_event_loop().time()
        result = await crawler.arun(config=run_cfg)
        duration = asyncio.get_event_loop().time() - start_time
        
        if result.success:
            print(f"Crawl completed (likely after wait_for_timeout). Duration: {duration:.2f}s")
            # The crawl might succeed if page_timeout is longer and wait_for_timeout is just a pause point
            # Playwright's wait_for_selector with timeout usually throws an error if not found.
            # Crawl4ai's behavior might proceed if overall page_timeout isn't hit by this.
            # Let's check if the error message indicates a timeout related to the selector.
            # The current implementation of AsyncPlaywrightCrawlerStrategy would lead to an overall timeout.
            # If the wait_for_timeout is meant to be non-fatal, the strategy logic would need adjustment.
            # For now, we expect the page.wait_for_selector to raise a TimeoutError.
            print(f"Crawl error_message (if any): {result.error_message}")
            # assert "Timeout" in result.error_message if result.error_message else False
            # This behavior depends on how Playwright's timeout for wait_for_selector is handled.
            # If it raises and is caught, error_message will have it. If it's ignored, it might proceed.
            # Given current code, it's more likely the overall page_timeout will be hit if wait_for never resolves.
            # Let's assume for this test that wait_for failing to find element leads to crawl failing.
            if result.error_message and "Timeout" in result.error_message: # Playwright TimeoutError
                 print("Wait_for timed out as expected, and crawl might have failed due to it.")
            else:
                 print("Crawl succeeded, wait_for_timeout might have just passed and execution continued.")

        else: # More likely scenario if wait_for is critical
            print(f"Crawl failed as expected. Duration: {duration:.2f}s. Error: {result.error_message}")
            assert "Timeout" in result.error_message # Expecting a timeout error

if __name__ == "__main__":
    asyncio.run(wait_for_timeout_example())
```

---
#### 6.7.6. Example: Enabling `wait_for_images=True` to ensure images are loaded.
Attempts to wait until all (or most) images on the page have finished loading.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def wait_for_images_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    # A page with several images. Wikipedia pages are good for this.
    url_with_images = "https://en.wikipedia.org/wiki/Cat"
    
    run_cfg = CrawlerRunConfig(
        url=url_with_images,
        wait_for_images=True,
        screenshot=True # Take a screenshot to visually verify if images loaded
    )
    
    print(f"Attempting to crawl {url_with_images} with wait_for_images=True.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful for {url_with_images}.")
            if result.screenshot:
                print("Screenshot captured. Manually inspect 'wait_for_images_screenshot.png' to see if images are loaded.")
                with open("wait_for_images_screenshot.png", "wb") as f:
                    import base64
                    f.write(base64.b64decode(result.screenshot))
            # Check if media extraction found images (though this happens after HTML retrieval)
            if result.media and result.media.get("images"):
                print(f"Found {len(result.media['images'])} image entries.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(wait_for_images_example())
```

---
#### 6.7.7. Example: Setting `delay_before_return_html` to pause before final HTML retrieval.
Adds a fixed delay (in seconds) just before the final HTML content is grabbed from the page.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def delay_before_html_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        delay_before_return_html=2.0 # Wait 2 seconds
    )
    print(f"Configured delay_before_return_html: {run_cfg.delay_before_return_html}s")
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Crawling {run_cfg.url} with a 2s delay before HTML retrieval.")
        start_time = time.perf_counter()
        result = await crawler.arun(config=run_cfg)
        duration = time.perf_counter() - start_time
        
        if result.success:
            print(f"Crawl successful. Total duration: {duration:.2f}s (should be > 2s).")
            assert duration >= 2.0
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(delay_before_html_example())
```

---
### 6.8. `arun_many` Specific Timing and Concurrency

#### 6.8.1. Example: Configuring `mean_delay` and `max_range` for randomized delays between `arun_many` requests.
This adds a random delay between `(mean_delay - max_range)` and `(mean_delay + max_range)` seconds for each request in `arun_many`.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def arun_many_delay_example():
    browser_cfg = BrowserConfig(headless=True, verbose=False) # Keep logs cleaner for this demo
    urls = [f"https://example.com?page={i}" for i in range(3)] # 3 dummy URLs

    run_cfg = CrawlerRunConfig(
        mean_delay=1.0,  # Average 1 second delay
        max_range=0.5,   # Delay will be between 0.5s and 1.5s
        cache_mode=CacheMode.BYPASS # Ensure actual fetches
    )
    print(f"Configured for arun_many: mean_delay={run_cfg.mean_delay}, max_range={run_cfg.max_range}")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print("Running arun_many with delays...")
        start_time = time.perf_counter()
        # arun_many itself doesn't take these; they are interpreted by dispatchers.
        # The default MemoryAdaptiveDispatcher uses these.
        results = await crawler.arun_many(urls=urls, config=run_cfg)
        total_duration = time.perf_counter() - start_time
        
        success_count = sum(1 for r in results if r.success)
        print(f"Finished {success_count}/{len(urls)} crawls.")
        print(f"Total duration for {len(urls)} crawls: {total_duration:.2f}s")
        
        # Expected duration: roughly num_urls * mean_delay + (sum of actual fetch times)
        # For 3 URLs, with mean_delay=1, expect at least 2 * 0.5 = 1s of added delay, up to 2 * 1.5 = 3s
        # This is a rough check, actual network time also contributes.
        # If all example.com fetches are very fast (<0.1s), total should be around 1-3s + (3*~0.1s)
        # A more robust test would mock the fetch time.
        # For now, we just check it took longer than if no delays.
        assert total_duration > (len(urls) -1) * (run_cfg.mean_delay - run_cfg.max_range) if len(urls)>1 else True

if __name__ == "__main__":
    asyncio.run(arun_many_delay_example())
```

---
#### 6.8.2. Example: Adjusting `semaphore_count` to control concurrency in `arun_many`.
Limits the number of concurrent crawl operations when using `arun_many` with dispatchers that respect it (like `SemaphoreDispatcher`).

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from crawl4ai.async_dispatcher import SemaphoreDispatcher # Explicitly use SemaphoreDispatcher

async def arun_many_semaphore_example():
    browser_cfg = BrowserConfig(headless=True, verbose=False)
    # More URLs to demonstrate concurrency clearly
    urls = [f"https://httpbin.org/delay/1?id={i}" for i in range(6)] # Each takes ~1s

    run_cfg_concurrency_2 = CrawlerRunConfig(
        semaphore_count=2, # Allow only 2 concurrent crawls
        cache_mode=CacheMode.BYPASS
    )
    print(f"Configured for arun_many: semaphore_count={run_cfg_concurrency_2.semaphore_count}")

    # Use SemaphoreDispatcher to respect semaphore_count
    dispatcher = SemaphoreDispatcher(max_concurrent_tasks=run_cfg_concurrency_2.semaphore_count)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"Running arun_many with {len(urls)} URLs and semaphore_count=2...")
        start_time = time.perf_counter()
        results = await crawler.arun_many(urls=urls, config=run_cfg_concurrency_2, dispatcher=dispatcher)
        total_duration = time.perf_counter() - start_time
        
        success_count = sum(1 for r in results if r.success)
        print(f"Finished {success_count}/{len(urls)} crawls.")
        print(f"Total duration: {total_duration:.2f}s")
        
        # With 6 URLs, each taking ~1s, and concurrency of 2:
        # Expected time is roughly (num_urls / concurrency) * per_url_time
        # (6 / 2) * 1s = 3s (plus overhead)
        # Without semaphore, it might be closer to 1s if all run truly parallel (unlikely for 6 heavy tasks)
        # or up to 6s if strictly sequential.
        # This is a heuristic check
        assert total_duration >= (len(urls) / run_cfg_concurrency_2.semaphore_count) * 1.0 
        assert total_duration < len(urls) * 1.0 # Should be faster than purely sequential

if __name__ == "__main__":
    asyncio.run(arun_many_semaphore_example())
```

---
### 6.9. Page Interaction, Anti-Bot, and Dynamic Content

#### 6.9.1. Example: Executing a single JavaScript string using `js_code`.
Runs arbitrary JavaScript on the page after it loads.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def single_js_code_example():
    browser_cfg = BrowserConfig(headless=True)
    
    js_to_run = "document.body.innerHTML = '<h1>JavaScript Was Here!</h1>';"
    run_cfg = CrawlerRunConfig(
        url="https://example.com", # Original page content will be replaced
        js_code=js_to_run
    )
    
    print(f"Running JS: {js_to_run}")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} after JS execution.")
            print(f"Cleaned HTML: {result.cleaned_html}")
            assert "JavaScript Was Here!" in result.cleaned_html
            assert "Example Domain" not in result.cleaned_html # Original H1 should be gone
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(single_js_code_example())
```

---
#### 6.9.2. Example: Executing a list of JavaScript snippets using `js_code`.
Runs multiple JavaScript snippets sequentially.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def list_js_code_example():
    browser_cfg = BrowserConfig(headless=True)
    
    js_snippets = [
        "document.body.insertAdjacentHTML('beforeend', '<p id=\\'js_step1\\'>Step 1 Complete</p>');",
        "document.getElementById('js_step1').style.color = 'blue';",
        "window.js_result = document.getElementById('js_step1').outerHTML;" # Store for verification
    ]
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        js_code=js_snippets,
        # We need to retrieve the window.js_result
        # This can be done by having the last JS snippet return something,
        # or by executing another JS snippet to retrieve it in a subsequent step.
        # For simplicity, let's assume the crawler strategy can pick up the last expression value
        # (if it's a string, Playwright's page.evaluate often returns it).
        # More reliably, explicitly return the value:
        # js_snippets.append("window.js_result;")
    )
    
    print(f"Running JS snippets sequentially.")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # To get a JS execution result, the last JS statement must evaluate to a serializable value.
        # Let's modify js_snippets for this.
        run_cfg.js_code.append("window.js_result;") # type: ignore
        
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawled {result.url} after multiple JS executions.")
            print(f"Cleaned HTML (should contain 'Step 1 Complete'):\n{result.cleaned_html[:300]}")
            assert "Step 1 Complete" in result.cleaned_html
            
            print(f"JS Execution Result (outerHTML of #js_step1): {result.js_execution_result}")
            assert result.js_execution_result and 'style="color: blue;"' in result.js_execution_result
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(list_js_code_example())
```

---
#### 6.9.3. Example: Using `js_only=True` for subsequent calls within a session to only execute JS.
Useful for multi-step interactions where you only want to run JS on an already loaded page in a session, without a full page navigation.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def js_only_example():
    session_id = "js_only_session"
    browser_cfg = BrowserConfig(headless=True, verbose=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Step 1: Load the page
        run_cfg1 = CrawlerRunConfig(url="https://example.com", session_id=session_id)
        print("\n--- Step 1: Loading initial page ---")
        result1 = await crawler.arun(config=run_cfg1)
        if not result1.success:
            print(f"Initial page load failed: {result1.error_message}")
            return
        print(f"Initial page '{result1.url}' loaded. HTML length: {len(result1.html)}")

        # Step 2: Execute JS only, no re-navigation
        js_to_modify = "document.body.innerHTML = '<h1>JS Only Update!</h1>'; 'JS Executed';"
        run_cfg2 = CrawlerRunConfig(
            url=result1.url, # Important: use the same URL or the one currently loaded in session
            session_id=session_id,
            js_code=js_to_modify,
            js_only=True # This is key
        )
        print("\n--- Step 2: Executing JS only on the current page ---")
        result2 = await crawler.arun(config=run_cfg2)
        if result2.success:
            print(f"JS-only update successful for {result2.url}.")
            print(f"Cleaned HTML after JS-only: {result2.cleaned_html}")
            assert "JS Only Update!" in result2.cleaned_html
            assert "Example Domain" not in result2.cleaned_html
            print(f"JS Execution Result: {result2.js_execution_result}")
            assert result2.js_execution_result == "JS Executed"
        else:
            print(f"JS-only update failed: {result2.error_message}")
        
        await crawler.crawler_strategy.kill_session(session_id)
        print(f"\nSession '{session_id}' killed.")

if __name__ == "__main__":
    asyncio.run(js_only_example())
```

---
#### 6.9.4. Example: Enabling `scan_full_page=True` and setting `scroll_delay` to handle lazy-loaded content.
The crawler will attempt to scroll through the entire page, pausing between scrolls, to trigger lazy-loading.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def scan_full_page_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True, viewport_height=300) # Smaller viewport to make scrolling more evident
    
    # A page known for long content or lazy loading (e.g., a long blog post or news article)
    # For this demo, we'll use a simple page and check total scroll time.
    # Real effect is best seen on pages that actually lazy-load images/content.
    url_to_scan = "https://en.wikipedia.org/wiki/Web_scraping" 
    
    run_cfg_scan = CrawlerRunConfig(
        url=url_to_scan,
        scan_full_page=True,
        scroll_delay=0.2,  # 0.2 seconds delay between scroll steps
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling {url_to_scan} with scan_full_page=True, scroll_delay={run_cfg_scan.scroll_delay}s")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        start_time = time.perf_counter()
        result = await crawler.arun(config=run_cfg_scan)
        duration = time.perf_counter() - start_time
        
        if result.success:
            print(f"Crawl successful. Duration: {duration:.2f}s (includes scrolling time).")
            print(f"Markdown length: {len(result.markdown.raw_markdown)}")
            # On a long page, the duration should be noticeably longer than a non-scrolling crawl.
            # And more content/images might be present in result.html/result.media
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(scan_full_page_example())
```

---
#### 6.9.5. Example: Enabling `process_iframes=True` to attempt extracting content from iframes.
**Note:** Iframe processing can be complex and success depends heavily on iframe structure and cross-origin policies.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def process_iframes_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # Need a URL that reliably contains accessible iframes for a good demo.
    # W3Schools iframe example page:
    iframe_test_url = "https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_iframe_name"
    
    run_cfg_iframes = CrawlerRunConfig(
        url=iframe_test_url,
        process_iframes=True,
        cache_mode=CacheMode.BYPASS,
        wait_for="iframe#iframeResult" # Wait for the specific iframe to ensure it's targetable
    )
    print(f"Crawling {iframe_test_url} with process_iframes=True.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_iframes)
        if result.success:
            print(f"Crawl successful for {iframe_test_url}.")
            print(f"Cleaned HTML (first 1000 chars):\n{result.cleaned_html[:1000]}")
            # Check for content known to be inside an iframe on that page.
            # The W3Schools example iframe usually loads "demo_iframe.htm" which contains "This is an iframe".
            if "This is an iframe" in result.cleaned_html:
                print("SUCCESS: Content from iframe seems to be included in cleaned_html.")
            else:
                print("Content from iframe not found or iframe structure is complex/cross-origin restricted.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(process_iframes_example())
```

---
#### 6.9.6. Example: Using `remove_overlay_elements=True` to dismiss popups/modals.
Attempts to automatically find and remove common overlay/popup elements before content extraction.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def remove_overlays_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # A page that might have a cookie consent banner or modal.
    # This is hard to test reliably with a public static URL as overlays are dynamic.
    # We'll simulate an overlay with raw HTML for this example.
    html_with_overlay = """
    <html><body>
        <div id="page-content">Main page text here.</div>
        <div id="cookie-banner" style="position:fixed; bottom:0; background:lightgray; width:100%; padding:10px; text-align:center;">
            This is a cookie banner! <button onclick="this.parentElement.remove()">Accept</button>
        </div>
        <div class="modal-overlay" style="position:fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.5);">
            <div class="modal-content" style="background:white; margin:15% auto; padding:20px; width:50%;">A promotional modal!</div>
        </div>
    </body></html>
    """
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_overlay}",
        remove_overlay_elements=True,
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling content with remove_overlay_elements=True.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful.")
            cleaned_html = result.cleaned_html
            print(f"Cleaned HTML (should not contain overlay/banner):\n{cleaned_html}")
            assert "cookie-banner" not in cleaned_html.lower() # Check by ID
            assert "modal-overlay" not in cleaned_html.lower() # Check by class
            assert "Main page text here." in cleaned_html # Main content should remain
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(remove_overlays_example())
```

---
#### 6.9.7. Example: Enabling `simulate_user=True` for basic user interaction simulation.
Triggers basic mouse movements and scrolls to mimic user presence, potentially bypassing some simple anti-bot measures.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def simulate_user_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True) # Can be False to observe
    
    run_cfg = CrawlerRunConfig(
        url="https://example.com", # A simple page for demonstration
        simulate_user=True,
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling {run_cfg.url} with simulate_user=True.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful with user simulation.")
            # The effect of simulate_user is subtle and hard to verify programmatically
            # on a simple page. It's more about behavior during the crawl.
            # If there were JS event listeners for mouse/scroll, they might have been triggered.
            print(f"Markdown (first 300 chars):\n{result.markdown.raw_markdown[:300]}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(simulate_user_example())
```

---
#### 6.9.8. Example: Enabling `override_navigator=True` to modify browser navigator properties.
Modifies properties like `navigator.webdriver` to make the browser appear less like an automated tool.

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def override_navigator_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # JS to check navigator.webdriver property
    js_check_webdriver = "JSON.stringify({webdriver: navigator.webdriver})"
    
    run_cfg_override = CrawlerRunConfig(
        url="https://httpbin.org/anything", # Any page will do for this JS check
        override_navigator=True,
        js_code=js_check_webdriver,
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling with override_navigator=True.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_override)
        if result.success and result.js_execution_result:
            print(f"Crawl successful with navigator override.")
            webdriver_status = result.js_execution_result
            print(f"navigator.webdriver status: {webdriver_status}")
            # When overridden, navigator.webdriver should be false
            assert webdriver_status.get("webdriver") is False
        elif not result.js_execution_result:
             print(f"JS execution did not return a result for webdriver check.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(override_navigator_example())
```

---
#### 6.9.9. Example: Using `magic=True` for automated handling of common anti-bot measures like overlays.
`magic=True` is a convenience flag that enables several anti-bot evasion techniques, including `remove_overlay_elements` and `override_navigator`.

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def magic_mode_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # HTML with an overlay and JS that checks navigator.webdriver
    html_with_magic_challenges = """
    <html><body>
        <div id="page-content">Main content.</div>
        <div id="my-overlay" style="position:fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.3);">Overlay</div>
        <script>
            window.webdriverStatus = navigator.webdriver;
        </script>
    </body></html>
    """
    js_get_webdriver = "JSON.stringify({webdriver: window.webdriverStatus})"

    run_cfg_magic = CrawlerRunConfig(
        url=f"raw://{html_with_magic_challenges}",
        magic=True,
        js_code=js_get_webdriver, # Check webdriver status after magic applies
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling with magic=True.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg_magic)
        if result.success:
            print(f"Crawl successful with magic mode.")
            
            # Check overlay removal
            print(f"Cleaned HTML (overlay should be gone):\n{result.cleaned_html}")
            assert "my-overlay" not in result.cleaned_html # Magic mode should remove it
            assert "Main content." in result.cleaned_html

            # Check navigator override
            if result.js_execution_result:
                webdriver_status = result.js_execution_result
                print(f"navigator.webdriver status (via JS): {webdriver_status}")
                assert webdriver_status.get("webdriver") is False # Magic mode should override this
            else:
                print("JS execution for webdriver check did not yield result.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(magic_mode_example())
```

---
#### 6.9.10. Example: Using `adjust_viewport_to_content=True` to dynamically resize viewport.
Resizes the browser viewport to fit the dimensions of the rendered page content. This is useful for accurate screenshots of the entire logical page.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def adjust_viewport_example():
    # A page that's taller than the default viewport
    # We'll use a raw HTML example with a defined large height for predictability
    tall_page_html = """
    <html><head><style>body {{ margin: 0; }} #tall-div {{ height: 2000px; width: 500px; background: lightblue; }}</style></head>
    <body><div id="tall-div">This is a tall div.</div></body></html>
    """
    
    browser_cfg = BrowserConfig(
        headless=True, 
        verbose=True,
        viewport_width=800, # Initial viewport
        viewport_height=600
    )
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{tall_page_html}",
        adjust_viewport_to_content=True,
        screenshot=True, # Capture screenshot to see the effect
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling with adjust_viewport_to_content=True. Initial viewport: 800x600.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print(f"Crawl successful.")
            if result.screenshot:
                print("Screenshot captured. If adjust_viewport_to_content worked, it should show the full 2000px height.")
                # Playwright's screenshot of full page might not directly reflect viewport size change,
                # but the internal logic for determining content height would have used the adjusted viewport.
                # The primary effect is on how Playwright calculates "full page".
                # It's hard to verify viewport size change directly without inspecting browser internals during run.
                # The key is that `page.content()` or screenshot operations consider the full content.
                print("Effect is primarily on internal full-page calculations for Playwright.")
            else:
                print("Screenshot not captured.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(adjust_viewport_example())
```

---
### 6.10. Media Capturing and Processing Options

#### 6.10.1. Example: Enabling screenshots with `screenshot=True`.
Captures a screenshot of the page.

```python
import asyncio
import base64
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def enable_screenshot_example():
    browser_cfg = BrowserConfig(headless=True)
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        screenshot=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.screenshot:
            print(f"Screenshot captured for {result.url}.")
            screenshot_path = Path("./example_com_screenshot.png")
            with open(screenshot_path, "wb") as f:
                f.write(base64.b64decode(result.screenshot))
            print(f"Screenshot saved to {screenshot_path.resolve()}")
            assert screenshot_path.exists() and screenshot_path.stat().st_size > 0
            screenshot_path.unlink() # Clean up
        elif not result.screenshot:
            print("Screenshot was enabled but not captured.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(enable_screenshot_example())
```

---
#### 6.10.2. Example: Using `screenshot_wait_for` (float) to add a delay before taking a screenshot.
Adds a specific delay (in seconds) before the screenshot is taken, useful for animations or late-loading elements.

```python
import asyncio
import base64
from pathlib import Path
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def screenshot_wait_for_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # Page that might have some animation or late change
    # For demo, we'll just time it.
    url_to_crawl = "https://example.com" 
    delay_seconds = 1.5
    
    run_cfg = CrawlerRunConfig(
        url=url_to_crawl,
        screenshot=True,
        screenshot_wait_for=delay_seconds, # Wait 1.5 seconds before screenshot
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling {url_to_crawl}, will wait {delay_seconds}s before screenshot.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        start_time = time.perf_counter()
        result = await crawler.arun(config=run_cfg)
        duration = time.perf_counter() - start_time
        
        if result.success and result.screenshot:
            print(f"Screenshot captured. Total crawl duration: {duration:.2f}s (should be >= {delay_seconds}s).")
            assert duration >= delay_seconds
            # Save for manual inspection if needed
            # screenshot_path = Path("./delayed_screenshot.png")
            # with open(screenshot_path, "wb") as f:
            #     f.write(base64.b64decode(result.screenshot))
            # print(f"Screenshot saved to {screenshot_path.resolve()}")
            # screenshot_path.unlink()
        else:
            print(f"Crawl or screenshot failed: {result.error_message if result else 'Unknown error'}")

if __name__ == "__main__":
    asyncio.run(screenshot_wait_for_example())
```

---
#### 6.10.3. Example: Customizing `screenshot_height_threshold` for full-page screenshot strategy.
If page height exceeds this threshold, Playwright might use a different strategy for full-page screenshots (e.g. stitching). Default is 20000.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def screenshot_height_threshold_example():
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    
    # This primarily affects how Playwright takes the full page screenshot internally.
    # The output (base64 image) will still be the full page.
    run_cfg = CrawlerRunConfig(
        url="https://en.wikipedia.org/wiki/Python_(programming_language)", # A long page
        screenshot=True,
        screenshot_height_threshold=5000, # Lower threshold, might trigger stitching earlier
        cache_mode=CacheMode.BYPASS
    )
    print(f"Crawling with screenshot_height_threshold={run_cfg.screenshot_height_threshold}.")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.screenshot:
            print(f"Screenshot captured for long page.")
            # Verification of the *strategy* change is internal to Playwright
            # and not easily asserted from outside. The result is still a full screenshot.
            assert len(result.screenshot) > 1000 # Check if screenshot data exists
        else:
            print(f"Crawl or screenshot failed: {result.error_message if result else 'Unknown error'}")

if __name__ == "__main__":
    asyncio.run(screenshot_height_threshold_example())
```

---
#### 6.10.4. Example: Enabling PDF generation with `pdf=True`.
Generates a PDF of the rendered page.

```python
import asyncio
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def enable_pdf_generation():
    browser_cfg = BrowserConfig(headless=True) # PDF generation requires headless
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        pdf=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.pdf:
            print(f"PDF generated for {result.url}.")
            pdf_path = Path("./example_com_page.pdf")
            with open(pdf_path, "wb") as f:
                f.write(result.pdf)
            print(f"PDF saved to {pdf_path.resolve()}")
            assert pdf_path.exists() and pdf_path.stat().st_size > 0
            pdf_path.unlink() # Clean up
        elif not result.pdf:
            print("PDF was enabled but not generated.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(enable_pdf_generation())
```

---
#### 6.10.5. Example: Enabling MHTML archive capture with `capture_mhtml=True`.
Saves the page as an MHTML archive, which includes all resources (CSS, images, JS) in a single file.

```python
import asyncio
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def capture_mhtml_example():
    browser_cfg = BrowserConfig(headless=True)
    run_cfg = CrawlerRunConfig(
        url="https://example.com",
        capture_mhtml=True,
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.mhtml:
            print(f"MHTML archive captured for {result.url}.")
            mhtml_path = Path("./example_com_page.mhtml")
            with open(mhtml_path, "w", encoding="utf-8") as f: # MHTML is text-based
                f.write(result.mhtml)
            print(f"MHTML saved to {mhtml_path.resolve()}")
            assert mhtml_path.exists() and mhtml_path.stat().st_size > 0
            mhtml_path.unlink() # Clean up
        elif not result.mhtml:
            print("MHTML was enabled but not captured.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(capture_mhtml_example())
```

---
#### 6.10.6. Example: Setting `image_description_min_word_threshold` for image alt/desc processing.
Filters image descriptions (alt text or surrounding text) based on word count.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def image_desc_threshold_example():
    # HTML with images having varying alt text lengths
    html_with_img_alts = """
    <html><body>
        <img src="img1.jpg" alt="A single word">
        <img src="img2.jpg" alt="This is a short description of five words">
        <img src="img3.jpg" alt="This alt text is definitely longer and should pass a higher threshold.">
    </body></html>
    """
    # Default for image_description_min_word_threshold is 50 in config.py, but often effectively lower if not using LLM for desc.
    # The WebScrapingStrategy uses it for finding desc from surrounding text.
    # Here, we test its effect on alt text directly (assuming internal logic considers alt text length)
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_img_alts}",
        image_description_min_word_threshold=4, # Only consider alts/desc with >= 4 words
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with image_description_min_word_threshold={run_cfg.image_description_min_word_threshold}.")
    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.media and result.media.get("images"):
            print(f"Images found: {len(result.media['images'])}")
            for img in result.media["images"]:
                print(f"  src: {img.get('src')}, alt: '{img.get('alt')}', desc: '{img.get('desc')}'")
            
            # Expect only img2 and img3 to have their alt text considered substantial enough for desc
            # (assuming desc field is populated from alt if alt meets threshold)
            # This depends on the scraping strategy's logic for populating 'desc'.
            # The default WebScrapingStrategy populates 'desc' from parent text if alt is short.
            # A direct test of the threshold on 'alt' itself is tricky without knowing exact internal logic.
            # Let's assume the intent is that 'alt' contributes to 'desc' if long enough.
            
            # This test is more conceptual about the parameter's existence.
            # The actual filtering based on this threshold happens within the scraping strategy logic.
            # For now, we confirm that images are extracted.
            assert len(result.media["images"]) == 3 
            print("Parameter set. Actual filtering of descriptions occurs in scraping strategy.")

        else:
            print(f"Crawl failed or no images: {result.error_message if result else 'No result'}")

if __name__ == "__main__":
    asyncio.run(image_desc_threshold_example())
```

---
#### 6.10.7. Example: Setting `image_score_threshold` for filtering images by relevance.
Images scoring below this heuristic threshold might be discarded.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def image_score_threshold_example():
    # WebScrapingStrategy scores images. Default threshold is 3.
    # Images with good alt text, dimensions, and not from common "ignore" patterns get higher scores.
    
    html_with_varied_images = """
    <html><body>
        <img src="good_image.jpg" alt="A very descriptive alt text for a good image" width="300" height="200"> <!-- High score -->
        <img src="icon.png" alt="icon" width="16" height="16"> <!-- Low score -->
        <img src="decorative.gif"> <!-- No alt, no dimensions, likely low score -->
    </body></html>
    """
    
    run_cfg_high_thresh = CrawlerRunConfig(
        url=f"raw://{html_with_varied_images}",
        image_score_threshold=4, # Higher threshold, expect fewer images
        cache_mode=CacheMode.BYPASS
    )
    run_cfg_low_thresh = CrawlerRunConfig(
        url=f"raw://{html_with_varied_images}",
        image_score_threshold=1, # Lower threshold, expect more images
        cache_mode=CacheMode.BYPASS
    )
    
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"\n--- Crawling with image_score_threshold={run_cfg_high_thresh.image_score_threshold} ---")
        result_high = await crawler.arun(config=run_cfg_high_thresh)
        images_high = result_high.media.get("images", []) if result_high.success else []
        print(f"Images found (high threshold): {len(images_high)}")
        for img in images_high: print(f"  - {img.get('src')}, score: {img.get('score')}")

        print(f"\n--- Crawling with image_score_threshold={run_cfg_low_thresh.image_score_threshold} ---")
        result_low = await crawler.arun(config=run_cfg_low_thresh)
        images_low = result_low.media.get("images", []) if result_low.success else []
        print(f"Images found (low threshold): {len(images_low)}")
        for img in images_low: print(f"  - {img.get('src')}, score: {img.get('score')}")
        
        if result_high.success and result_low.success:
             assert len(images_high) <= len(images_low)
             if images_high: # Check if the good image passed the high threshold
                 assert any(img.get('src') == "good_image.jpg" for img in images_high)
             if len(images_low) > len(images_high): # Check if lower threshold included more
                 assert any(img.get('src') == "icon.png" for img in images_low) or \
                        any(img.get('src') == "decorative.gif" for img in images_low)

if __name__ == "__main__":
    asyncio.run(image_score_threshold_example())
```

---
#### 6.10.8. Example: Setting `table_score_threshold` for filtering tables by relevance.
Tables scoring below this heuristic threshold might be discarded. Default is 7.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def table_score_threshold_example():
    html_with_tables = """
    <html><body>
        <p>Layout table (low score):</p>
        <table><tr><td>Cell 1</td><td>Cell 2</td></tr></table>

        <p>Data table (high score):</p>
        <table id="data-table-1">
            <caption>My Data</caption>
            <thead><tr><th>Header 1</th><th>Header 2</th></tr></thead>
            <tbody>
                <tr><td>Data A1</td><td>Data B1</td></tr>
                <tr><td>Data A2</td><td>Data B2</td></tr>
            </tbody>
        </table>
    </body></html>
    """
    
    run_cfg_high_thresh = CrawlerRunConfig(
        url=f"raw://{html_with_tables}",
        table_score_threshold=10, # Higher threshold, expect fewer/no tables
        cache_mode=CacheMode.BYPASS
    )
    run_cfg_low_thresh = CrawlerRunConfig(
        url=f"raw://{html_with_tables}",
        table_score_threshold=5,  # Lower threshold, data table should pass
        cache_mode=CacheMode.BYPASS
    )
    
    browser_cfg = BrowserConfig(headless=True, verbose=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print(f"\n--- Crawling with table_score_threshold={run_cfg_high_thresh.table_score_threshold} ---")
        result_high = await crawler.arun(config=run_cfg_high_thresh)
        tables_high = result_high.media.get("tables", []) if result_high.success else []
        print(f"Tables found (high threshold): {len(tables_high)}")
        for i, tbl in enumerate(tables_high): print(f"  - Table {i} caption: {tbl.get('caption', 'N/A')}")

        print(f"\n--- Crawling with table_score_threshold={run_cfg_low_thresh.table_score_threshold} ---")
        result_low = await crawler.arun(config=run_cfg_low_thresh)
        tables_low = result_low.media.get("tables", []) if result_low.success else []
        print(f"Tables found (low threshold): {len(tables_low)}")
        for i, tbl in enumerate(tables_low): print(f"  - Table {i} caption: {tbl.get('caption', 'N/A')}")
        
        if result_high.success and result_low.success:
            assert len(tables_high) <= len(tables_low)
            if tables_low: # The data table should be found with lower threshold
                assert any("My Data" in tbl.get("caption", "") for tbl in tables_low)
            if not tables_high: # High threshold might exclude all
                print("High threshold correctly excluded tables.")

if __name__ == "__main__":
    asyncio.run(table_score_threshold_example())
```

---
#### 6.10.9. Example: Enabling `exclude_external_images=True` to ignore images from other domains.
Only images hosted on the same domain (or subdomains) as the crawled URL will be included.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def exclude_external_images_example():
    html_with_mixed_images = """
    <html><body>
        <p>Internal Image:</p>
        <img src="/images/local_image.png" alt="Local">
        <p>External Image:</p>
        <img src="https://cdn.anotherdomain.com/image.jpg" alt="External CDN">
        <p>Same domain, different subdomain (should be kept if base domain matches):</p>
        <img src="https://img.example.com/another_local.png" alt="Subdomain Local">
    </body></html>
    """
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_mixed_images}", # Base URL for context
        # url="https://example.com", # If using a live page, set base_url for raw HTML
        base_url="https://example.com",   # Needed for raw HTML to determine internal/external
        exclude_external_images=True,
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with exclude_external_images=True. Base URL for resolving: {run_cfg.base_url}")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success and result.media and result.media.get("images"):
            print(f"Images found: {len(result.media['images'])}")
            external_found = False
            internal_found_count = 0
            for img in result.media["images"]:
                print(f"  - Src: {img.get('src')}, Alt: {img.get('alt')}")
                if "anotherdomain.com" in img.get('src', ''):
                    external_found = True
                if "example.com" in img.get('src', '') or img.get('src', '').startswith("/images"):
                    internal_found_count +=1
            
            assert not external_found, "External image was not excluded"
            assert internal_found_count == 2, f"Expected 2 internal/same-domain images, found {internal_found_count}"
            print("SUCCESS: External images excluded, internal/same-domain images kept.")
        else:
            print(f"Crawl failed or no images: {result.error_message if result else 'No result'}")

if __name__ == "__main__":
    asyncio.run(exclude_external_images_example())
```

---
#### 6.10.10. Example: Enabling `exclude_all_images=True` to remove all images.
No images will be processed or included in `result.media["images"]`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def exclude_all_images_example():
    html_with_images = """
    <html><body>
        <img src="local.jpg" alt="Local">
        <img src="https://cdn.example.com/external.png" alt="External">
    </body></html>
    """
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_images}",
        exclude_all_images=True,
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with exclude_all_images=True.")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            images_found = result.media.get("images", [])
            print(f"Images found: {len(images_found)}")
            assert not images_found, "Images were found even though exclude_all_images was True."
            print("SUCCESS: All images were excluded.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(exclude_all_images_example())
```

---
### 6.11. Link and Domain Filtering Options

#### 6.11.1. Example: Customizing the list of `exclude_social_media_domains`.
Provide your own list of social media domains to exclude, overriding or extending the default list.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def custom_social_media_domains_example():
    html_with_links = """
    <html><body>
        <a href="https://facebook.com/somepage">Facebook</a>
        <a href="https://linkedin.com/in/someone">LinkedIn</a>
        <a href="https://mycustomsocial.net/profile">My Custom Social</a>
        <a href="https://example.com/internal">Internal</a>
    </body></html>
    """
    
    # Default list includes facebook.com, linkedin.com, etc.
    # Let's exclude only "mycustomsocial.net" and keep others for this test.
    # To do this, we must also enable exclude_social_media_links=True.
    custom_social_domains = ["mycustomsocial.net"]
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_links}",
        base_url="https://anotherexample.com", # To make example.com external
        exclude_social_media_links=True, # This flag enables the filtering
        exclude_social_media_domains=custom_social_domains, # Provide our custom list
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with custom exclude_social_media_domains: {custom_social_domains}")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            print("Links found in result.links['external']:")
            mycustomsocial_found = False
            facebook_found = False
            linkedin_found = False
            for link_info in result.links.get("external", []):
                print(f"  - {link_info.get('href')}")
                if "mycustomsocial.net" in link_info.get('href', ''):
                    mycustomsocial_found = True
                if "facebook.com" in link_info.get('href', ''):
                    facebook_found = True
                if "linkedin.com" in link_info.get('href', ''):
                    linkedin_found = True
            
            assert not mycustomsocial_found, "Custom social domain 'mycustomsocial.net' was not excluded."
            # Since we overrode the list, default social domains should now be included if not in our custom list.
            # However, exclude_social_media_links = True with a custom list should *only* exclude those in the custom list from the "social" category.
            # The behavior of exclude_social_media_domains is to *add* to the default list if not replacing.
            # The provided code has exclude_social_media_domains take precedence and replaces the default if exclude_social_media_links is true
            # (Looking at `async_webcrawler.py` processing of these flags).
            # The code's logic is: if `exclude_social_media_links` is true, it uses `self.config.exclude_social_media_domains` (which defaults to `config.SOCIAL_MEDIA_DOMAINS`)
            # So, if `exclude_social_media_domains` is set in `CrawlerRunConfig`, it effectively *replaces* the default list for that run.
            print("As exclude_social_media_domains was set, it replaces the default list.")
            assert facebook_found, "Facebook link was unexpectedly excluded with custom list."
            assert linkedin_found, "LinkedIn link was unexpectedly excluded with custom list."
            print("SUCCESS: Custom social media domain excluded, others (not in custom list) were kept.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(custom_social_media_domains_example())
```

---
#### 6.11.2. Example: Enabling `exclude_external_links=True` to remove links to other domains.
Only links within the same base domain as the crawled URL will be kept in `result.links`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def exclude_external_links_example():
    html_with_mixed_links = """
    <html><body>
        <a href="/internal-page">Internal Page</a>
        <a href="https://example.com/another-internal">Another Internal</a>
        <a href="https://sub.example.com/sub-internal">Subdomain Internal</a>
        <a href="https://otherdomain.com/external-page">External Page</a>
    </body></html>
    """
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_mixed_links}",
        base_url="https://example.com", # Define the base domain for this raw HTML
        exclude_external_links=True,
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with exclude_external_links=True. Base URL: {run_cfg.base_url}")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            external_links = result.links.get("external", [])
            internal_links = result.links.get("internal", [])
            print(f"External links found: {len(external_links)}")
            for link in external_links: print(f"  - {link.get('href')}")
            print(f"Internal links found: {len(internal_links)}")
            for link in internal_links: print(f"  - {link.get('href')}")

            assert not external_links, "External links were not excluded."
            assert len(internal_links) == 3, f"Expected 3 internal links, found {len(internal_links)}."
            print("SUCCESS: External links excluded, internal links kept.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(exclude_external_links_example())
```

---
#### 6.11.3. Example: Enabling `exclude_social_media_links=True` (uses `exclude_social_media_domains` list).
Filters out links to common social media platforms using the default or a custom list.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode, config as crawl4ai_config

async def exclude_social_media_links_example():
    html_with_social_links = f"""
    <html><body>
        <a href="https://example.com/normal">Normal Link</a>
        <a href="https://{crawl4ai_config.SOCIAL_MEDIA_DOMAINS[0]}/someuser">Social Media Link 1</a>
        <a href="https://{crawl4ai_config.SOCIAL_MEDIA_DOMAINS[1]}/anotheruser">Social Media Link 2</a>
    </body></html>
    """
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_social_links}",
        base_url="https://example.com",
        exclude_social_media_links=True, # Uses default SOCIAL_MEDIA_DOMAINS list
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with exclude_social_media_links=True.")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            external_links = result.links.get("external", [])
            print(f"External links found ({len(external_links)}):")
            social_links_found = 0
            for link_info in external_links:
                print(f"  - {link_info.get('href')}")
                if any(domain in link_info.get('href', '') for domain in crawl4ai_config.SOCIAL_MEDIA_DOMAINS):
                    social_links_found +=1
            
            assert social_links_found == 0, f"Found {social_links_found} social media links, expected 0."
            # There are no "internal" links to example.com in this raw HTML case, so internal will be empty.
            # The "Normal Link" is considered external here if base_url is example.com and link is relative
            # For this test, it is important that social links are not present in *any* list.
            # Since they are external, they would appear in external_links if not filtered.
            
            # Check internal links too (though none are social here)
            internal_links = result.links.get("internal", [])
            for link_info in internal_links:
                 if any(domain in link_info.get('href', '') for domain in crawl4ai_config.SOCIAL_MEDIA_DOMAINS):
                    social_links_found +=1 # Should not happen
            assert social_links_found == 0, "Social media links found in internal links."

            print("SUCCESS: Social media links were excluded.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(exclude_social_media_links_example())
```

---
#### 6.11.4. Example: Providing a custom list of `exclude_domains` for link filtering.
Blacklist specific domains from appearing in the extracted links.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def custom_exclude_domains_example():
    html_with_various_links = """
    <html><body>
        <a href="https://example.com/page1">Allowed Internal</a>
        <a href="https://www.allowed-external.com/path">Allowed External</a>
        <a href="https://www.blocked-domain1.com/content">Blocked Domain 1</a>
        <a href="http://sub.blocked-domain2.org/another">Blocked Domain 2</a>
    </body></html>
    """
    excluded_domains_list = ["blocked-domain1.com", "blocked-domain2.org"]
    
    run_cfg = CrawlerRunConfig(
        url=f"raw://{html_with_various_links}",
        base_url="https://example.com",
        exclude_domains=excluded_domains_list,
        cache_mode=CacheMode.BYPASS
    )
    
    print(f"Crawling with exclude_domains: {excluded_domains_list}")
    browser_cfg = BrowserConfig(headless=True)
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(config=run_cfg)
        if result.success:
            all_links = result.links.get("internal", []) + result.links.get("external", [])
            print(f"Total links found after filtering: {len(all_links)}")
            
            blocked_found = False
            for link_info in all_links:
                print(f"  - {link_info.get('href')}")
                if any(blocked_domain in link_info.get('href', '') for blocked_domain in excluded_domains_list):
                    blocked_found = True
                    print(f"ERROR: Found a blocked domain link: {link_info.get('href')}")
            
            assert not blocked_found, "One or more explicitly excluded domains were found in links."
            assert len(all_links) == 2 # Only the two allowed links should remain
            print("SUCCESS: Specified domains were excluded from links.")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(custom_exclude_domains_example())
```

---
#### 6.11.5. Example: Enabling `exclude_internal_links=True` to remove links within the same domain.
Only external links will be kept in `result.links`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode

async def exclude_internal_links_example():
    html_with_mixed_links = """
    <html><body>
        <a href="/internal-page1">Internal Page 1</a>
        <a href="https://example.com/internal-page2">Internal Page 2</a>
        <

---


## Deep Crawling - Memory
Source: crawl4ai_deep_crawling_memory_content.llm.md

Okay, I have read and processed the information. I will now generate the detailed "Foundational Memory" document for the `deep_crawling` component of `crawl4ai`, based on the outline from the previous step and the provided code snippets.

```markdown
# Detailed Foundational Memory for crawl4ai - deep_crawling Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_deep_crawling.md`
**Library Version Context:** 0.6.3 (from `crawl4ai/__version__.py`)
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Deep Crawling

*   1.1. Purpose: The `deep_crawling` component provides functionalities for recursively crawling web pages starting from an initial URL. It includes strategies for different traversal orders (BFS, DFS, Best-First), mechanisms for filtering which URLs to visit, and methods for scoring URLs to prioritize crawling.
*   1.2. Core Concepts:
    *   1.2.1. Definition of Deep Crawling in Crawl4ai context: The process of discovering and fetching multiple web pages by following links from an initial set of URLs, adhering to specified depth, page limits, and filtering/scoring rules.
    *   1.2.2. Key Abstractions:
        *   `DeepCrawlStrategy`: Defines the algorithm for traversing linked web pages (e.g., BFS, DFS).
        *   `URLFilter`: Determines whether a discovered URL should be considered for crawling.
        *   `URLScorer`: Assigns a score to URLs to influence crawling priority, especially in strategies like Best-First.

## 2. `DeepCrawlStrategy` Interface and Implementations

*   **2.1. `DeepCrawlStrategy` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/base_strategy.py`
    *   2.1.1. Purpose: Defines the abstract base class for all deep crawling strategies, outlining the core methods required for traversal logic, resource management, URL validation, and link discovery.
    *   2.1.2. Key Abstract Methods:
        *   `async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]`:
            *   Description: Core logic for batch (non-streaming) deep crawling. Processes URLs level by level (or according to strategy) and returns all results once the crawl is complete or limits are met.
        *   `async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]`:
            *   Description: Core logic for streaming deep crawling. Processes URLs and yields `CrawlResult` objects as they become available.
        *   `async def shutdown(self) -> None`:
            *   Description: Cleans up any resources used by the deep crawl strategy, such as signaling cancellation events.
        *   `async def can_process_url(self, url: str, depth: int) -> bool`:
            *   Description: Validates a given URL and current depth against configured filters and limits to decide if it should be processed.
        *   `async def link_discovery(self, result: CrawlResult, source_url: str, current_depth: int, visited: Set[str], next_level: List[tuple], depths: Dict[str, int]) -> None`:
            *   Description: Extracts links from a `CrawlResult`, validates them using `can_process_url`, optionally scores them, and appends valid URLs (and their parent references) to the `next_level` list. Updates the `depths` dictionary for newly discovered URLs.
    *   2.1.3. Key Concrete Methods:
        *   `async def arun(self, start_url: str, crawler: AsyncWebCrawler, config: Optional[CrawlerRunConfig] = None) -> RunManyReturn`:
            *   Description: Main entry point for initiating a deep crawl. It checks if a `CrawlerRunConfig` is provided and then delegates to either `_arun_stream` or `_arun_batch` based on the `config.stream` flag.
        *   `def __call__(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig)`:
            *   Description: Makes the strategy instance callable, directly invoking the `arun` method.
    *   2.1.4. Attributes:
        *   `_cancel_event (asyncio.Event)`: Event to signal cancellation of the crawl.
        *   `_pages_crawled (int)`: Counter for the number of pages successfully crawled.

*   **2.2. `BFSDeepCrawlStrategy`**
    *   Source: `crawl4ai/deep_crawling/bfs_strategy.py`
    *   2.2.1. Purpose: Implements a Breadth-First Search (BFS) deep crawling strategy, exploring all URLs at the current depth level before moving to the next.
    *   2.2.2. Inheritance: `DeepCrawlStrategy`
    *   2.2.3. Initialization (`__init__`)
        *   2.2.3.1. Signature:
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                score_threshold: float = -float('inf'),
                max_pages: int = float('inf'),
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.2.3.2. Parameters:
            *   `max_depth (int)`: Maximum depth to crawl relative to the `start_url`.
            *   `filter_chain (FilterChain`, default: `FilterChain()`)`: A `FilterChain` instance to apply to discovered URLs.
            *   `url_scorer (Optional[URLScorer]`, default: `None`)`: An optional `URLScorer` to score URLs. If provided, URLs below `score_threshold` are skipped, and for crawls exceeding `max_pages`, higher-scored URLs are prioritized.
            *   `include_external (bool`, default: `False`)`: If `True`, allows crawling of URLs from external domains.
            *   `score_threshold (float`, default: `-float('inf')`)`: Minimum score (if `url_scorer` is used) for a URL to be processed.
            *   `max_pages (int`, default: `float('inf')`)`: Maximum total number of pages to crawl.
            *   `logger (Optional[logging.Logger]`, default: `None`)`: An optional logger instance. If `None`, a default logger is created.
    *   2.2.4. Key Implemented Methods:
        *   `_arun_batch(...)`: Implements BFS traversal by processing URLs level by level. It collects all results from a level before discovering links for the next level. All results are returned as a list upon completion.
        *   `_arun_stream(...)`: Implements BFS traversal, yielding `CrawlResult` objects as soon as they are processed within a level. Link discovery for the next level happens after all URLs in the current level are processed and their results yielded.
        *   `can_process_url(...)`: Validates URL format, applies the `filter_chain`, and checks depth limits. For the start URL (depth 0), filtering is bypassed.
        *   `link_discovery(...)`: Extracts internal (and optionally external) links, normalizes them, checks against `visited` set and `can_process_url`. If a `url_scorer` is present and `max_pages` limit is a concern, it scores and sorts valid links, selecting the top ones within `remaining_capacity`.
        *   `shutdown(...)`: Sets an internal `_cancel_event` to signal graceful termination and records the end time in `stats`.
    *   2.2.5. Key Attributes/Properties:
        *   `stats (TraversalStats)`: [Read-only] - Instance of `TraversalStats` tracking the progress and statistics of the crawl.
        *   `max_depth (int)`: Maximum crawl depth.
        *   `filter_chain (FilterChain)`: The filter chain used.
        *   `url_scorer (Optional[URLScorer])`: The URL scorer used.
        *   `include_external (bool)`: Flag for including external URLs.
        *   `score_threshold (float)`: URL score threshold.
        *   `max_pages (int)`: Maximum pages to crawl.

*   **2.3. `DFSDeepCrawlStrategy`**
    *   Source: `crawl4ai/deep_crawling/dfs_strategy.py`
    *   2.3.1. Purpose: Implements a Depth-First Search (DFS) deep crawling strategy, exploring as far as possible along each branch before backtracking.
    *   2.3.2. Inheritance: `BFSDeepCrawlStrategy` (Note: Leverages much of the `BFSDeepCrawlStrategy`'s infrastructure but overrides traversal logic to use a stack.)
    *   2.3.3. Initialization (`__init__`)
        *   2.3.3.1. Signature: (Same as `BFSDeepCrawlStrategy`)
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                score_threshold: float = -float('inf'),
                max_pages: int = infinity,
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.3.3.2. Parameters: Same as `BFSDeepCrawlStrategy`.
    *   2.3.4. Key Overridden/Implemented Methods:
        *   `_arun_batch(...)`: Implements DFS traversal using a LIFO stack. Processes one URL at a time, discovers its links, and adds them to the stack (typically in reverse order of discovery to maintain a natural DFS path). Collects all results in a list.
        *   `_arun_stream(...)`: Implements DFS traversal using a LIFO stack, yielding `CrawlResult` for each processed URL as it becomes available. Discovered links are added to the stack for subsequent processing.

*   **2.4. `BestFirstCrawlingStrategy`**
    *   Source: `crawl4ai/deep_crawling/bff_strategy.py`
    *   2.4.1. Purpose: Implements a Best-First Search deep crawling strategy, prioritizing URLs based on scores assigned by a `URLScorer`. It uses a priority queue to manage URLs to visit.
    *   2.4.2. Inheritance: `DeepCrawlStrategy`
    *   2.4.3. Initialization (`__init__`)
        *   2.4.3.1. Signature:
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                max_pages: int = float('inf'),
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.4.3.2. Parameters:
            *   `max_depth (int)`: Maximum depth to crawl.
            *   `filter_chain (FilterChain`, default: `FilterChain()`)`: Chain of filters to apply.
            *   `url_scorer (Optional[URLScorer]`, default: `None`)`: Scorer to rank URLs. Crucial for this strategy; if not provided, URLs might effectively be processed in FIFO order (score 0).
            *   `include_external (bool`, default: `False`)`: Whether to include external links.
            *   `max_pages (int`, default: `float('inf')`)`: Maximum number of pages to crawl.
            *   `logger (Optional[logging.Logger]`, default: `None`)`: Logger instance.
    *   2.4.4. Key Implemented Methods:
        *   `_arun_batch(...)`: Aggregates results from `_arun_best_first` into a list.
        *   `_arun_stream(...)`: Yields results from `_arun_best_first` as they are generated.
        *   `_arun_best_first(...)`: Core logic for best-first traversal. Uses an `asyncio.PriorityQueue` where items are `(score, depth, url, parent_url)`. URLs are processed in batches (default size 10) from the priority queue. Discovered links are scored and added to the queue.
    *   2.4.5. Key Attributes/Properties:
        *   `stats (TraversalStats)`: [Read-only] - Traversal statistics object.
        *   `BATCH_SIZE (int)`: [Class constant, default: 10] - Number of URLs to process concurrently from the priority queue.

## 3. URL Filtering Mechanisms

*   **3.1. `URLFilter` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.1.1. Purpose: Defines the abstract base class for all URL filters, providing a common interface for deciding whether a URL should be processed.
    *   3.1.2. Key Abstract Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Abstract method that must be implemented by subclasses. It takes a URL string and returns `True` if the URL passes the filter (should be processed), and `False` otherwise.
    *   3.1.3. Key Attributes/Properties:
        *   `name (str)`: [Read-only] - The name of the filter, typically the class name.
        *   `stats (FilterStats)`: [Read-only] - An instance of `FilterStats` to track how many URLs were processed, passed, and rejected by this filter.
        *   `logger (logging.Logger)`: [Read-only] - A logger instance specific to this filter, initialized lazily.
    *   3.1.4. Key Concrete Methods:
        *   `_update_stats(self, passed: bool) -> None`: Updates the `stats` object (total, passed, rejected counts).

*   **3.2. `FilterChain`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.2.1. Purpose: Manages a sequence of `URLFilter` instances. A URL must pass all filters in the chain to be considered valid.
    *   3.2.2. Initialization (`__init__`)
        *   3.2.2.1. Signature:
            ```python
            def __init__(self, filters: List[URLFilter] = None):
            ```
        *   3.2.2.2. Parameters:
            *   `filters (List[URLFilter]`, default: `None`)`: An optional list of `URLFilter` instances to initialize the chain with. If `None`, an empty chain is created.
    *   3.2.3. Key Public Methods:
        *   `add_filter(self, filter_: URLFilter) -> FilterChain`:
            *   Description: Adds a new `URLFilter` instance to the end of the chain.
            *   Returns: `(FilterChain)` - The `FilterChain` instance itself, allowing for method chaining.
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Applies each filter in the chain to the given URL. If any filter returns `False` (rejects the URL), this method immediately returns `False`. If all filters pass, it returns `True`. Handles both synchronous and asynchronous `apply` methods of individual filters.
            *   Returns: `(bool)` - `True` if the URL passes all filters, `False` otherwise.
    *   3.2.4. Key Attributes/Properties:
        *   `filters (Tuple[URLFilter, ...])`: [Read-only] - An immutable tuple containing the `URLFilter` instances in the chain.
        *   `stats (FilterStats)`: [Read-only] - An instance of `FilterStats` tracking the aggregated statistics for the entire chain (total URLs processed, passed, and rejected by the chain as a whole).

*   **3.3. `URLPatternFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.3.1. Purpose: Filters URLs based on whether they match a list of specified string patterns. Supports glob-style wildcards and regular expressions.
    *   3.3.2. Inheritance: `URLFilter`
    *   3.3.3. Initialization (`__init__`)
        *   3.3.3.1. Signature:
            ```python
            def __init__(
                self,
                patterns: Union[str, Pattern, List[Union[str, Pattern]]],
                use_glob: bool = True, # Deprecated, glob is always used for strings if not regex
                reverse: bool = False,
            ):
            ```
        *   3.3.3.2. Parameters:
            *   `patterns (Union[str, Pattern, List[Union[str, Pattern]]])`: A single pattern string/compiled regex, or a list of such patterns. String patterns are treated as glob patterns by default unless they are identifiable as regex (e.g., start with `^`, end with `$`, contain `\d`).
            *   `use_glob (bool`, default: `True`)`: [Deprecated] This parameter's functionality is now implicitly handled by pattern detection.
            *   `reverse (bool`, default: `False`)`: If `True`, the filter rejects URLs that match any of the patterns. If `False` (default), it accepts URLs that match any pattern and rejects those that don't match any.
    *   3.3.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Checks if the URL matches any of the configured patterns. Simple suffix/prefix/domain patterns are checked first for performance. For more complex patterns, it uses `fnmatch.translate` (for glob-like strings) or compiled regex objects. The outcome is affected by the `reverse` flag.
    *   3.3.5. Internal Categorization:
        *   `PATTERN_TYPES`: A dictionary mapping pattern types (SUFFIX, PREFIX, DOMAIN, PATH, REGEX) to integer constants.
        *   `_simple_suffixes (Set[str])`: Stores simple suffix patterns (e.g., `.html`).
        *   `_simple_prefixes (Set[str])`: Stores simple prefix patterns (e.g., `/blog/`).
        *   `_domain_patterns (List[Pattern])`: Stores compiled regex for domain-specific patterns (e.g., `*.example.com`).
        *   `_path_patterns (List[Pattern])`: Stores compiled regex for more general path patterns.

*   **3.4. `ContentTypeFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.4.1. Purpose: Filters URLs based on their expected content type, primarily by inferring it from the file extension in the URL.
    *   3.4.2. Inheritance: `URLFilter`
    *   3.4.3. Initialization (`__init__`)
        *   3.4.3.1. Signature:
            ```python
            def __init__(
                self,
                allowed_types: Union[str, List[str]],
                check_extension: bool = True,
                ext_map: Dict[str, str] = _MIME_MAP, # _MIME_MAP is internal
            ):
            ```
        *   3.4.3.2. Parameters:
            *   `allowed_types (Union[str, List[str]])`: A single MIME type string (e.g., "text/html") or a list of allowed MIME types. Can also be partial types like "image/" to allow all image types.
            *   `check_extension (bool`, default: `True`)`: If `True` (default), the filter attempts to determine the content type by looking at the URL's file extension. If `False`, all URLs pass this filter (unless `allowed_types` is empty).
            *   `ext_map (Dict[str, str]`, default: `ContentTypeFilter._MIME_MAP`)`: A dictionary mapping file extensions to their corresponding MIME types. A comprehensive default map is provided.
    *   3.4.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Extracts the file extension from the URL. If `check_extension` is `True` and an extension is found, it checks if the inferred MIME type (or the extension itself if MIME type is unknown) is among the `allowed_types`. If no extension is found, it typically allows the URL (assuming it might be an HTML page or similar).
    *   3.4.5. Static Methods:
        *   `_extract_extension(url: str) -> str`: [Cached] Extracts the file extension from a URL path, handling query parameters and fragments.
    *   3.4.6. Class Variables:
        *   `_MIME_MAP (Dict[str, str])`: A class-level dictionary mapping common file extensions to MIME types.

*   **3.5. `DomainFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.5.1. Purpose: Filters URLs based on a whitelist of allowed domains or a blacklist of blocked domains. Supports subdomain matching.
    *   3.5.2. Inheritance: `URLFilter`
    *   3.5.3. Initialization (`__init__`)
        *   3.5.3.1. Signature:
            ```python
            def __init__(
                self,
                allowed_domains: Union[str, List[str]] = None,
                blocked_domains: Union[str, List[str]] = None,
            ):
            ```
        *   3.5.3.2. Parameters:
            *   `allowed_domains (Union[str, List[str]]`, default: `None`)`: A single domain string or a list of domain strings. If provided, only URLs whose domain (or a subdomain thereof) is in this list will pass.
            *   `blocked_domains (Union[str, List[str]]`, default: `None`)`: A single domain string or a list of domain strings. URLs whose domain (or a subdomain thereof) is in this list will be rejected.
    *   3.5.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Extracts the domain from the URL. First, checks if the domain is in `_blocked_domains` (rejects if true). Then, if `_allowed_domains` is specified, checks if the domain is in that list (accepts if true). If `_allowed_domains` is not specified and the URL was not blocked, it passes.
    *   3.5.5. Static Methods:
        *   `_normalize_domains(domains: Union[str, List[str]]) -> Set[str]`: Converts input domains to a set of lowercase strings.
        *   `_is_subdomain(domain: str, parent_domain: str) -> bool`: Checks if `domain` is a subdomain of (or equal to) `parent_domain`.
        *   `_extract_domain(url: str) -> str`: [Cached] Extracts the domain name from a URL.

*   **3.6. `ContentRelevanceFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.6.1. Purpose: Filters URLs by fetching their `<head>` section, extracting text content (title, meta tags), and scoring its relevance against a given query using the BM25 algorithm.
    *   3.6.2. Inheritance: `URLFilter`
    *   3.6.3. Initialization (`__init__`)
        *   3.6.3.1. Signature:
            ```python
            def __init__(
                self,
                query: str,
                threshold: float,
                k1: float = 1.2,
                b: float = 0.75,
                avgdl: int = 1000,
            ):
            ```
        *   3.6.3.2. Parameters:
            *   `query (str)`: The query string to assess relevance against.
            *   `threshold (float)`: The minimum BM25 score required for the URL to be considered relevant and pass the filter.
            *   `k1 (float`, default: `1.2`)`: BM25 k1 parameter (term frequency saturation).
            *   `b (float`, default: `0.75`)`: BM25 b parameter (length normalization).
            *   `avgdl (int`, default: `1000`)`: Assumed average document length for BM25 calculations (typically based on the head content).
    *   3.6.4. Key Implemented Methods:
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Asynchronously fetches the HTML `<head>` content of the URL using `HeadPeeker.peek_html`. Extracts title and meta description/keywords. Calculates the BM25 score of this combined text against the `query`. Returns `True` if the score is >= `threshold`.
    *   3.6.5. Helper Methods:
        *   `_build_document(self, fields: Dict) -> str`: Constructs a weighted document string from title and meta tags.
        *   `_tokenize(self, text: str) -> List[str]`: Simple whitespace tokenizer.
        *   `_bm25(self, document: str) -> float`: Calculates the BM25 score.

*   **3.7. `SEOFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.7.1. Purpose: Filters URLs by performing a quantitative SEO quality assessment based on the content of their `<head>` section (e.g., title length, meta description presence, canonical tags, robots meta tags, schema.org markup).
    *   3.7.2. Inheritance: `URLFilter`
    *   3.7.3. Initialization (`__init__`)
        *   3.7.3.1. Signature:
            ```python
            def __init__(
                self,
                threshold: float = 0.65,
                keywords: List[str] = None,
                weights: Dict[str, float] = None,
            ):
            ```
        *   3.7.3.2. Parameters:
            *   `threshold (float`, default: `0.65`)`: The minimum aggregated SEO score (typically 0.0 to 1.0 range, though individual factor weights can exceed 1) required for the URL to pass.
            *   `keywords (List[str]`, default: `None`)`: A list of keywords to check for presence in the title.
            *   `weights (Dict[str, float]`, default: `None`)`: A dictionary to override default weights for various SEO factors (e.g., `{"title_length": 0.2, "canonical": 0.15}`).
    *   3.7.4. Key Implemented Methods:
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Asynchronously fetches the HTML `<head>` content. Calculates scores for individual SEO factors (title length, keyword presence, meta description, canonical tag, robots meta tag, schema.org presence, URL quality). Aggregates these scores using the defined `weights`. Returns `True` if the total score is >= `threshold`.
    *   3.7.5. Helper Methods (Scoring Factors):
        *   `_score_title_length(self, title: str) -> float`
        *   `_score_keyword_presence(self, text: str) -> float`
        *   `_score_meta_description(self, desc: str) -> float`
        *   `_score_canonical(self, canonical: str, original: str) -> float`
        *   `_score_schema_org(self, html: str) -> float`
        *   `_score_url_quality(self, parsed_url) -> float`
    *   3.7.6. Class Variables:
        *   `DEFAULT_WEIGHTS (Dict[str, float])`: Default weights for each SEO factor.

*   **3.8. `FilterStats` Data Class**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.8.1. Purpose: A data class to track statistics for URL filtering operations, including total URLs processed, passed, and rejected.
    *   3.8.2. Fields:
        *   `_counters (array.array)`: An array of unsigned integers storing counts for `[total, passed, rejected]`.
    *   3.8.3. Properties:
        *   `total_urls (int)`: Returns the total number of URLs processed.
        *   `passed_urls (int)`: Returns the number of URLs that passed the filter.
        *   `rejected_urls (int)`: Returns the number of URLs that were rejected.

## 4. URL Scoring Mechanisms

*   **4.1. `URLScorer` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.1.1. Purpose: Defines the abstract base class for all URL scorers. Scorers assign a numerical value to URLs, which can be used to prioritize crawling.
    *   4.1.2. Key Abstract Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Abstract method to be implemented by subclasses. It takes a URL string and returns a raw numerical score.
    *   4.1.3. Key Concrete Methods:
        *   `score(self, url: str) -> float`:
            *   Description: Calculates the final score for a URL by calling `_calculate_score` and multiplying the result by the scorer's `weight`. It also updates the internal `ScoringStats`.
            *   Returns: `(float)` - The weighted score.
    *   4.1.4. Key Attributes/Properties:
        *   `weight (ctypes.c_float)`: [Read-write] - The weight assigned to this scorer. The raw score calculated by `_calculate_score` will be multiplied by this weight. Default is 1.0. Stored as `ctypes.c_float` for memory efficiency.
        *   `stats (ScoringStats)`: [Read-only] - An instance of `ScoringStats` that tracks statistics for this scorer (number of URLs scored, total score, min/max scores).

*   **4.2. `KeywordRelevanceScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.2.1. Purpose: Scores URLs based on the presence and frequency of specified keywords within the URL string itself.
    *   4.2.2. Inheritance: `URLScorer`
    *   4.2.3. Initialization (`__init__`)
        *   4.2.3.1. Signature:
            ```python
            def __init__(self, keywords: List[str], weight: float = 1.0, case_sensitive: bool = False):
            ```
        *   4.2.3.2. Parameters:
            *   `keywords (List[str])`: A list of keyword strings to search for in the URL.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
            *   `case_sensitive (bool`, default: `False`)`: If `True`, keyword matching is case-sensitive. Otherwise, both the URL and keywords are converted to lowercase for matching.
    *   4.2.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Counts how many of the provided `keywords` are present in the `url`. The score is the ratio of matched keywords to the total number of keywords (0.0 to 1.0).
    *   4.2.5. Helper Methods:
        *   `_url_bytes(self, url: str) -> bytes`: [Cached] Converts URL to bytes, lowercasing if not case-sensitive.

*   **4.3. `PathDepthScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.3.1. Purpose: Scores URLs based on their path depth (number of segments in the URL path). It favors URLs closer to an `optimal_depth`.
    *   4.3.2. Inheritance: `URLScorer`
    *   4.3.3. Initialization (`__init__`)
        *   4.3.3.1. Signature:
            ```python
            def __init__(self, optimal_depth: int = 3, weight: float = 1.0):
            ```
        *   4.3.3.2. Parameters:
            *   `optimal_depth (int`, default: `3`)`: The path depth considered ideal. URLs at this depth get the highest score.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
    *   4.3.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Calculates the path depth of the URL. The score is `1.0 / (1.0 + abs(depth - optimal_depth))`, meaning URLs at `optimal_depth` score 1.0, and scores decrease as depth deviates. Uses a lookup table for common small differences for speed.
    *   4.3.5. Static Methods:
        *   `_quick_depth(path: str) -> int`: [Cached] Efficiently calculates path depth without full URL parsing.

*   **4.4. `ContentTypeScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.4.1. Purpose: Scores URLs based on their inferred content type, typically derived from the file extension.
    *   4.4.2. Inheritance: `URLScorer`
    *   4.4.3. Initialization (`__init__`)
        *   4.4.3.1. Signature:
            ```python
            def __init__(self, type_weights: Dict[str, float], weight: float = 1.0):
            ```
        *   4.4.3.2. Parameters:
            *   `type_weights (Dict[str, float])`: A dictionary mapping file extensions (e.g., "html", "pdf") or MIME type patterns (e.g., "text/html", "image/") to scores. Patterns ending with '$' are treated as exact extension matches.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
    *   4.4.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Extracts the file extension from the URL. Looks up the score in `type_weights` first by exact extension match (if pattern ends with '$'), then by general extension. If no direct match, it might try matching broader MIME type categories if defined in `type_weights`. Returns 0.0 if no match found.
    *   4.4.5. Static Methods:
        *   `_quick_extension(url: str) -> str`: [Cached] Efficiently extracts file extension.

*   **4.5. `FreshnessScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.5.1. Purpose: Scores URLs based on dates found within the URL string, giving higher scores to more recent dates.
    *   4.5.2. Inheritance: `URLScorer`
    *   4.5.3. Initialization (`__init__`)
        *   4.5.3.1. Signature:
            ```python
            def __init__(self, weight: float = 1.0, current_year: int = [datetime.date.today().year]): # Actual default is dynamic
            ```
        *   4.5.3.2. Parameters:
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
            *   `current_year (int`, default: `datetime.date.today().year`)`: The reference year to calculate freshness against.
    *   4.5.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Uses a regex to find year patterns (YYYY) in the URL. If multiple years are found, it uses the latest valid year. The score is higher for years closer to `current_year`, using a predefined lookup for small differences or a decay function for larger differences. If no year is found, a default score (0.5) is returned.
    *   4.5.5. Helper Methods:
        *   `_extract_year(self, url: str) -> Optional[int]`: [Cached] Extracts the most recent valid year from the URL.

*   **4.6. `DomainAuthorityScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.6.1. Purpose: Scores URLs based on a predefined list of domain authority weights. This allows prioritizing or de-prioritizing URLs from specific domains.
    *   4.6.2. Inheritance: `URLScorer`
    *   4.6.3. Initialization (`__init__`)
        *   4.6.3.1. Signature:
            ```python
            def __init__(
                self,
                domain_weights: Dict[str, float],
                default_weight: float = 0.5,
                weight: float = 1.0,
            ):
            ```
        *   4.6.3.2. Parameters:
            *   `domain_weights (Dict[str, float])`: A dictionary mapping domain names (e.g., "example.com") to their authority scores (typically between 0.0 and 1.0).
            *   `default_weight (float`, default: `0.5`)`: The score to assign to URLs whose domain is not found in `domain_weights`.
            *   `weight (float`, default: `1.0`)`: The overall weight to apply to the calculated score.
    *   4.6.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Extracts the domain from the URL. If the domain is in `_domain_weights`, its corresponding score is returned. Otherwise, `_default_weight` is returned. Prioritizes top domains for faster lookup.
    *   4.6.5. Static Methods:
        *   `_extract_domain(url: str) -> str`: [Cached] Efficiently extracts the domain from a URL.

*   **4.7. `CompositeScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.7.1. Purpose: Combines the scores from multiple `URLScorer` instances. Each constituent scorer contributes its weighted score to the final composite score.
    *   4.7.2. Inheritance: `URLScorer`
    *   4.7.3. Initialization (`__init__`)
        *   4.7.3.1. Signature:
            ```python
            def __init__(self, scorers: List[URLScorer], normalize: bool = True):
            ```
        *   4.7.3.2. Parameters:
            *   `scorers (List[URLScorer])`: A list of `URLScorer` instances to be combined.
            *   `normalize (bool`, default: `True`)`: If `True`, the final composite score is normalized by dividing the sum of weighted scores by the number of scorers. This can help keep scores in a more consistent range.
    *   4.7.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Iterates through all scorers in its list, calls their `score(url)` method (which applies individual weights), and sums up these scores. If `normalize` is `True`, divides the total sum by the number of scorers.
    *   4.7.5. Key Concrete Methods (overrides `URLScorer.score`):
        *   `score(self, url: str) -> float`:
            *   Description: Calculates the composite score and updates its own `ScoringStats`. Note: The individual scorers' stats are updated when their `score` methods are called internally.

*   **4.8. `ScoringStats` Data Class**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.8.1. Purpose: A data class to track statistics for URL scoring operations, including the number of URLs scored, total score, and min/max scores.
    *   4.8.2. Fields:
        *   `_urls_scored (int)`: Count of URLs scored.
        *   `_total_score (float)`: Sum of all scores.
        *   `_min_score (Optional[float])`: Minimum score encountered.
        *   `_max_score (Optional[float])`: Maximum score encountered.
    *   4.8.3. Key Methods:
        *   `update(self, score: float) -> None`: Updates the statistics with a new score.
        *   `get_average(self) -> float`: Calculates and returns the average score.
        *   `get_min(self) -> float`: Lazily initializes and returns the minimum score.
        *   `get_max(self) -> float`: Lazily initializes and returns the maximum score.

## 5. `DeepCrawlDecorator`

*   Source: `crawl4ai/deep_crawling/base_strategy.py`
*   5.1. Purpose: A decorator class that transparently adds deep crawling functionality to the `AsyncWebCrawler.arun` method if a `deep_crawl_strategy` is specified in the `CrawlerRunConfig`.
*   5.2. Initialization (`__init__`)
    *   5.2.1. Signature:
        ```python
        def __init__(self, crawler: AsyncWebCrawler):
        ```
    *   5.2.2. Parameters:
        *   `crawler (AsyncWebCrawler)`: The `AsyncWebCrawler` instance whose `arun` method is to be decorated.
*   5.3. `__call__` Method
    *   5.3.1. Signature:
        ```python
        @wraps(original_arun)
        async def wrapped_arun(url: str, config: CrawlerRunConfig = None, **kwargs):
        ```
    *   5.3.2. Functionality: This method wraps the original `arun` method of the `AsyncWebCrawler`.
        *   It checks if `config` is provided, has a `deep_crawl_strategy` set, and if `DeepCrawlDecorator.deep_crawl_active` context variable is `False` (to prevent recursion).
        *   If these conditions are met:
            *   It sets `DeepCrawlDecorator.deep_crawl_active` to `True`.
            *   It calls the `arun` method of the specified `config.deep_crawl_strategy`.
            *   It handles potential streaming results from the strategy by wrapping them in an async generator.
            *   Finally, it resets `DeepCrawlDecorator.deep_crawl_active` to `False`.
        *   If the conditions are not met, it calls the original `arun` method of the crawler.
*   5.4. Class Variable:
    *   `deep_crawl_active (ContextVar)`:
        *   Purpose: A `contextvars.ContextVar` used as a flag to indicate if a deep crawl is currently in progress for the current asynchronous context. This prevents the decorator from re-triggering deep crawling if the strategy itself calls the crawler's `arun` or `arun_many` methods.
        *   Default Value: `False`.

## 6. `TraversalStats` Data Model

*   Source: `crawl4ai/models.py`
*   6.1. Purpose: A data class for storing and tracking statistics related to a deep crawl traversal.
*   6.2. Fields:
    *   `start_time (datetime)`: The timestamp (Python `datetime` object) when the traversal process began. Default: `datetime.now()`.
    *   `end_time (Optional[datetime])`: The timestamp when the traversal process completed. Default: `None`.
    *   `urls_processed (int)`: The total number of URLs that were successfully fetched and processed. Default: `0`.
    *   `urls_failed (int)`: The total number of URLs that resulted in an error during fetching or processing. Default: `0`.
    *   `urls_skipped (int)`: The total number of URLs that were skipped (e.g., due to filters, already visited, or depth limits). Default: `0`.
    *   `total_depth_reached (int)`: The maximum depth reached from the start URL during the crawl. Default: `0`.
    *   `current_depth (int)`: The current depth level being processed by the crawler (can fluctuate during the crawl, especially for BFS). Default: `0`.

## 7. Configuration for Deep Crawling (`CrawlerRunConfig`)

*   Source: `crawl4ai/async_configs.py`
*   7.1. Purpose: `CrawlerRunConfig` is the primary configuration object passed to `AsyncWebCrawler.arun()` and `AsyncWebCrawler.arun_many()`. It contains various settings that control the behavior of a single crawl run, including those specific to deep crawling.
*   7.2. Relevant Fields:
    *   `deep_crawl_strategy (Optional[DeepCrawlStrategy])`:
        *   Type: `Optional[DeepCrawlStrategy]` (where `DeepCrawlStrategy` is the ABC from `crawl4ai.deep_crawling.base_strategy`)
        *   Default: `None`
        *   Description: Specifies the deep crawling strategy instance (e.g., `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, `BestFirstCrawlingStrategy`) to be used for the crawl. If `None`, deep crawling is disabled, and only the initial URL(s) will be processed.
    *   *Note: Parameters like `max_depth`, `max_pages`, `filter_chain`, `url_scorer`, `score_threshold`, and `include_external` are not direct attributes of `CrawlerRunConfig` for deep crawling. Instead, they are passed to the constructor of the chosen `DeepCrawlStrategy` instance, which is then assigned to `CrawlerRunConfig.deep_crawl_strategy`.*

## 8. Utility Functions

*   **8.1. `normalize_url_for_deep_crawl(url: str, source_url: str) -> str`**
    *   Source: `crawl4ai/deep_crawling/utils.py` (or `crawl4ai/utils.py` if it's a general utility)
    *   8.1.1. Purpose: Normalizes a URL found during deep crawling. This typically involves resolving relative URLs against the `source_url` to create absolute URLs and removing URL fragments (`#fragment`).
    *   8.1.2. Signature: `def normalize_url_for_deep_crawl(url: str, source_url: str) -> str:`
    *   8.1.3. Parameters:
        *   `url (str)`: The URL string to be normalized.
        *   `source_url (str)`: The URL of the page where the `url` was discovered. This is used as the base for resolving relative paths.
    *   8.1.4. Returns: `(str)` - The normalized, absolute URL without fragments.

*   **8.2. `efficient_normalize_url_for_deep_crawl(url: str, source_url: str) -> str`**
    *   Source: `crawl4ai/deep_crawling/utils.py` (or `crawl4ai/utils.py`)
    *   8.2.1. Purpose: Provides a potentially more performant version of URL normalization specifically for deep crawling scenarios, likely employing optimizations to avoid repeated or complex parsing operations. (Note: Based on the provided code, this appears to be the same as `normalize_url_for_deep_crawl` if only one is present, or it might contain specific internal optimizations not exposed differently at the API level but used by strategies).
    *   8.2.2. Signature: `def efficient_normalize_url_for_deep_crawl(url: str, source_url: str) -> str:`
    *   8.2.3. Parameters:
        *   `url (str)`: The URL string to be normalized.
        *   `source_url (str)`: The URL of the page where the `url` was discovered.
    *   8.2.4. Returns: `(str)` - The normalized, absolute URL, typically without fragments.

## 9. PDF Processing Integration (`crawl4ai.processors.pdf`)
    *   9.1. Overview of PDF processing in Crawl4ai: While not directly part of the `deep_crawling` package, PDF processing components can be used in conjunction if a deep crawl discovers PDF URLs and they need to be processed. The `PDFCrawlerStrategy` can fetch PDFs, and `PDFContentScrapingStrategy` can extract content from them.
    *   **9.2. `PDFCrawlerStrategy`**
        *   Source: `crawl4ai/processors/pdf/__init__.py`
        *   9.2.1. Purpose: An `AsyncCrawlerStrategy` designed to "crawl" PDF files. In practice, this usually means downloading the PDF content. It returns a minimal `AsyncCrawlResponse` that signals to a `ContentScrapingStrategy` (like `PDFContentScrapingStrategy`) that the content is a PDF.
        *   9.2.2. Inheritance: `AsyncCrawlerStrategy`
        *   9.2.3. Initialization (`__init__`)
            *   9.2.3.1. Signature: `def __init__(self, logger: AsyncLogger = None):`
            *   9.2.3.2. Parameters:
                *   `logger (AsyncLogger`, default: `None`)`: An optional logger instance.
        *   9.2.4. Key Methods:
            *   `async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse`:
                *   Description: For a PDF URL, this method typically signifies that the URL points to a PDF. It constructs an `AsyncCrawlResponse` with a `Content-Type` header of `application/pdf` and a placeholder HTML. The actual PDF processing (downloading and content extraction) is usually handled by a subsequent scraping strategy.
    *   **9.3. `PDFContentScrapingStrategy`**
        *   Source: `crawl4ai/processors/pdf/__init__.py`
        *   9.3.1. Purpose: A `ContentScrapingStrategy` specialized in extracting text, images (optional), and metadata from PDF files. It uses a `PDFProcessorStrategy` (like `NaivePDFProcessorStrategy`) internally.
        *   9.3.2. Inheritance: `ContentScrapingStrategy`
        *   9.3.3. Initialization (`__init__`)
            *   9.3.3.1. Signature:
                ```python
                def __init__(self,
                             save_images_locally: bool = False,
                             extract_images: bool = False,
                             image_save_dir: str = None,
                             batch_size: int = 4,
                             logger: AsyncLogger = None):
                ```
            *   9.3.3.2. Parameters:
                *   `save_images_locally (bool`, default: `False`)`: If `True`, extracted images will be saved to the local disk.
                *   `extract_images (bool`, default: `False`)`: If `True`, attempts to extract images from the PDF.
                *   `image_save_dir (str`, default: `None`)`: The directory where extracted images will be saved if `save_images_locally` is `True`.
                *   `batch_size (int`, default: `4`)`: The number of PDF pages to process in parallel batches (if the underlying processor supports it).
                *   `logger (AsyncLogger`, default: `None`)`: An optional logger instance.
        *   9.3.4. Key Methods:
            *   `scrape(self, url: str, html: str, **params) -> ScrapingResult`:
                *   Description: Takes the URL (which should point to a PDF or a local PDF path) and processes it. It downloads the PDF if it's a remote URL, then uses the internal `pdf_processor` to extract content. It formats the extracted text into basic HTML and collects image and link information.
            *   `async def ascrape(self, url: str, html: str, **kwargs) -> ScrapingResult`:
                *   Description: Asynchronous version of the `scrape` method, typically by running the synchronous `scrape` method in a separate thread.
        *   9.3.5. Helper Methods:
            *   `_get_pdf_path(self, url: str) -> str`: Downloads a PDF from a URL to a temporary file if it's not a local path.
    *   **9.4. `NaivePDFProcessorStrategy`**
        *   Source: `crawl4ai/processors/pdf/processor.py`
        *   9.4.1. Purpose: A concrete implementation of `PDFProcessorStrategy` that uses `PyPDF2` (or similar libraries if extended) to extract text, images, and metadata from PDF documents page by page or in batches.
        *   9.4.2. Initialization (`__init__`)
            *   Signature: `def __init__(self, image_dpi: int = 144, image_quality: int = 85, extract_images: bool = True, save_images_locally: bool = False, image_save_dir: Optional[Path] = None, batch_size: int = 4)`
            *   Parameters: [Details parameters for image extraction quality, saving, and batch processing size.]
        *   9.4.3. Key Methods:
            *   `process(self, pdf_path: Path) -> PDFProcessResult`:
                *   Description: Processes a single PDF file sequentially, page by page. Extracts metadata, text, and optionally images from each page.
            *   `process_batch(self, pdf_path: Path) -> PDFProcessResult`:
                *   Description: Processes a PDF file by dividing its pages into batches and processing these batches in parallel using a thread pool, potentially speeding up extraction for large PDFs.
        *   9.4.4. Helper Methods:
            *   `_process_page(self, page, image_dir: Optional[Path]) -> PDFPage`: Processes a single PDF page object.
            *   `_extract_images(self, page, image_dir: Optional[Path]) -> List[Dict]`: Extracts images from a page.
            *   `_extract_links(self, page) -> List[str]`: Extracts hyperlinks from a page.
            *   `_extract_metadata(self, pdf_path: Path, reader=None) -> PDFMetadata`: Extracts metadata from the PDF.
    *   **9.5. PDF Data Models**
        *   Source: `crawl4ai/processors/pdf/processor.py`
        *   9.5.1. `PDFMetadata`:
            *   Purpose: Stores metadata extracted from a PDF document.
            *   Fields:
                *   `title (Optional[str])`: The title of the PDF.
                *   `author (Optional[str])`: The author(s) of the PDF.
                *   `producer (Optional[str])`: The software used to produce the PDF.
                *   `created (Optional[datetime])`: The creation date of the PDF.
                *   `modified (Optional[datetime])`: The last modification date of the PDF.
                *   `pages (int)`: The total number of pages in the PDF. Default: `0`.
                *   `encrypted (bool)`: `True` if the PDF is encrypted, `False` otherwise. Default: `False`.
                *   `file_size (Optional[int])`: The size of the PDF file in bytes. Default: `None`.
        *   9.5.2. `PDFPage`:
            *   Purpose: Stores content extracted from a single page of a PDF document.
            *   Fields:
                *   `page_number (int)`: The page number (1-indexed).
                *   `raw_text (str)`: The raw text extracted from the page. Default: `""`.
                *   `markdown (str)`: Markdown representation of the page content. Default: `""`.
                *   `html (str)`: Basic HTML representation of the page content. Default: `""`.
                *   `images (List[Dict])`: A list of dictionaries, each representing an extracted image with details like format, path/data, dimensions. Default: `[]`.
                *   `links (List[str])`: A list of hyperlink URLs found on the page. Default: `[]`.
                *   `layout (List[Dict])`: Information about the layout of text elements on the page (e.g., coordinates). Default: `[]`.
        *   9.5.3. `PDFProcessResult`:
            *   Purpose: Encapsulates the results of processing a PDF document.
            *   Fields:
                *   `metadata (PDFMetadata)`: The metadata of the processed PDF.
                *   `pages (List[PDFPage])`: A list of `PDFPage` objects, one for each page processed.
                *   `processing_time (float)`: The time taken to process the PDF, in seconds. Default: `0.0`.
                *   `version (str)`: The version of the PDF processor. Default: `"1.1"`.

## 10. Version Information (`crawl4ai.__version__`)
*   Source: `crawl4ai/__version__.py`
*   10.1. `__version__ (str)`: A string representing the current installed version of the `crawl4ai` library (e.g., "0.6.3").

## 11. Asynchronous Configuration (`crawl4ai.async_configs`)
    *   11.1. Overview: The `crawl4ai.async_configs` module contains configuration classes used throughout the library, including those relevant for network requests like proxies (`ProxyConfig`) and general crawler/browser behavior.
    *   **11.2. `ProxyConfig`**
        *   Source: `crawl4ai/async_configs.py` (and `crawl4ai/proxy_strategy.py`)
        *   11.2.1. Purpose: Represents the configuration for a single proxy server, including its address, port, and optional authentication credentials.
        *   11.2.2. Initialization (`__init__`)
            *   11.2.2.1. Signature:
                ```python
                def __init__(
                    self,
                    server: str,
                    username: Optional[str] = None,
                    password: Optional[str] = None,
                    ip: Optional[str] = None,
                ):
                ```
            *   11.2.2.2. Parameters:
                *   `server (str)`: The proxy server URL (e.g., "http://proxy.example.com:8080", "socks5://proxy.example.com:1080").
                *   `username (Optional[str]`, default: `None`)`: The username for proxy authentication, if required.
                *   `password (Optional[str]`, default: `None`)`: The password for proxy authentication, if required.
                *   `ip (Optional[str]`, default: `None`)`: Optionally, the specific IP address of the proxy server. If not provided, it's inferred from the `server` URL.
        *   11.2.3. Key Static Methods:
            *   `from_string(proxy_str: str) -> ProxyConfig`:
                *   Description: Creates a `ProxyConfig` instance from a string representation. Expected format is "ip:port:username:password" or "ip:port".
                *   Returns: `(ProxyConfig)`
            *   `from_dict(proxy_dict: Dict) -> ProxyConfig`:
                *   Description: Creates a `ProxyConfig` instance from a dictionary.
                *   Returns: `(ProxyConfig)`
            *   `from_env(env_var: str = "PROXIES") -> List[ProxyConfig]`:
                *   Description: Loads a list of proxy configurations from a comma-separated string in an environment variable.
                *   Returns: `(List[ProxyConfig])`
        *   11.2.4. Key Methods:
            *   `to_dict(self) -> Dict`: Converts the `ProxyConfig` instance to a dictionary.
            *   `clone(self, **kwargs) -> ProxyConfig`: Creates a copy of the instance, optionally updating attributes with `kwargs`.

    *   **11.3. `ProxyRotationStrategy` (ABC)**
        *   Source: `crawl4ai/proxy_strategy.py`
        *   11.3.1. Purpose: Abstract base class defining the interface for proxy rotation strategies.
        *   11.3.2. Key Abstract Methods:
            *   `async def get_next_proxy(self) -> Optional[ProxyConfig]`: Asynchronously gets the next `ProxyConfig` from the strategy.
            *   `def add_proxies(self, proxies: List[ProxyConfig])`: Adds a list of `ProxyConfig` objects to the strategy's pool.
    *   **11.4. `RoundRobinProxyStrategy`**
        *   Source: `crawl4ai/proxy_strategy.py`
        *   11.4.1. Purpose: A simple proxy rotation strategy that cycles through a list of proxies in a round-robin fashion.
        *   11.4.2. Inheritance: `ProxyRotationStrategy`
        *   11.4.3. Initialization (`__init__`)
            *   11.4.3.1. Signature: `def __init__(self, proxies: List[ProxyConfig] = None):`
            *   11.4.3.2. Parameters:
                *   `proxies (List[ProxyConfig]`, default: `None`)`: An optional initial list of `ProxyConfig` objects.
        *   11.4.4. Key Implemented Methods:
            *   `add_proxies(self, proxies: List[ProxyConfig])`: Adds new proxies to the internal list and reinitializes the cycle.
            *   `async def get_next_proxy(self) -> Optional[ProxyConfig]`: Returns the next proxy from the cycle. Returns `None` if no proxies are available.

## 12. HTML to Markdown Conversion (`crawl4ai.markdown_generation_strategy`)
    *   12.1. `MarkdownGenerationStrategy` (ABC)
        *   Source: `crawl4ai/markdown_generation_strategy.py`
        *   12.1.1. Purpose: Abstract base class defining the interface for strategies that convert HTML content to Markdown.
        *   12.1.2. Key Abstract Methods:
            *   `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`:
                *   Description: Abstract method to convert the given `input_html` string into a `MarkdownGenerationResult` object.
                *   Parameters:
                    *   `input_html (str)`: The HTML content to convert.
                    *   `base_url (str`, default: `""`)`: The base URL used for resolving relative links within the HTML.
                    *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: Options to pass to the underlying HTML-to-text conversion library.
                    *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional filter to apply to the HTML before Markdown conversion, potentially to extract only relevant parts.
                    *   `citations (bool`, default: `True`)`: If `True`, attempts to convert hyperlinks into Markdown citations with a reference list.
                    *   `**kwargs`: Additional keyword arguments.
                *   Returns: `(MarkdownGenerationResult)`
    *   12.2. `DefaultMarkdownGenerator`
        *   Source: `crawl4ai/markdown_generation_strategy.py`
        *   12.2.1. Purpose: The default implementation of `MarkdownGenerationStrategy`. It uses the `CustomHTML2Text` class (an enhanced `html2text.HTML2Text`) for the primary conversion and can optionally apply a `RelevantContentFilter`.
        *   12.2.2. Inheritance: `MarkdownGenerationStrategy`
        *   12.2.3. Initialization (`__init__`)
            *   12.2.3.1. Signature:
                ```python
                def __init__(
                    self,
                    content_filter: Optional[RelevantContentFilter] = None,
                    options: Optional[Dict[str, Any]] = None,
                    content_source: str = "cleaned_html", # "raw_html", "fit_html"
                ):
                ```
            *   12.2.3.2. Parameters:
                *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An instance of a content filter strategy (e.g., `BM25ContentFilter`, `PruningContentFilter`) to be applied to the `input_html` before Markdown conversion. If `None`, no pre-filtering is done.
                *   `options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary of options to configure the `CustomHTML2Text` converter (e.g., `{"body_width": 0, "ignore_links": False}`).
                *   `content_source (str`, default: `"cleaned_html"`)`: Specifies which HTML source to use for Markdown generation if multiple are available (e.g., from `CrawlResult`). Options: `"cleaned_html"` (default), `"raw_html"`, `"fit_html"`. This parameter is primarily used when the generator is part of a larger crawling pipeline.
        *   12.2.4. Key Methods:
            *   `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`:
                *   Description: Converts HTML to Markdown. If a `content_filter` is provided (either at init or as an argument), it's applied first to get "fit_html". Then, `CustomHTML2Text` converts the chosen HTML (input_html or fit_html) to raw Markdown. If `citations` is True, links in the raw Markdown are converted to citation format.
                *   Returns: `(MarkdownGenerationResult)`
            *   `convert_links_to_citations(self, markdown: str, base_url: str = "") -> Tuple[str, str]`:
                *   Description: Parses Markdown text, identifies links, replaces them with citation markers (e.g., `[text]^(1)`), and generates a corresponding list of references.
                *   Returns: `(Tuple[str, str])` - A tuple containing the Markdown with citations and the Markdown string of references.

## 13. Content Filtering (`crawl4ai.content_filter_strategy`)
    *   13.1. `RelevantContentFilter` (ABC)
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.1.1. Purpose: Abstract base class for strategies that filter HTML content to extract only the most relevant parts, typically before Markdown conversion or further processing.
        *   13.1.2. Key Abstract Methods:
            *   `filter_content(self, html: str) -> List[str]`:
                *   Description: Abstract method that takes an HTML string and returns a list of strings, where each string is a chunk of HTML deemed relevant.
    *   13.2. `BM25ContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.2.1. Purpose: Filters HTML content by extracting text chunks and scoring their relevance to a user query (or an inferred page query) using the BM25 algorithm.
        *   13.2.2. Inheritance: `RelevantContentFilter`
        *   13.2.3. Initialization (`__init__`)
            *   13.2.3.1. Signature:
                ```python
                def __init__(
                    self,
                    user_query: Optional[str] = None,
                    bm25_threshold: float = 1.0,
                    language: str = "english",
                ):
                ```
            *   13.2.3.2. Parameters:
                *   `user_query (Optional[str]`, default: `None`)`: The query to compare content against. If `None`, the filter attempts to extract a query from the page's metadata.
                *   `bm25_threshold (float`, default: `1.0`)`: The minimum BM25 score for a text chunk to be considered relevant.
                *   `language (str`, default: `"english"`)`: The language used for stemming tokens.
        *   13.2.4. Key Implemented Methods:
            *   `filter_content(self, html: str, min_word_threshold: int = None) -> List[str]`: Parses HTML, extracts text chunks (paragraphs, list items, etc.), scores them with BM25 against the query, and returns the HTML of chunks exceeding the threshold.
    *   13.3. `PruningContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.3.1. Purpose: Filters HTML content by recursively pruning less relevant parts of the DOM tree based on a composite score (text density, link density, tag weights, etc.).
        *   13.3.2. Inheritance: `RelevantContentFilter`
        *   13.3.3. Initialization (`__init__`)
            *   13.3.3.1. Signature:
                ```python
                def __init__(
                    self,
                    user_query: Optional[str] = None,
                    min_word_threshold: Optional[int] = None,
                    threshold_type: str = "fixed", # or "dynamic"
                    threshold: float = 0.48,
                ):
                ```
            *   13.3.3.2. Parameters:
                *   `user_query (Optional[str]`, default: `None`)`: [Not directly used by pruning logic but inherited].
                *   `min_word_threshold (Optional[int]`, default: `None`)`: Minimum word count for an element to be considered for scoring initially (default behavior might be more nuanced).
                *   `threshold_type (str`, default: `"fixed"`)`: Specifies how the `threshold` is applied. "fixed" uses the direct value. "dynamic" adjusts the threshold based on content characteristics.
                *   `threshold (float`, default: `0.48`)`: The score threshold for pruning. Elements below this score are removed.
        *   13.3.4. Key Implemented Methods:
            *   `filter_content(self, html: str, min_word_threshold: int = None) -> List[str]`: Parses HTML, applies the pruning algorithm to the body, and returns the remaining significant HTML blocks as a list of strings.
    *   13.4. `LLMContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.4.1. Purpose: Uses a Large Language Model (LLM) to determine the relevance of HTML content chunks based on a given instruction.
        *   13.4.2. Inheritance: `RelevantContentFilter`
        *   13.4.3. Initialization (`__init__`)
            *   13.4.3.1. Signature:
                ```python
                def __init__(
                    self,
                    llm_config: Optional[LLMConfig] = None,
                    instruction: Optional[str] = None,
                    chunk_token_threshold: int = CHUNK_TOKEN_THRESHOLD, # Default from config
                    overlap_rate: float = OVERLAP_RATE,            # Default from config
                    word_token_rate: float = WORD_TOKEN_RATE,        # Default from config
                    verbose: bool = False,
                    logger: Optional[AsyncLogger] = None,
                    ignore_cache: bool = True
                ):
                ```
            *   13.4.3.2. Parameters:
                *   `llm_config (Optional[LLMConfig])`: Configuration for the LLM (provider, API key, model, etc.).
                *   `instruction (Optional[str])`: The instruction given to the LLM to guide content filtering (e.g., "Extract only the main article content, excluding headers, footers, and ads.").
                *   `chunk_token_threshold (int)`: Maximum number of tokens per chunk sent to the LLM.
                *   `overlap_rate (float)`: Percentage of overlap between consecutive chunks.
                *   `word_token_rate (float)`: Estimated ratio of words to tokens, used for chunking.
                *   `verbose (bool`, default: `False`)`: Enables verbose logging for LLM operations.
                *   `logger (Optional[AsyncLogger]`, default: `None`)`: Custom logger instance.
                *   `ignore_cache (bool`, default: `True`)`: If `True`, bypasses any LLM response caching for this operation.
        *   13.4.4. Key Implemented Methods:
            *   `filter_content(self, html: str, ignore_cache: bool = True) -> List[str]`:
                *   Description: Chunks the input HTML. For each chunk, it sends a request to the configured LLM with the chunk and the `instruction`. The LLM is expected to return the relevant part of the chunk. These relevant parts are then collected and returned.
```

---


## Deep Crawling - Reasoning
Source: crawl4ai_deep_crawling_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - deep_crawling Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `reasoning_deep_crawling.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Deep Crawling with Crawl4ai

Deep crawling is a fundamental capability for comprehensive web data extraction. This section introduces what deep crawling means in the context of Crawl4ai, why it's essential, and provides an overview of how Crawl4ai empowers you to perform sophisticated, multi-page crawls.

*   **1.1. What is Deep Crawling and Why Do You Need It?**
    *   **Explanation of deep crawling:** Deep crawling, unlike single-page scraping, involves systematically discovering and fetching web pages by following hyperlinks from an initial set of "seed" URLs. It's the process of exploring a website's structure to gather information spread across multiple pages. Crawl4ai's deep crawling component automates this exploration, allowing you to define the boundaries and priorities of your crawl.
    *   **Common scenarios requiring deep crawling:**
        *   **Building a comprehensive site index:** Discovering all pages within a website for search engine indexing or sitemap generation. For example, indexing all articles on a news website or all products on an e-commerce site.
        *   **Scraping data from multiple interconnected pages:** Extracting detailed information that isn't available on a single page, such as product specifications from individual product pages linked from a category page.
        *   **Discovering all content within a specific domain or sub-domain:** Ensuring all relevant content under `blog.example.com` is found and processed.
        *   **SEO analysis and site structure understanding:** Mapping out how pages are linked, identifying orphaned pages, or analyzing internal link distribution.
        *   **Monitoring website changes:** Regularly crawling a site to detect new content, updated pages, or broken links.
    *   **Core problems solved by Crawl4ai's `deep_crawling` component:**
        *   **URL Frontier Management:** Efficiently managing the queue of URLs to visit.
        *   **Visited URL Tracking:** Preventing re-crawling of already processed pages.
        *   **Depth Control:** Limiting how many "hops" the crawler takes from the seed URL.
        *   **Scope Management:** Using filters to define which URLs are relevant and should be processed.
        *   **Crawl Prioritization:** Using scorers to decide which URLs are more important to visit next.
        *   **Resource Management:** Providing mechanisms (`max_pages`) to limit the overall crawl size.

*   **1.2. When to Choose Deep Crawling Over Single-Page Crawling**
    *   **Decision factors:**
        *   **Data Distribution:** If the information you need is spread across multiple interlinked pages (e.g., an e-commerce site with category pages, product listing pages, and individual product detail pages), deep crawling is necessary. Single-page crawling is sufficient if all required data is on one page or a known, small set of URLs.
        *   **Link Discovery:** If you need to discover new URLs dynamically based on the content of previously crawled pages, deep crawling is the way to go.
        *   **Site Mapping/Full Site Analysis:** If your goal is to understand the structure of an entire site or a significant portion of it, deep crawling is essential.
    *   **Trade-offs:**
        *   **Comprehensiveness vs. Speed/Resources:** Deep crawling provides more comprehensive data but typically takes longer and consumes more bandwidth and processing resources than single-page crawls.
        *   **Complexity:** Configuring an effective deep crawl (with appropriate strategies, filters, and scorers) can be more complex than a simple single-page fetch.
        *   **Scope Control:** Without proper filters and limits (`max_depth`, `max_pages`), deep crawls can easily become too broad and inefficient.

*   **1.3. Overview of Crawl4ai's Deep Crawling Architecture**
    *   **High-level explanation:**
        Crawl4ai's deep crawling is orchestrated by a `DeepCrawlStrategy` (like `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, or `BestFirstCrawlingStrategy`). When a page is crawled, this strategy is responsible for:
        1.  Extracting new links from the page.
        2.  Applying a `FilterChain` (a sequence of `URLFilter` instances) to determine if a discovered URL should be considered for further crawling.
        3.  Optionally, using a `URLScorer` (especially with `BestFirstCrawlingStrategy`) to assign a priority to valid URLs, influencing the order in which they are visited.
        4.  Adding valid (and potentially scored) URLs to a frontier (queue or priority queue) for future processing.
        5.  Managing visited URLs to avoid redundant crawls and controlling the depth and extent of the crawl.
    *   **Role of `DeepCrawlDecorator`:**
        This decorator is an internal mechanism that transparently adds deep crawling capabilities to the standard `AsyncWebCrawler.arun()` method when a `deep_crawl_strategy` is specified in `CrawlerRunConfig`. Users typically don't interact with it directly but should be aware that it's the component enabling this extended functionality.
    *   `* Diagram: [Conceptual diagram of the deep crawling workflow:
        Seed URL -> AsyncWebCrawler.arun() -> DeepCrawlDecorator -> (if deep_crawl_strategy in CrawlerRunConfig) -> DeepCrawlStrategy.arun()
        Within DeepCrawlStrategy.arun():
            Fetch Page -> Process Page (Extract Links) -> For each Link:
                -> FilterChain.apply(link) -> (if valid) -> URLScorer.score(link) -> Add to Frontier -> Select Next URL from Frontier -> Fetch Page ... ]`

## 2. Core Concepts: Strategies, Filters, and Scorers

To effectively use deep crawling, understanding its three main pillars is crucial: the strategy dictates *how* you explore, filters decide *what* to explore, and scorers (especially for Best-First) determine *in what order* to explore.

*   **2.1. Understanding `DeepCrawlStrategy`**
    *   **Purpose:** The `DeepCrawlStrategy` is the heart of the deep crawling process. It's an interface (an abstract base class) that defines the logic for traversing a website. Concrete implementations provide different exploration patterns.
    *   **Why different strategies exist:**
        *   **BFS (Breadth-First Search):** Explores websites level by level. Good for a complete, systematic scan up to a certain depth.
        *   **DFS (Depth-First Search):** Explores one branch of a website as deeply as possible before backtracking. Useful for following specific paths.
        *   **Best-First Search:** Uses a scoring mechanism to prioritize URLs, visiting the most "promising" ones first. Ideal for targeted crawling where relevance is key.
    *   **How to select the right strategy for your goal:**
        *   **Comprehensive Site Mapping:** BFS is often preferred.
        *   **Finding Specific Content Quickly (if path is known or can be guided):** DFS can be efficient.
        *   **Targeted Crawling (e.g., based on keywords, freshness, authority):** Best-First is the most powerful.
        *   **Resource Constraints:** BFS can be memory-intensive for wide sites. DFS might be better for deep, narrow sites if `max_depth` is managed. Best-First's resource usage depends on the scorer and queue size.
        *   `* Decision Table:
            | Goal                          | Recommended Strategy | Key Considerations                      |
            |-------------------------------|----------------------|-----------------------------------------|
            | Full site index up to depth X | BFS                  | Memory for wide sites, `max_depth`      |
            | Explore specific section deep | DFS                  | `max_depth`, avoiding traps             |
            | Find most relevant pages      | Best-First           | Scorer quality, `max_pages`             |
            | Quick overview of a site      | BFS with low `max_depth`| Speed vs. completeness                |
            `
*   **2.2. The Role of URL Filters (`URLFilter` & `FilterChain`)**
    *   **Purpose:** Filters are essential for controlling the scope and efficiency of your deep crawl. They decide whether a discovered URL should be added to the crawling queue or discarded. Without filters, a crawler might wander into irrelevant parts of a website, get stuck in "crawler traps" (like infinite calendars), or consume excessive resources.
    *   **How `FilterChain` allows combining multiple filters:** `FilterChain` takes a list of `URLFilter` instances. When a URL is evaluated, it's passed through each filter in the chain sequentially. If *any* filter in the chain rejects the URL (returns `False`), the URL is discarded. It must pass *all* filters to be considered valid. This allows for creating sophisticated, layered filtering logic.
    *   **Benefits of effective filtering:**
        *   **Efficiency:** Reduces the number of pages fetched and processed, saving time and bandwidth.
        *   **Relevance:** Focuses the crawl on content that matches your objectives.
        *   **Resource Management:** Prevents excessive memory usage by keeping the URL frontier manageable.
        *   **Avoiding Traps:** Helps avoid sections of a website that might lead to an infinite number of unique URLs (e.g., calendars, faceted search results with many parameter combinations).

*   **2.3. The Power of URL Scoring (`URLScorer` & `CompositeScorer`)**
    *   **Purpose:** URL scoring is primarily used by the `BestFirstCrawlingStrategy`. It assigns a numerical score to each valid URL, indicating its priority. The strategy then picks URLs from the frontier based on these scores (typically highest score first). This allows the crawler to intelligently prioritize which parts of a website to explore.
    *   **How `CompositeScorer` enables multi-faceted URL evaluation:** Often, a single criterion isn't enough to determine a URL's importance. `CompositeScorer` allows you to combine multiple individual `URLScorer` instances (e.g., one for keyword relevance, one for freshness, one for domain authority). Each individual scorer contributes to an overall score, often with weights you can define, providing a more nuanced and effective prioritization.
    *   **Impact of scoring on crawl efficiency and result quality:**
        *   **Efficiency:** Good scoring can dramatically improve efficiency by guiding the crawler to relevant content much faster, especially if you have a `max_pages` limit.
        *   **Result Quality:** By prioritizing high-value pages, scoring ensures that the most important data is collected even if the crawl is stopped before exploring the entire site. The definition of "high-value" is determined by your scoring logic.

## 3. Deep Crawling Strategies In-Depth

Let's dive into each specific strategy, understanding its mechanics, use cases, and how to configure it.

*   **3.1. Breadth-First Search (`BFSDeepCrawlStrategy`)**
    *   **3.1.1. Understanding BFS Traversal**
        *   **What is BFS?** Breadth-First Search explores a website layer by layer. It starts with the seed URL(s) (level 0), then crawls all pages directly linked from the seeds (level 1), then all pages linked from level 1 pages (level 2), and so on. It uses a FIFO (First-In, First-Out) queue to manage URLs for each level.
        *   **Pros:**
            *   Finds the shortest path to all reachable pages.
            *   Systematic and predictable exploration pattern.
            *   Good for getting a broad overview of a site's structure quickly, especially at shallow depths.
        *   **Cons:**
            *   Can consume significant memory for websites with a large number of links per page (wide sites), as it needs to store all URLs of a given level before moving to the next.
            *   May take a long time to reach content buried deep within the site structure.
        *   **Typical Use Cases:**
            *   Full site mapping up to a certain depth.
            *   Discovering all pages for a small to medium-sized website.
            *   Finding broken links or orphaned pages (when combined with analysis of all discovered URLs).
        *   `* Diagram: [Visual representation of BFS traversal, showing levels and queue behavior.
            Example:
                Level 0: A
                Queue: [A] -> Process A, discover B, C
                Level 1: B, C
                Queue: [B, C] -> Process B, discover D, E. Process C, discover F
                Level 2: D, E, F
                Queue: [D, E, F] -> ...
            ]`
    *   **3.1.2. Practical Usage of `BFSDeepCrawlStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import BFSDeepCrawlStrategy, CrawlerRunConfig

            bfs_strategy = BFSDeepCrawlStrategy(max_depth=3) # Example: crawl up to 3 levels deep
            run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)
            # ... then pass run_config to crawler.arun() or crawler.arun_many()
            ```
        *   **Key configuration parameters:**
            *   `max_depth (int)`: Crucial for BFS. Defines how many levels deep the crawl will go. A `max_depth` of 0 crawls only the seed URL(s). A `max_depth` of 1 crawls seeds and pages directly linked from them.
            *   `filter_chain (Optional[FilterChain])`: URLs discovered at each level are passed through this chain before being added to the next level's queue.
            *   `url_scorer (Optional[URLScorer])`: While BFS is primarily level-ordered, a scorer *can* be used to influence the processing order *within* a given level if multiple URLs are fetched concurrently in batches. However, it doesn't change the fundamental level-by-level exploration. (This is less common for pure BFS compared to Best-First).
            *   `max_pages (int, default=infinity)`: A global limit on the total number of pages to crawl. The crawl will stop if `max_pages` or `max_depth` is reached, whichever comes first.
            *   `include_external (bool, default=False)`: If `True`, BFS will also explore links to external domains, respecting `max_depth` for those external paths as well. Use with caution and strong `DomainFilter`s.
        *   `* Code Example: [Setting up a BFS crawl to explore 'example.com' up to depth 2, only HTML pages, max 100 pages]`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                BFSDeepCrawlStrategy, FilterChain, DomainFilter, ContentTypeFilter
            )
            import asyncio

            async def bfs_example_crawl():
                # Filters: Only allow 'example.com' and only HTML files
                filters = FilterChain(filters=[
                    DomainFilter(allowed_domains=["example.com"]),
                    ContentTypeFilter(allowed_types=['.html', '.htm'])
                ])

                # BFS Strategy: Max depth 2, max 100 pages, apply filters
                bfs_strategy = BFSDeepCrawlStrategy(
                    max_depth=2,
                    filter_chain=filters,
                    max_pages=100
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=bfs_strategy,
                    verbose=True
                )

                browser_config = BrowserConfig(headless=True)
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    result_container = await crawler.arun(
                        url="https://example.com",
                        config=run_config
                    )
                    # In batch mode (default for arun without stream=True in strategy),
                    # result_container will be a list of CrawlResult objects
                    for i, result in enumerate(result_container):
                        if result.success:
                            print(f"Crawled {i+1}: {result.url} (Depth: {result.metadata.get('depth')})")
                        else:
                            print(f"Failed {i+1}: {result.url} - {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(bfs_example_crawl())
            ```
    *   **3.1.3. Best Practices for BFS**
        *   **Memory Management:** For very wide sites (many links per page), BFS can consume a lot of memory because it holds all URLs of the current level. If memory is a concern, consider a lower `max_depth` or switching to DFS/Best-First for more targeted exploration.
        *   **Effective `max_depth`:** Choose `max_depth` carefully. A small increase in depth can lead to an exponential increase in pages crawled.
        *   **Filtering:** Always use `DomainFilter` to keep the crawl focused. Add other filters (`ContentTypeFilter`, `URLPatternFilter`) as needed to refine scope.
    *   **3.1.4. Common Pitfalls with BFS**
        *   **Excessive Memory on Large/Wide Sites:** Setting `max_depth` too high without considering site width can lead to out-of-memory errors.
        *   **Crawling Irrelevant Content:** Not using filters can result in crawling large, unwanted sections of a site or even external sites if `include_external` is accidentally enabled without proper domain filtering.
        *   **Time Consumption:** BFS aims for breadth, so reaching very specific, deep content might take longer than with DFS or a well-tuned Best-First strategy.

*   **3.2. Depth-First Search (`DFSDeepCrawlStrategy`)**
    *   **3.2.1. Understanding DFS Traversal**
        *   **What is DFS?** Depth-First Search explores as far as possible along each branch before backtracking. It uses a LIFO (Last-In, First-Out) stack to manage URLs. When it discovers new links on a page, those links are added to the top of the stack, and the crawler immediately proceeds to the newest link.
        *   **Pros:**
            *   Can reach deep content very quickly if it happens to be on the current exploration path.
            *   Potentially lower memory footprint for deep, narrow sites compared to BFS, as it doesn't need to store all URLs at a given level.
        *   **Cons:**
            *   Can get "stuck" exploring a very deep or infinite branch, potentially missing content in other, shallower branches if `max_pages` or another limit is hit.
            *   The order of discovery is less predictable than BFS and may not provide a balanced view of the site quickly.
        *   **Typical Use Cases:**
            *   Following a specific path through a website (e.g., a series of articles, a product configuration wizard).
            *   Exploring a single section of a website as deeply as possible.
            *   When memory is a primary concern and the target content is known to be deep.
        *   `* Diagram: [Visual representation of DFS traversal, showing stack behavior.
            Example:
                Stack: [A] -> Pop A, discover B, C. Push C, then B.
                Stack: [B, C] -> Pop B, discover D, E. Push E, then D.
                Stack: [D, E, C] -> Pop D ... and so on.
            ]`
    *   **3.2.2. Practical Usage of `DFSDeepCrawlStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import DFSDeepCrawlStrategy, CrawlerRunConfig

            dfs_strategy = DFSDeepCrawlStrategy(max_depth=5) # Example: explore up to 5 links deep
            run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)
            ```
        *   **Key configuration parameters:**
            *   `max_depth (int)`: Critically important for DFS to prevent infinite loops or excessively deep crawls.
            *   `filter_chain (Optional[FilterChain])`: Essential for guiding DFS and preventing it from exploring irrelevant paths.
            *   `url_scorer (Optional[URLScorer])`: Less commonly used with pure DFS, as the stack naturally dictates order. If used, it might influence which of the newly discovered links from a page gets pushed to the stack (and thus processed) first.
            *   `max_pages (int, default=infinity)`: Stops the crawl if this limit is reached.
            *   `include_external (bool, default=False)`: Controls whether DFS follows external links.
        *   `* Code Example: [Setting up a DFS crawl to explore a blog, prioritizing paths under '/blog/archive/']`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                DFSDeepCrawlStrategy, FilterChain, URLPatternFilter
            )
            import asyncio

            async def dfs_example_crawl():
                # Filter to keep crawl within /blog/ subdirectories
                filters = FilterChain(filters=[
                    URLPatternFilter(patterns=["https://example.com/blog/.*"])
                ])

                dfs_strategy = DFSDeepCrawlStrategy(
                    max_depth=10,      # Allow going reasonably deep
                    filter_chain=filters,
                    max_pages=50       # But limit total pages
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=dfs_strategy,
                    verbose=True
                )

                browser_config = BrowserConfig(headless=True)
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    # Using stream=True in strategy for immediate results
                    dfs_strategy.stream = True # Overriding here for demo
                    async for result in await crawler.arun(
                        url="https://example.com/blog/",
                        config=run_config
                    ):
                        if result.success:
                            print(f"Crawled (DFS): {result.url} (Depth: {result.metadata.get('depth')})")
                        else:
                            print(f"Failed (DFS): {result.url} - {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(dfs_example_crawl())
            ```
    *   **3.2.3. Best Practices for DFS**
        *   **Mandatory `max_depth`:** Always set a reasonable `max_depth` to prevent the crawler from getting lost in very deep or cyclical paths.
        *   **Targeted Filtering:** Use `URLPatternFilter` or other specific filters to guide the DFS along the paths you're interested in.
        *   **Monitor `max_pages`:** If `max_pages` is hit before `max_depth` in many branches, your DFS might not be exploring the site effectively.
    *   **3.2.4. Common Pitfalls with DFS**
        *   **Crawler Traps:** DFS is particularly susceptible to getting stuck in "crawler traps" (e.g., links generating unique URLs infinitely, like calendars or poorly designed filters).
        *   **Missing Broad Content:** If relevant content is spread across many shallow branches, DFS might miss much of it if it goes deep into one branch and hits `max_pages`.
        *   **Order of Discovery:** The order in which pages are discovered can feel random if the site structure isn't well understood or filters aren't guiding the crawl.

*   **3.3. Best-First Search (`BestFirstCrawlingStrategy`)**
    *   **3.3.1. Understanding Best-First Traversal**
        *   **What is Best-First?** This strategy uses a priority queue to manage the URL frontier. Each URL added to the frontier is assigned a score by a `URLScorer`. The crawler always picks the URL with the highest score from the priority queue to process next.
        *   **Pros:**
            *   Highly efficient for targeted crawling when you can define what makes a URL "good" or "relevant."
            *   Focuses crawler resources on the most promising areas of a website first.
            *   Adaptable: By changing the scoring logic, you can radically alter the crawl's focus.
        *   **Cons:**
            *   Effectiveness is *heavily* dependent on the quality and design of the `URLScorer`. A bad scorer leads to a bad crawl.
            *   Can be more complex to configure due to the need to design and implement scoring logic.
            *   Might miss some relevant content if it consistently scores low and `max_pages` is reached.
        *   **Typical Use Cases:**
            *   Finding pages most relevant to a specific set of keywords.
            *   Prioritizing pages from high-authority domains or known good sources.
            *   Crawling recently updated or fresh content first.
            *   Combining multiple factors (e.g., relevance, freshness, authority) for sophisticated prioritization.
        *   `* Diagram: [Visual representation of Best-First traversal.
            1. Seed URL -> Scorer -> Add to PriorityQueue (URL, Score)
            2. Pop highest score URL from PQ -> Fetch & Process -> Discover Links
            3. For each Link: Filter -> (if valid) -> Scorer -> Add to PQ (Link, Score)
            4. Repeat from step 2.
            Show PQ reordering as new items are added with different scores.]`
    *   **3.3.2. Practical Usage of `BestFirstCrawlingStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import (
                BestFirstCrawlingStrategy, CrawlerRunConfig,
                KeywordRelevanceScorer, DomainFilter, FilterChain
            )

            # Scorer: Prioritize URLs with 'ai' and 'ethics'
            keyword_scorer = KeywordRelevanceScorer(keywords=['ai', 'ethics'], weight=1.0)

            # Filter: Only 'example.com'
            domain_filter = DomainFilter(allowed_domains=['example.com'])
            filter_chain = FilterChain(filters=[domain_filter])

            best_first_strategy = BestFirstCrawlingStrategy(
                url_scorer=keyword_scorer,
                filter_chain=filter_chain,
                max_depth=5,
                max_pages=200
            )
            run_config = CrawlerRunConfig(deep_crawl_strategy=best_first_strategy)
            ```
        *   **Crucial role of `url_scorer`:** This is the defining component. You *must* provide a `URLScorer` instance. This could be a single scorer or a `CompositeScorer`.
        *   **Interaction with `filter_chain`:** Filters are applied *before* scoring. Only URLs that pass all filters are then scored and considered for the priority queue.
        *   **Parameters:**
            *   `max_depth (int)`: Still relevant to prevent excessively deep exploration, even if scores guide the way.
            *   `max_pages (int, default=infinity)`: Important for limiting the overall crawl size.
            *   `include_external (bool, default=False)`: If `True`, external URLs that pass filters will also be scored and added to the queue.
        *   `* Code Example: [Setting up a Best-First crawl using CompositeScorer to find recent articles about "AI in finance" from specific financial news domains]`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                BestFirstCrawlingStrategy, FilterChain, DomainFilter,
                KeywordRelevanceScorer, FreshnessScorer, CompositeScorer
            )
            import asyncio
            from datetime import datetime

            async def best_first_example_crawl():
                # Scorers
                keyword_scorer = KeywordRelevanceScorer(
                    keywords=['ai', 'finance', 'fintech'],
                    weight=0.6
                )
                freshness_scorer = FreshnessScorer(
                    current_year=datetime.now().year,
                    weight=0.4
                )
                composite_scorer = CompositeScorer(
                    scorers=[keyword_scorer, freshness_scorer]
                )

                # Filters
                allowed_domains = ["reputablefinance.news", "fintechinsider.com"]
                filters = FilterChain(filters=[
                    DomainFilter(allowed_domains=allowed_domains)
                ])

                best_first_strategy = BestFirstCrawlingStrategy(
                    url_scorer=composite_scorer,
                    filter_chain=filters,
                    max_depth=4,
                    max_pages=100,
                    stream=True # Get results as they come
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=best_first_strategy,
                    verbose=True
                )
                browser_config = BrowserConfig(headless=True)

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    start_urls = [f"https://{domain}/" for domain in allowed_domains]
                    async for result in await crawler.arun_many(
                        urls=start_urls,
                        config=run_config
                    ):
                        if result.success:
                            print(f"Crawled (Best-First): {result.url} (Score: {result.metadata.get('score', 'N/A')}, Depth: {result.metadata.get('depth')})")

            if __name__ == "__main__":
                asyncio.run(best_first_example_crawl())
            ```
    *   **3.3.3. Best Practices for Best-First**
        *   **Design Effective Scoring:** The success of Best-First hinges on this. Think carefully about what makes a URL valuable for your goal.
        *   **Iterative Refinement:** Test your scorers. Observe the crawl path. Adjust weights and logic in your `CompositeScorer` or custom scorers based on results.
        *   **Balance Complexity and Performance:** While `CompositeScorer` is powerful, very complex scoring logic involving many external calls or heavy computations per URL can slow down the decision-making process.
        *   **Combine with Strong Filters:** Filters reduce the number of URLs that need to be scored, improving efficiency.
    *   **3.3.4. Common Pitfalls with Best-First**
        *   **Poor Scorer Configuration:** If the scorer doesn't align with your goals, the crawl will be misguided and inefficient (e.g., a keyword scorer with irrelevant keywords).
        *   **Score Normalization (if building custom composite logic):** Ensure scores from different components are on a somewhat comparable scale or that weights account for differences. `CompositeScorer` handles weighting but doesn't inherently normalize scores from sub-scorers.
        *   **Ignoring Potentially Valuable Branches:** If a relevant section of a site consistently scores low due to a quirk in the scoring logic, it might be missed if `max_pages` is too restrictive. Consider periodic "exploration" phases or adjusting scores.
        *   **Over-reliance on a Single Metric:** A `CompositeScorer` is often better than relying on just one type of score (e.g., just keywords) which might be too narrow.

## 4. Fine-Tuning Your Crawl: URL Filtering

Filters are your first line of defense against an unmanageable or irrelevant crawl. They ensure that only URLs meeting your criteria are even considered for fetching and further processing.

*   **4.1. The Importance of Effective Filtering**
    *   **Why filter?**
        *   **Save Resources:** Every skipped URL saves bandwidth, processing time, and memory.
        *   **Improve Speed:** A focused crawl finishes faster.
        *   **Enhance Relevance:** Ensures the data you collect is pertinent to your objectives.
        *   **Avoid Crawler Traps:** Prevents the crawler from getting stuck in infinite loops (e.g., calendars, endlessly paginated archives with slight URL variations).
        *   **Respect Site Policies:** Can be used to avoid crawling sensitive or disallowed sections (though `robots.txt` is the primary mechanism for this).
    *   **How `FilterChain` processes filters sequentially:**
        When you provide a `FilterChain` with multiple filters, a URL must pass *all* of them to be accepted. If `Filter1.apply(url)` returns `False`, the URL is rejected, and `Filter2`, `Filter3`, etc., are not even called for that URL. This "short-circuiting" behavior means you should order your filters strategically.
        `* Diagram: [URL -> Filter1 -> (if True) -> Filter2 -> (if True) -> Filter3 -> (if True) -> Accepted | (if False at any step) -> Rejected]`

*   **4.2. `DomainFilter`**
    *   **4.2.1. Purpose:** The most fundamental filter. It restricts the crawl to specific domains or subdomains (`allowed_domains`) and/or explicitly blocks certain domains (`blocked_domains`).
    *   **4.2.2. How it Works:** It extracts the netloc (e.g., `www.example.com`) from a URL.
        *   If `allowed_domains` is specified, the URL's domain (or a parent domain) must be in this list.
        *   If `blocked_domains` is specified, the URL's domain (or a parent domain) must *not* be in this list.
        *   If both are specified, it must satisfy the allow condition AND not satisfy the block condition.
        *   It handles subdomains correctly: if `example.com` is allowed, `blog.example.com` is also allowed. If `example.com` is blocked, `blog.example.com` is also blocked.
    *   **4.2.3. Configuration & Usage:**
        ```python
        from crawl4ai import DomainFilter, FilterChain

        # Allow only 'example.com' and its subdomains
        allow_example = DomainFilter(allowed_domains=["example.com"])

        # Block 'ads.example.com' and 'tracker.com'
        block_ads = DomainFilter(blocked_domains=["ads.example.com", "tracker.com"])

        # Combine them: only example.com, but not ads.example.com
        combined_filter = FilterChain(filters=[allow_example, block_ads])
        ```
        *   Wildcard usage is not directly supported in the `allowed_domains` list itself (e.g., `*.example.com`). You'd typically allow `example.com` which implicitly covers subdomains. For more complex pattern matching, use `URLPatternFilter`.
        *   `* Code Example: [Allowing 'blog.example.com' and 'docs.example.com', explicitly blocking 'ads.example.com', assuming 'example.com' is the main domain for other content.]`
            ```python
            # This setup means only blog.example.com and docs.example.com are allowed
            # and ads.example.com (if it were a subdomain of an allowed domain) would be blocked.
            # A more typical setup to crawl ONLY these two subdomains:
            specific_subdomains_filter = DomainFilter(allowed_domains=["blog.example.com", "docs.example.com"])

            # If you wanted to crawl example.com but exclude ads.example.com:
            crawl_main_exclude_ads = FilterChain(filters=[
                DomainFilter(allowed_domains=["example.com"]),
                DomainFilter(blocked_domains=["ads.example.com"])
            ])
            ```
    *   **4.2.4. Best Practices:**
        *   Almost always start your `FilterChain` with a `DomainFilter` specifying `allowed_domains` to keep your crawl focused.
        *   Be precise if you only want specific subdomains. Allowing a TLD (Top-Level Domain) like `.com` is generally not what you want unless you intend a very broad crawl.

*   **4.3. `ContentTypeFilter`**
    *   **4.3.1. Purpose:** To filter URLs based on their likely file extension, thereby inferring the content type. This is a quick way to avoid downloading large binary files, images, or unwanted document types.
    *   **4.3.2. How it Works:** By default (`check_extension=True`), it extracts the extension from the URL's path (e.g., `.html`, `.pdf`, `.jpg`). It then checks if this extension (or its corresponding MIME type via an internal map) is present in the `allowed_types`.
    *   **4.3.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentTypeFilter

        # Only allow HTML and PDF files
        html_pdf_filter = ContentTypeFilter(allowed_types=['.html', '.htm', '.pdf'])
        # OR by MIME type partial match (less common for this filter but possible):
        # html_pdf_filter_mime = ContentTypeFilter(allowed_types=['text/html', 'application/pdf'])
        ```
        *   The `allowed_types` can be a list of extensions (e.g., `'.jpg'`) or partial MIME types (e.g., `'image/'`, `'text/html'`).
        *   `check_extension=True` (default) is generally faster as it avoids needing actual content type from HTTP headers. Set to `False` if you must rely on `Content-Type` headers (note: this filter as shown in `filters.py` primarily works on extensions for pre-fetch filtering).
        *   `* Code Example: [Filtering to only allow HTML, HTM, and PHP files]`
            ```python
            webpage_filter = ContentTypeFilter(allowed_types=['.html', '.htm', '.php'])
            # This would allow URLs like:
            # https://example.com/page.html
            # https://example.com/article.php
            # But would block:
            # https://example.com/image.jpg
            # https://example.com/document.pdf
            ```
    *   **4.3.4. Best Practices:** Use this early in your filter chain to quickly discard URLs pointing to unwanted file types, saving bandwidth and processing for HEAD requests or full fetches that later filters might trigger.

*   **4.4. `URLPatternFilter`**
    *   **4.4.1. Purpose:** Provides fine-grained control over which URLs to process based on matching them against regular expressions or glob-style patterns.
    *   **4.4.2. How it Works:** It takes a list of `patterns`. For each URL, it checks if the URL string matches any of these patterns. The behavior depends on the `reverse` flag.
    *   **4.4.3. Configuration & Usage:**
        ```python
        from crawl4ai import URLPatternFilter

        # Allow only URLs under '/blog/' or '/products/'
        blog_products_filter = URLPatternFilter(
            patterns=[r".*/blog/.*", r".*/products/.*"] # Regex patterns
        )

        # Exclude URLs containing '/archive/' or '/temp/'
        exclude_archive_filter = URLPatternFilter(
            patterns=[r".*/archive/.*", r".*/temp/.*"],
            reverse=True # Exclude if matches
        )
        ```
        *   `patterns`: A list of strings. These are treated as regular expressions by default (Python's `re` module). The provided code shows `fnmatch.translate` is used, which converts glob patterns to regex, but also handles direct regex if it detects regex-specific characters like `^`, `$`, `\d`.
        *   `reverse (bool, default=False)`: If `False` (default), the URL passes if it matches *any* pattern. If `True`, the URL passes only if it matches *none* of the patterns (effectively a blocklist).
        *   `* Code Example: [Allowing only URLs matching '/articles/[year]/[month]/[slug]' and excluding any URL containing '?replytocom=']`
            ```python
            article_path_filter = URLPatternFilter(
                patterns=[r"/articles/\d{4}/\d{2}/[\w-]+/?$"] # Matches /articles/YYYY/MM/slug
            )
            no_reply_filter = URLPatternFilter(
                patterns=[r"\?replytocom="],
                reverse=True # Exclude if it contains replytocom
            )
            # In a FilterChain, these would apply sequentially
            # final_chain = FilterChain(filters=[article_path_filter, no_reply_filter])
            ```
    *   **4.4.4. Best Practices:**
        *   Use for complex URL structures that `DomainFilter` or `ContentTypeFilter` can't handle.
        *   Test your regular expressions thoroughly to ensure they match what you intend and don't have unintended side effects or performance issues. Online regex testers can be very helpful.
        *   Prefer simpler string methods or other filters if a regex is overkill, as regex can be slower.

*   **4.5. `ContentRelevanceFilter` (Requires HEAD requests)**
    *   **4.5.1. Purpose:** To pre-filter URLs based on the relevance of their metadata (title, meta description, keywords found in the `<head>` section) to a given query, using the BM25 ranking algorithm. This helps in prioritizing or including only pages that are likely to be about a specific topic *before* downloading the full content.
    *   **4.5.2. How it Works:**
        1.  Performs an HTTP HEAD request to fetch the headers of the URL.
        2.  If successful, it uses `HeadPeek` to extract text content from `<title>`, `<meta name="description">`, and `<meta name="keywords">` tags from the response (if the HEAD response contains enough of the head, which is not guaranteed and server-dependent. Often, servers don't send body content with HEAD).
        3.  Calculates a BM25 relevance score between the extracted text and the user-provided `query`.
        4.  The URL passes if the score is above the specified `threshold`.
        *   **Important Note on HEAD requests:** While HEAD requests are designed to be lightweight, not all servers implement them correctly or return meaningful content previews in the head. Some servers might return the full HTML head, others might return very little, and some might even block HEAD requests. The effectiveness of this filter depends heavily on server behavior.
    *   **4.5.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentRelevanceFilter

        relevance_filter = ContentRelevanceFilter(
            query="AI in healthcare applications",
            threshold=0.3,  # Adjust based on desired strictness
            # k1=1.2, b=0.75, avgdl=1000 are BM25 parameters, defaults are usually fine
        )
        ```
        *   `query (str)`: The search query to compare against.
        *   `threshold (float)`: The minimum BM25 score for a URL to pass.
        *   `k1`, `b`, `avgdl`: BM25 algorithm parameters. Defaults are generally reasonable.
        *   `* Code Example: [Filtering for pages potentially relevant to "sustainable energy solutions" with a score threshold of 0.25]`
            ```python
            # Assuming this filter is part of a FilterChain
            sustainable_energy_filter = ContentRelevanceFilter(
                query="sustainable energy solutions impact",
                threshold=0.25
            )
            # This would attempt to fetch HEAD for candidate URLs and check their head content.
            ```
    *   **4.5.4. When to Use:**
        *   When you need a preliminary content relevance check before committing to a full page download and processing.
        *   For highly targeted crawls where topic relevance is paramount.
        *   Be mindful of the performance implications: each HEAD request adds network latency. This filter is best used after broader, faster filters (like `DomainFilter`, `ContentTypeFilter`).
        *   Test with target sites to see if their HEAD responses are useful for this filter.

*   **4.6. `SEOFilter` (Requires HEAD requests)**
    *   **4.6.1. Purpose:** To filter URLs based on a quantitative assessment of their basic on-page SEO quality, derived from elements in the `<head>` section.
    *   **4.6.2. How it Works:**
        1.  Similar to `ContentRelevanceFilter`, it performs an HTTP HEAD request.
        2.  It then uses `HeadPeek` to extract information like title length, presence and length of meta description, canonical URL validity, robots meta tag status (e.g., `noindex`), schema.org markup presence, and general URL "quality" heuristics (length, parameters, underscores).
        3.  Each factor is scored, and a weighted total score is calculated.
        4.  The URL passes if the total score is above the specified `threshold`.
    *   **4.6.3. Configuration & Usage:**
        ```python
        from crawl4ai import SEOFilter

        seo_quality_filter = SEOFilter(
            threshold=0.65,  # Pages must meet at least 65% of SEO quality checks
            keywords=['data science', 'machine learning'], # Optional: boost score if these appear
            # weights: Optional dict to customize scoring of different SEO factors
        )
        ```
        *   `threshold (float)`: Minimum overall SEO score (0.0 to 1.0) for the URL to pass.
        *   `keywords (Optional[List[str]])`: If provided, presence of these keywords in title or meta description can boost the score.
        *   `weights (Optional[Dict[str, float]])`: Allows customization of how much each SEO factor (e.g., `"title_length"`, `"meta_description"`, `"canonical"`) contributes to the total score. See `SEOFilter.DEFAULT_WEIGHTS` for default factors and their weights.
        *   `* Code Example: [Filtering for pages with an SEO score > 0.7, particularly looking for pages optimized for "python programming tutorials"]`
            ```python
            python_tutorial_seo_filter = SEOFilter(
                threshold=0.7,
                keywords=["python programming tutorials", "learn python"]
            )
            # This filter would favor pages that are generally well-optimized for SEO
            # and also contain the specified keywords.
            ```
    *   **4.6.4. When to Use:**
        *   Performing SEO audits to quickly identify pages with potential on-page issues.
        *   Targeting well-optimized pages for content scraping or analysis.
        *   Like `ContentRelevanceFilter`, be aware of HEAD request overhead. Use after faster filters.

*   **4.7. Building Effective `FilterChain`s**
    *   **Order of filters matters:** This is crucial for performance.
        1.  **Fastest, broadest filters first:** Start with `DomainFilter` to immediately exclude irrelevant domains. Follow with `ContentTypeFilter` to discard unwanted file types by extension. `URLPatternFilter` with simple patterns can also be early.
        2.  **More expensive filters later:** Filters requiring network requests (like `ContentRelevanceFilter`, `SEOFilter`) or complex computations should come last, so they only operate on a reduced set of URLs.
    *   **Combining allow and deny logic:** Use multiple `DomainFilter` or `URLPatternFilter` instances (some with `reverse=True`) to create include/exclude rules. For example, allow `example.com` but block `example.com/private/`.
    *   `* Code Example: [A FilterChain demonstrating strategic ordering]`
        ```python
        from crawl4ai import (
            FilterChain, DomainFilter, ContentTypeFilter, URLPatternFilter,
            ContentRelevanceFilter
        )

        # Goal: Crawl blog posts about "AI ethics" on 'myblog.com',
        #       excluding PDFs and archive sections, ensuring basic relevance.

        # 1. Domain Filter: Only 'myblog.com'
        domain_filter = DomainFilter(allowed_domains=["myblog.com"])

        # 2. Content Type Filter: Only HTML
        content_type_filter = ContentTypeFilter(allowed_types=['.html', '.htm'])

        # 3. URL Pattern Filter: Exclude '/archive/'
        archive_exclude_filter = URLPatternFilter(patterns=[r"/archive/"], reverse=True)

        # 4. Content Relevance Filter: Must be somewhat about "AI ethics"
        relevance_filter = ContentRelevanceFilter(query="AI ethics", threshold=0.2)


        effective_chain = FilterChain(filters=[
            domain_filter,
            content_type_filter,
            archive_exclude_filter,
            relevance_filter  # This one makes HEAD requests, so it's last
        ])

        # This chain would be passed to a DeepCrawlStrategy
        # bfs_strategy = BFSDeepCrawlStrategy(max_depth=3, filter_chain=effective_chain)
        ```
        This example shows how to layer filters: first, quickly narrow down by domain and content type, then apply URL pattern rules, and finally, perform the more costly relevance check on the remaining candidates.

## 5. Prioritizing URLs: Scoring Mechanisms

URL scoring is the cornerstone of the `BestFirstCrawlingStrategy`. It allows you to define what "best" means for your crawl, guiding the crawler to explore the most promising URLs first.

*   **5.1. Why Score URLs?**
    *   **Guided Exploration:** Directs the `BestFirstCrawlingStrategy` to URLs that are most likely to contain the information you seek.
    *   **Resource Optimization:** If you have a `max_pages` limit or a time constraint, scoring helps ensure that the most valuable pages are processed before the limit is reached.
    *   **Relevance Ranking:** Allows you to implicitly rank discovered pages by their potential importance or relevance to your task.
    *   **Focus:** Helps concentrate crawling efforts on specific types of content (e.g., fresh news, product pages, high-authority articles).

*   **5.2. `CompositeScorer`: Combining Multiple Signals**
    *   **How it works:** `CompositeScorer` takes a list of individual `URLScorer` instances. For a given URL, it calls the `score()` method of each child scorer. The final score for the URL is typically a weighted sum of the scores from these individual scorers. Each child scorer's raw score is multiplied by its assigned `weight` (which defaults to 1.0 if not specified when adding the scorer to the `CompositeScorer`).
        ```python
        from crawl4ai import CompositeScorer, KeywordRelevanceScorer, PathDepthScorer

        keyword_scorer = KeywordRelevanceScorer(keywords=["news", "update"], weight=0.7)
        path_scorer = PathDepthScorer(optimal_depth=2, weight=0.3)

        # The CompositeScorer will calculate:
        # final_score = (keyword_scorer.score(url) * 0.7) + (path_scorer.score(url) * 0.3)
        # Note: The individual scorers' `weight` parameter is used by CompositeScorer.
        # It is more common to set weights when adding to CompositeScorer if that API exists,
        # or ensure individual scorers output in a way that their inherent weight makes sense.
        # Based on the code: CompositeScorer sums the results of `scorer.score(url)` directly,
        # which already includes the scorer's own weight.
        # So, the weights are applied within each scorer before summing.

        # Corrected understanding based on code:
        # Each scorer's score() method already incorporates its own weight.
        # CompositeScorer simply sums these pre-weighted scores.
        composite_scorer = CompositeScorer(scorers=[keyword_scorer, path_scorer])
        # final_score = keyword_scorer.score(url) + path_scorer.score(url)
        # where keyword_scorer.score(url) is raw_keyword_score * 0.7
        # and path_scorer.score(url) is raw_path_score * 0.3
        ```
    *   **Normalizing scores:** The individual scorers in Crawl4ai are generally designed to output scores that are somewhat normalized (often between 0 and 1 before their internal weight is applied). However, if you create custom scorers with vastly different output ranges, their contribution to the `CompositeScorer` might be skewed. It's good practice to design custom scorers to produce scores in a relatively consistent range (e.g., 0-1) before their `weight` is applied, or adjust their individual `weight`s accordingly to balance their influence in the `CompositeScorer`. The `CompositeScorer` itself simply sums the already weighted scores from its child scorers.
    *   `* Code Example: [Creating a CompositeScorer combining KeywordRelevanceScorer, FreshnessScorer, and DomainAuthorityScorer. Keyword relevance is most important, followed by freshness, then domain authority.]`
        ```python
        from crawl4ai import (
            CompositeScorer, KeywordRelevanceScorer, FreshnessScorer,
            DomainAuthorityScorer
        )
        from datetime import datetime

        # Scorer for keywords, highly weighted
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["financial analysis", "market trends"],
            weight=0.5 # This weight is applied internally by KeywordRelevanceScorer
        )

        # Scorer for freshness, moderately weighted
        freshness_scorer = FreshnessScorer(
            current_year=datetime.now().year,
            weight=0.3
        )

        # Scorer for domain authority, less weighted
        domain_scorer = DomainAuthorityScorer(
            domain_weights={"bloomberg.com": 0.9, "reuters.com": 0.85, "wsj.com": 0.95},
            default_weight=0.5, # For other domains
            weight=0.2
        )

        # CompositeScorer sums the weighted scores from each child scorer
        final_url_scorer = CompositeScorer(
            scorers=[keyword_scorer, freshness_scorer, domain_scorer]
        )
        # Example URL score calculation:
        # url = "https://www.bloomberg.com/news/articles/2023-10-26/ai-impact-on-finance"
        # score_for_url = final_url_scorer.score(url)
        # This would be:
        # (raw_keyword_score_for_url * 0.5) + \
        # (raw_freshness_score_for_url * 0.3) + \
        # (raw_domain_score_for_url * 0.2)
        ```

*   **5.3. `KeywordRelevanceScorer`**
    *   **5.3.1. Purpose:** Scores URLs based on how many of the specified keywords appear within the URL string itself. This is a simple way to prioritize URLs that seem topically relevant by their path or query parameters.
    *   **5.3.2. How it Works:** It iterates through the provided list of `keywords`. For each keyword, it checks if it's present in the URL (case-insensitively by default). The score is typically proportional to the number of matched keywords, normalized by the total number of keywords, and then multiplied by the scorer's `weight`.
    *   **5.3.3. Configuration & Usage:**
        ```python
        from crawl4ai import KeywordRelevanceScorer

        # Prioritize URLs related to Python or Machine Learning
        tech_keyword_scorer = KeywordRelevanceScorer(
            keywords=["python", "machine learning", "pytorch"],
            weight=1.0,       # Overall weight for this scorer
            case_sensitive=False # Default
        )
        ```
        *   `keywords (List[str])`: A list of keywords to search for in the URL.
        *   `weight (float, default=1.0)`: A multiplier applied to the raw score.
        *   `case_sensitive (bool, default=False)`: Whether keyword matching should be case-sensitive.
        *   `* Code Example: [Scoring URLs for a job board, prioritizing those containing "remote", "engineer", or "developer"]`
            ```python
            job_url_scorer = KeywordRelevanceScorer(
                keywords=["remote", "engineer", "developer", "software"],
                weight=1.0
            )
            # Example scores:
            # score1 = job_url_scorer.score("https://jobs.example.com/remote-software-engineer-position") # high score
            # score2 = job_url_scorer.score("https://jobs.example.com/marketing-manager-sf")         # lower score
            ```

*   **5.4. `DomainAuthorityScorer` (Conceptual/External Data)**
    *   **5.4.1. Purpose:** To give preference to URLs from domains that are considered more authoritative or trustworthy. This requires you to provide the authority scores.
    *   **5.4.2. How it Works:** It uses a dictionary (`domain_weights`) that maps domain names (e.g., `"wikipedia.org"`) to numerical authority scores (e.g., `0.9`). When a URL is scored, its domain is extracted. If the domain is in `domain_weights`, its score is used; otherwise, `default_weight` is applied. This raw score is then multiplied by the scorer's overall `weight`.
    *   **5.4.3. Configuration & Usage:**
        ```python
        from crawl4ai import DomainAuthorityScorer

        authority_scorer = DomainAuthorityScorer(
            domain_weights={
                "wikipedia.org": 0.9,
                "scholar.google.com": 0.85,
                "archive.org": 0.7
            },
            default_weight=0.3, # Score for domains not in the list
            weight=1.0          # Overall weight for this scorer's contribution
        )
        ```
        *   `domain_weights (Dict[str, float])`: A dictionary mapping domain strings to their authority scores (typically 0-1).
        *   `default_weight (float, default=0.5)`: Score assigned to URLs from domains not explicitly listed in `domain_weights`.
        *   `weight (float, default=1.0)`: Multiplier for the final score from this scorer.
        *   `* Code Example: [In a news crawl, giving higher scores to URLs from 'bbc.com', 'nytimes.com', and 'reuters.com']`
            ```python
            news_authority_scorer = DomainAuthorityScorer(
                domain_weights={
                    "bbc.com": 0.95,
                    "nytimes.com": 0.9,
                    "reuters.com": 0.88
                },
                default_weight=0.4, # Other news sources
                weight=1.0
            )
            # score_bbc = news_authority_scorer.score("https://www.bbc.com/news/world-europe-12345")
            # score_local_blog = news_authority_scorer.score("https://my-local-blog.com/news-update")
            ```
    *   **5.4.4. Note:** This scorer's effectiveness depends entirely on the quality and relevance of the `domain_weights` you provide. There's no built-in mechanism in Crawl4ai to automatically determine domain authority; you must supply this data.

*   **5.5. `FreshnessScorer`**
    *   **5.5.1. Purpose:** To prioritize URLs that appear to contain more recent content, typically by looking for date patterns (especially years) in the URL string.
    *   **5.5.2. How it Works:** It uses regular expressions to find date patterns (like YYYY/MM/DD, YYYY-MM-DD, YYYY_MM_DD, or just YYYY) in the URL. It extracts the most recent valid year found. The score is then calculated based on the difference between this extracted year and the `current_year` provided during initialization. More recent years get higher scores. The pre-defined `_FRESHNESS_SCORES` list provides a quick lookup for common year differences.
    *   **5.5.3. Configuration & Usage:**
        ```python
        from crawl4ai import FreshnessScorer
        from datetime import datetime

        # Prioritize content from the current year or last few years
        current_year = datetime.now().year
        freshness_scorer = FreshnessScorer(
            current_year=current_year,
            weight=1.0
        )
        ```
        *   `current_year (int)`: The reference year for calculating freshness.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   Date patterns detected: The `_date_pattern` regex in `scorers.py` looks for common date formats.
        *   `* Code Example: [Prioritizing news articles or blog posts from 2023 onwards, assuming current year is 2024]`
            ```python
            # Assuming it's 2024
            recent_content_scorer = FreshnessScorer(current_year=2024, weight=1.0)
            # score_2023 = recent_content_scorer.score("https://example.com/blog/2023/10/my-article") # High score
            # score_2020 = recent_content_scorer.score("https://example.com/archive/2020/old-post") # Lower score
            # score_no_date = recent_content_scorer.score("https://example.com/about-us")      # Default score (0.5)
            ```

*   **5.6. `PathDepthScorer`**
    *   **5.6.1. Purpose:** Scores URLs based on their path depth, which is the number of segments in the URL path (e.g., `/folder1/folder2/page.html` has depth 3). This can be used to prefer shallower pages (often more important) or pages around a specific `optimal_depth`.
    *   **5.6.2. How it Works:** It parses the URL to count the number of segments in its path. The scoring logic (from `_LOOKUP_SCORES`) gives higher scores to depths closer to `optimal_depth`. Depths further away receive progressively lower scores.
    *   **5.6.3. Configuration & Usage:**
        ```python
        from crawl4ai import PathDepthScorer

        # Prefer pages with a path depth of 2 or 3
        path_scorer = PathDepthScorer(
            optimal_depth=2, # Or 3, depending on preference
            weight=1.0
        )
        ```
        *   `optimal_depth (int, default=3)`: The path depth considered most desirable.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   `* Code Example: [Slightly preferring top-level category pages (depth 1) or main articles (depth 2)]`
            ```python
            shallow_page_scorer = PathDepthScorer(optimal_depth=1, weight=1.0)
            # score_depth1 = shallow_page_scorer.score("https://example.com/products/")  # High score (closer to 1.0)
            # score_depth3 = shallow_page_scorer.score("https://example.com/products/category/item") # Lower score
            ```
        The scoring is based on the difference from `optimal_depth`, using `_SCORE_LOOKUP = [1.0, 0.5, 0.333..., 0.25]`. A difference of 0 gets 1.0, 1 gets 0.5, etc.

*   **5.7. `ContentTypeScorer`**
    *   **5.7.1. Purpose:** Assigns scores to URLs based on their inferred content type, primarily determined by the file extension. This allows prioritizing certain types of content (e.g., HTML pages over images or documents).
    *   **5.7.2. How it Works:** It extracts the file extension from the URL. If this extension is found as a key in the `type_weights` dictionary provided during initialization, the corresponding score is used. If the extension is not found, a default score of 0.0 is typically assigned (unless `type_weights` provides a wildcard or default). The raw score is then multiplied by the scorer's `weight`.
    *   **5.7.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentTypeScorer

        html_priority_scorer = ContentTypeScorer(
            type_weights={
                '.html': 1.0,
                '.htm': 1.0,
                '.pdf': 0.7,
                '.doc': 0.5,
                '.jpg': 0.2,
                '.png': 0.2
            },
            weight=1.0
        )
        ```
        *   `type_weights (Dict[str, float])`: A dictionary mapping file extensions (including the dot, e.g., `'.html'`) to scores.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   `* Code Example: [Prioritizing HTML and PDF documents, while down-weighting images and executables for a document-focused crawl]`
            ```python
            document_content_scorer = ContentTypeScorer(
                type_weights={
                    '.html': 1.0, '.htm': 1.0,
                    '.pdf': 0.9, '.doc': 0.8, '.docx': 0.8,
                    '.txt': 0.7,
                    '.jpg': 0.1, '.png': 0.1, '.gif': 0.1,
                    '.exe': 0.0, '.zip': 0.05
                },
                weight=1.0
            )
            # score_html = document_content_scorer.score("https://example.com/index.html") # High
            # score_zip = document_content_scorer.score("https://example.com/archive.zip") # Low
            ```

*   **5.8. `URLScorer` (Base Class)**
    *   If the built-in scorers or their combination via `CompositeScorer` don't meet your specific needs, you can create a highly custom scorer by inheriting from the `URLScorer` base class.
    *   **Key method to implement:** `_calculate_score(self, url: str) -> float`. This method should take a URL string and return a float representing its score. Remember that the `score(self, url:str)` public method will automatically multiply this by `self._weight`.
    *   Consider the range of scores your custom scorer produces to ensure it integrates well if used within a `CompositeScorer`.

## 6. Configuring and Running Deep Crawls

The `CrawlerRunConfig` object is central to configuring how any specific crawl, including deep crawls, behaves. You'll pass your chosen `DeepCrawlStrategy` (along with its configured filters and scorers) to it.

*   **6.1. The Role of `CrawlerRunConfig`**
    *   The `deep_crawl_strategy` parameter of `CrawlerRunConfig` is how you enable and configure deep crawling for a specific `arun()` or `arun_many()` call.
    *   You instantiate your chosen strategy (e.g., `BFSDeepCrawlStrategy`), configure it with any `FilterChain` and `URLScorer` instances, and then assign this strategy object to `CrawlerRunConfig.deep_crawl_strategy`.
    *   `* Code Example: [Illustrating how strategy is passed to CrawlerRunConfig]`
        ```python
        from crawl4ai import (
            AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy,
            DomainFilter, FilterChain
        )
        import asyncio

        # 1. Define Filters and Scorers (if needed)
        my_filters = FilterChain(filters=[DomainFilter(allowed_domains=["example.com"])])

        # 2. Instantiate and Configure the Strategy
        my_bfs_strategy = BFSDeepCrawlStrategy(max_depth=2, filter_chain=my_filters)

        # 3. Create CrawlerRunConfig and assign the strategy
        my_run_config = CrawlerRunConfig(
            deep_crawl_strategy=my_bfs_strategy,
            # ... other run-specific settings like cache_mode, verbosity, etc.
            cache_mode=CacheMode.BYPASS,
            verbose=True
        )

        async def main():
            async with AsyncWebCrawler() as crawler:
                results = await crawler.arun(url="https://example.com", config=my_run_config)
                for result in results: # arun returns a CrawlResultContainer
                    if result.success:
                        print(f"Crawled: {result.url} - Depth: {result.metadata.get('depth')}")
        # asyncio.run(main())
        ```

*   **6.2. Essential Global Deep Crawl Parameters in `DeepCrawlStrategy` (and reflected in `CrawlerRunConfig`)**
    These parameters are generally set on the strategy object itself.
    *   **`max_depth`:**
        *   **Meaning:**
            *   **BFS:** The maximum number of levels to explore from the seed URL(s). Level 0 is the seed.
            *   **DFS:** The maximum number of links to follow down a single path before backtracking.
            *   **Best-First:** While primarily score-driven, `max_depth` still acts as an upper limit on how deep any single path can go, preventing infinite exploration even if scores are high.
        *   **Strategies for choosing `max_depth`:**
            *   Start small (e.g., 1 or 2) and observe the number of pages found.
            *   Increase incrementally based on your understanding of the target site's structure and your data needs.
            *   For very large sites, a high `max_depth` can lead to an enormous number of URLs.
        *   `* Diagram: [Show two small site graphs. One with max_depth=1, showing only direct links. Another with max_depth=2, showing links of links. Highlight the exponential growth potential.]`
    *   **`max_pages`:**
        *   **How it acts as a global stop condition:** Regardless of `max_depth` or strategy, the crawl will halt once `max_pages` have been successfully processed.
        *   **Interaction with `max_depth` and strategy:**
            *   A crawl might hit `max_pages` before reaching `max_depth` on all branches.
            *   For BFS, this means some deeper levels might not be touched.
            *   For DFS, this means some branches might not be fully explored.
            *   For Best-First, this means lower-scoring URLs might never be visited.
        *   **Use cases:**
            *   Budgeting crawl resources (time, bandwidth, API calls if any).
            *   Getting a quick sample or overview of a site.
            *   Preventing runaway crawls on unexpectedly large sites.
    *   **`include_external` (Context: Typically a parameter on the strategy, e.g., `BestFirstCrawlingStrategy` and other strategies in `crawl4ai`):**
        *   **What it does:** If `True`, the crawler will follow links to domains different from the seed URL's domain.
        *   **When to use it:**
            *   Discovering backlinks or references to your site from external sources (though this is usually done by starting crawls on those external sources).
            *   Exploring a small, trusted ecosystem of related websites.
        *   **Potential pitfalls:**
            *   **Crawl Scope Explosion:** The web is vast. Without *extremely* strict filters (`DomainFilter` for allowed external domains, `URLPatternFilter`), enabling `include_external` can lead to an unmanageably large and irrelevant crawl.
            *   **Resource Drain:** Crawling external sites consumes your resources for potentially off-topic content.
            *   **Best Practice:** Keep `include_external=False` (the default for most strategies) unless you have a very specific reason and robust filters in place. If you need to crawl multiple specific domains, it's often better to run separate, focused crawls for each or use `arun_many` with a list of seed URLs.

*   **6.3. Practical Examples of `CrawlerRunConfig` for Deep Crawls**
    *   `* Code Example: [BFS crawl limited to depth 3 within 'example.com', only HTML pages, verbose logging]`
        ```python
        bfs_strat_example = BFSDeepCrawlStrategy(
            max_depth=3,
            filter_chain=FilterChain(filters=[
                DomainFilter(allowed_domains=["example.com"]),
                ContentTypeFilter(allowed_types=['.html', '.htm'])
            ])
        )
        run_config_bfs = CrawlerRunConfig(
            deep_crawl_strategy=bfs_strat_example,
            verbose=True,
            cache_mode=CacheMode.BYPASS # Forcing fresh crawl for example
        )
        # Usage: await crawler.arun(url="https://example.com", config=run_config_bfs)
        ```
    *   `* Code Example: [DFS crawl following '/blog/' paths, max 50 pages, stream results]`
        ```python
        dfs_strat_example = DFSDeepCrawlStrategy(
            max_depth=10, # DFS can go deep
            max_pages=50,
            filter_chain=FilterChain(filters=[
                URLPatternFilter(patterns=[r"https://example.com/blog/.*"])
            ]),
            stream=True # Yield results as they are found
        )
        run_config_dfs = CrawlerRunConfig(
            deep_crawl_strategy=dfs_strat_example,
            verbose=True
        )
        # Usage:
        # async for result in await crawler.arun(url="https://example.com/blog/", config=run_config_dfs):
        #     # process result
        ```
    *   `* Code Example: [Best-First crawl for "AI ethics" articles, prioritizing recent, high-authority sources, excluding PDFs, max 100 pages]`
        ```python
        from crawl4ai import KeywordRelevanceScorer, FreshnessScorer, DomainAuthorityScorer, CompositeScorer, ContentTypeFilter
        from datetime import datetime

        # Scorers
        keyword_scorer_ai_ethics = KeywordRelevanceScorer(keywords=["AI ethics", "responsible AI"], weight=0.6)
        freshness_scorer_recent = FreshnessScorer(current_year=datetime.now().year, weight=0.25)
        authority_scorer_news = DomainAuthorityScorer(
            domain_weights={"techcrunch.com": 0.8, "wired.com": 0.85},
            default_weight=0.4,
            weight=0.15
        )
        composite_scorer_ai = CompositeScorer(scorers=[
            keyword_scorer_ai_ethics, freshness_scorer_recent, authority_scorer_news
        ])

        # Filters
        filter_chain_ai = FilterChain(filters=[
            DomainFilter(allowed_domains=["techcrunch.com", "wired.com", "another-news-site.com"]),
            ContentTypeFilter(allowed_types=['.html']) # Exclude PDFs by only allowing HTML
        ])

        best_first_strat_ai = BestFirstCrawlingStrategy(
            url_scorer=composite_scorer_ai,
            filter_chain=filter_chain_ai,
            max_pages=100,
            max_depth=5 # Still good to have a depth limit
        )
        run_config_best_first_ai = CrawlerRunConfig(
            deep_crawl_strategy=best_first_strat_ai,
            verbose=True
        )
        # Usage: await crawler.arun(url="https://techcrunch.com", config=run_config_best_first_ai)
        ```

*   **6.4. Integrating with `AsyncWebCrawler.arun()` and `AsyncWebCrawler.arun_many()`**
    *   **`arun(url="seed_url", config=run_config_with_deep_crawl)`:**
        When you call `arun` with a `CrawlerRunConfig` that includes a `deep_crawl_strategy`, the deep crawling process starts from the single `seed_url`. The `DeepCrawlDecorator` intercepts this and delegates to your strategy.
    *   **`arun_many(urls=["seed1", "seed2"], config=run_config_with_deep_crawl)`:**
        If you use `arun_many`, Crawl4ai will typically initiate *independent* deep crawls starting from each seed URL in the `urls` list. Each deep crawl will adhere to the `max_depth`, `max_pages`, filters, and scorers defined in the *same* `run_config_with_deep_crawl`.
        *   **Important Consideration:** The `max_pages` limit in this scenario usually applies *per seed URL's deep crawl task* if the dispatcher handles them as separate tasks. If you need a global `max_pages` across all seed URLs in an `arun_many` call, that would require a more custom dispatcher or a wrapper around `arun_many` to track the total pages. The default behavior is often per-task limits.
        *   If `stream=True` is set on the strategy (or within the `run_config`), results from the different deep crawls initiated by `arun_many` will be yielded as they become available.

## 7. Understanding the `DeepCrawlDecorator`

While you primarily interact with deep crawling through `CrawlerRunConfig` and strategy objects, it's helpful to have a conceptual understanding of the `DeepCrawlDecorator`.

*   **7.1. How Deep Crawling is Activated (Conceptual)**
    *   **Role of `DeepCrawlDecorator`:** The `DeepCrawlDecorator` is a Python decorator that wraps the `AsyncWebCrawler.arun()` method. When `arun()` is called, the decorator checks if the `config` argument (an instance of `CrawlerRunConfig`) has a `deep_crawl_strategy` defined.
    *   If a `deep_crawl_strategy` is present *and* deep crawling is not already active (see `deep_crawl_active` below), the decorator intercepts the call. Instead of the standard single-page crawl, it invokes the `arun()` method of your specified `deep_crawl_strategy` instance, passing along the crawler, seed URL, and configuration.
    *   If no `deep_crawl_strategy` is set, or if deep crawling is already active, the decorator allows the original `arun()` method (for single-page crawling) to proceed.
    *   **The `deep_crawl_active` `ContextVar`:** This is a context variable (from Python's `contextvars` module). The decorator sets `deep_crawl_active` to `True` before calling the strategy's `arun()` method and resets it to `False` afterwards.
        *   **Purpose:** Its primary function is to prevent accidental recursive deep crawls. If your `deep_crawl_strategy` internally calls `crawler.arun()` (which it typically does to fetch individual pages), this flag ensures that those internal calls perform standard single-page fetches and don't try to initiate another layer of deep crawling using the same strategy.

*   **7.2. What This Means for You as a User**
    *   **Transparency:** For most use cases, the `DeepCrawlDecorator` operates transparently. You don't need to instantiate or call it directly.
    *   **Centralized Configuration:** You enable and configure deep crawling by setting the `deep_crawl_strategy` attribute in your `CrawlerRunConfig` object. This is the main entry point.
    *   **Awareness for Advanced Scenarios:** Understanding its existence is useful if:
        *   You are debugging complex deep crawling behavior and want to trace the execution flow.
        *   You are developing very custom strategies that might need to interact with or be aware of this context.

## 8. Monitoring and Analyzing Deep Crawl Performance

Understanding what happened during your deep crawl is key to optimizing it.

*   **8.1. Using `TraversalStats`**
    *   **Accessing `TraversalStats`:**
        *   The `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, and `BestFirstCrawlingStrategy` (and their base class `DeepCrawlStrategy`) maintain a `self.stats` attribute of type `TraversalStats`.
        *   After a crawl initiated by one of these strategies completes (or even during, if you have access to the strategy instance), you can inspect `strategy_instance.stats`.
        *   The strategies also log these stats upon completion (or at intervals if verbose logging is high).
    *   **Key Metrics and Their Meaning (from `TraversalStats` in `crawl4ai/deep_crawling/base_strategy.py` and related files):**
        *   `start_time`, `end_time`: The overall start and end Python `datetime` objects for the crawl.
        *   `urls_processed` (often part of `FilterStats` within a strategy or a general counter): Total number of unique URLs that were actually fetched and processed by the crawler.
        *   `urls_failed`: Count of URLs that resulted in an error during fetching or processing.
        *   `urls_skipped`: Count of URLs that were discovered but discarded by the `FilterChain` or other conditions (e.g., already visited, exceeded `max_depth`).
        *   `total_depth_reached`: The maximum depth level explored during the crawl.
        *   `current_depth` (Relevant for strategies like BFS/DFS): The current depth level being explored.
        *   Individual filters (`URLFilter` subclasses) also have their own `FilterStats` (`filter.stats`) which track `total_urls` processed by that filter, `passed_urls`, and `rejected_urls`. This is very useful for seeing which filter is having the most impact.
    *   **Interpreting Stats for Optimization:**
        *   **High `urls_skipped` (overall) or high `rejected_urls` (for a specific filter):** This indicates your filters are very active. Review them to ensure they aren't too restrictive or are correctly configured.
        *   **`total_depth_reached` < `max_depth` (when `max_pages` is high):** This could mean the crawl exhausted all discoverable links within the filter scope before reaching the maximum depth, or filters are preventing deeper exploration.
        *   **Crawl finishes too quickly and `urls_processed` is low:** Check seed URLs, initial filters, and `max_depth/max_pages` limits.
        *   **Crawl takes too long:**
            *   Are filters too loose?
            *   Is `max_depth` or `max_pages` too high for the site?
            *   Are expensive filters/scorers (requiring network or heavy computation) being used excessively?
    *   `* Code Example: [Accessing stats after a BFS crawl (assuming batch mode)]`
        ```python
        # ... (setup bfs_strategy and run_config as in previous examples) ...
        # async with AsyncWebCrawler(config=browser_config) as crawler:
        #     results_container = await crawler.arun(
        #         url="https://example.com",
        #         config=run_config_with_bfs_strategy # run_config_with_bfs_strategy.deep_crawl_strategy is bfs_strategy
        #     )
        #
        #     # Access stats from the strategy instance
        #     crawl_stats = run_config_with_bfs_strategy.deep_crawl_strategy.stats
        #     print(f"\n--- Crawl Statistics ---")
        #     print(f"Start Time: {crawl_stats.start_time}")
        #     # Note: urls_processed might be better tracked by summing successful results
        #     # or by inspecting filter stats on the FilterChain if available.
        #     # TraversalStats itself in the provided code doesn't explicitly have urls_processed.
        #     # Let's assume we count successful results:
        #     print(f"URLs Successfully Processed: {len([r for r in results_container if r.success])}")
        #     print(f"URLs Failed: {len([r for r in results_container if not r.success])}") # Approximation
        #     print(f"URLs Skipped by Filters (Example): {getattr(run_config_with_bfs_strategy.deep_crawl_strategy.filter_chain, 'stats', FilterStats()).rejected_urls if run_config_with_bfs_strategy.deep_crawl_strategy.filter_chain else 'N/A'}")
        #     print(f"Max Depth Reached: {crawl_stats.total_depth_reached}")
        #     if crawl_stats.end_time:
        #          print(f"End Time: {crawl_stats.end_time}")
        #          print(f"Duration: {crawl_stats.end_time - crawl_stats.start_time}")
        #     else:
        #          print("Crawl may still be in progress or ended prematurely.")

        # Note: The TraversalStats model has start_time, end_time, urls_processed, urls_failed, urls_skipped, total_depth_reached, current_depth
        # The strategy code increments self._pages_crawled for successful crawls towards max_pages
        # and self.stats.urls_skipped for links skipped by filters.
        ```

*   **8.2. Logging in Deep Crawls**
    *   Crawl4ai's `AsyncLogger` (if `verbose=True` in `CrawlerRunConfig` or `BrowserConfig`) provides valuable insights. During deep crawls, you'll see:
        *   URLs being added to the frontier/queue/stack.
        *   Actions taken by filters (e.g., "URL rejected by DomainFilter").
        *   Scores assigned to URLs if using `BestFirstCrawlingStrategy` with a `URLScorer`.
        *   Newly discovered links from each page.
        *   Errors encountered during fetching or processing.
    *   **Setting verbosity:**
        *   `verbose=True` (default) provides a good level of detail.
        *   For extremely detailed debugging, you might need to delve into the library's source or add custom logging within custom components.
        *   The logger uses tags (e.g., `[BFS]`, `[FILTER]`, `[SCORE]`) to help identify the source of log messages.

## 9. Best Practices for Effective Deep Crawling

Crafting an effective deep crawl involves more than just setting a strategy; it requires planning, careful configuration, and ethical considerations.

*   **9.1. Planning Your Crawl Strategy**
    *   **Define Clear Objectives:**
        *   What specific data are you trying to collect? (e.g., all product names and prices, all blog post titles and content, site structure).
        *   What is the precise scope of your crawl? (e.g., a single subdomain, specific path patterns, content related to certain keywords).
        *   Clearly defined objectives will guide your choice of strategy, filters, and scorers.
    *   **Analyze Target Website Structure:**
        *   **`robots.txt`:** Always check `robots.txt` first (e.g., `https://example.com/robots.txt`) to understand disallowed paths. Crawl4ai can do this automatically if `check_robots_txt=True` (in `CrawlerRunConfig`).
        *   **Sitemaps:** Look for XML sitemaps (`/sitemap.xml`). They often provide a good list of canonical URLs and can be a great source of seed URLs.
        *   **URL Patterns:** Observe common URL structures for different content types (e.g., `/blog/YYYY/MM/DD/slug`, `/product/category/item-id`). This helps in crafting `URLPatternFilter`s.
        *   **Navigation:** Understand how users (and thus crawlers) navigate the site. Are there clear menus, breadcrumbs, pagination?
    *   **Estimate Crawl Size:**
        *   Before launching a full deep crawl, try a very limited crawl (e.g., `max_depth=1` or `2`, `max_pages=50`) to get a sense of how many URLs are discovered per level.
        *   This helps in estimating resource consumption and setting realistic `max_depth` and `max_pages` for the full crawl.

*   **9.2. Configuring for Efficiency and Relevance**
    *   **Filter Aggressively, Then Loosen:**
        *   Start with a restrictive `DomainFilter` to stay within your target domain(s).
        *   Add `ContentTypeFilter` early to exclude unwanted file types.
        *   Use `URLPatternFilter` to include/exclude specific paths.
        *   Only if necessary, add more computationally expensive filters like `ContentRelevanceFilter` or `SEOFilter` later in the chain.
    *   **Control Scope with `max_depth` and `max_pages`:**
        *   These are your primary safety nets against runaway crawls.
        *   Set them based on your objectives and initial site analysis.
    *   **Choose the Right Strategy:**
        *   BFS for broad, systematic coverage of shallow sites.
        *   DFS for deep dives into specific paths (with careful depth control).
        *   Best-First for targeted crawling based on relevance, freshness, or other criteria (requires good scorer design).
    *   **Iterate on Scorer Design (for Best-First):**
        *   Start with simple scorers.
        *   Run test crawls, analyze the types of URLs being prioritized, and refine your scorer weights or logic.
        *   Use `CompositeScorer` to combine multiple weak signals into a stronger one.
    *   **Test Filter and Scorer Logic:**
        *   Before a large crawl, test your `FilterChain` and `URLScorer` logic with a small, representative set of sample URLs to ensure they behave as expected.

*   **9.3. Ethical and Respectful Crawling**
    *   **Respect `robots.txt`:** Set `check_robots_txt=True` in your `CrawlerRunConfig`. Crawl4ai's `RobotsParser` will then automatically check `robots.txt` for each domain.
    *   **Implement Politeness Delays:**
        *   Crawl4ai's default dispatcher strategies often include rate limiting and backoff mechanisms.
        *   You can configure `mean_delay` and `max_range` in `CrawlerRunConfig` if using `arun_many` to introduce delays between individual `arun` calls managed by the dispatcher.
        *   Avoid hitting any single server too frequently.
    *   **Identify and Handle Sensitive Data Responsibly:** If your crawl might encounter personally identifiable information (PII) or other sensitive data, ensure you have mechanisms to either avoid it (via filters) or handle it according to privacy regulations and ethical guidelines.
    *   **Avoid Overwhelming Servers:** Monitor your crawl's impact. If you notice server errors (5xx status codes) or very slow responses, reduce your crawl rate or concurrency.
    *   **User-Agent:** While Crawl4ai provides a default User-Agent, consider setting a custom one that identifies your bot and provides a way to contact you if site administrators have questions (e.g., `MyCoolBot/1.0 (+http://mybotinfo.example.com)`).

*   **9.4. Handling Common Challenges**
    *   **Crawler Traps:**
        *   **Solution:** Use a sensible `max_depth`. Employ `URLPatternFilter` to exclude patterns that lead to traps (e.g., infinitely deep calendar links, search facets creating unique URLs for every combination).
        *   **Example:** A filter like `URLPatternFilter(patterns=[r"/calendar/\d{4}/\d{2}/\d{2}/.*"], reverse=True)` could block deep calendar paths.
    *   **Session-Dependent Content:**
        *   **Challenge:** If deep pages require a login session established on an earlier page.
        *   **Solution:** Use Crawl4ai's session persistence features.
            1.  Perform an initial crawl/interaction to log in, then save the `storage_state` from the browser context.
            2.  For subsequent deep crawls, load this `storage_state` into `BrowserConfig` or `CrawlerRunConfig` to reuse the session.
            *   Refer to the [Session Management](./session-management.md) and [Advanced Features](./advanced-features.md#5-session-persistence--local-storage) guides for details.
    *   **AJAX-Loaded Content / JavaScript-Generated Links:**
        *   **Challenge:** If links are not present in the initial HTML but are loaded or generated by JavaScript.
        *   **Solution:** Ensure your base crawling strategy (e.g., `AsyncPlaywrightCrawlerStrategy`, which is default) is used, as it executes JavaScript. You might need to use `wait_for` in `CrawlerRunConfig` to allow time for JS to execute and render links before link discovery happens.
    *   **Large-Scale Crawls:**
        *   **Challenge:** In-memory `visited` sets and URL frontiers can become bottlenecks for very large crawls (millions of pages).
        *   **Solution:** For enterprise-scale crawls, consider:
            *   Distributed crawling architecture (breaking the crawl into smaller, manageable parts).
            *   Using a persistent, disk-based queue (e.g., Redis, RabbitMQ) for the URL frontier.
            *   Using a distributed database or bloom filter service for the `visited` set.
            *   Crawl4ai's current built-in strategies are primarily designed for single-instance operation with in-memory tracking.

## 10. Troubleshooting Common Deep Crawling Issues

Even with careful planning, deep crawls can sometimes behave unexpectedly. Here's how to diagnose common problems.

*   **10.1. Crawl Not Going Deep Enough / Stopping Prematurely**
    *   **Check `max_depth` and `max_pages`:** Are these limits set too low for your target?
        *   `print(f"Config: max_depth={my_strategy.max_depth}, max_pages={my_strategy.max_pages}")`
    *   **Inspect Filters:**
        *   Are your filters (`DomainFilter`, `URLPatternFilter`, `ContentTypeFilter`, etc.) too aggressive?
        *   **Debugging Tip:** Temporarily disable filters one by one or simplify them to see if they are the cause. Add logging within custom filters or enable verbose logging in Crawl4ai to see filter decisions.
        *   `* Code Example: [Temporarily disabling a filter chain for debugging]`
            ```python
            # original_filter_chain = my_strategy.filter_chain
            # my_strategy.filter_chain = None # Temporarily disable for a test run
            # ... run crawl ...
            # my_strategy.filter_chain = original_filter_chain # Restore
            ```
    *   **Examine Link Discovery:**
        *   Are links being correctly extracted from the initial pages? After an `arun` call on a seed URL, inspect `result.links`.
        *   If links are JavaScript-generated, ensure your `wait_for` or other JS execution settings are adequate.
    *   **`include_external` Behavior:** If you *intend* to crawl subdomains but they are treated as external (and `include_external=False`), they won't be followed. Ensure your `DomainFilter` correctly specifies all allowed (sub)domains if `include_external` is `False`.

*   **10.2. Crawling Too Many Irrelevant Pages**
    *   **Tighten Filters:** This is the most common solution.
        *   Make `DomainFilter` more specific.
        *   Refine `URLPatternFilter` to exclude unwanted paths.
        *   Add a `ContentTypeFilter` if you're getting many non-HTML pages.
    *   **Refine Scoring (for Best-First):** If using `BestFirstCrawlingStrategy`, improve your `URLScorer` to give lower scores to irrelevant URL patterns or domains.
    *   **Verify `include_external`:** Ensure it's `False` (default for most strategies) if you don't intend to leave your primary domain(s).

*   **10.3. Performance Bottlenecks**
    *   **Network-Intensive Filters:** `ContentRelevanceFilter` and `SEOFilter` make HEAD requests for each URL they evaluate. If your `FilterChain` applies these to many URLs, it will significantly slow down the link processing phase.
        *   **Solution:** Place these filters *after* faster filters that can eliminate many URLs without network calls.
    *   **Complex Regex:** Very complex or inefficient regular expressions in `URLPatternFilter` can be slow.
        *   **Solution:** Simplify regex where possible. Test regex performance independently.
    *   **Complex Scoring Logic:** If your custom `URLScorer` or `CompositeScorer` performs heavy computations or external API calls for every URL, it will become a bottleneck for `BestFirstCrawlingStrategy`.
        *   **Solution:** Optimize scorer logic. Cache external API results if possible.
    *   **BFS Memory Usage:** As mentioned, BFS on very wide sites can lead to high memory usage.
        *   **Solution:** Limit `max_depth`, or consider DFS/Best-First for parts of the crawl.
    *   **Logging Overhead:** Extremely verbose logging to the console or file can add overhead. Reduce verbosity for production runs once debugging is complete.

*   **10.4. Using Logs for Diagnosis**
    *   **Enable Verbose Logging:** Set `verbose=True` in `CrawlerRunConfig` and/or `BrowserConfig`.
    *   **Look for Key Log Messages:**
        *   Messages indicating a URL is being added to the queue/stack.
        *   Filter actions: "URL [url] rejected by [FilterName]" or "URL [url] passed [FilterName]".
        *   Scorer outputs: "URL [url] scored [score] by [ScorerName]" (if scorers log verbosely).
        *   Link discovery: "Discovered [N] links on [page_url]".
        *   Errors: Any exceptions or error messages during fetching, processing, or strategy execution.
    *   **Custom Logging:** Add `self.logger.debug(...)` or `self.logger.info(...)` statements within your custom filters or scorers to trace their specific behavior.

## 11. Advanced Deep Crawling: Customization and Integration

When built-in components aren't enough, Crawl4ai's modular design allows you to create custom strategies, filters, and scorers.

*   **11.1. Why and When to Create Custom Components**
    *   **Unique Filtering Logic:** You might need to filter URLs based on criteria not covered by existing filters, e.g., checking against a dynamic blocklist from an API, or complex domain-specific path rules.
    *   **Domain-Specific Scoring Heuristics:** Your definition of a "valuable" URL might involve proprietary business logic, data from your own databases, or very specific content cues that standard scorers don't address.
    *   **Novel Traversal Strategies:** While BFS, DFS, and Best-First cover many cases, you might envision a hybrid approach or a strategy tailored to a very unusual site structure.
    *   **Integration with External Systems:** Custom components can interact with external APIs or databases during the filtering or scoring process.

*   **11.2. Implementing a Custom `DeepCrawlStrategy`**
    *   **Key methods to override (from `DeepCrawlStrategy` base class):**
        *   `async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]:` Implement the core non-streaming traversal logic here. You'll manage the frontier (queue/stack/priority queue), call `crawler.arun_many()` for batches of URLs, and use `self.link_discovery()` and `self.can_process_url()`.
        *   `async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]:` Implement the streaming version of the traversal logic. Yield `CrawlResult` objects as they become available.
        *   `async def can_process_url(self, url: str, depth: int) -> bool:` (Often relies on `self.filter_chain`) This method determines if a given URL should be processed based on depth and filters.
        *   `async def link_discovery(self, result: CrawlResult, source_url: str, current_depth: int, visited: Set[str], next_level: List[Tuple], depths: Dict[str, int]) -> None:` (Helper method) Extracts links from a `CrawlResult`, applies `can_process_url`, and adds valid URLs to `next_level` and updates `depths`.
    *   **Managing state:** Your custom strategy will need to manage:
        *   A **frontier** of URLs to visit (e.g., `asyncio.Queue` for BFS, `list` for DFS stack, `asyncio.PriorityQueue` for Best-First).
        *   A **visited set** to avoid re-processing URLs.
        *   **URL depths** to respect `max_depth`.
        *   Counters for `max_pages`.
    *   `* Code Example: [Skeleton for a custom strategy that crawls based on page title length (longer titles first - a simplistic example)]`
        ```python
        from crawl4ai import DeepCrawlStrategy, CrawlResult, AsyncWebCrawler, CrawlerRunConfig
        import asyncio
        from typing import List, AsyncGenerator, Set, Dict, Tuple

        class TitleLengthPriorityStrategy(DeepCrawlStrategy):
            def __init__(self, max_depth: int = 3, max_pages: int = 100, **kwargs):
                super().__init__(max_depth=max_depth, max_pages=max_pages, **kwargs)
                self.priority_queue = asyncio.PriorityQueue() # (score, url, depth, parent_url)
                # Note: Lower numbers = higher priority for asyncio.PriorityQueue

            async def _process_page_and_discover_links(
                self, url: str, depth: int, parent_url: str,
                crawler: AsyncWebCrawler, config: CrawlerRunConfig,
                visited: Set[str]
            ) -> AsyncGenerator[CrawlResult, None]:
                if url in visited or depth > self.max_depth or self._pages_crawled >= self.max_pages:
                    return

                visited.add(url)
                # Create a config for fetching a single page
                page_config = config.clone(deep_crawl_strategy=None) # Ensure no recursive deep crawl
                
                # arun returns a CrawlResultContainer, access the first (and only) result
                crawl_result_container = await crawler.arun(url=url, config=page_config)
                page_result = crawl_result_container.results[0] if crawl_result_container.results else None

                if page_result and page_result.success:
                    self._pages_crawled += 1
                    page_result.metadata = page_result.metadata or {}
                    page_result.metadata["depth"] = depth
                    page_result.metadata["parent_url"] = parent_url
                    yield page_result

                    # Discover links
                    new_links_to_score = []
                    # Simplified link discovery for example
                    if page_result.links:
                        for link_type in ["internal", "external"]: # Or just internal
                            for link_info in page_result.links.get(link_type, []):
                                next_url = link_info.get("href")
                                if next_url and await self.can_process_url(next_url, depth + 1):
                                     if next_url not in visited: # Check visited again before adding to score
                                        new_links_to_score.append(next_url)
                    
                    for new_url in set(new_links_to_score): # Process unique new links
                        # Fetch title (simplified, real scorer would be more robust)
                        # This is inefficient here, a real scorer would be separate
                        temp_page_config = config.clone(deep_crawl_strategy=None, only_text=True, word_count_threshold=0) # very light fetch
                        try:
                            temp_result_container = await crawler.arun(url=new_url, config=temp_page_config)
                            temp_result = temp_result_container.results[0] if temp_result_container.results else None
                            title_len = len(temp_result.metadata.get("title", "")) if temp_result and temp_result.metadata else 0
                            score = -title_len # Negative because lower number = higher priority
                            if new_url not in visited: # Final check before putting in queue
                                await self.priority_queue.put((score, new_url, depth + 1, url))
                        except Exception as e:
                            if self.logger: self.logger.warning(f"Scoring error for {new_url}: {e}")


            async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]:
                self.reset_stats() # Important!
                visited: Set[str] = set()
                await self.priority_queue.put((0, start_url, 0, None)) # Initial score 0 for seed

                while not self.priority_queue.empty() and self._pages_crawled < self.max_pages:
                    _score, current_url, current_depth, parent_url = await self.priority_queue.get()
                    
                    if current_url in visited:
                        continue

                    async for page_result in self._process_page_and_discover_links(
                        current_url, current_depth, parent_url, crawler, config, visited
                    ):
                        yield page_result
                self.log_stats()
            
            # _arun_batch would collect all yields from _arun_stream
            async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]:
                results = []
                async for result in self._arun_stream(start_url, crawler, config):
                    results.append(result)
                return results

        # Usage:
        # title_strategy = TitleLengthPriorityStrategy(max_depth=3)
        # run_config_custom = CrawlerRunConfig(deep_crawl_strategy=title_strategy)
        ```

*   **11.3. Developing a Custom `URLFilter`**
    *   **Inheriting from `URLFilter`:** Your custom filter class must inherit from `crawl4ai.deep_crawling.filters.URLFilter`.
    *   **Implementing `apply(self, url: str) -> bool`:** This is the core method. It takes a URL string and must return `True` if the URL should pass the filter, or `False` if it should be rejected.
        *   You can also make `apply` an `async def` if it needs to perform asynchronous operations (e.g., an API call to check a dynamic blocklist).
    *   **Leveraging `FilterStats`:**
        *   Call `self._update_stats(passed=True/False)` at the end of your `apply` method to correctly update the filter's statistics (`total_urls`, `passed_urls`, `rejected_urls`).
    *   `* Code Example: [A filter that blocks URLs with more than 5 path segments or URLs containing a specific query parameter 'sessionid']`
        ```python
        from crawl4ai import URLFilter, FilterStats
        from urllib.parse import urlparse, parse_qs

        class PathAndQueryParamFilter(URLFilter):
            def __init__(self, max_segments=5, forbidden_param="sessionid", **kwargs):
                super().__init__(**kwargs) # Pass name if desired
                self.max_segments = max_segments
                self.forbidden_param = forbidden_param

            # This can be async if needed: async def apply(self, url: str) -> bool:
            def apply(self, url: str) -> bool:
                parsed_url = urlparse(url)
                path_segments = parsed_url.path.strip('/').split('/')
                
                # Check path depth
                if len(path_segments) > self.max_segments:
                    self._update_stats(passed=False)
                    return False

                # Check for forbidden query parameter
                query_params = parse_qs(parsed_url.query)
                if self.forbidden_param in query_params:
                    self._update_stats(passed=False)
                    return False
                
                self._update_stats(passed=True)
                return True

        # Usage:
        # custom_filter = PathAndQueryParamFilter(max_segments=4, forbidden_param="tracking_id")
        # filter_chain = FilterChain(filters=[custom_filter])
        ```

*   **11.4. Designing a Custom `URLScorer`**
    *   **Inheriting from `URLScorer`:** Your custom scorer class must inherit from `crawl4ai.deep_crawling.scorers.URLScorer`.
    *   **Implementing `_calculate_score(self, url: str) -> float`:** This method takes a URL string and must return a float representing its "raw" score (before the scorer's `weight` is applied).
        *   This method can also be `async def` if it needs to perform asynchronous operations (e.g., fetching external data to inform the score).
    *   **Score Range:** It's good practice to design your `_calculate_score` to return values in a somewhat consistent range (e.g., 0.0 to 1.0) to make it easier to combine with other scorers in a `CompositeScorer`. The final score returned by the public `score()` method will be `_calculate_score(url) * self.weight`.
    *   `* Code Example: [A scorer that assigns higher scores to URLs ending with '.html' or '.php' and containing the word 'article']`
        ```python
        from crawl4ai import URLScorer

        class ArticlePageScorer(URLScorer):
            def __init__(self, weight: float = 1.0, **kwargs):
                super().__init__(weight=weight, **kwargs) # Pass name if desired

            # Can be async: async def _calculate_score(self, url: str) -> float:
            def _calculate_score(self, url: str) -> float:
                score = 0.0
                if url.lower().endswith(('.html', '.htm', '.php')):
                    score += 0.5
                if 'article' in url.lower():
                    score += 0.5
                return min(score, 1.0) # Cap score at 1.0

        # Usage:
        # article_scorer = ArticlePageScorer(weight=0.8)
        # composite_scorer = CompositeScorer(scorers=[article_scorer, ...])
        ```

*   **11.5. Integrating Deep Crawling with Other Crawl4ai Features**
    *   **Combining with Extraction Strategies:**
        *   **How it works:** The `DeepCrawlStrategy` (BFS, DFS, Best-First) is responsible for discovering and fetching pages. Each `CrawlResult` object it yields (in stream mode) or returns (in batch mode) contains the HTML content. This `CrawlResult` is then passed to the extraction pipeline defined in your `CrawlerRunConfig` (e.g., `LLMExtractionStrategy`, `JsonCssExtractionStrategy`).
        *   **Data Flow:** `DeepCrawlStrategy` produces `CrawlResult` -> `AsyncWebCrawler`'s main loop -> `ExtractionStrategy` (if defined) consumes `CrawlResult.html` or `CrawlResult.markdown` -> Populates `CrawlResult.extracted_content`.
        *   **Ensuring Compatibility:** Built-in deep crawl strategies yield standard `CrawlResult` objects, which are directly usable by extraction strategies. If you build a very custom strategy, ensure it also yields or returns `CrawlResult` instances.
    *   **Authenticated Deep Crawls:**
        *   **Challenge:** Many websites require login to access deeper content. A deep crawl needs to maintain this session.
        *   **Solution:**
            1.  **Initial Login:** Perform a separate, initial `crawler.arun()` call to the login page. Use `js_code` to fill in login forms and submit.
            2.  **Save Session State:** After successful login, save the browser's `storage_state` (cookies, localStorage):
                ```python
                # In the login part of your script
                # login_page_result = await crawler.arun(...)
                # await page.context.storage_state(path="my_session_state.json")
                ```
                (Assuming `page` is accessible, or via a hook that gets the context). A more robust way is to use a dedicated `BrowserConfig` with `user_data_dir` for persistence across crawler instances, or use the `session_id` feature if you keep the same crawler instance.
            3.  **Deep Crawl with Session:** For the subsequent deep crawl, configure `BrowserConfig` to use this saved state or `CrawlerRunConfig` to reuse a session via `session_id`:
                ```python
                # Option A: Persistent context via user_data_dir (for multiple crawler instances)
                # browser_config_authed = BrowserConfig(user_data_dir="path/to/my_profile_with_login")
                # async with AsyncWebCrawler(config=browser_config_authed) as authed_crawler:
                #    await authed_crawler.arun(..., config=deep_crawl_run_config)

                # Option B: Reusing a session within the same crawler instance
                # deep_crawl_run_config.session_id = "my_authed_session"
                # (after initial login that established this session)
                # await crawler.arun(..., config=deep_crawl_run_config)
                ```
        *   **Considerations:**
            *   **Token Refresh/Session Expiry:** Long crawls might encounter session expiry. More advanced solutions might need hooks or custom logic to detect expired sessions and re-authenticate.
            *   **AJAX/SPA Logins:** Ensure login interactions are fully completed (e.g., using `wait_for` for redirection or dashboard elements) before saving state or proceeding.
        *   `* Scenario Walkthrough: [Conceptual steps for deep crawling a members-only forum]`
            1.  Create `CrawlerRunConfig` for login: `js_code` to fill login form, `wait_for` a dashboard element.
            2.  `crawler.arun()` to login page with this config. Save `session_id` (e.g., "forum_session").
            3.  Create `CrawlerRunConfig` for deep crawl:
                *   `deep_crawl_strategy` (e.g., BFS to find all threads).
                *   `session_id="forum_session"` to reuse the logged-in state.
                *   Filters to stay within forum sections.
            4.  `crawler.arun()` with the forum's starting URL and deep crawl config.

## 12. Conclusion and Further Exploration

Crawl4ai's `deep_crawling` component offers a powerful and flexible toolkit for exploring websites beyond a single page. By understanding and combining strategies, filters, and scorers, you can tailor your crawls to a wide variety of tasks, from comprehensive site indexing to highly targeted data extraction.

*   **Recap:**
    *   Choose the right **strategy** (BFS, DFS, Best-First) based on your exploration goals.
    *   Use **filters** (`DomainFilter`, `ContentTypeFilter`, `URLPatternFilter`, etc.) to precisely define the scope of your crawl and improve efficiency.
    *   Leverage **scorers** (especially with `BestFirstCrawlingStrategy`) to prioritize URLs and focus on the most relevant content.
    *   Configure everything through `CrawlerRunConfig` and its `deep_crawl_strategy` parameter.
    *   Monitor your crawls using `TraversalStats` and logs to optimize performance.
*   **Encouragement:** The best way to master deep crawling is to experiment!
    *   Start with simple configurations and gradually add complexity.
    *   Test different filter combinations and scorer weightings.
    *   Observe how your changes affect the crawl path and results.
*   **Pointers to Other Relevant Documentation:**
    *   **Basic Crawling:** [Simple Crawling Guide](../core/simple-crawling.md)
    *   **Configuration:** [Browser, Crawler & LLM Configuration](../core/browser-crawler-config.md)
    *   **Specific Filters/Scorers API:** (Refer to API documentation if available, or source code comments)
    *   **Extraction Strategies:** [No-LLM Extraction Strategies](../extraction/no-llm-strategies.md), [LLM-based Extraction](../extraction/llm-extraction.md)
    *   **Session Management & Authentication:** [Session Management](./session-management.md), [Hooks & Auth](./hooks-auth.md)
    *   **Advanced Page Interaction:** [Page Interaction](./page-interaction.md)

Happy deep crawling with Crawl4ai!
```

---


## Deep Crawling - Examples
Source: crawl4ai_deep_crawling_examples_content.llm.md

```markdown
# Examples Outline for crawl4ai - deep_crawling Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_deep_crawling.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

This document provides a collection of runnable Python code examples for the `deep_crawling` component of the `crawl4ai` library. Each example aims to demonstrate a specific feature or usage pattern.

---
## 1. Deep Crawling Strategies

This section will cover the different traversal and processing strategies available for deep crawling.

### 1.1. `DeepCrawlDecorator`

#### 1.1.1. Example: Basic application of `DeepCrawlDecorator` to an `AsyncWebCrawler` instance.

This example shows how to apply the `DeepCrawlDecorator` to an `AsyncWebCrawler` instance, which augments its `arun` method with deep crawling capabilities.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DeepCrawlDecorator, BFSDeeepCrawlStrategy

# Basic setup - crawl4ai_server needs to be running for live examples
# For simplicity, we'll often use example.com or raw HTML for self-contained tests.

async def decorator_basic_application():
    crawler = AsyncWebCrawler()
    # The decorator enhances the crawler instance
    deep_crawl_decorator = DeepCrawlDecorator(crawler)
    
    # The original arun method is still available if needed
    # For deep crawling, the decorated arun will be used implicitly 
    # when a deep_crawl_strategy is set in CrawlerRunConfig.
    
    print(f"Crawler 'arun' method before decoration: {crawler.arun}")
    
    # Apply the decorator
    # This typically happens inside the AsyncWebCrawler when a strategy is provided
    # but for demonstration, we can show it being applied manually.
    # In practice, you don't call DeepCrawlDecorator directly like this for arun.
    # The AsyncWebCrawler's __init__ or arun method would handle this.
    # This example is more conceptual to show the decorator's existence.

    # A more realistic scenario is providing the strategy to CrawlerRunConfig:
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=0)
    config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    # When arun is called with a config that has a deep_crawl_strategy,
    # the decorator's logic (if active) would take over.
    # Let's simulate a simple crawl
    try:
        async with crawler: # Ensure crawler is started and closed
            result = await crawler.arun(url="http://example.com", config=config)
            if result.success:
                print(f"Successfully crawled {result.url} using decorator (implicitly).")
                print(f"Decorator active status: {deep_crawl_decorator.deep_crawl_active.get()}")
            else:
                print(f"Crawl failed: {result.error_message}")
    except Exception as e:
        print(f"An error occurred: {e}")


asyncio.run(decorator_basic_application())
```

#### 1.1.2. Example: Triggering a deep crawl via the decorated `arun` method using `BFSDeeepCrawlStrategy`.

This example demonstrates how providing a `deep_crawl_strategy` in `CrawlerRunConfig` automatically triggers the deep crawling logic managed by `DeepCrawlDecorator`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def trigger_deep_crawl_with_decorator():
    # Define a BFS strategy
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=3) 
    
    # Configure the crawler run to use this strategy
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=bfs_strategy,
        # For real crawls, ensure verbosity or specific logging for clarity
        # For this example, we'll print basic info from results
    )

    async with AsyncWebCrawler() as crawler:
        # The DeepCrawlDecorator is implicitly active due to deep_crawl_strategy in config
        print(f"Starting deep crawl with BFS strategy for http://example.com (max_depth=1, max_pages=3)")
        results_container = await crawler.arun(url="http://example.com", config=run_config)
        
        crawled_count = 0
        if results_container: # arun returns a list/generator container
            print("\n--- Crawl Results ---")
            async for result in results_container: # If stream=True
                if result.success:
                    crawled_count += 1
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth', 'N/A')})")
                else:
                    print(f"Failed: {result.url} - {result.error_message}")
            if not isinstance(results_container, types.AsyncGeneratorType): # if stream=False (batch mode)
                 for result in results_container:
                    if result.success:
                        crawled_count +=1
                        print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth', 'N/A')})")
                    else:
                        print(f"Failed: {result.url} - {result.error_message}")


        print(f"\nTotal pages processed by deep crawl: {crawled_count}")

# Note: example.com might not have many links or varied depth.
# For a more illustrative output, a mock server or a known simple site would be better.
import types
asyncio.run(trigger_deep_crawl_with_decorator())
```

#### 1.1.3. Example: Showing `DeepCrawlDecorator` respects the `deep_crawl_active` context to prevent recursion.

This example conceptually illustrates how `DeepCrawlDecorator` uses a `ContextVar` (`deep_crawl_active`) to prevent recursive deep crawls if the decorated method were to call itself indirectly with another deep crawl strategy.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DeepCrawlDecorator, BFSDeeepCrawlStrategy

async def decorator_recursion_prevention():
    # This is a conceptual example. In a real scenario, the AsyncWebCrawler
    # manages the decorator and its context.
    
    crawler_instance = AsyncWebCrawler()
    decorator = DeepCrawlDecorator(crawler_instance)

    # Simulate an initial call that sets the context
    print(f"Initial deep_crawl_active: {decorator.deep_crawl_active.get()}")
    token = decorator.deep_crawl_active.set(True)
    print(f"After setting, deep_crawl_active: {decorator.deep_crawl_active.get()}")

    # Now, imagine original_arun (if called within the strategy) tries to start another deep crawl
    # The decorator's __call__ would check deep_crawl_active.get()
    # If True, it would call the original_arun directly, not the strategy.

    # Simulate what would happen if a nested deep crawl was attempted:
    # A config that would normally trigger a deep crawl
    nested_strategy = BFSDeeepCrawlStrategy(max_depth=0)
    nested_config = CrawlerRunConfig(deep_crawl_strategy=nested_strategy)

    # If decorator.__call__(original_arun) is invoked while deep_crawl_active is True:
    if nested_config.deep_crawl_strategy and not decorator.deep_crawl_active.get():
        print("This part (nested deep crawl strategy execution) should NOT be reached if active.")
        # strategy_result = await nested_strategy.arun(...) 
    elif nested_config.deep_crawl_strategy and decorator.deep_crawl_active.get():
        print("Deep crawl already active. Nested strategy will be bypassed, original_arun would be called.")
        # original_arun_result = await original_arun(...)
    else:
        print("Not a deep crawl config or context not active.")

    decorator.deep_crawl_active.reset(token)
    print(f"After reset, deep_crawl_active: {decorator.deep_crawl_active.get()}")
    
    # In a real run with AsyncWebCrawler, this management is internal.
    # This example is to show the ContextVar's role.

asyncio.run(decorator_recursion_prevention())
```

### 1.2. `DeepCrawlStrategy` (Abstract Base Class - Conceptual)

#### 1.2.1. Note: This is an ABC.
Examples will use concrete implementations like `BFSDeeepCrawlStrategy`, `DFSDeeepCrawlStrategy`, and `BestFirstCrawlingStrategy`. This section provides conceptual examples of how the base strategy's `arun` method works.

#### 1.2.2. Example: Conceptual demonstration of the `arun` method dispatching to `_arun_batch` (non-streaming).

This shows that if `config.stream` is `False` (or not set, as `False` is default for strategies not explicitly setting it), the `arun` method of a `DeepCrawlStrategy` (like `BFSDeeepCrawlStrategy`) will internally call its `_arun_batch` method.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy # Using BFS as a concrete example

async def conceptual_arun_batch_dispatch():
    strategy = BFSDeeepCrawlStrategy(max_depth=0) # max_depth=0 for minimal crawl
    config = CrawlerRunConfig(stream=False) # Explicitly non-streaming

    # Mock the _arun_batch method to confirm it's called
    original_arun_batch = strategy._arun_batch
    called_arun_batch = False
    async def mock_arun_batch(*args, **kwargs):
        nonlocal called_arun_batch
        called_arun_batch = True
        # Simulate returning a list of results
        return await original_arun_batch(*args, **kwargs) 
    strategy._arun_batch = mock_arun_batch

    async with AsyncWebCrawler() as crawler:
        await strategy.arun(start_url="http://example.com", crawler=crawler, config=config)

    if called_arun_batch:
        print("Conceptual: strategy.arun() correctly dispatched to _arun_batch() for non-streaming mode.")
    else:
        print("Conceptual: strategy.arun() DID NOT dispatch to _arun_batch().")
    
    # Restore original method
    strategy._arun_batch = original_arun_batch

asyncio.run(conceptual_arun_batch_dispatch())
```

#### 1.2.3. Example: Conceptual demonstration of the `arun` method dispatching to `_arun_stream` (streaming).

This shows that if `config.stream` is `True`, the `arun` method of a `DeepCrawlStrategy` (like `BFSDeeepCrawlStrategy`) will internally call its `_arun_stream` method.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy # Using BFS as a concrete example

async def conceptual_arun_stream_dispatch():
    strategy = BFSDeeepCrawlStrategy(max_depth=0)
    config = CrawlerRunConfig(stream=True) # Explicitly streaming

    # Mock the _arun_stream method
    original_arun_stream = strategy._arun_stream
    called_arun_stream = False
    async def mock_arun_stream(*args, **kwargs):
        nonlocal called_arun_stream
        called_arun_stream = True
        # Simulate yielding results from an async generator
        async for item in original_arun_stream(*args, **kwargs):
            yield item
    strategy._arun_stream = mock_arun_stream
    
    async with AsyncWebCrawler() as crawler:
        async for _ in strategy.arun(start_url="http://example.com", crawler=crawler, config=config):
            pass # Consume the generator

    if called_arun_stream:
        print("Conceptual: strategy.arun() correctly dispatched to _arun_stream() for streaming mode.")
    else:
        print("Conceptual: strategy.arun() DID NOT dispatch to _arun_stream().")

    # Restore original method
    strategy._arun_stream = original_arun_stream

asyncio.run(conceptual_arun_stream_dispatch())
```

#### 1.2.4. Example: Demonstrating `ValueError` when `CrawlerRunConfig` is not provided to `arun`.

This example demonstrates that calling `strategy.arun(...)` without a `config` (or with `config=None`) raises a `ValueError`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def arun_without_config():
    strategy = BFSDeeepCrawlStrategy(max_depth=0)
    async with AsyncWebCrawler() as crawler:
        try:
            await strategy.arun(start_url="http://example.com", crawler=crawler, config=None)
        except ValueError as e:
            print(f"Caught expected ValueError: {e}")
        except Exception as e:
            print(f"Caught unexpected error: {e}")
        else:
            print("ValueError was not raised as expected.")

asyncio.run(arun_without_config())
```

#### 1.2.5. Example: Demonstrating the `__call__` method making the strategy instance callable.

The `__call__` method allows a strategy instance to be called directly like a function, which internally calls its `arun` method.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def strategy_is_callable():
    strategy = BFSDeeepCrawlStrategy(max_depth=0, max_pages=1)
    config = CrawlerRunConfig()

    async with AsyncWebCrawler() as crawler:
        print("Calling strategy instance directly using __call__...")
        # This is equivalent to strategy.arun(...)
        results_container = await strategy(start_url="http://example.com", crawler=crawler, config=config)
        
        if results_container:
            for result in results_container: # If batch mode
                 if result.success:
                    print(f"Callable strategy crawled: {result.url}")
                 else:
                    print(f"Callable strategy failed for {result.url}: {result.error_message}")
        else:
            print("Strategy call did not return results.")

asyncio.run(strategy_is_callable())
```

### 1.3. `BFSDeeepCrawlStrategy` (Breadth-First Search)

#### 1.3.1. **Initialization & Basic Usage**

##### 1.3.1.1. Example: Initializing `BFSDeeepCrawlStrategy` with a `max_depth` of 1 and performing a basic batch crawl.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def bfs_basic_batch_crawl():
    # Initialize BFS strategy: crawl up to 1 level deep from the start URL
    # and fetch a maximum of 5 pages in total.
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=5)
    
    # Create a run configuration using this strategy
    # stream=False is the default for batch mode
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS batch crawl (max_depth=1, max_pages=5) on http://example.com...")
        # arun returns a list of CrawlResult objects in batch mode
        results_list = await crawler.arun(url="http://example.com", config=run_config)
        
        print(f"\n--- BFS Batch Crawl Results (max_depth=1, max_pages=5) ---")
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')})")
                else:
                    print(f"Failed: {result.url} - {result.error_message}")
            print(f"Total pages processed: {len(results_list)}")
        else:
            print("No results returned from crawl.")

# Note: example.com has very few links, so it might not reach max_pages=5 or depth=1.
# A site with more links like docs.crawl4ai.com would be more illustrative if accessible.
asyncio.run(bfs_basic_batch_crawl())
```

##### 1.3.1.2. Example: Performing a BFS crawl in stream mode (`config.stream=True`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def bfs_stream_crawl():
    # Initialize BFS strategy
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=3)
    
    # Enable stream mode in the run configuration
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy, stream=True)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS stream crawl (max_depth=1, max_pages=3) on http://example.com...")
        # arun returns an async generator in stream mode
        results_generator = await crawler.arun(url="http://example.com", config=run_config)
        
        print(f"\n--- BFS Stream Crawl Results ---")
        processed_count = 0
        async for result in results_generator:
            processed_count +=1
            if result.success:
                print(f"Streamed: {result.url} (Depth: {result.metadata.get('depth')})")
            else:
                print(f"Stream Failed: {result.url} - {result.error_message}")
        print(f"Total pages streamed: {processed_count}")

asyncio.run(bfs_stream_crawl())
```

#### 1.3.2. **Controlling Crawl Depth and Scope**

##### 1.3.2.1. Example: Demonstrating `max_depth` limiting the crawl (e.g., `max_depth=0` for only the start URL).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def bfs_max_depth_zero():
    # max_depth=0 means only the starting URL will be crawled.
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=0, max_pages=5) # max_pages won't be hit
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS crawl with max_depth=0 on http://example.com...")
        results_list = await crawler.arun(url="http://example.com", config=run_config)
        
        print(f"\n--- BFS Crawl Results (max_depth=0) ---")
        if results_list:
            for result in results_list:
                print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')})")
            print(f"Total pages processed: {len(results_list)}")
            assert len(results_list) == 1, "Should only crawl the start URL with max_depth=0"
            assert results_list[0].metadata.get('depth') == 0, "Depth should be 0"
        else:
            print("No results returned.")

asyncio.run(bfs_max_depth_zero())
```

##### 1.3.2.2. Example: Demonstrating `max_pages` limiting the number of crawled pages.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

# For this example, we'll use a site that likely has more than 2 links accessible
# from the start page to demonstrate max_pages.
# If docs.crawl4ai.com is not available, replace with another suitable site.
# Or, use raw HTML with multiple links.
TARGET_URL = "https://docs.crawl4ai.com/core/async-web-crawler/" 

async def bfs_max_pages_limit():
    # Crawl up to depth 2, but stop after 2 pages.
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=2, max_pages=2)
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print(f"Starting BFS crawl with max_pages=2 on {TARGET_URL}...")
        results_list = await crawler.arun(url=TARGET_URL, config=run_config)
        
        print(f"\n--- BFS Crawl Results (max_pages=2) ---")
        crawled_urls = []
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')})")
                    crawled_urls.append(result.url)
            print(f"Total pages processed: {len(results_list)}")
            # The number of results might be slightly more than max_pages
            # due to how BFS processes levels, but pages_crawled stat should be accurate.
            # print(f"Strategy stats: Pages crawled = {bfs_strategy.stats._pages_crawled}")
            # For simplicity, we check len(results_list) but acknowledge bfs_strategy.stats is more precise.
            assert len(crawled_urls) <= 2, "Should process at most max_pages"
        else:
            print("No results returned.")

asyncio.run(bfs_max_pages_limit())
```

##### 1.3.2.3. Example: BFS crawl including external links (`include_external=True`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

# We need a page with known external links. example.com might have iana.org.
# For a more robust test, a mock HTML would be better.
RAW_HTML_WITH_EXTERNAL_LINK = """
<html><body>
    <a href="http://example.com/internal">Internal Link</a>
    <a href="https://www.iana.org/domains/reserved">External IANA Link</a>
    <a href="http://another-external.com">Another External</a>
</body></html>
"""
START_URL_RAW = f"raw://{RAW_HTML_WITH_EXTERNAL_LINK}"


async def bfs_include_external():
    # Crawl depth 1, include external links, limit to 3 pages to see some externals.
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=3, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print(f"Starting BFS crawl including external links (max_depth=1, max_pages=3)...")
        results_list = await crawler.arun(url=START_URL_RAW, config=run_config)
        
        print(f"\n--- BFS Crawl Results (include_external=True) ---")
        external_found = False
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')})")
                    if "iana.org" in result.url or "another-external.com" in result.url:
                        external_found = True
                else:
                    print(f"Failed: {result.url} - {result.error_message}")
            print(f"Total pages processed: {len(results_list)}")
            assert external_found, "Expected to crawl at least one external link."
        else:
            print("No results returned.")

asyncio.run(bfs_include_external())
```

#### 1.3.3. **Filtering and Scoring**

##### 1.3.3.1. Example: Using `BFSDeeepCrawlStrategy` with a `FilterChain` to include/exclude specific URLs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy, FilterChain, URLPatternFilter, DomainFilter

RAW_HTML_FOR_FILTERING = """
<html><body>
    <a href="http://example.com/page1.html">Page 1 (HTML)</a>
    <a href="http://example.com/image.png">Image (PNG)</a>
    <a href="http://example.com/docs/doc1.pdf">Doc 1 (PDF)</a>
    <a href="http://external.com/anotherpage">External Page</a>
    <a href="http://example.com/blog/post1">Blog Post 1</a>
</body></html>
"""
START_URL_RAW_FILTER = f"raw://{RAW_HTML_FOR_FILTERING}"

async def bfs_with_filter_chain():
    # Allow only HTML files from example.com
    filter_chain = FilterChain(filters=[
        DomainFilter(allowed_domains=["example.com"]),
        URLPatternFilter(patterns=["*.html"]) 
    ])
    
    bfs_strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=5, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS crawl with FilterChain (allow *.html from example.com)...")
        results_list = await crawler.arun(url=START_URL_RAW_FILTER, config=run_config)
        
        print(f"\n--- BFS Crawl Results (with FilterChain) ---")
        crawled_urls = []
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url}")
                    crawled_urls.append(result.url)
            print(f"Total pages processed: {len(results_list)}")
            
            assert all("example.com" in url for url in crawled_urls if url != START_URL_RAW_FILTER), "Only example.com URLs should be crawled (excluding start)."
            assert all(url.endswith(".html") for url in crawled_urls if url != START_URL_RAW_FILTER), "Only .html URLs should be crawled (excluding start)."
            assert "http://example.com/page1.html" in crawled_urls
            assert "http://example.com/image.png" not in crawled_urls
            assert "http://example.com/docs/doc1.pdf" not in crawled_urls
            assert "http://external.com/anotherpage" not in crawled_urls
            assert "http://example.com/blog/post1" not in crawled_urls

        else:
            print("No results returned.")

asyncio.run(bfs_with_filter_chain())
```

##### 1.3.3.2. Example: Using `BFSDeeepCrawlStrategy` with a `url_scorer` and `score_threshold` to prune links.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy, PathDepthScorer

RAW_HTML_FOR_SCORING = """
<html><body>
    <a href="/page1">Page 1 (depth 1)</a>
    <a href="/category/page2">Page 2 (depth 2)</a>
    <a href="/category/sub/page3">Page 3 (depth 3)</a>
    <a href="/very/deep/path/page4">Page 4 (depth 4)</a>
</body></html>
"""
START_URL_RAW_SCORE = f"raw://{RAW_HTML_FOR_SCORING}"


async def bfs_with_scorer_and_threshold():
    # Score by path depth, optimal depth is 1.
    # Threshold will prune links that are too deep (lower score).
    # PathDepthScorer gives 1.0 for optimal, 0.5 for diff 1, 0.33 for diff 2, etc.
    url_scorer = PathDepthScorer(optimal_depth=1) 
    score_threshold = 0.4  # This should allow depth 1 (score 1.0) and depth 2 (score 0.5) links, but not depth 3 (score 0.33)

    bfs_strategy = BFSDeeepCrawlStrategy(
        max_depth=3, 
        max_pages=5, 
        url_scorer=url_scorer, 
        score_threshold=score_threshold
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS crawl with URL scorer and threshold (optimal_depth=1, threshold=0.4)...")
        results_list = await crawler.arun(url=START_URL_RAW_SCORE, config=run_config)
        
        print(f"\n--- BFS Crawl Results (with Scorer & Threshold) ---")
        crawled_urls = []
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 'N/A')})")
                    crawled_urls.append(result.url)
            print(f"Total pages processed: {len(results_list)}")
            
            assert any("/page1" in url for url in crawled_urls)
            assert any("/category/page2" in url for url in crawled_urls)
            assert not any("/category/sub/page3" in url for url in crawled_urls)
            assert not any("/very/deep/path/page4" in url for url in crawled_urls)
        else:
            print("No results returned.")

asyncio.run(bfs_with_scorer_and_threshold())
```

#### 1.3.4. **URL Processing Logic**

##### 1.3.4.1. Example: Demonstrating `can_process_url` for valid and invalid URL formats (e.g., missing scheme, unsupported scheme).

The `can_process_url` method is primarily used internally by strategies. We can test its behavior directly for demonstration.

```python
import asyncio
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def demo_can_process_url_formats():
    strategy = BFSDeeepCrawlStrategy(max_depth=1) # Filters are not applied at depth 0 by default

    print("Testing can_process_url (at depth=1, default filters):")
    
    valid_http_url = "http://example.com/page"
    can_process_http = await strategy.can_process_url(valid_http_url, depth=1)
    print(f"Can process '{valid_http_url}'? {can_process_http}")
    assert can_process_http

    valid_https_url = "https://example.com/secure"
    can_process_https = await strategy.can_process_url(valid_https_url, depth=1)
    print(f"Can process '{valid_https_url}'? {can_process_https}")
    assert can_process_https

    url_missing_scheme = "example.com/page" # This would typically be resolved by normalize_url_for_deep_crawl
                                          # but can_process_url expects a full URL.
                                          # Let's assume it's already normalized to http://example.com/page
    # For direct test of can_process_url, it would fail if scheme is missing before normalization
    # Assuming it's passed post-normalization phase where scheme is present for this check
    
    url_ftp_scheme = "ftp://example.com/file"
    can_process_ftp = await strategy.can_process_url(url_ftp_scheme, depth=1)
    print(f"Can process '{url_ftp_scheme}' (unsupported scheme)? {can_process_ftp}")
    assert not can_process_ftp # FTP is not a supported scheme by default

    url_no_netloc = "http:///page" # Invalid netloc
    can_process_no_netloc = await strategy.can_process_url(url_no_netloc, depth=1)
    print(f"Can process '{url_no_netloc}' (no netloc)? {can_process_no_netloc}")
    assert not can_process_no_netloc
    
    # Depth 0 always bypasses filters in default can_process_url logic of strategies
    can_process_depth_zero = await strategy.can_process_url(url_ftp_scheme, depth=0)
    print(f"Can process '{url_ftp_scheme}' at depth 0 (bypassing filters)? {can_process_depth_zero}")
    assert can_process_depth_zero

asyncio.run(demo_can_process_url_formats())
```

##### 1.3.4.2. Example: Demonstrating `can_process_url` with a `filter_chain` that rejects certain URLs.

```python
import asyncio
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy, FilterChain, URLPatternFilter

async def demo_can_process_url_with_filter():
    # Filter chain that rejects URLs containing "admin"
    filter_chain = FilterChain(filters=[
        URLPatternFilter(patterns=["*admin*"], reverse=True) # reverse=True means reject if matches
    ])
    strategy = BFSDeeepCrawlStrategy(max_depth=1, filter_chain=filter_chain)

    print("Testing can_process_url with a filter_chain (rejecting '*admin*'):")

    url_to_allow = "http://example.com/dashboard"
    can_process_allow = await strategy.can_process_url(url_to_allow, depth=1)
    print(f"Can process '{url_to_allow}'? {can_process_allow}")
    assert can_process_allow

    url_to_reject = "http://example.com/admin/login"
    can_process_reject = await strategy.can_process_url(url_to_reject, depth=1)
    print(f"Can process '{url_to_reject}'? {can_process_reject}")
    assert not can_process_reject

asyncio.run(demo_can_process_url_with_filter())
```

#### 1.3.5. **Link Discovery**

The `link_discovery` method is internal. Its effects are best observed through the URLs selected for the next level of a crawl.

##### 1.3.5.1. Example: Showing how `link_discovery` populates the `next_level` for BFS.

This conceptual example shows how `link_discovery` (if it were public and directly callable for this purpose) would take a `CrawlResult` and populate a `next_level` list.

```python
import asyncio
from crawl4ai.models import CrawlResult, Links, Link
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy
from crawl4ai.utils import normalize_url_for_deep_crawl

# Simulate a CrawlResult
mock_crawl_result = CrawlResult(
    url="http://example.com/source",
    html="<a href='/page1'>Page 1</a> <a href='page2.html'>Page 2</a>",
    success=True,
    links=Links(
        internal=[
            Link(href="/page1", text="Page 1"),
            Link(href="page2.html", text="Page 2")
        ],
        external=[]
    )
)

async def demo_link_discovery_populates_next_level():
    strategy = BFSDeeepCrawlStrategy(max_depth=1)
    
    # Simulate internal state for link_discovery
    source_url = mock_crawl_result.url
    current_depth = 0
    visited = {source_url}
    next_level_links = [] # This is what link_discovery populates
    depths = {source_url: 0}

    # Call link_discovery (conceptually)
    # In real code, this is an internal method: strategy.link_discovery(...)
    # We'll manually simulate its core logic for this demonstration.
    
    next_depth = current_depth + 1
    if next_depth <= strategy.max_depth:
        discovered_links_from_result = []
        for link_type in ["internal", "external"] if strategy.include_external else ["internal"]:
            for link_obj in getattr(mock_crawl_result.links, link_type, []):
                link_href = link_obj.href
                if link_href:
                    # Normalize URL (simplified for example)
                    abs_url = normalize_url_for_deep_crawl(link_href, source_url)
                    if abs_url and abs_url not in visited:
                        if await strategy.can_process_url(abs_url, next_depth):
                             if strategy.url_scorer:
                                score = await strategy.url_scorer.score(abs_url)
                                if score < strategy.score_threshold:
                                    strategy.stats.urls_skipped +=1
                                    continue
                             discovered_links_from_result.append((abs_url, source_url))
                             visited.add(abs_url)
                             depths[abs_url] = next_depth
        
        next_level_links.extend(discovered_links_from_result)


    print(f"Source URL: {source_url}")
    print(f"Discovered and added to next_level (url, parent_url):")
    for link_info in next_level_links:
        print(f"  - {link_info[0]} (from {link_info[1]}) at depth {depths[link_info[0]]}")

    assert ("http://example.com/page1", "http://example.com/source") in next_level_links
    assert ("http://example.com/page2.html", "http://example.com/source") in next_level_links
    assert depths.get("http://example.com/page1") == 1
    assert depths.get("http://example.com/page2.html") == 1

asyncio.run(demo_link_discovery_populates_next_level())
```

##### 1.3.5.2. Example: How `link_discovery` respects `max_pages` when adding new links.

This conceptual example shows how `link_discovery` would limit adding new links if `max_pages` is about to be reached.

```python
import asyncio
from crawl4ai.models import CrawlResult, Links, Link
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy
from crawl4ai.utils import normalize_url_for_deep_crawl


mock_crawl_result_many_links = CrawlResult(
    url="http://example.com/source_many",
    html="""
        <a href="/link1">1</a> <a href="/link2">2</a> <a href="/link3">3</a>
        <a href="/link4">4</a> <a href="/link5">5</a>
    """,
    success=True,
    links=Links(
        internal=[
            Link(href="/link1"), Link(href="/link2"), Link(href="/link3"),
            Link(href="/link4"), Link(href="/link5")
        ]
    )
)

async def demo_link_discovery_respects_max_pages():
    # Max 3 pages total, already crawled 1 (the start URL)
    # So, only 2 more pages can be added from discovered links.
    strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=3)
    strategy._pages_crawled = 1 # Simulate start URL already crawled

    source_url = mock_crawl_result_many_links.url
    current_depth = 0
    visited = {source_url}
    next_level_links_tuples = []
    depths = {source_url: 0}

    # Manual simulation of link_discovery's core logic for this specific scenario
    next_depth = current_depth + 1
    valid_links_to_consider = []
    
    if next_depth <= strategy.max_depth:
        for link_type in ["internal"]: # Assuming include_external is False
            for link_obj in getattr(mock_crawl_result_many_links.links, link_type, []):
                link_href = link_obj.href
                if link_href:
                    abs_url = normalize_url_for_deep_crawl(link_href, source_url)
                    if abs_url and abs_url not in visited:
                        if await strategy.can_process_url(abs_url, next_depth):
                            valid_links_to_consider.append(abs_url)
                            # visited.add(abs_url) # Add to visited only if selected
                            # depths[abs_url] = next_depth # Add depth only if selected
    
    remaining_capacity = strategy.max_pages - strategy._pages_crawled
    
    if remaining_capacity > 0 and valid_links_to_consider:
        # If scoring is involved, links would be sorted by score here. For simplicity, we take first N.
        selected_links = valid_links_to_consider[:remaining_capacity]
        if len(valid_links_to_consider) > remaining_capacity and strategy.logger:
            strategy.logger.info(f"Limiting to {remaining_capacity} URLs due to max_pages limit")
            
        for sel_url in selected_links:
            next_level_links_tuples.append((sel_url, source_url))
            visited.add(sel_url)
            depths[sel_url] = next_depth

    print(f"Source URL: {source_url}")
    print(f"Pages already crawled: {strategy._pages_crawled}")
    print(f"Max pages: {strategy.max_pages}, Remaining capacity: {remaining_capacity}")
    print(f"Discovered and added to next_level due to max_pages limit:")
    for link_url, parent_url in next_level_links_tuples:
        print(f"  - {link_url} (from {parent_url}) at depth {depths.get(link_url)}")
    
    assert len(next_level_links_tuples) <= remaining_capacity, f"Expected {remaining_capacity} links, got {len(next_level_links_tuples)}"
    assert len(next_level_links_tuples) == 2 # 3 (max_pages) - 1 (already_crawled) = 2

asyncio.run(demo_link_discovery_respects_max_pages())
```

#### 1.3.6. **Shutdown**

##### 1.3.6.1. Example: Demonstrating the `shutdown` method and its effect on crawl stats (e.g., `end_time`).

```python
import asyncio
import time
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

async def demo_shutdown_method():
    strategy = BFSDeeepCrawlStrategy(max_depth=1)
    print(f"Initial strategy stats: Start time: {strategy.stats.start_time}, End time: {strategy.stats.end_time}")
    
    # Simulate some activity
    strategy.stats.urls_processed = 5
    await asyncio.sleep(0.1) # Simulate time passing
    
    await strategy.shutdown()
    
    print(f"Stats after shutdown: Start time: {strategy.stats.start_time}, End time: {strategy.stats.end_time}")
    assert strategy.stats.end_time is not None, "End time should be set after shutdown"
    assert strategy.stats.end_time > strategy.stats.start_time, "End time should be after start time"

asyncio.run(demo_shutdown_method())
```

### 1.4. `DFSDeeepCrawlStrategy` (Depth-First Search)

#### 1.4.1. **Initialization & Basic Usage**

##### 1.4.1.1. Example: Initializing `DFSDeeepCrawlStrategy` with a `max_depth` of 2 and performing a basic batch DFS crawl.
For DFS, a site with clear branching is needed to see the depth-first behavior. `example.com` has limited depth. We'll use a mock structure.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DFSDeeepCrawlStrategy

# Mock HTML for DFS demonstration
# Page A -> B, C
# Page B -> D
# Page C -> E
RAW_HTML_DFS_A = "<html><body><a href='raw://PAGE_B'>B</a> <a href='raw://PAGE_C'>C</a></body></html>"
RAW_HTML_DFS_B = "<html><body><a href='raw://PAGE_D'>D</a></body></html>"
RAW_HTML_DFS_C = "<html><body><a href='raw://PAGE_E'>E</a></body></html>"
RAW_HTML_DFS_D = "<html><body>Depth 2 D</body></html>"
RAW_HTML_DFS_E = "<html><body>Depth 2 E</body></html>"

# Replace placeholders with actual raw content for the crawler
START_URL_DFS = f"raw://{RAW_HTML_DFS_A.replace('PAGE_B', RAW_HTML_DFS_B).replace('PAGE_C', RAW_HTML_DFS_C).replace('PAGE_D', RAW_HTML_DFS_D).replace('PAGE_E', RAW_HTML_DFS_E)}"

async def dfs_basic_batch_crawl():
    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=2, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting DFS batch crawl (max_depth=2, max_pages=5)...")
        # arun for DFS is expected to return results in a somewhat depth-first order
        results_list = await crawler.arun(url=START_URL_DFS, config=run_config)
        
        print(f"\n--- DFS Batch Crawl Results (Order may vary based on internal async processing but generally depth-first) ---")
        crawled_urls_with_depth = []
        if results_list:
            for result in results_list:
                if result.success:
                    depth = result.metadata.get('depth', 'N/A')
                    print(f"Crawled: {result.url_for_display()} (Depth: {depth})") # Using url_for_display for raw URLs
                    crawled_urls_with_depth.append((result.url_for_display(), depth))
            print(f"Total pages processed: {len(results_list)}")
            
            # Assertions to check if expected pages were crawled (order can be tricky with async)
            # We expect all 5 mock pages if max_pages=5 and max_depth=2 allows
            page_names = [url_display.split("<body>")[1].split("</body>")[0].strip() for url_display, _ in crawled_urls_with_depth if "raw://" in url_display and "<body>" in url_display]
            assert "Depth 2 D" in page_names or "Depth 2 E" in page_names, "Expected to reach depth 2"

        else:
            print("No results returned from crawl.")
            
# Replace placeholders correctly for the crawler to understand nested raw URLs
# This is tricky because the raw URL itself contains other raw URLs.
# A proper mock server or carefully crafted single raw HTML would be better.
# For now, this is a conceptual example of the expected DFS behavior.
# The current raw URL setup is too complex for simple string replacement.

async def dfs_basic_batch_crawl_simplified():
    # Simpler mock for easier verification.
    # A -> B ; B -> C
    html_c = "<html><body>Page C (depth 2)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_dfs_simple = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a></body></html>"

    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=2, max_pages=3)
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting DFS batch crawl (max_depth=2, max_pages=3) on simplified mock...")
        results_list = await crawler.arun(url=start_url_dfs_simple, config=run_config)
        
        print(f"\n--- DFS Batch Crawl Results (Simplified Mock) ---")
        crawled_info = []
        if results_list:
            for result in results_list:
                if result.success:
                    depth = result.metadata.get('depth', 'N/A')
                    # Extract a simple identifier from the raw HTML for easier assertion
                    content_id = "Unknown"
                    if "Page A" in result.html: content_id="A"
                    elif "Page B" in result.html: content_id="B"
                    elif "Page C" in result.html: content_id="C"
                    print(f"Crawled: Page {content_id} (Depth: {depth})")
                    crawled_info.append({"id": content_id, "depth": depth, "url": result.url_for_display()})
            print(f"Total pages processed: {len(results_list)}")
            
            # Expected order for DFS: A, B, C (or A, C, B depending on link order in A if it had multiple)
            # With the given structure (A -> B, B -> C), order should be A, B, C
            if len(crawled_info) == 3:
                assert crawled_info[0]["id"] == "A" and crawled_info[0]["depth"] == 0
                assert crawled_info[1]["id"] == "B" and crawled_info[1]["depth"] == 1
                assert crawled_info[2]["id"] == "C" and crawled_info[2]["depth"] == 2
                print("DFS order appears correct for this simplified structure.")
            else:
                print(f"Expected 3 pages, got {len(crawled_info)}. Crawled: {crawled_info}")


asyncio.run(dfs_basic_batch_crawl_simplified())
```

##### 1.4.1.2. Example: Performing a DFS crawl in stream mode (`config.stream=True`), highlighting the order of results.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DFSDeeepCrawlStrategy

# Simplified mock for DFS streaming order demonstration
# A -> B, D
# B -> C
# D -> E
async def dfs_stream_crawl_order():
    html_e = "<html><body>Page E (depth 2 from D)</body></html>"
    html_d = f"<html><body>Page D (depth 1) <a href='raw://{html_e}'>Link to E</a></body></html>"
    html_c = "<html><body>Page C (depth 2 from B)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_dfs_stream = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a> <a href='raw://{html_d}'>Link to D</a></body></html>"


    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=2, max_pages=5)
    # Stream mode
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy, stream=True)

    async with AsyncWebCrawler() as crawler:
        print("Starting DFS stream crawl (max_depth=2, max_pages=5)...")
        results_generator = await crawler.arun(url=start_url_dfs_stream, config=run_config)
        
        print(f"\n--- DFS Stream Crawl Results (Order of processing) ---")
        crawled_order_ids = []
        async for result in results_generator:
            if result.success:
                depth = result.metadata.get('depth', 'N/A')
                content_id = "Unknown"
                if "Page A" in result.html: content_id="A"
                elif "Page B" in result.html: content_id="B"
                elif "Page C" in result.html: content_id="C"
                elif "Page D" in result.html: content_id="D"
                elif "Page E" in result.html: content_id="E"
                print(f"Streamed: Page {content_id} (Depth: {depth})")
                crawled_order_ids.append(content_id)
            else:
                print(f"Stream Failed: {result.url_for_display()} - {result.error_message}")
        
        print(f"Crawled order: {crawled_order_ids}")
        # Expected DFS orders (stack behavior, depends on which link is pushed last/first):
        # If D is processed before B from A: A, D, E, B, C
        # If B is processed before D from A: A, B, C, D, E
        # The `reverse=True` on new_links in dfs_strategy.py means the *first* link in HTML is processed *last* by stack.
        # So, if A has links to B then D, D will be put on stack last, thus popped (processed) first.
        expected_order_1 = ['A', 'D', 'E', 'B', 'C'] # If "Link to D" is processed from stack first
        expected_order_2 = ['A', 'B', 'C', 'D', 'E'] # If "Link to B" is processed from stack first

        # Given the current DFS implementation pushes in reversed order of discovery
        # and HTML link order is B then D, D gets added to stack on top of B.
        # So D's branch should be explored first.
        assert crawled_order_ids == expected_order_1 or crawled_order_ids == expected_order_2, \
            f"DFS order incorrect. Expected one of {expected_order_1} or {expected_order_2}, got {crawled_order_ids}"


asyncio.run(dfs_stream_crawl_order())
```

#### 1.4.2. **Controlling Crawl Depth and Scope**

##### 1.4.2.1. Example: Demonstrating `max_depth` limiting the DFS crawl.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DFSDeeepCrawlStrategy

async def dfs_max_depth_limit():
    html_c = "<html><body>Page C (depth 2)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_dfs_simple = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a></body></html>"

    # Set max_depth to 1. Page C (depth 2) should not be crawled.
    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=1, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting DFS crawl with max_depth=1...")
        results_list = await crawler.arun(url=start_url_dfs_simple, config=run_config)
        
        print(f"\n--- DFS Crawl Results (max_depth=1) ---")
        crawled_pages_at_depth = {}
        if results_list:
            for result in results_list:
                if result.success:
                    depth = result.metadata.get('depth')
                    crawled_pages_at_depth.setdefault(depth, []).append(result.url_for_display())
                    print(f"Crawled: {result.url_for_display()} (Depth: {depth})")
            print(f"Total pages processed: {len(results_list)}")
            
            assert 0 in crawled_pages_at_depth
            assert 1 in crawled_pages_at_depth
            assert 2 not in crawled_pages_at_depth, "Should not crawl beyond max_depth=1"
            assert len(results_list) == 2 # Page A and Page B
        else:
            print("No results returned.")

asyncio.run(dfs_max_depth_limit())
```

##### 1.4.2.2. Example: Demonstrating `max_pages` limiting the number of crawled pages during DFS.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DFSDeeepCrawlStrategy

async def dfs_max_pages_limit():
    html_c = "<html><body>Page C (depth 2)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_dfs_simple = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a></body></html>"

    # max_pages = 2 means only Page A and Page B should be crawled.
    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=2, max_pages=2)
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting DFS crawl with max_pages=2...")
        results_list = await crawler.arun(url=start_url_dfs_simple, config=run_config)
        
        print(f"\n--- DFS Crawl Results (max_pages=2) ---")
        crawled_count = 0
        if results_list:
            for result in results_list:
                if result.success:
                    crawled_count +=1
                    print(f"Crawled: {result.url_for_display()} (Depth: {result.metadata.get('depth')})")
            print(f"Total pages processed: {crawled_count}")
            assert crawled_count <= 2, "Should process at most max_pages"
             # Check strategy's internal count for precision
            assert strategy._pages_crawled <= 2
        else:
            print("No results returned.")

asyncio.run(dfs_max_pages_limit())
```

#### 1.4.3. **Traversal Order**

##### 1.4.3.1. Example: A small site crawl demonstrating the LIFO (stack-like) behavior of DFS link processing.
(This is effectively covered by 1.4.1.2. The stream mode example shows the order of processing which is a direct result of LIFO.)
We can re-emphasize it or consider this covered. For completeness, a slight variation:

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import DFSDeeepCrawlStrategy

async def dfs_lifo_demonstration():
    # Structure: A -> [B, D], B -> C, D -> E
    # HTML links in order: B then D
    # DFS stack (simplified, top is right):
    # Initial: [A]
    # Pop A, discover B, D. Push D, then B. Stack: [B, D]
    # Pop D, discover E. Push E. Stack: [B, E]
    # Pop E. Stack: [B]
    # Pop B, discover C. Push C. Stack: [C]
    # Pop C. Stack: []
    # Expected processing order: A, D, E, B, C (because D is pushed last from A's links, so processed first)
    
    html_e = "<html><body>Page E (child of D)</body></html>"
    html_d = f"<html><body>Page D <a href='raw://{html_e}'>To E</a></body></html>"
    html_c = "<html><body>Page C (child of B)</body></html>"
    html_b = f"<html><body>Page B <a href='raw://{html_c}'>To C</a></body></html>"
    # Links in A are ordered B then D
    start_url_dfs_lifo = f"raw://<html><body>Page A <a href='raw://{html_b}'>To B</a> <a href='raw://{html_d}'>To D</a></body></html>"

    dfs_strategy = DFSDeeepCrawlStrategy(max_depth=2, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy, stream=True) # Stream to see order

    async with AsyncWebCrawler() as crawler:
        print("Demonstrating DFS LIFO behavior (stream mode)...")
        crawled_ids_in_order = []
        async for result in await crawler.arun(url=start_url_dfs_lifo, config=run_config):
            if result.success:
                content_id = "Unknown"
                if "Page A" in result.html: content_id="A"
                elif "Page B" in result.html: content_id="B"
                elif "Page C" in result.html: content_id="C"
                elif "Page D" in result.html: content_id="D"
                elif "Page E" in result.html: content_id="E"
                print(f"Processed: Page {content_id} (Depth: {result.metadata.get('depth')})")
                crawled_ids_in_order.append(content_id)
        
        print(f"\nActual processing order: {crawled_ids_in_order}")
        # Based on current DFS strategy (reversing links before adding to stack):
        # Links from A: B, D. Reversed: D, B. Stack (top right): [B, D]
        # Pop D, links: E. Stack: [B, E]
        # Pop E. Stack: [B]
        # Pop B, links: C. Stack: [C]
        # Pop C. Stack: []
        # Order: A, D, E, B, C
        expected_order = ['A', 'D', 'E', 'B', 'C']
        assert crawled_ids_in_order == expected_order, f"Expected LIFO order {expected_order}, got {crawled_ids_in_order}"

asyncio.run(dfs_lifo_demonstration())
```

### 1.5. `BestFirstCrawlingStrategy`

#### 1.5.1. **Initialization & Basic Usage**

##### 1.5.1.1. Example: Initializing `BestFirstCrawlingStrategy` with `max_depth` and a `url_scorer`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer

async def best_first_init_and_usage():
    # Score URLs based on keyword "product"
    keyword_scorer = KeywordRelevanceScorer(keywords=["product"])
    
    best_first_strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        max_pages=3,
        url_scorer=keyword_scorer
    )
    
    run_config = CrawlerRunConfig(deep_crawl_strategy=best_first_strategy)

    # Mock HTML for demonstration
    html_product = "<html><body><a href='/product-page.html'>Cool Product</a> Product details...</body></html>"
    html_blog = "<html><body><a href='/blog-post.html'>Blog Post</a> News and updates...</body></html>"
    html_contact = "<html><body><a href='/contact.html'>Contact Us</a> Get in touch...</body></html>"
    
    start_url_best_first = f"""raw://<html><body>
        <h1>Welcome</h1>
        <a href="raw://{html_product.replace('"', '&quot;')}">View Product</a>
        <a href="raw://{html_blog.replace('"', '&quot;')}">Read Blog</a>
        <a href="raw://{html_contact.replace('"', '&quot;')}">Contact</a>
    </body></html>"""


    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First crawl (scoring for 'product')...")
        results_list = await crawler.arun(url=start_url_best_first, config=run_config)
        
        print(f"\n--- Best-First Crawl Results (Batch) ---")
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url_for_display()} (Score: {result.metadata.get('score', 'N/A')}, Depth: {result.metadata.get('depth')})")
            # Expect product page to be crawled due to higher score
            assert any("product-page.html" in res.url_for_display() for res in results_list if res.success), "Product page should have been crawled."
        else:
            print("No results from crawl.")

asyncio.run(best_first_init_and_usage())
```

##### 1.5.1.2. Example: Performing a Best-First crawl in batch mode.
(Covered by 1.5.1.1, as batch mode is default if `stream=False` is not specified or is False in `CrawlerRunConfig` for the strategy)

##### 1.5.1.3. Example: Performing a Best-First crawl in stream mode.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer

async def best_first_stream_mode():
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature"])
    
    best_first_strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        max_pages=3,
        url_scorer=keyword_scorer
    )
    
    run_config = CrawlerRunConfig(deep_crawl_strategy=best_first_strategy, stream=True)

    html_feature = "<html><body><a href='/new-feature.html'>New Feature</a> Details about feature...</body></html>"
    html_about = "<html><body><a href='/about-us.html'>About Us</a> Company info...</body></html>"
    
    start_url_stream = f"""raw://<html><body>
        <a href="raw://{html_feature.replace('"', '&quot;')}">Amazing Feature</a>
        <a href="raw://{html_about.replace('"', '&quot;')}">About Page</a>
    </body></html>"""

    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First stream crawl (scoring for 'feature')...")
        results_generator = await crawler.arun(url=start_url_stream, config=run_config)
        
        print(f"\n--- Best-First Stream Crawl Results ---")
        async for result in results_generator:
            if result.success:
                print(f"Streamed: {result.url_for_display()} (Score: {result.metadata.get('score', 'N/A')}, Depth: {result.metadata.get('depth')})")
            else:
                print(f"Stream Failed: {result.url_for_display()} - {result.error_message}")

asyncio.run(best_first_stream_mode())
```

#### 1.5.2. **Priority-Based Crawling**

##### 1.5.2.1. Example: Demonstrating how `url_scorer` (e.g., `KeywordRelevanceScorer`) influences the crawl order.
(Effectively demonstrated in 1.5.1.1 and 1.5.1.3 where pages containing the keyword are prioritized. We can make a more direct comparison here.)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer

async def best_first_order_influence():
    # Scenario 1: Prioritize "tutorials"
    tutorial_scorer = KeywordRelevanceScorer(keywords=["tutorial"])
    strategy_tut = BestFirstCrawlingStrategy(max_depth=1, max_pages=3, url_scorer=tutorial_scorer)
    config_tut = CrawlerRunConfig(deep_crawl_strategy=strategy_tut, stream=True)

    # Scenario 2: Prioritize "pricing"
    pricing_scorer = KeywordRelevanceScorer(keywords=["pricing"])
    strategy_price = BestFirstCrawlingStrategy(max_depth=1, max_pages=3, url_scorer=pricing_scorer)
    config_price = CrawlerRunConfig(deep_crawl_strategy=strategy_price, stream=True)

    html_home = """<html><body>
        <a href="raw_tut.html">Tutorials</a>
        <a href="raw_price.html">Pricing Info</a>
        <a href="raw_blog.html">Blog</a>
    </body></html>"""
    html_tut = "<html><body>Learn with our tutorial.</body></html>"
    html_price = "<html><body>Check our pricing plans.</body></html>"
    html_blog = "<html><body>Latest news from our blog.</body></html>"
    
    # Create self-contained raw URLs
    start_url = f"raw://{html_home.replace('raw_tut.html', f'raw://{html_tut.replace(&quot;,&quot;&amp;quot;&quot;)}').replace('raw_price.html', f'raw://{html_price.replace(&quot;,&quot;&amp;quot;&quot;)}').replace('raw_blog.html', f'raw://{html_blog.replace(&quot;,&quot;&amp;quot;&quot;)}')}"


    async with AsyncWebCrawler() as crawler:
        print("\n--- Crawling with 'tutorial' priority ---")
        order_tut = []
        async for result in await crawler.arun(url=start_url, config=config_tut):
            if result.success:
                order_tut.append(result.url_for_display())
                print(f"  Crawled (tut): {result.url_for_display()} Score: {result.metadata.get('score')}")
        
        print("\n--- Crawling with 'pricing' priority ---")
        order_price = []
        async for result in await crawler.arun(url=start_url, config=config_price): # Re-crawl for new strategy application
            if result.success:
                order_price.append(result.url_for_display())
                print(f"  Crawled (price): {result.url_for_display()} Score: {result.metadata.get('score')}")
        
        # Assertions are tricky due to async nature and BATCH_SIZE. 
        # The goal is that the higher-scored item (if within the first BATCH_SIZE) appears earlier.
        # For a small number of links (3 in this case), it should be quite deterministic.
        # Assuming BATCH_SIZE is >= 3 or more links are processed per batch.

        # Check if "tutorial" related page is prioritized in first run (could be 2nd after start_url)
        if len(order_tut) > 1:
            assert any("tutorial" in url_disp.lower() for url_disp in order_tut[1:2]), \
                f"Tutorial link was not prioritized. Order: {order_tut}"
        
        # Check if "pricing" related page is prioritized in second run
        if len(order_price) > 1:
            assert any("pricing" in url_disp.lower() for url_disp in order_price[1:2]), \
                f"Pricing link was not prioritized. Order: {order_price}"

asyncio.run(best_first_order_influence())
```

##### 1.5.2.2. Example: Showing how `BATCH_SIZE` affects the processing of URLs from the priority queue.

`BATCH_SIZE` in `BestFirstCrawlingStrategy` determines how many URLs are fetched from the priority queue at once to be processed by `crawler.arun_many`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer
from crawl4ai.deep_crawling.bff_strategy import BATCH_SIZE as BFF_BATCH_SIZE # Import to show its value

# This example is more conceptual as BATCH_SIZE is a module-level constant.
# We can illustrate its effect by showing how many URLs are processed in parallel
# if we could control it or if the number of high-priority URLs is less/more than BATCH_SIZE.

# For demonstration, let's assume we have many high-priority links.
# The strategy will pull BATCH_SIZE of them at a time.
async def best_first_batch_size_effect():
    print(f"Current BATCH_SIZE for BestFirstCrawlingStrategy: {BFF_BATCH_SIZE}")

    # Create more links than BATCH_SIZE to see the batching effect
    num_links = BFF_BATCH_SIZE + 5 
    links_html = ""
    for i in range(num_links):
        # All links will have the keyword to make them high priority
        html_content = f"<html><body>Content with keyword 'important' number {i}</body></html>"
        links_html += f"<a href='raw_link_{i}.html'>Link {i} (important)</a>\n"
        # This creates placeholder names, actual content needs to be embedded for raw://
        # For simplicity in this example, we won't fully embed, but focus on the number of links
        # that *would* be processed.

    start_url_batch_effect = f"raw://<html><body>{links_html}</body></html>"
    
    # Mock the crawler.arun_many to see how many URLs it receives in one call
    class MockCrawler(AsyncWebCrawler):
        async def arun_many(self, urls, config, **kwargs):
            print(f"MockCrawler.arun_many called with {len(urls)} URLs: {urls[:3]}...") # Show first 3
            # Simulate successful crawl for all
            mock_results = []
            for i, url_str in enumerate(urls):
                # Simplified HTML content based on the URL
                html_content = f"<html><body>Mock content for {url_str.split('/')[-1]}</body></html>"
                # Determine depth; for this mock, assume all are depth 1 if not start_url
                depth = 1 if url_str != start_url_batch_effect else 0
                
                mock_results.append(self._create_crawl_result(
                    url=url_str,
                    html=html_content, # Create some HTML for each
                    success=True,
                    status_code=200,
                    metadata={"depth": depth, "score": 1.0}, # Mock score
                    config=config
                ))
            return mock_results

    scorer = KeywordRelevanceScorer(keywords=["important"])
    strategy = BestFirstCrawlingStrategy(max_depth=1, max_pages=num_links + 1, url_scorer=scorer)
    # Note: max_pages is set high to not interfere with BATCH_SIZE demonstration for link discovery
    
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy)

    async with MockCrawler() as crawler:
        print(f"Starting Best-First crawl with {num_links} high-priority links...")
        # This will call arun_many multiple times if num_links > BATCH_SIZE
        results_list = await crawler.arun(url=start_url_batch_effect, config=run_config)
        
        print(f"\n--- Best-First Crawl (BATCH_SIZE effect) ---")
        if results_list:
            print(f"Total pages processed: {len(results_list)-1}") # -1 for the start URL itself
            # Further assertions could be made if arun_many was more intricately mocked to track calls.
        else:
            print("No results returned.")
    print("Observe the 'MockCrawler.arun_many called with...' print statements.")
    print("If num_links > BATCH_SIZE, you should see multiple calls to arun_many, "
          f"each with up to {BFF_BATCH_SIZE} URLs.")

asyncio.run(best_first_batch_size_effect())
```

#### 1.5.3. **Controlling Crawl Scope**

##### 1.5.3.1. Example: Best-First crawl with `max_depth` reached.
(Similar to BFS/DFS max_depth, BestFirst also respects it.)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, PathDepthScorer

async def best_first_max_depth():
    html_c = "<html><body>Page C (depth 2)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_simple = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a></body></html>"

    # Score by path depth, optimal is 0 to ensure links are explored if possible
    scorer = PathDepthScorer(optimal_depth=0) 
    
    # max_depth=1 should stop before Page C
    strategy = BestFirstCrawlingStrategy(max_depth=1, max_pages=5, url_scorer=scorer)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First crawl with max_depth=1...")
        results_list = await crawler.arun(url=start_url_simple, config=run_config)
        
        print(f"\n--- Best-First Crawl Results (max_depth=1) ---")
        crawled_depths = set()
        if results_list:
            for result in results_list:
                if result.success:
                    depth = result.metadata.get('depth')
                    crawled_depths.add(depth)
                    print(f"Crawled: {result.url_for_display()} (Depth: {depth})")
            print(f"Depths crawled: {crawled_depths}")
            assert 2 not in crawled_depths, "Should not have crawled to depth 2"
            assert len(results_list) == 2 # Page A and Page B
        else:
            print("No results.")

asyncio.run(best_first_max_depth())
```

##### 1.5.3.2. Example: Best-First crawl with `max_pages` reached.
(Similar to BFS/DFS max_pages, BestFirst also respects it.)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, PathDepthScorer

async def best_first_max_pages():
    html_c = "<html><body>Page C (depth 2)</body></html>"
    html_b = f"<html><body>Page B (depth 1) <a href='raw://{html_c}'>Link to C</a></body></html>"
    start_url_simple = f"raw://<html><body>Page A (depth 0) <a href='raw://{html_b}'>Link to B</a></body></html>"
    
    scorer = PathDepthScorer(optimal_depth=0) # Score to explore if possible
    
    # max_pages=2 should process only Page A and Page B
    strategy = BestFirstCrawlingStrategy(max_depth=2, max_pages=2, url_scorer=scorer)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First crawl with max_pages=2...")
        results_list = await crawler.arun(url=start_url_simple, config=run_config)
        
        crawled_count = 0
        if results_list:
            print(f"\n--- Best-First Crawl Results (max_pages=2) ---")
            for result in results_list:
                if result.success:
                    crawled_count += 1
                    print(f"Crawled: {result.url_for_display()} (Depth: {result.metadata.get('depth')})")
            print(f"Total pages processed: {crawled_count}")
            assert crawled_count <= 2
            # Check strategy's internal count for precision
            assert strategy._pages_crawled <= 2
        else:
            print("No results.")
            
asyncio.run(best_first_max_pages())
```

#### 1.5.4. **Link Discovery and Scoring**

##### 1.5.4.1. Example: `link_discovery` adding new links to the priority queue based on their scores.
(This is implicitly shown in examples 1.5.1.1 and 1.5.2.1, where higher-scored links are processed. The priority queue mechanism itself is internal to `BestFirstCrawlingStrategy`.)
This conceptual example will show how `link_discovery` calculates scores and would (internally) add items with scores to the priority queue.

```python
import asyncio
from crawl4ai.models import CrawlResult, Links, Link
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer
from crawl4ai.utils import normalize_url_for_deep_crawl

mock_crawl_result_for_scoring = CrawlResult(
    url="http://example.com/home",
    html="""
        <a href="/product-feature.html">Feature Page</a>
        <a href="/about-us.html">About Page</a>
        <a href="/another-feature.html">Another Feature</a>
    """,
    success=True,
    links=Links(
        internal=[
            Link(href="/product-feature.html"), Link(href="/about-us.html"), Link(href="/another-feature.html")
        ]
    )
)

async def demo_best_first_link_discovery_scoring():
    scorer = KeywordRelevanceScorer(keywords=["feature"]) # Prioritize "feature"
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer)
    
    # Simulate internal state for link_discovery
    source_url = mock_crawl_result_for_scoring.url
    current_depth = 0 # Depth of the source_url
    visited = {source_url}
    # PriorityQueue would store (negative_score, depth, url, parent_url)
    # We'll simulate the items that would be put into it.
    priority_queue_items_to_add = [] 
    depths = {source_url: 0}

    # Manual simulation of link_discovery's core logic
    next_depth = current_depth + 1
    if next_depth <= strategy.max_depth:
        for link_obj in mock_crawl_result_for_scoring.links.internal:
            abs_url = normalize_url_for_deep_crawl(link_obj.href, source_url)
            if abs_url and abs_url not in visited:
                if await strategy.can_process_url(abs_url, next_depth): # Standard checks
                    score = await strategy.url_scorer.score(abs_url) # Score the URL
                    # Store with negative score because asyncio.PriorityQueue is a min-heap
                    priority_queue_items_to_add.append((-score, next_depth, abs_url, source_url))
                    visited.add(abs_url) # Mark as "to be processed"
                    depths[abs_url] = next_depth
    
    # Sort by score (descending, as priority queue would handle)
    priority_queue_items_to_add.sort(key=lambda x: x[0], reverse=True) # Actually sorts by -score asc -> score desc

    print(f"Source URL: {source_url}")
    print(f"Items that would be added to priority queue (score, depth, url, parent):")
    for neg_score, depth_val, url_val, parent_val in priority_queue_items_to_add:
        print(f"  - Score: {-neg_score:.2f}, Depth: {depth_val}, URL: {url_val}")

    assert len(priority_queue_items_to_add) == 3
    # Check if "feature" links have higher scores (lower negative_score, appear earlier after sort)
    assert "product-feature.html" in priority_queue_items_to_add[0][2] or "another-feature.html" in priority_queue_items_to_add[0][2]
    assert "product-feature.html" in priority_queue_items_to_add[1][2] or "another-feature.html" in priority_queue_items_to_add[1][2]
    assert "about-us.html" in priority_queue_items_to_add[2][2] # Should have lower score

asyncio.run(demo_best_first_link_discovery_scoring())
```

---
## 2. URL Filters

This section demonstrates how to use various filters to control which URLs are processed during a deep crawl.

### 2.1. `FilterStats`

#### 2.1.1. Example: Accessing `total_urls`, `passed_urls`, and `rejected_urls` from a `FilterStats` object after applying a filter.

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter, FilterStats

async def demo_filter_stats():
    # Create a filter (e.g., allow only .html files)
    html_filter = URLPatternFilter(patterns=["*.html"])

    urls_to_test = [
        "http://example.com/index.html",
        "http://example.com/script.js",
        "http://example.com/about.html",
        "http://example.com/image.png",
    ]

    print("Applying URLPatternFilter (allow *.html):")
    for url in urls_to_test:
        # The apply method updates stats internally
        passed = await html_filter.apply(url) 
        print(f"  URL: {url}, Passed: {passed}")

    # Access the stats from the filter instance
    stats = html_filter.stats
    print(f"\n--- Filter Stats ---")
    print(f"Total URLs processed: {stats.total_urls}")
    print(f"Passed URLs: {stats.passed_urls}")
    print(f"Rejected URLs: {stats.rejected_urls}")

    assert stats.total_urls == 4
    assert stats.passed_urls == 2
    assert stats.rejected_urls == 2

asyncio.run(demo_filter_stats())
```

### 2.2. `FilterChain`

#### 2.2.1. Example: Creating a `FilterChain` with `DomainFilter` and `URLPatternFilter`.

```python
import asyncio
from crawl4ai.deep_crawling import FilterChain, DomainFilter, URLPatternFilter

async def create_filter_chain():
    # Filter 1: Allow only 'example.com'
    domain_filter = DomainFilter(allowed_domains=["example.com"])
    
    # Filter 2: Allow only URLs ending with '.html' or '.htm'
    pattern_filter = URLPatternFilter(patterns=["*.html", "*.htm"])
    
    # Create a chain: URL must pass BOTH filters
    filter_chain = FilterChain(filters=[domain_filter, pattern_filter])
    
    print(f"FilterChain created with {len(filter_chain.filters)} filters.")

    url1 = "http://example.com/page.html" # Should pass
    url2 = "http://example.com/script.js" # Should fail pattern_filter
    url3 = "http://otherexample.com/page.html" # Should fail domain_filter
    url4 = "http://otherexample.com/image.png" # Should fail both

    print(f"\nTesting URL: {url1} -> Passed: {await filter_chain.apply(url1)}")
    print(f"Testing URL: {url2} -> Passed: {await filter_chain.apply(url2)}")
    print(f"Testing URL: {url3} -> Passed: {await filter_chain.apply(url3)}")
    print(f"Testing URL: {url4} -> Passed: {await filter_chain.apply(url4)}")

    assert await filter_chain.apply(url1) == True
    assert await filter_chain.apply(url2) == False
    assert await filter_chain.apply(url3) == False
    assert await filter_chain.apply(url4) == False
    
    print("\nDomainFilter stats:", domain_filter.stats.passed_urls, "passed,", domain_filter.stats.rejected_urls, "rejected.")
    print("PatternFilter stats:", pattern_filter.stats.passed_urls, "passed,", pattern_filter.stats.rejected_urls, "rejected.")
    print("FilterChain aggregated stats:", filter_chain.stats.passed_urls, "passed,", filter_chain.stats.rejected_urls, "rejected.")


asyncio.run(create_filter_chain())
```

#### 2.2.2. Example: Applying a `FilterChain` within a `BFSDeeepCrawlStrategy` and observing filtered results.
(This was effectively covered in 1.3.3.1. That example shows a `FilterChain` used with `BFSDeeepCrawlStrategy`.)

#### 2.2.3. Example: Checking aggregated `FilterStats` from a `FilterChain`.
(This was effectively covered in 2.2.1, where `filter_chain.stats` are printed.)

### 2.3. `URLPatternFilter`

#### 2.3.1. Example: Using `URLPatternFilter` to allow only URLs matching a specific regex pattern.

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_regex():
    # Allow URLs that look like product pages, e.g., /products/item123
    regex_pattern = r"/products/item\d+"
    product_filter = URLPatternFilter(patterns=[regex_pattern]) # use_glob=False is default for regex

    url_product = "http://example.com/products/item123"
    url_blog = "http://example.com/blog/my-post"

    print(f"Testing with regex pattern: {regex_pattern}")
    print(f"URL: {url_product}, Passed: {await product_filter.apply(url_product)}")
    print(f"URL: {url_blog}, Passed: {await product_filter.apply(url_blog)}")

    assert await product_filter.apply(url_product) == True
    assert await product_filter.apply(url_blog) == False

asyncio.run(url_pattern_regex())
```

#### 2.3.2. Example: Using `URLPatternFilter` with `reverse=True` to disallow URLs matching a pattern.

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_reverse():
    # Disallow URLs containing 'admin' or 'login'
    disallow_patterns = [r".*admin.*", r".*login.*"]
    admin_block_filter = URLPatternFilter(patterns=disallow_patterns, reverse=True)

    url_safe = "http://example.com/dashboard"
    url_admin = "http://example.com/admin/panel"
    url_login = "http://example.com/user/login/page"

    print(f"Testing with reverse patterns: {disallow_patterns}")
    print(f"URL: {url_safe}, Passed: {await admin_block_filter.apply(url_safe)}")
    print(f"URL: {url_admin}, Passed: {await admin_block_filter.apply(url_admin)}")
    print(f"URL: {url_login}, Passed: {await admin_block_filter.apply(url_login)}")

    assert await admin_block_filter.apply(url_safe) == True
    assert await admin_block_filter.apply(url_admin) == False
    assert await admin_block_filter.apply(url_login) == False

asyncio.run(url_pattern_reverse())
```

#### 2.3.3. Example: `URLPatternFilter` with `use_glob=True` for wildcard matching.

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_glob():
    # Allow only .jpg or .png images in an /assets/ directory using glob
    # use_glob=True is effectively default when patterns don't look like regex
    # but we can be explicit. The categorization logic determines this.
    # Forcing glob by not using regex-like characters.
    # This filter's internal logic auto-detects glob for simple patterns like "*.jpg"
    # If you want to force fnmatch style globbing over regex for ambiguous patterns,
    # there isn't a direct `use_glob` parameter on URLPatternFilter itself.
    # The categorization of pattern types (SUFFIX, PREFIX, DOMAIN, PATH, REGEX)
    # handles this. Let's demonstrate a simple suffix pattern.
    
    # Example: Allow only *.jpg or *.png
    image_filter_suffix = URLPatternFilter(patterns=["*.jpg", "*.png"]) # This will be categorized as SUFFIX

    url_jpg = "http://example.com/image.jpg"
    url_png = "http://example.com/photo.png"
    url_gif = "http://example.com/animation.gif"

    print("Testing with SUFFIX patterns: ['*.jpg', '*.png']")
    print(f"URL: {url_jpg}, Passed: {await image_filter_suffix.apply(url_jpg)}")
    print(f"URL: {url_png}, Passed: {await image_filter_suffix.apply(url_png)}")
    print(f"URL: {url_gif}, Passed: {await image_filter_suffix.apply(url_gif)}")

    assert await image_filter_suffix.apply(url_jpg) == True
    assert await image_filter_suffix.apply(url_png) == True
    assert await image_filter_suffix.apply(url_gif) == False

asyncio.run(url_pattern_glob())
```
*Self-correction: The `use_glob` parameter was mentioned in the prompt for `URLPatternFilter`, but it's not a direct constructor parameter. The filter categorizes patterns and applies fnmatch-like logic for SUFFIX, PREFIX, etc. The above example demonstrates this behavior for SUFFIX.*

#### 2.3.4. Example: `URLPatternFilter` demonstrating "SUFFIX" pattern type (e.g., `*.html`).
(Covered by 2.3.3 with `*.jpg`, `*.png`. A specific `*.html` example is similar)

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_suffix_html():
    html_filter = URLPatternFilter(patterns=["*.html"]) # Categorized as SUFFIX

    url_html = "http://example.com/index.html"
    url_php = "http://example.com/index.php"
    
    print("Testing SUFFIX pattern: '*.html'")
    print(f"URL: {url_html}, Passed: {await html_filter.apply(url_html)}")
    print(f"URL: {url_php}, Passed: {await html_filter.apply(url_php)}")

    assert await html_filter.apply(url_html) == True
    assert await html_filter.apply(url_php) == False

asyncio.run(url_pattern_suffix_html())
```

#### 2.3.5. Example: `URLPatternFilter` demonstrating "PREFIX" pattern type (e.g., `/blog/*`).

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_prefix_blog():
    # Note: For `URLPatternFilter`, a pattern like "/blog/*" would be treated as a REGEX
    # or a general PATH if it doesn't have special regex characters.
    # The internal categorization distinguishes between simple wildcards and full regex.
    # Let's use a pattern that is clearly a prefix and would be handled by fnmatch-like logic.
    # The `_categorize_pattern` method would likely treat "/blog/*" as PATH or REGEX.
    # To demonstrate a pure prefix, we'd use a simpler pattern without the trailing `*` if it's not a glob pattern.
    # If "/blog/*" is intended as a glob, it's treated as such for PATH.
    
    # Let's use a pattern that clearly falls into PREFIX category: "http://example.com/blog/"
    # And another that would be a PATH type: "/blog/*"
    
    # This will be categorized as PATH due to the wildcard if not complex enough for REGEX
    blog_path_filter = URLPatternFilter(patterns=["*/blog/*"]) 

    url_blog_post = "http://example.com/blog/my-first-post"
    url_blog_main = "http://example.com/blog/"
    url_products = "http://example.com/products/item"

    print("Testing PATH pattern: '*/blog/*'") # Will match URLs containing /blog/
    print(f"URL: {url_blog_post}, Passed: {await blog_path_filter.apply(url_blog_post)}")
    print(f"URL: {url_blog_main}, Passed: {await blog_path_filter.apply(url_blog_main)}")
    print(f"URL: {url_products}, Passed: {await blog_path_filter.apply(url_products)}")

    assert await blog_path_filter.apply(url_blog_post) == True
    assert await blog_path_filter.apply(url_blog_main) == True
    assert await blog_path_filter.apply(url_products) == False
    
    # To strictly test _simple_prefixes (checked before general path patterns):
    # This uses exact string startswith logic.
    simple_prefix_filter = URLPatternFilter(patterns=["http://example.com/blog"]) 
    # This will not match /blog/ as a prefix, but as a PATH pattern if it makes it there.
    # The internal categorization logic is: SUFFIX -> DOMAIN -> PREFIX -> PATH/REGEX
    # So, "http://example.com/blog" would be a simple prefix.

    print("\nTesting simple prefix pattern: 'http://example.com/blog'")
    print(f"URL: {url_blog_post}, Passed: {await simple_prefix_filter.apply(url_blog_post)}") # True
    print(f"URL: http://example.com/blog, Passed: {await simple_prefix_filter.apply('http://example.com/blog')}") # True
    print(f"URL: {url_products}, Passed: {await simple_prefix_filter.apply(url_products)}") # False
    
    assert await simple_prefix_filter.apply(url_blog_post) == True


asyncio.run(url_pattern_prefix_blog())
```

#### 2.3.6. Example: `URLPatternFilter` demonstrating "DOMAIN" pattern type (e.g., `*.example.com`).

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def url_pattern_domain():
    # This pattern type matches the domain part of the URL.
    # "*.example.com" would allow "www.example.com", "blog.example.com", but not "example.com" itself
    # or "badexample.com".
    # The internal logic converts "*.example.com" into a regex like r"[^/]+\.example\.com" for domain matching.
    
    domain_filter = URLPatternFilter(patterns=["*.example.com"]) 

    url_subdomain = "http://blog.example.com/article"
    url_maindomain = "http://example.com/main" # This should NOT pass if pattern is strictly *.example.com
    url_other_sub = "http://www.example.com/page"
    url_external = "http://another.domain.com"
    
    # If we want to include example.com itself, we'd need another pattern "example.com" or a regex.
    # Let's test with "example.com" to show it's treated as a domain pattern
    main_domain_filter = URLPatternFilter(patterns=["example.com"])


    print("Testing DOMAIN pattern: '*.example.com'")
    print(f"URL: {url_subdomain}, Passed: {await domain_filter.apply(url_subdomain)}")
    print(f"URL: {url_maindomain}, Passed: {await domain_filter.apply(url_maindomain)}") # Expected False
    print(f"URL: {url_other_sub}, Passed: {await domain_filter.apply(url_other_sub)}")
    print(f"URL: {url_external}, Passed: {await domain_filter.apply(url_external)}")
    
    assert await domain_filter.apply(url_subdomain) == True
    assert await domain_filter.apply(url_maindomain) == False # `*.example.com` does not match `example.com`
    assert await domain_filter.apply(url_other_sub) == True
    assert await domain_filter.apply(url_external) == False

    print("\nTesting DOMAIN pattern: 'example.com'")
    print(f"URL: {url_subdomain}, Passed: {await main_domain_filter.apply(url_subdomain)}") # True (subdomain of example.com)
    print(f"URL: {url_maindomain}, Passed: {await main_domain_filter.apply(url_maindomain)}") # True
    
    assert await main_domain_filter.apply(url_subdomain) == True
    assert await main_domain_filter.apply(url_maindomain) == True


asyncio.run(url_pattern_domain())
```

#### 2.3.7. Example: `URLPatternFilter` demonstrating "PATH" pattern type (e.g., `/products/electronics/*`).
(Covered by 2.3.5 with `*/blog/*`. The categorization logic handles these general path/glob-like patterns after specific prefix/suffix/domain.)

### 2.4. `ContentTypeFilter`

*Note: `ContentTypeFilter` examples ideally require a live server or a mock HTTP server to return actual `Content-Type` headers. For self-contained examples, we'll primarily rely on the extension-based fallback.*

#### 2.4.1. Example: Using `ContentTypeFilter` to allow only "text/html" URLs.

```python
import asyncio
from crawl4ai.deep_crawling import ContentTypeFilter

async def content_type_html_only():
    # This filter will primarily use URL extensions if check_extension=True (default)
    # as we are not making live HTTP requests in this isolated example.
    html_only_filter = ContentTypeFilter(allowed_types=["text/html"])

    url_html = "http://example.com/index.html"
    url_pdf = "http://example.com/document.pdf"
    url_no_ext = "http://example.com/api/data" # No extension, would rely on Content-Type header in live scenario

    print("Testing ContentTypeFilter (allow 'text/html', relies on extension here):")
    print(f"URL: {url_html}, Passed: {await html_only_filter.apply(url_html)}")
    print(f"URL: {url_pdf}, Passed: {await html_only_filter.apply(url_pdf)}")
    print(f"URL: {url_no_ext}, Passed: {await html_only_filter.apply(url_no_ext)}")
    print("Note: For URLs without extensions, this filter would need live HTTP HEAD requests to check Content-Type header.")

    assert await html_only_filter.apply(url_html) == True
    assert await html_only_filter.apply(url_pdf) == False
    # Without live request, url_no_ext might pass if default behavior is permissive or fail if strict.
    # The current implementation's apply method is synchronous and relies on _check_url_cached.
    # _check_url_cached primarily uses extension. If check_extension=False, it would always return True
    # unless a live check was made (which is not part of the filter's apply method directly).
    # For this test, it defaults to True if no extension and check_extension=True
    assert await html_only_filter.apply(url_no_ext) == True 


asyncio.run(content_type_html_only())
```

#### 2.4.2. Example: `ContentTypeFilter` allowing a list of types, e.g., ["text/html", "application/pdf"].

```python
import asyncio
from crawl4ai.deep_crawling import ContentTypeFilter

async def content_type_html_and_pdf():
    multi_type_filter = ContentTypeFilter(allowed_types=["text/html", "application/pdf"])

    url_html = "http://example.com/index.html"
    url_pdf = "http://example.com/report.pdf"
    url_jpg = "http://example.com/image.jpg"

    print("Testing ContentTypeFilter (allow 'text/html', 'application/pdf'):")
    print(f"URL: {url_html}, Passed: {await multi_type_filter.apply(url_html)}")
    print(f"URL: {url_pdf}, Passed: {await multi_type_filter.apply(url_pdf)}")
    print(f"URL: {url_jpg}, Passed: {await multi_type_filter.apply(url_jpg)}")
    
    assert await multi_type_filter.apply(url_html) == True
    assert await multi_type_filter.apply(url_pdf) == True
    assert await multi_type_filter.apply(url_jpg) == False

asyncio.run(content_type_html_and_pdf())
```

#### 2.4.3. Example: `ContentTypeFilter` with `check_extension=True` (default) vs. `check_extension=False` using URL extensions.

```python
import asyncio
from crawl4ai.deep_crawling import ContentTypeFilter

async def content_type_check_extension_toggle():
    url_html_like_no_ext = "http://example.com/about-us" # Could be HTML
    url_pdf_like_no_ext = "http://example.com/download/report" # Could be PDF

    # Default: check_extension=True
    filter_check_ext = ContentTypeFilter(allowed_types=["text/html"])
    # If no extension, it defaults to True (allows processing, hoping Content-Type header confirms)
    print(f"With check_extension=True (default):")
    print(f"  URL: {url_html_like_no_ext}, Passed: {await filter_check_ext.apply(url_html_like_no_ext)}")
    assert await filter_check_ext.apply(url_html_like_no_ext) == True

    # check_extension=False: Ignores extensions, always passes unless live HEAD request fails (not tested here)
    filter_no_check_ext = ContentTypeFilter(allowed_types=["text/html"], check_extension=False)
    # This will always return True from apply() because it skips extension check
    print(f"With check_extension=False:")
    print(f"  URL: {url_html_like_no_ext}, Passed: {await filter_no_check_ext.apply(url_html_like_no_ext)}")
    print(f"  URL: http://example.com/image.jpg, Passed: {await filter_no_check_ext.apply('http://example.com/image.jpg')}")
    
    assert await filter_no_check_ext.apply(url_html_like_no_ext) == True
    assert await filter_no_check_ext.apply("http://example.com/image.jpg") == True 
    # (Passes because it doesn't check extension; live HEAD would be needed to actually filter)

asyncio.run(content_type_check_extension_toggle())
```

### 2.5. `DomainFilter`

#### 2.5.1. Example: `DomainFilter` allowing only URLs from "example.com" and its subdomains.

```python
import asyncio
from crawl4ai.deep_crawling import DomainFilter

async def domain_filter_allow_specific():
    # Allows example.com and any subdomains like www.example.com, blog.example.com
    domain_filter = DomainFilter(allowed_domains=["example.com"])

    url_main = "http://example.com/page"
    url_sub = "http://blog.example.com/article"
    url_external = "http://anotherdomain.com"

    print("Testing DomainFilter (allow 'example.com' and subdomains):")
    print(f"URL: {url_main}, Passed: {await domain_filter.apply(url_main)}")
    print(f"URL: {url_sub}, Passed: {await domain_filter.apply(url_sub)}")
    print(f"URL: {url_external}, Passed: {await domain_filter.apply(url_external)}")

    assert await domain_filter.apply(url_main) == True
    assert await domain_filter.apply(url_sub) == True
    assert await domain_filter.apply(url_external) == False

asyncio.run(domain_filter_allow_specific())
```

#### 2.5.2. Example: `DomainFilter` blocking URLs from "ads.example.com".

```python
import asyncio
from crawl4ai.deep_crawling import DomainFilter

async def domain_filter_block_specific():
    # Blocks ads.example.com and its subdomains (e.g., tracker.ads.example.com)
    # but allows other example.com URLs if no allowed_domains is set (or if it includes example.com)
    domain_filter = DomainFilter(blocked_domains=["ads.example.com"])
    # By default, if allowed_domains is None, all non-blocked domains are permitted.

    url_allowed_sub = "http://blog.example.com/article"
    url_blocked_sub = "http://ads.example.com/banner"
    url_deep_blocked = "http://tracker.ads.example.com/pixel" # Also blocked as subdomain of ads.example.com
    url_main = "http://example.com/main"

    print("Testing DomainFilter (block 'ads.example.com'):")
    print(f"URL: {url_allowed_sub}, Passed: {await domain_filter.apply(url_allowed_sub)}")
    print(f"URL: {url_blocked_sub}, Passed: {await domain_filter.apply(url_blocked_sub)}")
    print(f"URL: {url_deep_blocked}, Passed: {await domain_filter.apply(url_deep_blocked)}")
    print(f"URL: {url_main}, Passed: {await domain_filter.apply(url_main)}")

    assert await domain_filter.apply(url_allowed_sub) == True
    assert await domain_filter.apply(url_blocked_sub) == False
    assert await domain_filter.apply(url_deep_blocked) == False
    assert await domain_filter.apply(url_main) == True

asyncio.run(domain_filter_block_specific())
```

#### 2.5.3. Example: Combining `allowed_domains` and `blocked_domains`.

```python
import asyncio
from crawl4ai.deep_crawling import DomainFilter

async def domain_filter_combined():
    # Allow example.com but specifically block sub.example.com
    domain_filter = DomainFilter(
        allowed_domains=["example.com"],
        blocked_domains=["sub.example.com"]
    )

    url_main_allowed = "http://www.example.com/page" # Subdomain of example.com, allowed
    url_blocked_sub = "http://sub.example.com/secret" # Specifically blocked
    url_other_sub_allowed = "http://blog.example.com/article" # Allowed as subdomain of example.com
    url_external = "http://another.com" # Not in allowed_domains

    print("Testing DomainFilter (allow 'example.com', block 'sub.example.com'):")
    print(f"URL: {url_main_allowed}, Passed: {await domain_filter.apply(url_main_allowed)}")
    print(f"URL: {url_blocked_sub}, Passed: {await domain_filter.apply(url_blocked_sub)}")
    print(f"URL: {url_other_sub_allowed}, Passed: {await domain_filter.apply(url_other_sub_allowed)}")
    print(f"URL: {url_external}, Passed: {await domain_filter.apply(url_external)}")

    assert await domain_filter.apply(url_main_allowed) == True
    assert await domain_filter.apply(url_blocked_sub) == False
    assert await domain_filter.apply(url_other_sub_allowed) == True
    assert await domain_filter.apply(url_external) == False

asyncio.run(domain_filter_combined())
```

### 2.6. `ContentRelevanceFilter`
*Note: Requires live HTTP requests to fetch `<head>` content. For isolated tests, mocking or a stable example site is needed. We'll use a simplified direct call for demonstration.*

#### 2.6.1. Example: Using `ContentRelevanceFilter` with a query and threshold to filter pages based on head content relevance.

```python
import asyncio
from crawl4ai.deep_crawling import ContentRelevanceFilter
# Mock HeadPeek for testing without live requests
from unittest.mock import patch

# This example will mock HeadPeek.peek_html to simulate responses
async def demo_content_relevance_filter():
    query = "Python programming tutorial"
    # Threshold: Score must be >= 0.1 to pass
    relevance_filter = ContentRelevanceFilter(query=query, threshold=0.1) 

    # Mock HTML heads
    html_head_relevant = """
    <head>
        <title>Advanced Python Programming Tutorial</title>
        <meta name="description" content="Learn Python programming concepts and best practices.">
        <meta name="keywords" content="Python, programming, tutorial, advanced">
    </head>
    """
    html_head_irrelevant = """
    <head>
        <title>Best Coffee Recipes</title>
        <meta name="description" content="Discover amazing coffee recipes.">
        <meta name="keywords" content="coffee, recipes, morning, brew">
    </head>
    """

    url_relevant = "http://example.com/python-tutorial"
    url_irrelevant = "http://example.com/coffee-recipes"

    print(f"Testing ContentRelevanceFilter with query: '{query}' and threshold: 0.1")

    # Patch HeadPeek.peek_html to return our mock heads
    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html') as mock_peek:
        # First call to apply (for url_relevant)
        mock_peek.return_value = html_head_relevant
        passed_relevant = await relevance_filter.apply(url_relevant)
        print(f"  URL: {url_relevant}, Passed: {passed_relevant}, Score: {relevance_filter._last_score:.2f}") # _last_score is for demo
        assert passed_relevant

        # Second call to apply (for url_irrelevant)
        mock_peek.return_value = html_head_irrelevant
        passed_irrelevant = await relevance_filter.apply(url_irrelevant)
        print(f"  URL: {url_irrelevant}, Passed: {passed_irrelevant}, Score: {relevance_filter._last_score:.2f}")
        assert not passed_irrelevant

asyncio.run(demo_content_relevance_filter())
```

#### 2.6.2. Example: Demonstrating different `k1` and `b` BM25 parameters.

```python
import asyncio
from crawl4ai.deep_crawling import ContentRelevanceFilter
from unittest.mock import patch

async def demo_bm25_params_relevance_filter():
    query = "web scraping tools"
    
    html_head_content = """
    <head>
        <title>Top Web Scraping Tools and Libraries</title>
        <meta name="description" content="A comprehensive list of web scraping tools.">
    </head>
    """
    url_test = "http://example.com/scraping-tools"

    # Filter 1: Default k1 and b
    filter_default_params = ContentRelevanceFilter(query=query, threshold=0.01)
    
    # Filter 2: Higher k1 (more sensitive to term frequency)
    filter_high_k1 = ContentRelevanceFilter(query=query, threshold=0.01, k1=2.0)

    # Filter 3: Lower b (less penalty for document length)
    filter_low_b = ContentRelevanceFilter(query=query, threshold=0.01, b=0.5)

    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html', return_value=html_head_content):
        await filter_default_params.apply(url_test)
        score_default = filter_default_params._last_score # Internal for demo
        print(f"Score with default BM25 params (k1={filter_default_params.k1}, b={filter_default_params.b}): {score_default:.4f}")

        await filter_high_k1.apply(url_test)
        score_high_k1 = filter_high_k1._last_score
        print(f"Score with high k1 (k1={filter_high_k1.k1}, b={filter_high_k1.b}): {score_high_k1:.4f}")
        
        await filter_low_b.apply(url_test)
        score_low_b = filter_low_b._last_score
        print(f"Score with low b (k1={filter_low_b.k1}, b={filter_low_b.b}): {score_low_b:.4f}")

    # Exact score assertions can be brittle, but we expect scores to change.
    # For this specific content and query, higher k1 might slightly increase score if query terms are frequent.
    # Lower b might increase score if avgdl is small relative to this doc's length.
    assert score_default != score_high_k1 or score_default != score_low_b, "BM25 scores should differ with parameter changes."

asyncio.run(demo_bm25_params_relevance_filter())
```

### 2.7. `SEOFilter`

#### 2.7.1. Example: Using `SEOFilter` with a threshold to filter pages based on SEO quality score.

```python
import asyncio
from crawl4ai.deep_crawling import SEOFilter
from unittest.mock import patch

async def demo_seo_filter_threshold():
    # Example: Require a minimum SEO score of 0.7
    seo_filter = SEOFilter(threshold=0.7)

    html_head_good_seo = """
    <head>
        <title>Optimize Your SEO: A Comprehensive Guide (55 chars)</title>
        <meta name="description" content="Learn SEO best practices to improve your website ranking. This guide covers keywords, on-page optimization, and link building. (150 chars)">
        <link rel="canonical" href="http://example.com/seo-guide" />
        <meta name="robots" content="index, follow">
        <script type="application/ld+json">{"@context": "https://schema.org"}</script>
    </head>
    """
    url_good_seo = "http://example.com/seo-guide"

    html_head_poor_seo = "<head><title>Seo</title></head>" # Short title, no meta, etc.
    url_poor_seo = "http://example.com/bad-seo"
    
    print(f"Testing SEOFilter with threshold 0.7:")
    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html') as mock_peek:
        mock_peek.return_value = html_head_good_seo
        passed_good = await seo_filter.apply(url_good_seo)
        print(f"  URL (Good SEO): {url_good_seo}, Passed: {passed_good}, Score: {seo_filter._last_score:.2f}")
        assert passed_good

        mock_peek.return_value = html_head_poor_seo
        passed_poor = await seo_filter.apply(url_poor_seo)
        print(f"  URL (Poor SEO): {url_poor_seo}, Passed: {passed_poor}, Score: {seo_filter._last_score:.2f}")
        assert not passed_poor

asyncio.run(demo_seo_filter_threshold())
```

#### 2.7.2. Example: `SEOFilter` with custom `keywords` to check for keyword presence in title/meta.

```python
import asyncio
from crawl4ai.deep_crawling import SEOFilter
from unittest.mock import patch

async def demo_seo_filter_keywords():
    # Filter requires "Crawl4AI" in title/meta, and overall score >= 0.5
    seo_filter = SEOFilter(threshold=0.5, keywords=["Crawl4AI", "web scraping"])

    html_head_with_keyword = """
    <head>
        <title>Crawl4AI: The Best Web Scraping Tool</title>
        <meta name="description" content="Discover Crawl4AI for efficient web scraping.">
        <meta name="robots" content="index, follow"> 
    </head>
    """ # Missing canonical but has keywords
    url_with_keyword = "http://example.com/crawl4ai-tool"

    html_head_no_keyword = """
    <head>
        <title>A Generic Web Tool</title>
        <meta name="description" content="This is a tool for the web.">
        <meta name="robots" content="index, follow">
        <link rel="canonical" href="http://example.com/generic-tool" />
    </head>
    """ # Good general SEO but missing keywords
    url_no_keyword = "http://example.com/generic-tool"

    print(f"Testing SEOFilter with keywords ['Crawl4AI', 'web scraping'] and threshold 0.5:")
    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html') as mock_peek:
        mock_peek.return_value = html_head_with_keyword
        passed_with_kw = await seo_filter.apply(url_with_keyword)
        print(f"  URL (With Keyword): {url_with_keyword}, Passed: {passed_with_kw}, Score: {seo_filter._last_score:.2f}")
        assert passed_with_kw # Keyword presence significantly boosts score

        mock_peek.return_value = html_head_no_keyword
        passed_no_kw = await seo_filter.apply(url_no_keyword)
        print(f"  URL (No Keyword): {url_no_keyword}, Passed: {passed_no_kw}, Score: {seo_filter._last_score:.2f}")
        assert not passed_no_kw # Lack of keyword drops score below threshold

asyncio.run(demo_seo_filter_keywords())
```

#### 2.7.3. Example: `SEOFilter` with custom `weights` for different SEO factors.

```python
import asyncio
from crawl4ai.deep_crawling import SEOFilter
from unittest.mock import patch

async def demo_seo_filter_weights():
    # Emphasize title length and canonical tag more than others
    custom_weights = {
        "title_length": 0.3,  # Default 0.15
        "title_kw": 0.1,      # Default 0.18
        "meta_description": 0.1, # Default 0.12
        "canonical": 0.3,     # Default 0.10
        "robot_ok": 0.1,      # Default 0.20
        "schema_org": 0.05,   # Default 0.10
        "url_quality": 0.05   # Default 0.15
    } # Sum should ideally be 1.0 but filter normalizes internally if not.
    
    seo_filter = SEOFilter(threshold=0.6, weights=custom_weights)

    html_strong_title_canonical = """
    <head>
        <title>Perfect Title Length For Custom SEO Weights Test (50char)</title>
        <meta name="description" content="Short description."> 
        <link rel="canonical" href="http://example.com/custom-weights" />
        <meta name="robots" content="index, follow">
    </head>
    """
    url_strong_tc = "http://example.com/custom-weights"

    html_weak_title_canonical = """
    <head>
        <title>Tiny</title> 
        <meta name="description" content="Very very very long description that will be penalized for length if that factor has weight, but here title and canonical matter most.">
        <meta name="robots" content="index, follow">
    </head>
    """ # No canonical, bad title length
    url_weak_tc = "http://example.com/weak-title-canonical"

    print(f"Testing SEOFilter with custom weights (emphasizing title_length, canonical):")
    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html') as mock_peek:
        mock_peek.return_value = html_strong_title_canonical
        passed_strong = await seo_filter.apply(url_strong_tc)
        print(f"  URL (Strong T/C): {url_strong_tc}, Passed: {passed_strong}, Score: {seo_filter._last_score:.2f}")
        assert passed_strong

        mock_peek.return_value = html_weak_title_canonical
        passed_weak = await seo_filter.apply(url_weak_tc)
        print(f"  URL (Weak T/C): {url_weak_tc}, Passed: {passed_weak}, Score: {seo_filter._last_score:.2f}")
        assert not passed_weak

asyncio.run(demo_seo_filter_weights())
```

---
## 3. URL Scorers

This section showcases how to score URLs to guide priority-based crawling strategies like `BestFirstCrawlingStrategy`.

### 3.1. `ScoringStats`

#### 3.1.1. Example: Accessing `urls_scored`, `total_score`, `min_score`, and `max_score` from a `ScoringStats` object.

```python
import asyncio
from crawl4ai.deep_crawling import KeywordRelevanceScorer # Any scorer will do

async def demo_scoring_stats():
    scorer = KeywordRelevanceScorer(keywords=["apple", "banana"])

    urls = [
        "http://example.com/apple-pie",       # Score: 0.5 (1/2 keywords)
        "http://example.com/banana-bread",    # Score: 0.5
        "http://example.com/apple-and-banana",# Score: 1.0 (2/2 keywords)
        "http://example.com/orange-juice"     # Score: 0.0
    ]
    
    scores_achieved = []
    for url in urls:
        score = await scorer.score(url) # score() updates stats
        scores_achieved.append(score)

    stats = scorer.stats
    print(f"--- Scoring Stats for KeywordRelevanceScorer ---")
    print(f"URLs Scored: {stats.urls_scored}")
    print(f"Total Score Sum: {stats.total_score:.2f}") # Sum of (score * weight), weight is 1.0 here
    print(f"Min Score: {stats.min_score:.2f}")
    print(f"Max Score: {stats.max_score:.2f}")
    print(f"Average Score: {stats.average_score:.2f}")

    assert stats.urls_scored == 4
    assert abs(stats.total_score - (0.5 + 0.5 + 1.0 + 0.0)) < 0.01
    assert abs(stats.min_score - 0.0) < 0.01
    assert abs(stats.max_score - 1.0) < 0.01
    assert abs(stats.average_score - 2.0/4) < 0.01


asyncio.run(demo_scoring_stats())
```

### 3.2. `CompositeScorer`

#### 3.2.1. Example: Creating a `CompositeScorer` with `KeywordRelevanceScorer` and `PathDepthScorer`, assigning weights.

```python
import asyncio
from crawl4ai.deep_crawling import CompositeScorer, KeywordRelevanceScorer, PathDepthScorer

async def demo_composite_scorer():
    # Scorer 1: Keyword relevance (weight 0.7)
    keyword_scorer = KeywordRelevanceScorer(keywords=["guide"], weight=0.7)
    
    # Scorer 2: Path depth, optimal at 2 (weight 0.3)
    depth_scorer = PathDepthScorer(optimal_depth=2, weight=0.3)

    composite_scorer = CompositeScorer(scorers=[keyword_scorer, depth_scorer])

    url1 = "http://example.com/guides/main-guide.html" # Keyword match, depth 2
    # kw_score = 1.0 * 0.7 = 0.7
    # depth_score (optimal_depth=2, current_depth=2, diff=0) = 1.0 * 0.3 = 0.3
    # total = 0.7 + 0.3 = 1.0
    
    url2 = "http://example.com/blog/post" # No keyword, depth 2
    # kw_score = 0.0 * 0.7 = 0.0
    # depth_score (optimal_depth=2, current_depth=2, diff=0) = 1.0 * 0.3 = 0.3
    # total = 0.0 + 0.3 = 0.3

    url3 = "http://example.com/guides/intro" # Keyword match, depth 1
    # kw_score = 1.0 * 0.7 = 0.7
    # depth_score (optimal_depth=2, current_depth=1, diff=1) = 0.5 * 0.3 = 0.15
    # total = 0.7 + 0.15 = 0.85

    score1 = await composite_scorer.score(url1)
    score2 = await composite_scorer.score(url2)
    score3 = await composite_scorer.score(url3)

    print(f"URL: {url1}, Composite Score: {score1:.4f}")
    print(f"URL: {url2}, Composite Score: {score2:.4f}")
    print(f"URL: {url3}, Composite Score: {score3:.4f}")

    assert abs(score1 - 1.0) < 0.01
    assert abs(score2 - 0.3) < 0.01
    assert abs(score3 - 0.85) < 0.01
    
    # Check stats of individual scorers and composite
    print(f"\nComposite Scorer Stats: Avg={composite_scorer.stats.average_score:.2f}")
    print(f"  Keyword Scorer Stats: Avg={keyword_scorer.stats.average_score:.2f}")
    print(f"  Depth Scorer Stats: Avg={depth_scorer.stats.average_score:.2f}")


asyncio.run(demo_composite_scorer())
```

#### 3.2.2. Example: `CompositeScorer` with `normalize=True` to scale scores.

When `normalize=True`, the final score is divided by the number of scorers if there are scorers, ensuring it stays roughly in the 0-1 range if individual scorers are also in that range.

```python
import asyncio
from crawl4ai.deep_crawling import CompositeScorer, KeywordRelevanceScorer, PathDepthScorer

async def demo_composite_scorer_normalized():
    keyword_scorer = KeywordRelevanceScorer(keywords=["guide"], weight=1.0) # Unweighted for clarity
    depth_scorer = PathDepthScorer(optimal_depth=2, weight=1.0)       # Unweighted for clarity

    # With normalize=True, final score will be (kw_score + depth_score) / 2
    composite_scorer_norm = CompositeScorer(
        scorers=[keyword_scorer, depth_scorer], 
        normalize=True
    )

    url1 = "http://example.com/guides/main-guide.html" # Keyword match, depth 2
    # kw_score = 1.0
    # depth_score = 1.0
    # total_raw = 2.0; normalized = 2.0 / 2 = 1.0
    
    url2 = "http://example.com/blog/post" # No keyword, depth 2
    # kw_score = 0.0
    # depth_score = 1.0
    # total_raw = 1.0; normalized = 1.0 / 2 = 0.5

    score1_norm = await composite_scorer_norm.score(url1)
    score2_norm = await composite_scorer_norm.score(url2)

    print(f"URL: {url1}, Normalized Composite Score: {score1_norm:.4f}")
    print(f"URL: {url2}, Normalized Composite Score: {score2_norm:.4f}")
    
    assert abs(score1_norm - 1.0) < 0.01
    assert abs(score2_norm - 0.5) < 0.01

asyncio.run(demo_composite_scorer_normalized())
```

### 3.3. `KeywordRelevanceScorer`

#### 3.3.1. Example: Scoring URLs based on the presence of specific keywords.

```python
import asyncio
from crawl4ai.deep_crawling import KeywordRelevanceScorer

async def demo_keyword_relevance_scorer():
    scorer = KeywordRelevanceScorer(keywords=["apple", "banana", "cherry"])

    url1 = "http://example.com/apple-pie-recipe" # 1 keyword
    url2 = "http://example.com/banana-and-cherry-smoothie" # 2 keywords
    url3 = "http://example.com/orange-juice" # 0 keywords
    url4 = "http://example.com/apple-banana-cherry-fruit-salad" # 3 keywords

    score1 = await scorer.score(url1)
    score2 = await scorer.score(url2)
    score3 = await scorer.score(url3)
    score4 = await scorer.score(url4)

    print(f"URL: {url1}, Score: {score1:.2f} (Expected: ~0.33)")
    print(f"URL: {url2}, Score: {score2:.2f} (Expected: ~0.67)")
    print(f"URL: {url3}, Score: {score3:.2f} (Expected: 0.00)")
    print(f"URL: {url4}, Score: {score4:.2f} (Expected: 1.00)")
    
    assert abs(score1 - 1/3) < 0.01
    assert abs(score2 - 2/3) < 0.01
    assert abs(score3 - 0.0) < 0.01
    assert abs(score4 - 1.0) < 0.01

asyncio.run(demo_keyword_relevance_scorer())
```

#### 3.3.2. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.

```python
import asyncio
from crawl4ai.deep_crawling import KeywordRelevanceScorer

async def demo_keyword_relevance_case_sensitive():
    # Case-sensitive matching
    scorer_cs = KeywordRelevanceScorer(keywords=["Apple"], case_sensitive=True)
    # Default: case-insensitive
    scorer_ci = KeywordRelevanceScorer(keywords=["Apple"])


    url_exact_match = "http://example.com/Apple-iPhone"
    url_lowercase_match = "http://example.com/apple-ipad"
    url_no_match = "http://example.com/orange-device"

    print("--- Case-Sensitive Scorer (keyword: 'Apple') ---")
    print(f"URL: {url_exact_match}, Score: {await scorer_cs.score(url_exact_match):.2f}")
    print(f"URL: {url_lowercase_match}, Score: {await scorer_cs.score(url_lowercase_match):.2f}")
    print(f"URL: {url_no_match}, Score: {await scorer_cs.score(url_no_match):.2f}")

    assert abs(await scorer_cs.score(url_exact_match) - 1.0) < 0.01
    assert abs(await scorer_cs.score(url_lowercase_match) - 0.0) < 0.01
    assert abs(await scorer_cs.score(url_no_match) - 0.0) < 0.01

    print("\n--- Case-Insensitive Scorer (keyword: 'Apple') ---")
    print(f"URL: {url_exact_match}, Score: {await scorer_ci.score(url_exact_match):.2f}")
    print(f"URL: {url_lowercase_match}, Score: {await scorer_ci.score(url_lowercase_match):.2f}")
    
    assert abs(await scorer_ci.score(url_exact_match) - 1.0) < 0.01
    assert abs(await scorer_ci.score(url_lowercase_match) - 1.0) < 0.01

asyncio.run(demo_keyword_relevance_case_sensitive())
```

### 3.4. `PathDepthScorer`

#### 3.4.1. Example: `PathDepthScorer` with an `optimal_depth` of 2.

```python
import asyncio
from crawl4ai.deep_crawling import PathDepthScorer

async def demo_path_depth_scorer_optimal_2():
    scorer = PathDepthScorer(optimal_depth=2)

    url_depth0 = "http://example.com"                  # diff = 2, score ~0.33
    url_depth1 = "http://example.com/category"         # diff = 1, score 0.5
    url_depth2 = "http://example.com/category/product" # diff = 0, score 1.0
    url_depth3 = "http://example.com/cat/prod/details" # diff = 1, score 0.5
    url_depth4 = "http://example.com/cat/prod/det/rev" # diff = 2, score ~0.33

    print(f"PathDepthScorer with optimal_depth=2:")
    print(f"URL: {url_depth0} (depth 0), Score: {await scorer.score(url_depth0):.3f}")
    print(f"URL: {url_depth1} (depth 1), Score: {await scorer.score(url_depth1):.3f}")
    print(f"URL: {url_depth2} (depth 2), Score: {await scorer.score(url_depth2):.3f}")
    print(f"URL: {url_depth3} (depth 3), Score: {await scorer.score(url_depth3):.3f}")
    print(f"URL: {url_depth4} (depth 4), Score: {await scorer.score(url_depth4):.3f}")
    
    assert abs(await scorer.score(url_depth2) - 1.0) < 0.01
    assert abs(await scorer.score(url_depth1) - 0.5) < 0.01
    assert abs(await scorer.score(url_depth3) - 0.5) < 0.01

asyncio.run(demo_path_depth_scorer_optimal_2())
```

#### 3.4.2. Example: Demonstrating how scores decrease as path depth deviates from `optimal_depth`.
(This is covered by 3.4.1.)

### 3.5. `ContentTypeScorer`

#### 3.5.1. Example: `ContentTypeScorer` prioritizing ".html" and ".pdf" URLs over others based on `type_weights`.
*Note: This scorer relies on URL extensions primarily, as live HEAD requests are not part of its `score` method.*

```python
import asyncio
from crawl4ai.deep_crawling import ContentTypeScorer

async def demo_content_type_scorer():
    type_weights = {
        ".html": 1.0,
        ".pdf": 0.8,
        ".jpg": 0.2,
        # Other types get default 0.1 (not explicitly shown here)
    }
    scorer = ContentTypeScorer(type_weights=type_weights)

    url_html = "http://example.com/index.html"
    url_pdf = "http://example.com/document.pdf"
    url_jpg = "http://example.com/image.jpg"
    url_txt = "http://example.com/notes.txt" # Not in weights, gets default 0.1

    print(f"ContentTypeScorer with weights: {type_weights}")
    print(f"URL: {url_html}, Score: {await scorer.score(url_html):.2f}")
    print(f"URL: {url_pdf}, Score: {await scorer.score(url_pdf):.2f}")
    print(f"URL: {url_jpg}, Score: {await scorer.score(url_jpg):.2f}")
    print(f"URL: {url_txt}, Score: {await scorer.score(url_txt):.2f}")
    
    assert abs(await scorer.score(url_html) - 1.0) < 0.01
    assert abs(await scorer.score(url_pdf) - 0.8) < 0.01
    assert abs(await scorer.score(url_jpg) - 0.2) < 0.01
    assert abs(await scorer.score(url_txt) - 0.1) < 0.01 # Default for unlisted

asyncio.run(demo_content_type_scorer())
```

### 3.6. `FreshnessScorer`

#### 3.6.1. Example: `FreshnessScorer` scoring URLs with recent years higher.

```python
import asyncio
from datetime import datetime
from crawl4ai.deep_crawling import FreshnessScorer

async def demo_freshness_scorer():
    current_year = datetime.now().year
    scorer = FreshnessScorer(current_year=current_year)

    url_current_year = f"http://example.com/news/{current_year}/article"
    url_last_year = f"http://example.com/archive/{current_year - 1}/report"
    url_five_years_ago = f"http://example.com/blog/{current_year - 5}/post"
    url_ten_years_ago = f"http://example.com/old/{current_year - 10}/story"
    url_no_year = "http://example.com/static-page"

    print(f"FreshnessScorer (current year: {current_year}):")
    score_current = await scorer.score(url_current_year)
    score_last = await scorer.score(url_last_year)
    score_five_ago = await scorer.score(url_five_years_ago)
    score_ten_ago = await scorer.score(url_ten_years_ago)
    score_no_year = await scorer.score(url_no_year)
    
    print(f"URL: {url_current_year}, Score: {score_current:.2f} (Expected: 1.0)")
    print(f"URL: {url_last_year}, Score: {score_last:.2f} (Expected: 0.9)")
    print(f"URL: {url_five_years_ago}, Score: {score_five_ago:.2f} (Expected: 0.5)")
    print(f"URL: {url_ten_years_ago}, Score: {score_ten_ago:.2f} (Expected: ~0.1)") # 1.0 - 10 * 0.1 = 0, capped at 0.1
    print(f"URL: {url_no_year}, Score: {score_no_year:.2f} (Expected: 0.5 - default)")

    assert abs(score_current - 1.0) < 0.01
    assert abs(score_last - 0.9) < 0.01
    assert abs(score_five_ago - 0.5) < 0.01
    assert abs(score_ten_ago - 0.1) < 0.01 # Max(0.1, 1.0 - 10*0.1)
    assert abs(score_no_year - 0.5) < 0.01


asyncio.run(demo_freshness_scorer())
```

#### 3.6.2. Example: Demonstrating `FreshnessScorer` with a custom `current_year`.

```python
import asyncio
from crawl4ai.deep_crawling import FreshnessScorer

async def demo_freshness_scorer_custom_year():
    # Pretend it's 2030 for scoring purposes
    custom_current_year = 2030
    scorer = FreshnessScorer(current_year=custom_current_year)

    url_actually_2024 = "http://example.com/event/2024/details" # 6 years old relative to 2030
    # Expected score: max(0.1, 1.0 - 6 * 0.1) = max(0.1, 0.4) = 0.4
    # If using lookup: index 6 is out of bounds, so fallback calculation.
    # _FRESHNESS_SCORES has 6 elements (index 0-5). Year diff 6 is 1.0 - 6*0.1 = 0.4

    score_2024 = await scorer.score(url_actually_2024)
    print(f"FreshnessScorer (custom current year: {custom_current_year}):")
    print(f"URL: {url_actually_2024}, Score: {score_2024:.2f}")
    
    assert abs(score_2024 - 0.4) < 0.01

asyncio.run(demo_freshness_scorer_custom_year())
```

### 3.7. `DomainAuthorityScorer`

#### 3.7.1. Example: `DomainAuthorityScorer` with a custom `domain_weights` dictionary.

```python
import asyncio
from crawl4ai.deep_crawling import DomainAuthorityScorer

async def demo_domain_authority_custom_weights():
    domain_weights = {
        "wikipedia.org": 0.9,
        "github.com": 0.8,
        "example.com": 0.5  # Default is 0.5, but explicitly setting
    }
    scorer = DomainAuthorityScorer(domain_weights=domain_weights)

    url_wiki = "https://en.wikipedia.org/wiki/Web_scraping"
    url_github = "https://github.com/crawl4ai/crawl4ai"
    url_example = "http://www.example.com/somepage" # Subdomain is fine
    url_unknown = "http://unknownsite.net/path"

    print(f"DomainAuthorityScorer with custom weights:")
    print(f"URL: {url_wiki}, Score: {await scorer.score(url_wiki):.2f}")
    print(f"URL: {url_github}, Score: {await scorer.score(url_github):.2f}")
    print(f"URL: {url_example}, Score: {await scorer.score(url_example):.2f}")
    print(f"URL: {url_unknown}, Score: {await scorer.score(url_unknown):.2f} (default weight)")

    assert abs(await scorer.score(url_wiki) - 0.9) < 0.01
    assert abs(await scorer.score(url_github) - 0.8) < 0.01
    assert abs(await scorer.score(url_example) - 0.5) < 0.01
    assert abs(await scorer.score(url_unknown) - scorer.default_weight) < 0.01 # default_weight is 0.5

asyncio.run(demo_domain_authority_custom_weights())
```

#### 3.7.2. Example: `DomainAuthorityScorer` showing the effect of `default_weight`.

```python
import asyncio
from crawl4ai.deep_crawling import DomainAuthorityScorer

async def demo_domain_authority_default_weight():
    # No custom weights, so all domains get default_weight unless they are in the pre-cached top domains
    # Default_weight defaults to 0.5
    scorer_default = DomainAuthorityScorer() 
    
    # Custom default weight
    scorer_custom_default = DomainAuthorityScorer(default_weight=0.3)

    url_random1 = "http://random-new-site.xyz/page"
    url_random2 = "http://another-unknown-domain.io/blog"

    print(f"DomainAuthorityScorer with default_weight={scorer_default.default_weight}:")
    print(f"  URL: {url_random1}, Score: {await scorer_default.score(url_random1):.2f}")
    
    print(f"\nDomainAuthorityScorer with custom default_weight={scorer_custom_default.default_weight}:")
    print(f"  URL: {url_random2}, Score: {await scorer_custom_default.score(url_random2):.2f}")

    # Check if the score matches the default_weight for domains not in the top_domains cache
    # (Assuming these random domains are not in the small pre-cached list)
    assert abs(await scorer_default.score(url_random1) - 0.5) < 0.01 
    assert abs(await scorer_custom_default.score(url_random2) - 0.3) < 0.01

asyncio.run(demo_domain_authority_default_weight())
```

---
## 4. Integration Examples

Showcasing how different deep crawling components can be combined.

### 4.1. Example: `BFSDeeepCrawlStrategy` with a `FilterChain` (`DomainFilter` + `URLPatternFilter`) and a simple `PathDepthScorer`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import (
    BFSDeeepCrawlStrategy, FilterChain, DomainFilter, URLPatternFilter, PathDepthScorer
)

RAW_HTML_INTEGRATION_1 = """
<html><body>
    <a href="http://example.com/docs/page1.html">Docs Page 1</a>
    <a href="http://example.com/blog/post1.html">Blog Post 1 (depth 2)</a>
    <a href="http://example.com/docs/page2.pdf">Docs Page 2 (PDF)</a>
    <a href="http://anotherexample.com/external.html">External HTML</a>
    <a href="http://example.com/very/deep/page.html">Deep Page (depth 3)</a>
</body></html>
"""
START_URL_INTEGRATION_1 = f"raw://{RAW_HTML_INTEGRATION_1}"

async def bfs_integration_example():
    # Filters: Only example.com, only .html files
    filter_chain = FilterChain(filters=[
        DomainFilter(allowed_domains=["example.com"]),
        URLPatternFilter(patterns=["*.html"])
    ])
    
    # Scorer: Prioritize pages with path depth 1 (e.g. /docs/)
    # Threshold will allow only high-scored links based on this.
    # Optimal depth 1, score for depth 1 = 1.0; depth 2 = 0.5; depth 3 = 0.33
    url_scorer = PathDepthScorer(optimal_depth=1)
    score_threshold = 0.6 # Allows only depth 1 pages

    bfs_strategy = BFSDeeepCrawlStrategy(
        max_depth=2, 
        max_pages=5,
        filter_chain=filter_chain,
        url_scorer=url_scorer,
        score_threshold=score_threshold
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    async with AsyncWebCrawler() as crawler:
        print("Starting BFS integration crawl...")
        results_list = await crawler.arun(url=START_URL_INTEGRATION_1, config=run_config)
        
        print(f"\n--- BFS Integration Results ---")
        crawled_urls = []
        if results_list:
            for result in results_list:
                if result.success:
                    print(f"Crawled: {result.url} (Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score')})")
                    crawled_urls.append(result.url)
            
            # Expected: Only "http://example.com/docs/page1.html" (depth 1, .html, on domain, score 1.0)
            # "/blog/post1.html" is depth 2, score 0.5 (below threshold)
            # "/docs/page2.pdf" fails pattern filter
            # "external.html" fails domain filter
            # "/very/deep/page.html" is depth 3, score 0.33 (below threshold)
            assert "http://example.com/docs/page1.html" in crawled_urls
            assert len(crawled_urls) == 2 # Start URL + one matched page
            assert "http://example.com/blog/post1.html" not in crawled_urls
            assert "http://example.com/docs/page2.pdf" not in crawled_urls
            assert "http://anotherexample.com/external.html" not in crawled_urls
            assert "http://example.com/very/deep/page.html" not in crawled_urls
        else:
            print("No results.")

asyncio.run(bfs_integration_example())
```

### 4.2. Example: `BestFirstCrawlingStrategy` driven by a `CompositeScorer` (`KeywordRelevanceScorer` + `FreshnessScorer`) and filtered by `ContentTypeFilter`.

```python
import asyncio
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import (
    BestFirstCrawlingStrategy, CompositeScorer, KeywordRelevanceScorer, 
    FreshnessScorer, ContentTypeFilter
)

RAW_HTML_INTEGRATION_2 = f"""
<html><body>
    <a href="/news/{datetime.now().year}/ai-breakthrough.html">AI Breakthrough ({datetime.now().year})</a>
    <a href="/news/{datetime.now().year-1}/old-ai-news.html">Old AI News ({datetime.now().year-1})</a>
    <a href="/tech/general-tech.html">General Tech ({datetime.now().year})</a>
    <a href="/news/{datetime.now().year}/ai-update.pdf">AI Update PDF ({datetime.now().year})</a>
</body></html>
"""
START_URL_INTEGRATION_2 = f"raw://{RAW_HTML_INTEGRATION_2}"

async def best_first_integration_example():
    # Scorers
    keyword_scorer = KeywordRelevanceScorer(keywords=["ai"], weight=0.6)
    freshness_scorer = FreshnessScorer(current_year=datetime.now().year, weight=0.4)
    composite_scorer = CompositeScorer(scorers=[keyword_scorer, freshness_scorer])

    # Filter: Only HTML content
    content_filter = ContentTypeFilter(allowed_types=["text/html"])
    filter_chain = FilterChain(filters=[content_filter])

    strategy = BestFirstCrawlingStrategy(
        max_depth=1,
        max_pages=3, # Start URL + 2 more
        url_scorer=composite_scorer,
        filter_chain=filter_chain
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, stream=True) # Stream to see order

    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First integration crawl...")
        crawled_items_in_order = []
        async for result in await crawler.arun(url=START_URL_INTEGRATION_2, config=run_config):
            if result.success:
                print(f"Crawled: {result.url_for_display()} (Score: {result.metadata.get('score'):.2f}, Depth: {result.metadata.get('depth')})")
                crawled_items_in_order.append(result.url_for_display())
        
        print(f"\nCrawled order: {crawled_items_in_order}")
        # Expected:
        # 1. Start URL
        # 2. AI Breakthrough (current year, "ai" keyword) - Highest score
        # 3. General Tech (current year, no "ai") OR Old AI News (last year, "ai" keyword)
        #    - AI Breakthrough: kw_score=1*0.6=0.6, fresh_score=1*0.4=0.4. Total=1.0
        #    - Old AI News: kw_score=1*0.6=0.6, fresh_score=0.9*0.4=0.36. Total=0.96
        #    - General Tech: kw_score=0*0.6=0.0, fresh_score=1*0.4=0.4. Total=0.4
        #    - AI Update PDF: Filtered out by ContentTypeFilter
        # So, order after start URL should be AI Breakthrough, then Old AI News.
        if len(crawled_items_in_order) >= 3: # Start URL + 2 others
            assert "ai-breakthrough.html" in crawled_items_in_order[1]
            assert "old-ai-news.html" in crawled_items_in_order[2]
        assert not any("ai-update.pdf" in url for url in crawled_items_in_order)

asyncio.run(best_first_integration_example())
```

### 4.3. Example: `DFSDeeepCrawlStrategy` with `max_pages` and `max_depth` to show interaction.
(This has been demonstrated in 1.4.2.1 and 1.4.2.2 where either `max_depth` or `max_pages` can limit the crawl first.)

### 4.4. Example: Using `DeepCrawlDecorator` to initiate a `BFSDeeepCrawlStrategy` with a `FilterChain` including `ContentRelevanceFilter`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import (
    BFSDeeepCrawlStrategy, FilterChain, ContentRelevanceFilter
)
from unittest.mock import patch # To mock HeadPeek

async def decorator_bfs_content_relevance():
    query = "important information"
    # Filter chain with content relevance
    filter_chain = FilterChain(filters=[
        ContentRelevanceFilter(query=query, threshold=0.1)
    ])

    bfs_strategy = BFSDeeepCrawlStrategy(
        max_depth=1, 
        max_pages=3,
        filter_chain=filter_chain
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)

    html_relevant_head = "<head><title>Key Info</title><meta name='description' content='This page contains important information about our services.'></head>"
    html_irrelevant_head = "<head><title>Random Stuff</title><meta name='description' content='Some other details.'></head>"
    
    start_html = f"""<html><body>
        <a href="raw_relevant.html">Relevant Link</a>
        <a href="raw_irrelevant.html">Irrelevant Link</a>
    </body></html>"""
    
    # Create self-contained raw URLs (simplified)
    relevant_page_content = f"raw://{html_relevant_head}<body>Relevant content</body></html>"
    irrelevant_page_content = f"raw://{html_irrelevant_head}<body>Irrelevant content</body></html>"
    
    start_url_content_rel = start_html.replace("raw_relevant.html", relevant_page_content.replace('"', '&quot;')) \
                                     .replace("raw_irrelevant.html", irrelevant_page_content.replace('"', '&quot;'))
    start_url_content_rel = f"raw://{start_url_content_rel}"


    # Mock HeadPeek.peek_html to control responses for filter
    # This mapping helps return the correct head content for each URL
    mock_heads = {
        relevant_page_content: html_relevant_head, # Key by the full raw URL string
        irrelevant_page_content: html_irrelevant_head
    }
    
    async def mock_peek_html_func(url_to_peek):
        # For raw URLs, the actual URL is the content itself. We need to find the one that matches.
        for mock_url_key, head_content in mock_heads.items():
            if mock_url_key == url_to_peek: # Direct match for raw URLs
                 return head_content
        return "" # Default for unexpected URLs

    with patch('crawl4ai.deep_crawling.filters.HeadPeek.peek_html', side_effect=mock_peek_html_func):
        async with AsyncWebCrawler() as crawler: # Decorator is active via run_config
            print("Starting Decorator + BFS + ContentRelevanceFilter crawl...")
            results_list = await crawler.arun(url=start_url_content_rel, config=run_config)
            
            crawled_urls = []
            if results_list:
                print("\n--- Crawl Results ---")
                for result in results_list:
                    if result.success:
                        print(f"Crawled: {result.url_for_display()}")
                        crawled_urls.append(result.url_for_display())
                
                assert any(u == relevant_page_content for u in crawled_urls), "Relevant page should have been crawled."
                assert not any(u == irrelevant_page_content for u in crawled_urls), "Irrelevant page should have been filtered."
            else:
                print("No results.")

asyncio.run(decorator_bfs_content_relevance())
```

### 4.5. Example: `BestFirstCrawlingStrategy` using a `url_scorer` that includes `DomainAuthorityScorer` to prioritize high-authority external links when `include_external=True`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import (
    BestFirstCrawlingStrategy, DomainAuthorityScorer
)

RAW_HTML_INTEGRATION_DOMAIN_AUTH = """
<html><body>
    <a href="https://www.wikipedia.org">Wikipedia (High Auth)</a>
    <a href="http://example-blog.blogspot.com">Personal Blog (Low Auth)</a>
    <a href="https://github.com/crawl4ai">Crawl4AI GitHub (Mid/High Auth)</a>
</body></html>
"""
START_URL_INTEGRATION_DA = f"raw://{RAW_HTML_INTEGRATION_DOMAIN_AUTH}"

async def best_first_domain_authority_external():
    # Scorer: Prioritize by domain authority
    # Note: DomainAuthorityScorer has some pre-defined weights, wikipedia.org and github.com are usually high.
    domain_scorer = DomainAuthorityScorer() 

    strategy = BestFirstCrawlingStrategy(
        max_depth=1,
        max_pages=3, # Start URL + 2 external links
        url_scorer=domain_scorer,
        include_external=True # Crucial for this example
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, stream=True)

    async with AsyncWebCrawler() as crawler:
        print("Starting Best-First crawl prioritizing high-authority external links...")
        crawled_urls_in_order = []
        async for result in await crawler.arun(url=START_URL_INTEGRATION_DA, config=run_config):
            if result.success:
                score = result.metadata.get('score', 'N/A')
                print(f"Crawled: {result.url} (Score: {score:.2f})")
                crawled_urls_in_order.append(result.url)
        
        print(f"\nCrawled order (after start URL): {[url.split('//')[1] for url in crawled_urls_in_order[1:]]}")
        
        # Expected order (after start URL): wikipedia.org, then github.com (or vice-versa), then blogspot.com
        # This depends on the default scores in DomainAuthorityScorer.
        # Wikipedia usually has a very high score.
        if len(crawled_urls_in_order) > 1:
            assert "wikipedia.org" in crawled_urls_in_order[1], "Wikipedia (high authority) should be prioritized."
        if len(crawled_urls_in_order) > 2:
             assert "github.com" in crawled_urls_in_order[2] or "blogspot.com" in crawled_urls_in_order[2]

asyncio.run(best_first_domain_authority_external())
```

---
## 5. Advanced Scenarios & Edge Cases

### 5.1. Example: Deep crawling a site where initial pages have no links, but deeper pages (found via alternative means, if simulatable) do.
*This scenario is hard to demonstrate without a mock server or very specific site structure. The core idea is that if the `start_url` itself has no links, the crawl stops unless the strategy has other means to find URLs (not typical for these strategies without custom `link_discovery` or being fed URLs externally).*
*A conceptual example: if `arun` was called with multiple start URLs, and one of them had links.*

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy

RAW_HTML_NO_LINKS = "<html><body>No links here.</body></html>"
RAW_HTML_WITH_LINKS = "<html><body><a href='http://example.com/final'>Final Page</a></body></html>"

# Create a scenario where the first URL has no links, but we want to show that if other URLs
# were added to the queue (e.g., from a different source or a modified strategy), they'd be processed.
# This tests the strategy's ability to continue if the queue is populated externally/later.
# For this example, we'll use a simple BFS and "manually" add to its queue for demo.

async def advanced_no_initial_links():
    strategy = BFSDeeepCrawlStrategy(max_depth=1, max_pages=3)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy)
    
    start_url_no_links = f"raw://{RAW_HTML_NO_LINKS}"
    second_url_with_links = f"raw://{RAW_HTML_WITH_LINKS}"

    async with AsyncWebCrawler() as crawler:
        print(f"Attempting crawl starting with URL that has no links: {start_url_no_links}")
        
        # First, try to crawl the page with no links. Expected: only start_url is processed.
        results1_container = await crawler.arun(url=start_url_no_links, config=run_config)
        results1 = [res async for res in results1_container] if hasattr(results1_container, '__aiter__') else results1_container

        print(f"Results from first crawl (no links): {[r.url_for_display() for r in results1]}")
        assert len(results1) == 1

        # Now, let's conceptualize how a strategy might continue if new URLs are added.
        # The actual strategies' queues are internal. We'll simulate a new crawl.
        # If the strategy's queue was externally populated, it would process them.
        # For a direct test, we'd need to modify the strategy or use a more complex setup.
        # This is more about the crawler's ability to handle an empty next_level from one source.
        
        print(f"\nSimulating a scenario where another URL with links is processed by the *same strategy instance* (if possible):")
        # To truly test this with the same strategy instance maintaining state,
        # we'd need to call its internal methods or re-architect this test.
        # For simplicity, we show that a *new* crawl with a populated start will work.
        
        results2_container = await crawler.arun(url=second_url_with_links, config=run_config)
        results2 = [res async for res in results2_container] if hasattr(results2_container, '__aiter__') else results2_container

        print(f"Results from second crawl (with links):")
        found_final_page = False
        for r in results2:
            print(f"  - {r.url_for_display()} (Depth: {r.metadata.get('depth')})")
            if "example.com/final" in r.url_for_display():
                found_final_page = True
        assert len(results2) >= 2 # start URL + final page
        assert found_final_page

asyncio.run(advanced_no_initial_links())
```

### 5.2. Example: Handling a `score_threshold` so high in `BFSDeeepCrawlStrategy` or `BestFirstCrawlingStrategy` that no new links are added.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeeepCrawlStrategy, PathDepthScorer

RAW_HTML_FOR_HIGH_THRESHOLD = """
<html><body>
    <a href="/page1">Page 1</a>
    <a href="/category/page2">Page 2</a>
</body></html>
"""
START_URL_RAW_HIGH_THRESHOLD = f"raw://{RAW_HTML_FOR_HIGH_THRESHOLD}"

async def high_score_threshold_no_new_links():
    # Scorer that gives scores between 0 and 1
    url_scorer = PathDepthScorer(optimal_depth=0) 
    
    # Set a threshold higher than any possible score (e.g., PathDepthScorer max is 1.0)
    high_threshold = 1.1 

    strategy = BFSDeeepCrawlStrategy( # Could also be BestFirstCrawlingStrategy
        max_depth=1, 
        max_pages=5,
        url_scorer=url_scorer,
        score_threshold=high_threshold
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy)

    async with AsyncWebCrawler() as crawler:
        print(f"Starting crawl with very high score_threshold ({high_threshold})...")
        results_list = await crawler.arun(url=START_URL_RAW_HIGH_THRESHOLD, config=run_config)
        
        print(f"\n--- Crawl Results (High Threshold) ---")
        crawled_urls_count = 0
        if results_list:
            for result in results_list:
                if result.success:
                    crawled_urls_count +=1
                    print(f"Crawled: {result.url_for_display()} (Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 'N/A')})")
            
            # Expect only the start URL to be crawled, as all discovered links will fail threshold
            assert crawled_urls_count == 1, "Only the start URL should be crawled with such a high threshold."
            assert strategy.stats.urls_skipped > 0, "Links should have been skipped due to threshold."
            print(f"URLs skipped by scorer: {strategy.stats.urls_skipped}")
        else:
            print("No results (unexpected).")

asyncio.run(high_score_threshold_no_new_links())
```

### 5.3. Example: Demonstrating the behavior of `max_pages` in BFS when a level has more links than the remaining page capacity.
(This was covered effectively by 1.3.5.2, which simulates the internal logic of `link_discovery` respecting `max_pages`.)

### 5.4. Example: `URLPatternFilter` with complex regex to match very specific URL structures.

```python
import asyncio
from crawl4ai.deep_crawling import URLPatternFilter

async def complex_regex_url_pattern():
    # Regex to match URLs like: /archive/YYYY/MM/DD/article-slug-with-hyphens.html
    # Where YYYY is 20xx, MM is 01-12, DD is 01-31.
    complex_pattern = r"/archive/20\d{2}/(0[1-9]|1[0-2])/(0[1-9]|[12]\d|3[01])/[a-z0-9-]+(\.html)?$"
    
    archive_filter = URLPatternFilter(patterns=[complex_pattern])

    urls_to_test = [
        "http://example.com/archive/2023/05/15/my-great-article.html", # Match
        "http://example.com/archive/2024/12/31/another-post",          # Match (optional .html)
        "http://example.com/archive/2022/13/01/invalid-month.html",    # No Match (invalid month)
        "http://example.com/blog/2023/05/15/my-great-article.html",    # No Match (wrong base path)
        "http://example.com/archive/1999/01/01/old-article.html"      # No Match (year doesn't start with 20)
    ]

    print(f"Testing with complex regex pattern: {complex_pattern}")
    for url in urls_to_test:
        passed = await archive_filter.apply(url)
        print(f"  URL: {url}, Passed: {passed}")

    assert await archive_filter.apply(urls_to_test[0]) == True
    assert await archive_filter.apply(urls_to_test[1]) == True
    assert await archive_filter.apply(urls_to_test[2]) == False
    assert await archive_filter.apply(urls_to_test[3]) == False
    assert await archive_filter.apply(urls_to_test[4]) == False

asyncio.run(complex_regex_url_pattern())
```

### 5.5. Example: A `FilterChain` where one filter passes a URL but a subsequent filter rejects it.

```python
import asyncio
from crawl4ai.deep_crawling import FilterChain, DomainFilter, URLPatternFilter

async def filter_chain_sequential_rejection():
    # Filter 1: Allow 'example.com' (will pass 'http://example.com/admin/login')
    domain_filter = DomainFilter(allowed_domains=["example.com"])
    
    # Filter 2: Reject anything with 'admin' (will reject 'http://example.com/admin/login')
    no_admin_filter = URLPatternFilter(patterns=["*admin*"], reverse=True)
    
    filter_chain = FilterChain(filters=[domain_filter, no_admin_filter])

    url_admin_on_domain = "http://example.com/admin/login"
    url_safe_on_domain = "http://example.com/dashboard"

    print("Testing FilterChain: DomainFilter (allow example.com) -> URLPatternFilter (reject *admin*)")
    
    passed_admin = await filter_chain.apply(url_admin_on_domain)
    print(f"URL: {url_admin_on_domain}, Passed Chain: {passed_admin}")
    print(f"  DomainFilter passed: {await domain_filter.apply(url_admin_on_domain)}") # Apply individually to see
    print(f"  NoAdminFilter passed: {await no_admin_filter.apply(url_admin_on_domain)}")
    assert not passed_admin # Should be rejected by the second filter

    passed_safe = await filter_chain.apply(url_safe_on_domain)
    print(f"URL: {url_safe_on_domain}, Passed Chain: {passed_safe}")
    print(f"  DomainFilter passed: {await domain_filter.apply(url_safe_on_domain)}")
    print(f"  NoAdminFilter passed: {await no_admin_filter.apply(url_safe_on_domain)}")
    assert passed_safe # Should pass both filters

asyncio.run(filter_chain_sequential_rejection())
```

---
End of Examples Document.
```

---


## Deployment - Memory
Source: crawl4ai_deployment_memory_content.llm.md

```markdown
# Detailed Outline for crawl4ai - deployment Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_deployment.md`
**Library Version Context:** 0.6.0 (as per Dockerfile ARG `C4AI_VER` from provided `Dockerfile` content)
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Deployment
    * 1.1. Purpose: This document provides a factual reference for installing the `crawl4ai` library and deploying its server component using Docker. It covers basic and advanced library installation, various Docker deployment methods, server configuration, and an overview of the API for interaction.
    * 1.2. Scope:
        * Installation of the `crawl4ai` Python library.
        * Setup and diagnostic commands for the library.
        * Deployment of the `crawl4ai` server using Docker, including pre-built images, Docker Compose, and manual builds.
        * Explanation of Dockerfile parameters and server configuration via `config.yml`.
        * Details of API interaction, including the Playground UI, Python SDK, and direct REST API calls.
        * Overview of additional server API endpoints and Model Context Protocol (MCP) support.
        * High-level understanding of the server's internal logic relevant to users.
        * The library's version numbering scheme.

## 2. Library Installation

    * 2.1. **Basic Library Installation**
        * 2.1.1. Standard Installation
            * Command: `pip install crawl4ai`
            * Purpose: Installs the core `crawl4ai` library and its essential dependencies for performing web crawling and scraping tasks. This provides the fundamental `AsyncWebCrawler` and related configuration objects.
        * 2.1.2. Post-Installation Setup
            * Command: `crawl4ai-setup`
            * Purpose:
                * Initializes the user's home directory structure for Crawl4ai (e.g., `~/.crawl4ai/cache`).
                * Installs or updates necessary Playwright browsers (Chromium is installed by default) required for browser-based crawling. The `crawl4ai-setup` script internally calls `playwright install --with-deps chromium`.
                * Performs OS-level checks for common missing libraries that Playwright might depend on, providing guidance if issues are found.
                * Creates a default `global.yml` configuration file if one doesn't exist.
        * 2.1.3. Diagnostic Check
            * Command: `crawl4ai-doctor`
            * Purpose:
                * Verifies Python version compatibility.
                * Confirms Playwright installation and browser integrity by attempting a simple crawl of `https://crawl4ai.com`.
                * Inspects essential environment variables and potential library conflicts that might affect Crawl4ai's operation.
                * Provides diagnostic messages indicating success or failure of these checks, with suggestions for resolving common issues.
        * 2.1.4. Verification Process
            * Purpose: To confirm that the basic installation and setup were successful and Crawl4ai can perform a simple crawl.
            * Script Example (as inferred from `crawl4ai-doctor` logic and typical usage):
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

                async def main():
                    browser_config = BrowserConfig(
                        headless=True,
                        browser_type="chromium",
                        ignore_https_errors=True,
                        light_mode=True,
                        viewport_width=1280,
                        viewport_height=720,
                    )
                    run_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        screenshot=True,
                    )
                    async with AsyncWebCrawler(config=browser_config) as crawler:
                        print("Testing crawling capabilities...")
                        result = await crawler.arun(url="https://crawl4ai.com", config=run_config)
                        if result and result.markdown:
                            print("✅ Crawling test passed!")
                            return True
                        else:
                            print("❌ Test failed: Failed to get content")
                            return False

                if __name__ == "__main__":
                    asyncio.run(main())
                ```
            * Expected Outcome: The script should print "✅ Crawling test passed!" and successfully output Markdown content from the crawled page.

    * 2.2. **Advanced Library Installation (Optional Features)**
        * 2.2.1. Installation of Optional Extras
            * Purpose: To install additional dependencies required for specific advanced features of Crawl4ai, such as those involving machine learning models.
            * Options (as defined in `pyproject.toml`):
                * `pip install crawl4ai[pdf]`:
                    * Purpose: Installs `PyPDF2` for PDF processing capabilities.
                * `pip install crawl4ai[torch]`:
                    * Purpose: Installs `torch`, `nltk`, and `scikit-learn`. Enables features relying on PyTorch models, such as some advanced text clustering or semantic analysis within extraction strategies.
                * `pip install crawl4ai[transformer]`:
                    * Purpose: Installs `transformers` and `tokenizers`. Enables the use of Hugging Face Transformers models for tasks like summarization, question answering, or other advanced NLP features within Crawl4ai.
                * `pip install crawl4ai[cosine]`:
                    * Purpose: Installs `torch`, `transformers`, and `nltk`. Specifically for features utilizing cosine similarity with embeddings (implies model usage).
                * `pip install crawl4ai[sync]`:
                    * Purpose: Installs `selenium` for synchronous crawling capabilities (less common, as Crawl4ai primarily focuses on async).
                * `pip install crawl4ai[all]`:
                    * Purpose: Installs all optional dependencies listed above (`PyPDF2`, `torch`, `nltk`, `scikit-learn`, `transformers`, `tokenizers`, `selenium`), providing the complete suite of Crawl4ai capabilities.
        * 2.2.2. Model Pre-fetching
            * Command: `crawl4ai-download-models` (maps to `crawl4ai.model_loader:main`)
            * Purpose: Downloads and caches machine learning models (e.g., specific sentence transformers or classification models from Hugging Face) that are used by certain optional features, particularly those installed via `crawl4ai[transformer]` or `crawl4ai[cosine]`. This avoids runtime downloads and ensures models are available offline.

## 3. Docker Deployment (Server Mode)

    * 3.1. **Prerequisites**
        * 3.1.1. Docker: A working Docker installation. (Link: `https://docs.docker.com/get-docker/`)
        * 3.1.2. Git: Required for cloning the `crawl4ai` repository if building locally or using Docker Compose from the repository. (Link: `https://git-scm.com/book/en/v2/Getting-Started-Installing-Git`)
        * 3.1.3. RAM Requirements:
            * Minimum: 2GB for the basic server without intensive LLM tasks. The `Dockerfile` HEALTCHECK indicates a warning if less than 2GB RAM is available.
            * Recommended for LLM support: 4GB+ (as specified in `docker-compose.yml` limits).
            * Shared Memory (`/dev/shm`): Recommended size is 1GB (`--shm-size=1g`) for optimal Chromium browser performance, as specified in `docker-compose.yml` and run commands.
    * 3.2. **Installation Options**
        * 3.2.1. **Using Pre-built Images from Docker Hub**
            * 3.2.1.1. Image Source: `unclecode/crawl4ai:<tag>`
                * Explanation of `<tag>`:
                    * `latest`: Points to the most recent stable release of Crawl4ai.
                    * Specific version tags (e.g., `0.6.0`, `0.5.1`): Correspond to specific library releases.
                    * Pre-release tags (e.g., `0.6.0-rc1`, `0.7.0-devN`): Development or release candidate versions for testing.
            * 3.2.1.2. Pulling the Image
                * Command: `docker pull unclecode/crawl4ai:<tag>` (e.g., `docker pull unclecode/crawl4ai:latest`)
            * 3.2.1.3. Environment Setup (`.llm.env`)
                * File Name: `.llm.env` (to be created by the user in the directory where `docker run` or `docker-compose` commands are executed).
                * Purpose: To securely provide API keys for various LLM providers used by Crawl4ai for features like LLM-based extraction or Q&A.
                * Example Content (based on `docker-compose.yml`):
                    ```env
                    OPENAI_API_KEY=your_openai_api_key
                    DEEPSEEK_API_KEY=your_deepseek_api_key
                    ANTHROPIC_API_KEY=your_anthropic_api_key
                    GROQ_API_KEY=your_groq_api_key
                    TOGETHER_API_KEY=your_together_api_key
                    MISTRAL_API_KEY=your_mistral_api_key
                    GEMINI_API_TOKEN=your_gemini_api_token
                    ```
                * Creation: Users should create this file and populate it with their API keys. An example (`.llm.env.example`) might be provided in the repository.
            * 3.2.1.4. Running the Container
                * Basic Run (without LLM support):
                    * Command: `docker run -d -p 11235:11235 --shm-size=1g --name crawl4ai-server unclecode/crawl4ai:<tag>`
                    * Port Mapping: `-p 11235:11235` maps port 11235 on the host to port 11235 in the container (default server port).
                    * Shared Memory: `--shm-size=1g` allocates 1GB of shared memory for the browser.
                * Run with LLM Support (mounting `.llm.env`):
                    * Command: `docker run -d -p 11235:11235 --env-file .llm.env --shm-size=1g --name crawl4ai-server unclecode/crawl4ai:<tag>`
            * 3.2.1.5. Stopping the Container
                * Command: `docker stop crawl4ai-server`
                * Command (to remove): `docker rm crawl4ai-server`
            * 3.2.1.6. Docker Hub Versioning:
                * Docker image tags on Docker Hub (e.g., `unclecode/crawl4ai:0.6.0`) directly correspond to `crawl4ai` library releases. The `latest` tag usually points to the most recent stable release. Pre-release tags include suffixes like `-devN`, `-aN`, `-bN`, or `-rcN`.

        * 3.2.2. **Using Docker Compose (`docker-compose.yml`)**
            * 3.2.2.1. Cloning the Repository
                * Command: `git clone https://github.com/unclecode/crawl4ai.git`
                * Command: `cd crawl4ai`
            * 3.2.2.2. Environment Setup (`.llm.env`)
                * File Name: `.llm.env` (should be created in the root of the cloned `crawl4ai` repository).
                * Purpose: Same as above, to provide LLM API keys.
            * 3.2.2.3. Running Pre-built Images
                * Command: `docker-compose up -d`
                * Behavior: Uses the image specified in `docker-compose.yml` (e.g., `${IMAGE:-unclecode/crawl4ai}:${TAG:-latest}`).
                * Overriding image tag: `TAG=0.6.0 docker-compose up -d` or `IMAGE=mycustom/crawl4ai TAG=mytag docker-compose up -d`.
            * 3.2.2.4. Building Locally with Docker Compose
                * Command: `docker-compose up -d --build`
                * Build Arguments (passed from environment variables to `docker-compose.yml` which then passes to `Dockerfile`):
                    * `INSTALL_TYPE`: (e.g., `default`, `torch`, `all`)
                        * Purpose: To include optional Python dependencies during the Docker image build process.
                        * Example: `INSTALL_TYPE=all docker-compose up -d --build`
                    * `ENABLE_GPU`: (e.g., `true`, `false`)
                        * Purpose: To include GPU support (e.g., CUDA toolkits) in the Docker image if the build hardware and target runtime support it.
                        * Example: `ENABLE_GPU=true docker-compose up -d --build`
            * 3.2.2.5. Stopping Docker Compose Services
                * Command: `docker-compose down`

        * 3.2.3. **Manual Local Build & Run**
            * 3.2.3.1. Cloning the Repository: (As above)
            * 3.2.3.2. Environment Setup (`.llm.env`): (As above)
            * 3.2.3.3. Building with `docker buildx`
                * Command Example:
                    ```bash
                    docker buildx build --platform linux/amd64,linux/arm64 \
                      --build-arg C4AI_VER=0.6.0 \
                      --build-arg INSTALL_TYPE=all \
                      --build-arg ENABLE_GPU=false \
                      --build-arg USE_LOCAL=true \
                      -t my-crawl4ai-image:custom .
                    ```
                * Purpose of `docker buildx`: A Docker CLI plugin that extends the `docker build` command with full support for BuildKit builder capabilities, including multi-architecture builds.
                * Explanation of `--platform`: Specifies the target platform(s) for the build (e.g., `linux/amd64`, `linux/arm64`).
                * Explanation of `--build-arg`: Passes build-time variables defined in the `Dockerfile` (see section 3.3).
            * 3.2.3.4. Running the Custom-Built Container
                * Basic Run: `docker run -d -p 11235:11235 --shm-size=1g --name my-crawl4ai-server my-crawl4ai-image:custom`
                * Run with LLM Support: `docker run -d -p 11235:11235 --env-file .llm.env --shm-size=1g --name my-crawl4ai-server my-crawl4ai-image:custom`
            * 3.2.3.5. Stopping the Container: (As above)

    * 3.3. **Dockerfile Parameters (`ARG` values)**
        * 3.3.1. `C4AI_VER`: (Default: `0.6.0`)
            * Role: Specifies the version of the `crawl4ai` library. Used for labeling the image and potentially for version-specific logic.
        * 3.3.2. `APP_HOME`: (Default: `/app`)
            * Role: Defines the working directory inside the Docker container where the application code and related files are stored and executed.
        * 3.3.3. `GITHUB_REPO`: (Default: `https://github.com/unclecode/crawl4ai.git`)
            * Role: The URL of the GitHub repository to clone if `USE_LOCAL` is set to `false`.
        * 3.3.4. `GITHUB_BRANCH`: (Default: `main`)
            * Role: The specific branch of the GitHub repository to clone if `USE_LOCAL` is `false`.
        * 3.3.5. `USE_LOCAL`: (Default: `true`)
            * Role: A boolean flag. If `true`, the `Dockerfile` installs `crawl4ai` from the local source code copied into `/tmp/project/` during the build context. If `false`, it clones the repository specified by `GITHUB_REPO` and `GITHUB_BRANCH`.
        * 3.3.6. `PYTHON_VERSION`: (Default: `3.12`)
            * Role: Specifies the Python version for the base image (e.g., `python:3.12-slim-bookworm`).
        * 3.3.7. `INSTALL_TYPE`: (Default: `default`)
            * Role: Controls which optional dependencies of `crawl4ai` are installed. Possible values: `default` (core), `pdf`, `torch`, `transformer`, `cosine`, `sync`, `all`.
        * 3.3.8. `ENABLE_GPU`: (Default: `false`)
            * Role: A boolean flag. If `true` and `TARGETARCH` is `amd64`, the `Dockerfile` attempts to install the NVIDIA CUDA toolkit for GPU acceleration.
        * 3.3.9. `TARGETARCH`:
            * Role: An automatic build argument provided by Docker, indicating the target architecture of the build (e.g., `amd64`, `arm64`). Used for conditional logic in the `Dockerfile`, such as installing platform-specific optimized libraries or CUDA for `amd64`.

    * 3.4. **Server Configuration (`config.yml`)**
        * 3.4.1. Location: The server loads its configuration from `/app/config.yml` inside the container by default. This path is relative to `APP_HOME`.
        * 3.4.2. Structure Overview (based on `deploy/docker/config.yml`):
            * `app`: General application settings.
                * `title (str)`: API title (e.g., "Crawl4AI API").
                * `version (str)`: API version (e.g., "1.0.0").
                * `host (str)`: Host address for the server to bind to (e.g., "0.0.0.0").
                * `port (int)`: Port for the server to listen on (e.g., 11234, though Docker usually maps to 11235).
                * `reload (bool)`: Enable/disable auto-reload for development (default: `false`).
                * `workers (int)`: Number of worker processes (default: 1).
                * `timeout_keep_alive (int)`: Keep-alive timeout in seconds (default: 300).
            * `llm`: Default LLM configuration.
                * `provider (str)`: Default LLM provider string (e.g., "openai/gpt-4o-mini").
                * `api_key_env (str)`: Environment variable name to read the API key from (e.g., "OPENAI_API_KEY").
                * `api_key (Optional[str])`: Directly pass API key (overrides `api_key_env`).
            * `redis`: Redis connection details.
                * `host (str)`: Redis host (e.g., "localhost").
                * `port (int)`: Redis port (e.g., 6379).
                * `db (int)`: Redis database number (e.g., 0).
                * `password (str)`: Redis password (default: "").
                * `ssl (bool)`: Enable SSL for Redis connection (default: `false`).
                * `ssl_cert_reqs (Optional[str])`: SSL certificate requirements (e.g., "none", "optional", "required").
                * `ssl_ca_certs (Optional[str])`: Path to CA certificate file.
                * `ssl_certfile (Optional[str])`: Path to SSL certificate file.
                * `ssl_keyfile (Optional[str])`: Path to SSL key file.
            * `rate_limiting`: Configuration for API rate limits.
                * `enabled (bool)`: Enable/disable rate limiting (default: `true`).
                * `default_limit (str)`: Default rate limit (e.g., "1000/minute").
                * `trusted_proxies (List[str])`: List of trusted proxy IP addresses.
                * `storage_uri (str)`: Storage URI for rate limit counters (e.g., "memory://", "redis://localhost:6379").
            * `security`: Security-related settings.
                * `enabled (bool)`: Master switch for security features (default: `false`).
                * `jwt_enabled (bool)`: Enable/disable JWT authentication (default: `false`).
                * `https_redirect (bool)`: Enable/disable HTTPS redirection (default: `false`).
                * `trusted_hosts (List[str])`: List of allowed host headers (e.g., `["*"]` or specific domains).
                * `headers (Dict[str, str])`: Default security headers to add to responses (e.g., `X-Content-Type-Options`, `Content-Security-Policy`).
            * `crawler`: Default crawler behavior.
                * `base_config (Dict[str, Any])`: Base parameters for `CrawlerRunConfig`.
                    * `simulate_user (bool)`: (default: `true`).
                * `memory_threshold_percent (float)`: Memory usage threshold for adaptive dispatcher (default: `95.0`).
                * `rate_limiter (Dict[str, Any])`: Configuration for the internal rate limiter for crawling.
                    * `enabled (bool)`: (default: `true`).
                    * `base_delay (List[float, float])`: Min/max delay range (e.g., `[1.0, 2.0]`).
                * `timeouts (Dict[str, float])`: Timeouts for different crawler operations.
                    * `stream_init (float)`: Timeout for stream initialization (default: `30.0`).
                    * `batch_process (float)`: Timeout for batch processing (default: `300.0`).
                * `pool (Dict[str, Any])`: Browser pool settings.
                    * `max_pages (int)`: Max concurrent browser pages (default: `40`).
                    * `idle_ttl_sec (int)`: Time-to-live for idle crawlers in seconds (default: `1800`).
                * `browser (Dict[str, Any])`: Default `BrowserConfig` parameters.
                    * `kwargs (Dict[str, Any])`: Keyword arguments for `BrowserConfig`.
                        * `headless (bool)`: (default: `true`).
                        * `text_mode (bool)`: (default: `true`).
                    * `extra_args (List[str])`: List of additional browser launch arguments (e.g., `"--no-sandbox"`).
            * `logging`: Logging configuration.
                * `level (str)`: Logging level (e.g., "INFO", "DEBUG").
                * `format (str)`: Log message format string.
            * `observability`: Observability settings.
                * `prometheus (Dict[str, Any])`: Prometheus metrics configuration.
                    * `enabled (bool)`: (default: `true`).
                    * `endpoint (str)`: Metrics endpoint path (e.g., "/metrics").
                * `health_check (Dict[str, str])`: Health check endpoint configuration.
                    * `endpoint (str)`: Health check endpoint path (e.g., "/health").
        * 3.4.3. JWT Authentication
            * Enabling: Set `security.enabled: true` and `security.jwt_enabled: true` in `config.yml`.
            * Secret Key: Configured via `security.jwt_secret_key`. This value can be overridden by the environment variable `JWT_SECRET_KEY`.
            * Algorithm: Configured via `security.jwt_algorithm` (default: `HS256`).
            * Token Expiry: Configured via `security.jwt_expire_minutes` (default: `30`).
            * Usage:
                * 1. Client obtains a token by sending a POST request to the `/token` endpoint with an email in the request body (e.g., `{"email": "user@example.com"}`). The email domain might be validated if configured.
                * 2. Client includes the received token in the `Authorization` header of subsequent requests to protected API endpoints: `Authorization: Bearer <your_jwt_token>`.
        * 3.4.4. Customizing `config.yml`
            * 3.4.4.1. Modifying Before Build:
                * Method: Edit the `deploy/docker/config.yml` file within the cloned `crawl4ai` repository before building the Docker image. This new configuration will be baked into the image.
            * 3.4.4.2. Runtime Mount:
                * Method: Mount a custom `config.yml` file from the host machine to `/app/config.yml` (or the path specified by `APP_HOME`) inside the running Docker container.
                * Example Command: `docker run -d -p 11235:11235 -v /path/on/host/my-config.yml:/app/config.yml --name crawl4ai-server unclecode/crawl4ai:latest`
        * 3.4.5. Key Configuration Recommendations
            * Security:
                * Enable JWT (`security.jwt_enabled: true`) if the server is exposed to untrusted networks.
                * Use a strong, unique `jwt_secret_key`.
                * Configure `security.trusted_hosts` to a specific list of allowed hostnames instead of `["*"]` for production.
                * If using a reverse proxy for SSL termination, ensure `https_redirect` is appropriately configured or disabled if the proxy handles it.
            * Resource Management:
                * Adjust `crawler.pool.max_pages` based on server resources to prevent overwhelming the system.
                * Tune `crawler.pool.idle_ttl_sec` to balance resource usage and responsiveness for pooled browser instances.
            * Monitoring:
                * Keep `observability.prometheus.enabled: true` for production monitoring via the `/metrics` endpoint.
                * Ensure the `/health` endpoint is accessible to health checking systems.
            * Performance:
                * Review and customize `crawler.browser.extra_args` for headless browser optimization (e.g., disabling GPU, sandbox if appropriate for your environment).
                * Set reasonable `crawler.timeouts` to prevent long-stalled crawls.

    * 3.5. **API Usage (Interacting with the Dockerized Server)**
        * 3.5.1. **Playground Interface**
            * Access URL: `http://localhost:11235/playground` (assuming default port mapping).
            * Purpose: An interactive web UI (Swagger UI/OpenAPI) allowing users to explore API endpoints, view schemas, construct requests, and test API calls directly from their browser.
        * 3.5.2. **Python SDK (`Crawl4aiDockerClient`)**
            * Class Name: `Crawl4aiDockerClient`
            * Location: (Typically imported as `from crawl4ai.docker_client import Crawl4aiDockerClient`) - Actual import might vary based on final library structure; refer to `docs/examples/docker_example.py` or `docs/examples/docker_python_sdk.py`.
            * Initialization:
                * Signature: `Crawl4aiDockerClient(base_url: str = "http://localhost:11235", api_token: Optional[str] = None, timeout: int = 300)`
                * Parameters:
                    * `base_url (str)`: The base URL of the Crawl4ai server. Default: `"http://localhost:11235"`.
                    * `api_token (Optional[str])`: JWT token for authentication if enabled on the server. Default: `None`.
                    * `timeout (int)`: Default timeout in seconds for HTTP requests to the server. Default: `300`.
            * Authentication (JWT):
                * Method: Pass the `api_token` during client initialization. The token can be obtained from the server's `/token` endpoint or other authentication mechanisms.
            * `crawl()` Method:
                * Signature (Conceptual, based on typical SDK patterns and server capabilities): `async def crawl(self, urls: Union[str, List[str]], browser_config: Optional[Dict] = None, crawler_config: Optional[Dict] = None, stream: bool = False) -> Union[List[Dict], AsyncGenerator[Dict, None]]`
                    *Note: SDK might take `BrowserConfig` and `CrawlerRunConfig` objects directly, which it then serializes.*
                * Key Parameters:
                    * `urls (Union[str, List[str]])`: A single URL string or a list of URL strings to crawl.
                    * `browser_config (Optional[Dict])`: A dictionary representing the `BrowserConfig` object, or a `BrowserConfig` instance itself.
                    * `crawler_config (Optional[Dict])`: A dictionary representing the `CrawlerRunConfig` object, or a `CrawlerRunConfig` instance itself.
                    * `stream (bool)`: If `True`, the method returns an async generator yielding individual `CrawlResult` dictionaries as they are processed by the server. If `False` (default), it returns a list containing all `CrawlResult` dictionaries after all URLs are processed.
                * Return Type: `List[Dict]` (for `stream=False`) or `AsyncGenerator[Dict, None]` (for `stream=True`), where each `Dict` represents a `CrawlResult`.
                * Streaming Behavior:
                    * `stream=True`: Allows processing of results incrementally, suitable for long crawl jobs or real-time data feeds.
                    * `stream=False`: Collects all results before returning, simpler for smaller batches.
            * `get_schema()` Method:
                * Signature: `async def get_schema(self) -> dict`
                * Return Type: `dict`.
                * Purpose: Fetches the JSON schemas for `BrowserConfig` and `CrawlerRunConfig` from the server's `/schema` endpoint. This helps in constructing valid configuration payloads.
        * 3.5.3. **JSON Request Schema for Configurations**
            * Structure: `{"type": "ClassName", "params": {...}}`
            * Purpose: This structure is used by the server (and expected by the Python SDK internally) to deserialize JSON payloads back into Pydantic configuration objects like `BrowserConfig`, `CrawlerRunConfig`, and their nested strategy objects (e.g., `LLMExtractionStrategy`, `PruningContentFilter`). The `type` field specifies the Python class name, and `params` holds the keyword arguments for its constructor.
            * Example (`BrowserConfig`):
                ```json
                {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": true,
                        "browser_type": "chromium",
                        "viewport_width": 1920,
                        "viewport_height": 1080
                    }
                }
                ```
            * Example (`CrawlerRunConfig` with a nested `LLMExtractionStrategy`):
                ```json
                {
                    "type": "CrawlerRunConfig",
                    "params": {
                        "cache_mode": {"type": "CacheMode", "params": "BYPASS"},
                        "screenshot": false,
                        "extraction_strategy": {
                            "type": "LLMExtractionStrategy",
                            "params": {
                                "llm_config": {
                                    "type": "LLMConfig",
                                    "params": {"provider": "openai/gpt-4o-mini"}
                                },
                                "instruction": "Extract the main title and summary."
                            }
                        }
                    }
                }
                ```
        * 3.5.4. **REST API Examples**
            * `/crawl` Endpoint:
                * URL: `http://localhost:11235/crawl`
                * HTTP Method: `POST`
                * Payload Structure (`CrawlRequest` model from `deploy/docker/schemas.py`):
                    ```json
                    {
                        "urls": ["https://example.com"],
                        "browser_config": { // JSON representation of BrowserConfig
                            "type": "BrowserConfig",
                            "params": {"headless": true}
                        },
                        "crawler_config": { // JSON representation of CrawlerRunConfig
                            "type": "CrawlerRunConfig",
                            "params": {"screenshot": true}
                        }
                    }
                    ```
                * Response Structure: A JSON object, typically `{"success": true, "results": [CrawlResult, ...], "server_processing_time_s": float, ...}`.
            * `/crawl/stream` Endpoint:
                * URL: `http://localhost:11235/crawl/stream`
                * HTTP Method: `POST`
                * Payload Structure: Same as `/crawl` (`CrawlRequest` model).
                * Response Structure: Newline Delimited JSON (NDJSON, `application/x-ndjson`). Each line is a JSON string representing a `CrawlResult` object.
                    * Headers: Includes `Content-Type: application/x-ndjson` and `X-Stream-Status: active` while streaming, and a final JSON object `{"status": "completed"}`.

    * 3.6. **Additional API Endpoints (from `server.py`)**
        * 3.6.1. `/html`
            * Endpoint URL: `/html`
            * HTTP Method: `POST`
            * Purpose: Crawls the given URL, preprocesses its raw HTML content specifically for schema extraction purposes (e.g., by sanitizing and simplifying the structure), and returns the processed HTML.
            * Request Body (`HTMLRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to fetch and process.
            * Response Structure (JSON):
                * `html (str)`: The preprocessed HTML string.
                * `url (str)`: The original URL requested.
                * `success (bool)`: Indicates if the operation was successful.
        * 3.6.2. `/screenshot`
            * Endpoint URL: `/screenshot`
            * HTTP Method: `POST`
            * Purpose: Captures a full-page PNG screenshot of the specified URL. Allows an optional delay before capture and an option to save the file server-side.
            * Request Body (`ScreenshotRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to take a screenshot of.
                * `screenshot_wait_for (Optional[float])`: Seconds to wait before taking the screenshot. Default: `2.0`.
                * `output_path (Optional[str])`: If provided, the screenshot is saved to this path on the server, and the path is returned. Otherwise, the base64 encoded image is returned. Default: `None`.
            * Response Structure (JSON):
                * `success (bool)`: Indicates if the screenshot was successfully taken.
                * `screenshot (Optional[str])`: Base64 encoded PNG image data, if `output_path` was not provided.
                * `path (Optional[str])`: The absolute server-side path to the saved screenshot, if `output_path` was provided.
        * 3.6.3. `/pdf`
            * Endpoint URL: `/pdf`
            * HTTP Method: `POST`
            * Purpose: Generates a PDF document of the rendered content of the specified URL.
            * Request Body (`PDFRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to convert to PDF.
                * `output_path (Optional[str])`: If provided, the PDF is saved to this path on the server, and the path is returned. Otherwise, the base64 encoded PDF data is returned. Default: `None`.
            * Response Structure (JSON):
                * `success (bool)`: Indicates if the PDF generation was successful.
                * `pdf (Optional[str])`: Base64 encoded PDF data, if `output_path` was not provided.
                * `path (Optional[str])`: The absolute server-side path to the saved PDF, if `output_path` was provided.
        * 3.6.4. `/execute_js`
            * Endpoint URL: `/execute_js`
            * HTTP Method: `POST`
            * Purpose: Executes a list of JavaScript snippets on the specified URL in the browser context and returns the full `CrawlResult` object, including any modifications or data retrieved by the scripts.
            * Request Body (`JSEndpointRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL on which to execute the JavaScript.
                * `scripts (List[str])`: A list of JavaScript code snippets to execute sequentially. Each script should be an expression that returns a value.
            * Response Structure (JSON): A `CrawlResult` object (serialized to a dictionary) containing the state of the page after JS execution, including `js_execution_result`.
        * 3.6.5. `/ask` (Endpoint defined as `/ask` in `server.py`)
            * Endpoint URL: `/ask`
            * HTTP Method: `GET`
            * Purpose: Retrieves context about the Crawl4ai library itself, either code snippets or documentation sections, filtered by a query. This is designed for AI assistants or RAG systems needing information about Crawl4ai.
            * Parameters (Query):
                * `context_type (str, default="all", enum=["code", "doc", "all"])`: Specifies whether to return "code", "doc", or "all" (both).
                * `query (Optional[str])`: A search query string used to filter relevant chunks using BM25 ranking. If `None`, returns all context of the specified type(s).
                * `score_ratio (float, default=0.5, ge=0.0, le=1.0)`: The minimum score (as a fraction of the maximum possible score for the query) for a chunk to be included in the results.
                * `max_results (int, default=20, ge=1)`: The maximum number of result chunks to return.
            * Response Structure (JSON):
                * If `query` is provided:
                    * `code_results (Optional[List[Dict[str, Union[str, float]]]])`: A list of dictionaries, where each dictionary contains `{"text": "code_chunk...", "score": bm25_score}`. Present if `context_type` is "code" or "all".
                    * `doc_results (Optional[List[Dict[str, Union[str, float]]]])`: A list of dictionaries, where each dictionary contains `{"text": "doc_chunk...", "score": bm25_score}`. Present if `context_type` is "doc" or "all".
                * If `query` is not provided:
                    * `code_context (Optional[str])`: The full concatenated code context as a single string. Present if `context_type` is "code" or "all".
                    * `doc_context (Optional[str])`: The full concatenated documentation context as a single string. Present if `context_type` is "doc" or "all".

    * 3.7. **MCP (Model Context Protocol) Support**
        * 3.7.1. Explanation of MCP:
            * Purpose: The Model Context Protocol (MCP) is a standardized way for AI models (like Anthropic's Claude with Code Interpreter capabilities) to discover and interact with external tools and data sources. Crawl4ai's MCP server exposes its functionalities as tools that an MCP-compatible AI can use.
        * 3.7.2. Connection Endpoints (defined in `mcp_bridge.py` and attached to FastAPI app):
            * `/mcp/sse`: Server-Sent Events (SSE) endpoint for MCP communication.
            * `/mcp/ws`: WebSocket endpoint for MCP communication.
            * `/mcp/messages`: Endpoint for clients to POST messages in the SSE transport.
        * 3.7.3. Usage with Claude Code Example:
            * Command: `claude mcp add -t sse c4ai-sse http://localhost:11235/mcp/sse`
            * Purpose: This command (specific to the Claude Code CLI) registers the Crawl4ai MCP server as a tool provider named `c4ai-sse` using the SSE transport. The AI can then discover and invoke tools from this source.
        * 3.7.4. List of Available MCP Tools (defined by `@mcp_tool` decorators in `server.py`):
            * `md`: Fetches Markdown for a URL.
                * Parameters (derived from `get_markdown` function signature): `url (str)`, `filter_type (FilterType)`, `query (Optional[str])`, `cache (Optional[str])`.
            * `html`: Generates preprocessed HTML for a URL.
                * Parameters (derived from `generate_html` function signature): `url (str)`.
            * `screenshot`: Generates a screenshot of a URL.
                * Parameters (derived from `generate_screenshot` function signature): `url (str)`, `screenshot_wait_for (Optional[float])`, `output_path (Optional[str])`.
            * `pdf`: Generates a PDF of a URL.
                * Parameters (derived from `generate_pdf` function signature): `url (str)`, `output_path (Optional[str])`.
            * `execute_js`: Executes JavaScript on a URL.
                * Parameters (derived from `execute_js` function signature): `url (str)`, `scripts (List[str])`.
            * `crawl`: Performs a full crawl operation.
                * Parameters (derived from `crawl` function signature): `urls (List[str])`, `browser_config (Optional[Dict])`, `crawler_config (Optional[Dict])`.
            * `ask`: Retrieves library context.
                * Parameters (derived from `get_context` function signature): `context_type (str)`, `query (Optional[str])`, `score_ratio (float)`, `max_results (int)`.
        * 3.7.5. Testing MCP Connections:
            * Method: Use an MCP client tool (e.g., `claude mcp call c4ai-sse.md url=https://example.com`) to invoke a tool and verify the response.
        * 3.7.6. Accessing MCP Schemas:
            * Endpoint URL: `/mcp/schema`
            * Purpose: Returns a JSON response detailing all registered MCP tools, including their names, descriptions, and input schemas, enabling clients to understand how to use them.

    * 3.8. **Metrics & Monitoring Endpoints**
        * 3.8.1. `/health`
            * Purpose: Provides a basic health check for the server, indicating if it's running and responsive.
            * Response Structure (JSON from `server.py`): `{"status": "ok", "timestamp": float, "version": str}` (where version is `__version__` from `server.py`).
            * Configuration: Path configurable via `observability.health_check.endpoint` in `config.yml`.
        * 3.8.2. `/metrics`
            * Purpose: Exposes application metrics in a format compatible with Prometheus for monitoring and alerting.
            * Response Format: Prometheus text format.
            * Configuration: Enabled via `observability.prometheus.enabled: true` and endpoint path via `observability.prometheus.endpoint` in `config.yml`.

    * 3.9. **Underlying Server Logic (`server.py` - High-Level Understanding)**
        * 3.9.1. FastAPI Application:
            * Framework: The server is built using the FastAPI Python web framework for creating APIs.
        * 3.9.2. `crawler_pool` (`CrawlerPool` from `deploy.docker.crawler_pool`):
            * Role: Manages a pool of `AsyncWebCrawler` instances to reuse browser resources efficiently.
            * `get_crawler(BrowserConfig)`: Fetches an existing idle crawler compatible with the `BrowserConfig` or creates a new one if none are available or compatible.
            * `close_all()`: Iterates through all pooled crawlers and closes them.
            * `janitor()`: An `asyncio.Task` that runs periodically to close and remove crawler instances that have been idle for longer than `crawler.pool.idle_ttl_sec` (configured in `config.yml`).
        * 3.9.3. Global Page Semaphore (`GLOBAL_SEM`):
            * Type: `asyncio.Semaphore`.
            * Purpose: A global semaphore that limits the total number of concurrently open browser pages across all `AsyncWebCrawler` instances managed by the server. This acts as a hard cap to prevent excessive resource consumption.
            * Configuration: The maximum number of concurrent pages is set by `crawler.pool.max_pages` in `config.yml` (default: `30` in `server.py`, but `40` in `config.yml`). The `AsyncWebCrawler.arun` method acquires this semaphore.
        * 3.9.4. Job Router (`init_job_router` from `deploy.docker.job`):
            * Role: Manages asynchronous, long-running tasks, particularly for the `/crawl` (non-streaming batch) endpoint.
            * Mechanism: Uses Redis (configured in `config.yml`) as a backend for task queuing (storing task metadata like status, creation time, URL, result, error) and status tracking.
            * User Interaction: When a job is submitted to an endpoint using this router (e.g., `/crawl/job`), a `task_id` is returned. The client then polls an endpoint like `/task/{task_id}` to get the status and eventual result or error.
        * 3.9.5. Rate Limiting Middleware:
            * Implementation: Uses the `slowapi` library, integrated with FastAPI.
            * Purpose: To protect the server from abuse by limiting the number of requests an IP address can make within a specified time window.
            * Configuration: Settings like `enabled`, `default_limit`, `storage_uri` (e.g., `memory://` or `redis://...`) are managed in the `rate_limiting` section of `config.yml`.
        * 3.9.6. Security Middleware:
            * Implementations: `HTTPSRedirectMiddleware` and `TrustedHostMiddleware` from FastAPI, plus custom logic for adding security headers.
            * Purpose:
                * `HTTPSRedirectMiddleware`: Redirects HTTP requests to HTTPS if `security.https_redirect` is true.
                * `TrustedHostMiddleware`: Ensures requests are only served if their `Host` header matches an entry in `security.trusted_hosts`.
                * Custom header logic: Adds HTTP security headers like `X-Content-Type-Options`, `X-Frame-Options`, `Content-Security-Policy`, `Strict-Transport-Security` to all responses if `security.enabled` is true. These are defined in `security.headers` in `config.yml`.
        * 3.9.7. API Request Mapping:
            * Request Models: Pydantic models defined in `deploy/docker/schemas.py` (e.g., `CrawlRequest`, `MarkdownRequest`, `HTMLRequest`, `ScreenshotRequest`, `PDFRequest`, `JSEndpointRequest`, `TokenRequest`, `RawCode`) define the expected JSON structure for incoming API request bodies.
            * Endpoint Logic: Functions decorated with `@app.post(...)`, `@app.get(...)`, etc., in `server.py` handle incoming HTTP requests. These functions use FastAPI's dependency injection to parse and validate request bodies against the Pydantic models.
            * `AsyncWebCrawler` Interaction:
                * The parameters from the parsed request models (e.g., `CrawlRequest.urls`, `CrawlRequest.browser_config`, `CrawlRequest.crawler_config`) are used.
                * `BrowserConfig` and `CrawlerRunConfig` objects are created by calling their respective `.load()` class methods with the dictionary payloads received in the request (e.g., `BrowserConfig.load(crawl_request.browser_config)`).
                * These configuration objects are then passed to an `AsyncWebCrawler` instance obtained from the `crawler_pool`, typically to its `arun()` (for single URL or when JS execution context is critical) or `arun_many()` (for batch processing of multiple URLs) methods.
            * Result Serialization: The `CrawlResult` objects (or lists/generators of them) returned by the `AsyncWebCrawler` are usually serialized to JSON using their `.model_dump()` method before being included in the HTTP response. For streaming endpoints, each `CrawlResult` is serialized and sent as a separate NDJSON line.

## 4. Version Numbering Scheme

    * 4.1. **Standard Versioning (`MAJOR.MINOR.PATCH`)**
        * `MAJOR`: Incremented when incompatible API changes are made.
        * `MINOR`: Incremented when functionality is added in a backward-compatible manner.
        * `PATCH`: Incremented for backward-compatible bug fixes.
    * 4.2. **Pre-release Suffixes**
        * `devN`: (e.g., `0.6.0.dev1`) Development release. These are typically unstable and used for internal testing or early feedback on new, unrefined features.
        * `aN`: (e.g., `0.6.0a1`) Alpha release. Indicates an early preview of a new version, potentially unstable, and APIs might still change.
        * `bN`: (e.g., `0.6.0b1`) Beta release. Generally feature-complete for the targeted minor or major version but may still contain bugs. APIs are mostly stable at this point.
        * `rcN`: (e.g., `0.6.0rc1`) Release Candidate. A version that is potentially the final release, undergoing final testing to catch critical bugs before official release.
```

---


## Deployment - Reasoning
Source: crawl4ai_deployment_reasoning_content.llm.md

Okay, I'm ready to generate the "Reasoning & Problem-Solving Framework" document for the `deployment` component of `crawl4ai`, based on the outline you provided and the code/documentation context from our previous interaction.

```markdown
# Detailed Outline for crawl4ai - deployment Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_deployment.md`
**Library Version Context:** 0.6.0
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Crawl4ai Deployment

This guide focuses on deploying and configuring `crawl4ai`, whether you're using it as a simple Python library or as a robust Dockerized server. Understanding the different deployment strategies will help you choose the best approach for your specific needs, from quick local scripts to scalable, API-driven crawling services.

*   1.1. Why Different Deployment Strategies Matter
    *   1.1.1. Explaining the trade-offs: Library vs. Server (Docker) mode.
        *   **Library Mode:**
            *   **Pros:** Simplest to get started with for Python developers, direct integration into existing Python projects, easier debugging of Python-specific logic.
            *   **Cons:** Requires Python environment setup on every machine, can be harder to manage dependencies for larger teams or across different OS, resource management (browsers, memory) is directly tied to the script's host.
            *   **Why choose it?** Ideal for individual developers, small scripts, quick prototyping, or when `crawl4ai` is a component within a larger Python application.
        *   **Server (Docker) Mode:**
            *   **Pros:** Consistent environment (Docker handles dependencies), easy to scale, API-first (accessible from any language), better resource isolation and management, simplified deployment to cloud or on-premise servers.
            *   **Cons:** Requires Docker knowledge, slightly more setup initially, debugging might involve looking at container logs in addition to application logs.
            *   **Why choose it?** Best for team collaboration, production deployments, providing crawling as a service, language-agnostic access, or when you need robust, isolated browser instances.
    *   1.1.2. When to choose simple library installation.
        *   Choose simple library installation when:
            *   You are primarily working in a Python environment.
            *   You need to quickly integrate crawling into an existing Python script or application.
            *   Your deployment target is a machine where you can easily manage Python environments and Playwright browser installations.
            *   You are prototyping or working on a small-scale project.
    *   1.1.3. When a Dockerized server deployment is beneficial (scalability, isolation, API access).
        *   Opt for a Dockerized server when:
            *   You need a consistent, reproducible crawling environment across different machines or team members.
            *   You plan to offer crawling capabilities as an API to other services or applications (potentially written in different languages).
            *   You require better resource isolation for browser instances to prevent them from impacting other processes on the host machine.
            *   You anticipate needing to scale your crawling operations up or down based on demand.
            *   You are deploying to a cloud environment or a server where Docker is the preferred deployment method.

*   1.2. Overview of Installation Paths
    *   1.2.1. Quick guide to choosing your installation path based on needs.
        *   **For local Python development/scripting:** Start with "Core Library Installation." Add "Advanced Library Installation" if you need features like local ML model inference.
        *   **For a standalone, API-accessible server:** Jump to "Docker Deployment." You can choose between pre-built images (easiest), Docker Compose (good for managing related services like Redis), or manual builds (for full control).
    *   1.2.2. What this guide will cover for each path.
        *   This guide will provide step-by-step instructions, explanations of "why" certain steps are necessary, best practices, and troubleshooting tips for both library installation and the various Docker deployment options.

## 2. Core Library Installation & Usage

This section details how to get the `crawl4ai` library up and running directly in your Python environment.

*   2.1. Understanding the Basic Installation
    *   2.1.1. **How-to:** Installing the core `crawl4ai` library.
        *   **Command:**
            ```bash
            pip install crawl4ai
            ```
        *   **What core functionalities this provides:**
            *   The `AsyncWebCrawler` class and its associated configuration objects (`BrowserConfig`, `CrawlerRunConfig`).
            *   Core scraping capabilities (HTML, Markdown, links, media).
            *   Basic content processing and filtering.
            *   Support for Playwright-driven browser automation.
            *   The `crawl4ai-setup` and `crawl4ai-doctor` CLI tools.
    *   2.1.2. The Importance of Post-Installation Setup (`crawl4ai-setup`)
        *   **Why `crawl4ai-setup` is crucial:** `crawl4ai` relies on Playwright for browser automation. Playwright, in turn, needs browser executables (like Chromium, Firefox, WebKit) to be downloaded and installed in a location it can find. `crawl4ai-setup` automates this process.
        *   **What it does:**
            *   Invokes Playwright's browser installation mechanism (e.g., `playwright install --with-deps chromium`).
            *   Performs OS-specific checks to ensure necessary libraries or dependencies for running headless browsers are present (especially important on Linux).
            *   Sets up the local Crawl4ai home directory structure (e.g., `~/.crawl4ai/cache`).
        *   **Troubleshooting common `crawl4ai-setup` issues:**
            *   **Permission errors:** Ensure you have write permissions to the Playwright browser installation directory (often in your user's home directory or a system-wide location if installing as root).
            *   **Network issues:** Browser downloads can be large; ensure a stable internet connection. Proxies might interfere if not configured correctly for Playwright.
            *   **Missing OS dependencies (Linux):** The script attempts to guide you, but you might need to manually install packages like `libnss3`, `libatk1.0-0`, etc.
        *   *Code Example: Running `crawl4ai-setup` and interpreting its output.*
            ```bash
            crawl4ai-setup
            ```
            **Expected Output (Success):**
            ```
            [INIT] Running post-installation setup...
            [SETUP] Playwright browser installation complete.
            [COMPLETE] Post-installation setup completed!
            ```
            **Potential Issue Output:**
            ```
            [ERROR] Failed to install Playwright browsers. Please run 'playwright install --with-deps' manually.
            ```
    *   2.1.3. Diagnosing Your Environment with `crawl4ai-doctor`
        *   **When and why to use `crawl4ai-doctor`:** Run this command if you encounter issues after installation, or if crawls are failing unexpectedly. It performs a series of checks to verify that your Python environment, Playwright installation, and browser executables are correctly set up and accessible.
        *   **Interpreting `crawl4ai-doctor` output for common problems:**
            *   It will check Python version compatibility.
            *   It verifies if Playwright is installed and if browsers can be launched.
            *   It might suggest solutions for common issues it detects.
        *   *Code Example: Running `crawl4ai-doctor` and typical successful/problematic outputs.*
            ```bash
            crawl4ai-doctor
            ```
            **Expected Output (Success):**
            ```
            [INIT] Running Crawl4ai health check...
            [INFO] Python version: 3.X.X
            [INFO] Playwright version: X.Y.Z
            [TEST] Testing crawling capabilities...
            [COMPLETE] ✅ Crawling test passed!
            Crawl4ai doctor check completed. All systems operational.
            ```
            **Potential Issue Output:**
            ```
            [ERROR] ❌ Test failed: Could not launch browser. Ensure Playwright browsers are installed (run 'crawl4ai-setup' or 'playwright install --with-deps chromium').
            ```
    *   2.1.4. Verifying Your Basic Installation: Your First Simple Crawl
        *   **Step-by-step guide:**
            1.  Create a new Python file (e.g., `test_crawl.py`).
            2.  Import necessary classes: `AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`.
            3.  Write an `async` function.
            4.  Inside the function, create a `BrowserConfig` instance (defaults are usually fine for a first test).
            5.  Create an `AsyncWebCrawler` instance, passing the `BrowserConfig`. Use an `async with` statement for proper resource management.
            6.  Create a `CrawlerRunConfig` instance (again, defaults are fine).
            7.  Call `crawler.arun(url="https://example.com", config=run_config)`.
            8.  Print a part of the result, e.g., `result.markdown[:300]`.
            9.  Use `asyncio.run()` to execute your `async` function.
        *   **Expected output:** You should see the first 300 characters of the Markdown content extracted from `example.com`.
        *   **How to confirm success:** If the script runs without errors and prints Markdown content, your basic installation is working.
        *   *Code Example: A minimal Python script to crawl `example.com`.*
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

            async def main():
                browser_cfg = BrowserConfig(headless=True) # Keep headless for non-UI environments
                run_cfg = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Bypass cache for a fresh fetch

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    print("Attempting to crawl https://example.com...")
                    result = await crawler.arun(url="https://example.com", config=run_cfg)
                    if result.success:
                        print("Crawl successful!")
                        print("Markdown (first 300 chars):")
                        if result.markdown:
                            print(result.markdown.raw_markdown[:300])
                        else:
                            print("No markdown content generated.")
                    else:
                        print(f"Crawl failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(main())
            ```

*   2.2. Advanced Library Installation: Extending Functionality
    *   2.2.1. When to Consider Optional Features
        *   **Identifying use cases:**
            *   **Local Machine Learning/NLP tasks:** If you plan to use features like `CosineSimilarityFilter`, advanced `LLMContentFilter` modes that might leverage local sentence transformers, or other AI-driven text processing directly within your Python script without relying on an external LLM API for everything.
            *   **PyTorch-dependent features:** Some advanced filters or future AI integrations might specifically require PyTorch.
            *   **Hugging Face Transformers:** If you intend to use models directly from the Hugging Face Hub for tasks like summarization, classification, or custom embedding generation within your Crawl4ai workflow.
        *   **Understanding the additional capabilities:** These extras typically bring in libraries like `torch`, `transformers`, `scikit-learn`, and `nltk`, enabling more sophisticated local data processing and AI model inference.
    *   2.2.2. **How-to:** Installing Optional Extras
        *   **Explaining `crawl4ai[torch]`:**
            *   Installs `torch` and related dependencies.
            *   **Why:** Necessary for features that perform local neural network inference, such as certain embedding models or advanced NLP tasks that are PyTorch-based.
        *   **Explaining `crawl4ai[transformer]`:**
            *   Installs `transformers` (from Hugging Face) and `tokenizers`.
            *   **Why:** Enables the use of a wide range of pre-trained transformer models for tasks like text generation, summarization, and classification directly within Crawl4ai, often in conjunction with `torch`.
        *   **Explaining `crawl4ai[all]`:**
            *   Installs all optional dependencies, including `torch`, `transformers`, `nltk`, `scikit-learn`, `PyPDF2`, etc.
            *   **When to use:** If you anticipate needing a broad range of features and don't mind a larger installation footprint. Convenient for development environments.
            *   **Potential downsides:** Significantly larger installation size and more dependencies to manage, which might increase the chance of conflicts in complex environments.
        *   *Code Example: `pip install crawl4ai[torch]` and `pip install crawl4ai[all]`.*
            ```bash
            # For PyTorch related features
            pip install crawl4ai[torch]

            # For Hugging Face Transformers related features
            pip install crawl4ai[transformer]

            # To install all optional features
            pip install crawl4ai[all]
            ```
    *   2.2.3. Pre-fetching Models with `crawl4ai-download-models`
        *   **Why pre-fetch models:**
            *   **Offline use:** Allows features dependent on these models (e.g., certain embedding generators, classifiers) to run without an internet connection after initial download.
            *   **Faster startup:** Avoids download time on the first run of a script that uses these models.
            *   **Controlled environment:** Ensures you have the specific model versions Crawl4ai expects.
        *   **How to use the command:**
            ```bash
            crawl4ai-download-models
            ```
        *   **Where models are stored:** Typically in a cache directory managed by the underlying libraries (e.g., Hugging Face's cache, usually in `~/.cache/huggingface`). `crawl4ai-download-models` simply triggers the download process via these libraries.

## 3. Docker Deployment: Running Crawl4ai as a Server

Deploying Crawl4ai with Docker provides a consistent, isolated, and scalable environment, making it ideal for production or when offering crawling as an API service.

*   3.1. Why Deploy Crawl4ai with Docker?
    *   3.1.1. **Benefits:**
        *   **Isolation:** Browser instances and dependencies are contained within the Docker image, preventing conflicts with your host system or other applications.
        *   **Reproducibility:** Ensures that Crawl4ai runs the same way across different environments (development, staging, production).
        *   **Scalability:** Docker containers can be easily scaled up or down using orchestration tools like Kubernetes or Docker Swarm.
        *   **API-first Access:** Exposes Crawl4ai's functionality via a REST API, allowing applications written in any language to utilize its crawling capabilities.
    *   3.1.2. Common use cases for a Dockerized Crawl4ai server.
        *   Providing a centralized crawling service for multiple applications or teams.
        *   Integrating Crawl4ai into non-Python microservices architectures.
        *   Deploying to cloud platforms that favor containerized applications.
        *   Ensuring consistent browser behavior and dependency management for critical crawling tasks.

*   3.2. Prerequisites for Docker Deployment
    *   3.2.1. **Docker:** Ensure Docker Desktop (for Windows/Mac) or Docker Engine (for Linux) is installed and the Docker daemon is running.
        *   *Decision:* If you're new to Docker, visit the official Docker website for installation instructions specific to your OS.
    *   3.2.2. **Git:** Required if you plan to build the Docker image locally from the source code or use Docker Compose with a local repository clone.
        *   *Decision:* If you only intend to use pre-built images from Docker Hub, Git might not be strictly necessary on the deployment machine, but it's good practice for managing configurations.
    *   3.2.3. **RAM Requirements:** Web browsers, especially multiple concurrent instances, can be memory-intensive.
        *   **Guidance:**
            *   Minimum: At least 2GB RAM for the Docker container itself, plus additional RAM per concurrent browser page (e.g., 250-500MB per page, can vary).
            *   A common starting point for a server expected to handle a few concurrent crawls might be 4GB-8GB total allocated to Docker.
            *   Monitor your container's memory usage (`docker stats <container_id>`) and adjust resources as needed. Insufficient RAM can lead to browser crashes or slow performance.
            *   Remember to configure `--shm-size` (shared memory size) for your Docker run command (e.g., `--shm-size=1g`), as Chromium-based browsers heavily use it. The `docker-compose.yml` already includes a `/dev/shm` mount.

*   3.3. Docker Installation Options: A Decision Guide
    *   3.3.1. Option 1: Using Pre-built Images from Docker Hub
        *   **When to use:** This is the **easiest and quickest** way to get started if you don't need custom modifications to the Crawl4ai server image. It's ideal for standard use cases and trying out the server.
        *   **How-to:**
            *   **Pulling the image:**
                ```bash
                docker pull unclecode/crawl4ai:latest # For the latest stable release
                # Or, for a specific version (recommended for production):
                docker pull unclecode/crawl4ai:0.6.0
                ```
            *   **Understanding Docker Hub Tags:**
                *   `latest`: Points to the most recent stable release. Use with caution in production as it can change unexpectedly.
                *   Specific versions (e.g., `0.6.0`): Recommended for production to ensure reproducibility and avoid breaking changes.
                *   `0.6.0-rc1`: Release candidates, nearly stable.
                *   `dev`: Development builds from the `main` branch, potentially unstable.
                *   **Decision:** For production, always pin to a specific version tag. Use `latest` for quick tests or when you always want the newest features and are prepared for potential changes.
            *   **Setting up the environment:** Create a `.llm.env` file in your current directory to store API keys for LLM providers if you plan to use LLM-based extraction or filtering features.
                *   *Example `.llm.env` content:*
                    ```env
                    OPENAI_API_KEY=sk-yourOpenAiApiKeyxxxxxxxxxxxx
                    ANTHROPIC_API_KEY=sk-ant-yourAnthropicApiKeyxxxxxxxx
                    GEMINI_API_TOKEN=yourGoogleAIGeminiApiKeyxxxxxxxx
                    # Add other LLM provider keys as needed
                    ```
            *   **Running the container (Basic, no LLM support initially):**
                ```bash
                docker run -d -p 11235:11235 --name crawl4ai-server --shm-size=1g unclecode/crawl4ai:0.6.0
                ```
                *   `-d`: Run in detached mode (background).
                *   `-p 11235:11235`: Map port 11235 on your host to port 11235 in the container.
                *   `--name crawl4ai-server`: Assign a name to the container for easier management.
                *   `--shm-size=1g`: Allocate 1GB of shared memory, crucial for browser stability.
            *   **Running with LLM Support (mounting `.llm.env`):**
                ```bash
                docker run -d -p 11235:11235 --name crawl4ai-server --shm-size=1g --env-file .llm.env unclecode/crawl4ai:0.6.0
                ```
            *   **Stopping and removing the container:**
                ```bash
                docker stop crawl4ai-server
                docker rm crawl4ai-server
                ```
        *   **Best practices:**
            *   Always use specific version tags in production.
            *   Manage API keys securely using `.env` files or Docker secrets, not by hardcoding them into run commands or Dockerfiles.
    *   3.3.2. Option 2: Using Docker Compose
        *   **When to use:**
            *   When you want an easier way to manage the container's configuration and lifecycle.
            *   If you plan to run related services (e.g., a dedicated Redis instance for rate limiting or job queues) alongside Crawl4ai.
            *   If you need to make minor local customizations to the build process (like choosing `INSTALL_TYPE`) without managing complex `docker build` commands.
        *   **How-to:**
            1.  **Cloning the `crawl4ai` repository:**
                ```bash
                git clone https://github.com/unclecode/crawl4ai.git
                cd crawl4ai
                ```
            2.  **Setting up `.llm.env`:** Create this file in the root of the cloned repository if you need LLM support (see example above).
            3.  **Running with Pre-built Images (default in `docker-compose.yml`):**
                ```bash
                # This will use the image specified in docker-compose.yml (e.g., unclecode/crawl4ai:latest or a specific version)
                docker-compose up -d
                ```
                *   The `docker-compose.yml` file is pre-configured to pull official images and set up necessary volumes (like `/dev/shm`).
            4.  **Building Images Locally with Docker Compose:**
                *   **When this is preferred:** If you need to build the image with specific optional features (`INSTALL_TYPE`) or enable GPU support, and you prefer the `docker-compose` workflow.
                *   **How:** You'll modify the `docker-compose.yml` to use the `build` context or pass build arguments via the command line.
                    ```bash
                    # Example: Build with all features
                    docker-compose build --build-arg INSTALL_TYPE=all
                    docker-compose up -d

                    # Example: Build with GPU support (ensure Dockerfile supports this and host has NVIDIA drivers/toolkit)
                    # Potentially requires modifying docker-compose.yml to pass GPU runtime flags
                    docker-compose build --build-arg ENABLE_GPU=true
                    docker-compose up -d
                    ```
                    *Note: The provided `docker-compose.yml` already has a `build` section, so `docker-compose build` will use it. You can uncomment/modify `args` in the `build` section of `docker-compose.yml` as well.*
            5.  **Stopping services:**
                ```bash
                docker-compose down
                ```
        *   **Advantages:** Simplifies managing container configurations, volumes, and networks, especially if you add more services later.
    *   3.3.3. Option 3: Manual Local Build & Run
        *   **When to use:**
            *   When you need to make significant customizations to the `Dockerfile` itself.
            *   For development and testing of changes to the Crawl4ai server codebase.
            *   If you need to build for a specific architecture not readily available as a pre-built image variant (though `buildx` helps with this).
        *   **How-to:**
            1.  **Cloning the repository:**
                ```bash
                git clone https://github.com/unclecode/crawl4ai.git
                cd crawl4ai
                ```
            2.  **Setting up `.llm.env`:** Create this file in the root directory.
            3.  **Building with `docker buildx` (recommended for multi-arch):**
                *   **Understanding multi-arch builds:** `docker buildx` allows you to build images for multiple architectures (e.g., `linux/amd64` for typical Intel/AMD servers, `linux/arm64` for ARM-based servers like AWS Graviton or Raspberry Pi).
                *   **Passing build arguments:**
                    ```bash
                    # Example: Build for amd64 and arm64, with all features, and tag it
                    docker buildx build \
                      --platform linux/amd64,linux/arm64 \
                      --build-arg INSTALL_TYPE=all \
                      --build-arg ENABLE_GPU=false \
                      -t my-custom-crawl4ai:latest \
                      --push .  # Use --load to load into local Docker images instead of pushing
                    ```
                    *   Replace `--push` with `--load` if you want to use the image locally immediately.
            4.  **Running the locally built container:**
                ```bash
                docker run -d -p 11235:11235 --name my-crawl4ai-server --shm-size=1g --env-file .llm.env my-custom-crawl4ai:latest
                ```
            5.  **Stopping and removing the container:**
                ```bash
                docker stop my-crawl4ai-server
                docker rm my-crawl4ai-server
                ```
        *   **Considerations:** This method gives you the most control but requires a deeper understanding of Docker image building. Build times can be longer, especially with `INSTALL_TYPE=all`.

*   3.4. Understanding Dockerfile Build Parameters (`ARG` values)
    *   These arguments allow you to customize the Docker image during the build process (`docker build` or `docker-compose build`).
    *   `C4AI_VER`:
        *   **Role:** Specifies the version of Crawl4ai to install if not using local source. It's used in the Dockerfile if `USE_LOCAL=false`.
        *   **Why change:** You might want to build an image based on a specific older version or a development tag.
    *   `APP_HOME`:
        *   **Role:** Defines the working directory inside the container (e.g., `/app`).
        *   **Why change:** Rarely needed unless you have specific path requirements for integrations.
    *   `GITHUB_REPO`, `GITHUB_BRANCH`:
        *   **Role:** Used when `USE_LOCAL=false` to clone Crawl4ai from a specific GitHub repository and branch.
        *   **Why change:** To build from your own fork, a feature branch, or a specific commit for testing.
    *   `USE_LOCAL`:
        *   **Role:** A boolean (`true` or `false`). If `true`, the Docker build uses the local source code from the directory where the `Dockerfile` resides (copied via `COPY . /tmp/project/`). If `false`, it clones from `GITHUB_REPO` and `GITHUB_BRANCH`.
        *   **Why change:** Set to `true` when developing and wanting to build an image with your local changes. Set to `false` for CI/CD or building from a canonical Git source.
    *   `PYTHON_VERSION`:
        *   **Role:** Specifies the base Python slim image version (e.g., `3.12`).
        *   **Why change:** If you need to ensure compatibility with a specific Python version for your dependencies or environment.
    *   `INSTALL_TYPE`:
        *   **Role:** Controls which optional dependencies of `crawl4ai` are installed. Options include `default` (core), `all` (all extras), `torch`, `transformer`.
        *   **Impact:**
            *   `default`: Smallest image, fewest features.
            *   `all`: Largest image, all features (including ML/NLP capabilities).
            *   `torch`/`transformer`: Intermediate size, specific ML/NLP capabilities.
        *   **Why change:** To tailor the image size and included features to your specific needs, avoiding unnecessary bloat.
    *   `ENABLE_GPU`:
        *   **Role:** A boolean (`true` or `false`). If `true`, the Dockerfile attempts to install GPU-related dependencies (e.g., CUDA toolkit if `TARGETARCH` is compatible).
        *   **Why change:** Set to `true` if you have a compatible GPU on your Docker host and want to accelerate ML tasks (like local LLM inference or embeddings) inside the container. Requires appropriate Docker runtime configuration (e.g., `--gpus all`).
    *   `TARGETARCH`:
        *   **Role:** Automatically set by Docker Buildx based on the `--platform` flag. It informs the Dockerfile about the target architecture (e.g., `amd64`, `arm64`) so it can install architecture-specific dependencies (like OpenMP for AMD64 or OpenBLAS for ARM64, or CUDA for NVIDIA GPUs on compatible architectures).
        *   **Why be aware:** Essential for understanding multi-arch builds and ensuring correct dependencies are installed for the target platform.
    *   *Guidance: Best practices for setting these arguments:*
        *   For development with local changes: `USE_LOCAL=true`.
        *   For minimal production image: `INSTALL_TYPE=default` (if no advanced features needed).
        *   For ML-heavy tasks on GPU hardware: `ENABLE_GPU=true`, `INSTALL_TYPE=all` (or `torch`/`transformer`).
        *   Always specify `C4AI_VER` or `GITHUB_BRANCH` explicitly for reproducible builds if not using `USE_LOCAL=true`.

*   3.5. Server Configuration (`config.yml`)
    The `config.yml` file (located at `/app/config.yml` inside the container, and `deploy/docker/config.yml` in the source) controls various aspects of the Crawl4ai server's behavior.
    *   3.5.1. Overview of `config.yml` Structure
        *   **`app` section:**
            *   **Purpose:** Configures the FastAPI/Uvicorn server.
            *   `host`, `port`: Network interface and port the server listens on.
            *   `workers`: Number of Uvicorn worker processes (for handling concurrent requests).
            *   **Reasoning:** Adjust `workers` based on your server's CPU cores and expected load. `0.0.0.0` for `host` makes it accessible externally.
        *   **`llm` section:**
            *   **Purpose:** Default settings for LLM integrations.
            *   `provider`: Default LLM provider/model (e.g., `openai/gpt-4o-mini`).
            *   `api_key_env`: The environment variable name from which to read the API key for the default provider (e.g., `OPENAI_API_KEY`).
            *   `api_key`: (Optional, discouraged) Directly embed an API key. It's better to use `api_key_env`.
            *   **Reasoning:** Centralizes default LLM settings. API keys should almost always be managed via environment variables for security.
        *   **`redis` section:**
            *   **Purpose:** Configuration for connecting to a Redis instance.
            *   Used for distributed rate limiting (if `rate_limiting.storage_uri` points to Redis) and potentially for the job queue in future versions.
            *   **Reasoning:** Essential for robust rate limiting in a scaled environment. If not using distributed features, default `memory://` for rate limiting is simpler.
        *   **`rate_limiting` section:**
            *   **Purpose:** Controls API rate limiting to prevent abuse.
            *   `enabled`: `true` or `false`.
            *   `default_limit`: E.g., "1000/minute".
            *   `storage_uri`: `memory://` (default, per-instance) or `redis://...` (for distributed).
            *   **Reasoning:** Always enable in production. Adjust limits based on expected traffic and capacity.
        *   **`security` section:**
            *   **Purpose:** Security-related settings.
            *   `enabled`: Master switch for security features below.
            *   `jwt_enabled`: Enable/disable JWT token authentication for API endpoints.
            *   `https_redirect`: If `true`, redirects HTTP to HTTPS (requires a reverse proxy like Nginx to handle SSL termination).
            *   `trusted_hosts`: List of allowed host headers. `["*"]` allows all, but be more specific in production.
            *   `headers`: Default security headers (X-Content-Type-Options, X-Frame-Options, CSP, HSTS).
            *   **Reasoning:** Crucial for production. `jwt_enabled` protects your API. `trusted_hosts` prevents host header attacks. Default headers provide good baseline security.
        *   **`crawler` section:**
            *   **Purpose:** Default behaviors for the crawler instances managed by the server.
            *   `base_config`: Default `CrawlerRunConfig` parameters if not specified in the API request.
            *   `memory_threshold_percent`: For `MemoryAdaptiveDispatcher`, at what system memory percentage to start throttling.
            *   `rate_limiter`: Default settings for the `RateLimiter` used by dispatchers.
            *   `pool`:
                *   `max_pages`: Corresponds to `GLOBAL_SEM` in `server.py`. Max concurrent browser pages server-wide.
                *   `idle_ttl_sec`: How long an idle browser instance remains in the pool before being cleaned up by the `janitor`.
            *   `browser`: Default `BrowserConfig` parameters.
                *   `kwargs`: Passed to Playwright's browser launch.
                *   `extra_args`: Additional browser command-line flags.
            *   **Reasoning:** Fine-tune these based on server resources and crawling needs. `max_pages` is critical for stability. `idle_ttl_sec` balances responsiveness with resource conservation.
        *   **`logging` section:**
            *   **Purpose:** Controls server-side logging.
            *   `level`: `INFO`, `DEBUG`, `WARNING`, `ERROR`.
            *   `format`: Log message format.
            *   **Reasoning:** Set to `DEBUG` for detailed troubleshooting, `INFO` for general production logs.
        *   **`observability` section:**
            *   **Purpose:** Endpoints for monitoring.
            *   `prometheus.endpoint`: Path for Prometheus metrics (e.g., `/metrics`).
            *   `health_check.endpoint`: Path for health checks (e.g., `/health`).
            *   **Reasoning:** Essential for production monitoring and integration with alerting systems.
    *   3.5.2. Securing Your Server: JWT Authentication
        *   **Why enable JWT authentication:** To protect your Crawl4ai server API from unauthorized access, especially if it's exposed to the internet or a shared network.
        *   **How to enable:** In `config.yml`, under the `security` section, set `jwt_enabled: true`.
        *   **Impact on API requests:** Most API endpoints (those decorated with `Depends(token_dep)`) will require an `Authorization: Bearer <your_jwt_token>` header.
        *   **Generating tokens via the `/token` endpoint:**
            *   The `/token` endpoint itself is *not* protected by JWT.
            *   You send a POST request with an email (currently, any email in a valid format works, but domain verification can be configured for more robust auth if needed for other systems; for Crawl4ai's purpose, the token is the primary gate).
            *   The server responds with an access token.
            *   *Example: Requesting a token with `curl`.*
                ```bash
                curl -X POST "http://localhost:11235/token" \
                     -H "Content-Type: application/json" \
                     -d '{"email": "user@example.com"}'
                ```
                **Expected Response:**
                ```json
                {
                  "email": "user@example.com",
                  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
                  "token_type": "bearer"
                }
                ```
            *   *Example: Requesting a token with Python `requests`.*
                ```python
                import requests
                import json

                response = requests.post(
                    "http://localhost:11235/token",
                    json={"email": "user@example.com"}
                )
                if response.status_code == 200:
                    token_data = response.json()
                    print(f"Access Token: {token_data['access_token']}")
                else:
                    print(f"Error getting token: {response.text}")
                ```
    *   3.5.3. Customizing `config.yml`
        *   **Method 1: Modifying `config.yml` before building a local Docker image.**
            *   **How:** If you're building your own Docker image (Option 3.3.3 or Docker Compose with a local build context), you can directly edit the `deploy/docker/config.yml` file in your cloned repository before running `docker build` or `docker-compose build`.
            *   **Why:** Best if you want the custom configuration to be part of the image itself, ensuring consistency if you distribute or version the image.
        *   **Method 2: Mounting a custom `config.yml` at runtime.**
            *   **How:** Create your custom `config.yml` file on your Docker host machine. Then, when running the container, use a volume mount to replace the default `config.yml` inside the container.
            *   *Code Example:*
                ```bash
                # Assuming your custom config is at ./my-custom-config.yml on the host
                docker run -d -p 11235:11235 \
                  --name crawl4ai-server \
                  --shm-size=1g \
                  --env-file .llm.env \
                  -v "$(pwd)/my-custom-config.yml:/app/config.yml" \
                  unclecode/crawl4ai:0.6.0
                ```
            *   **Why:** Useful for quick configuration changes without rebuilding the image, or for managing configurations separately from the image, especially if you use pre-built images.
    *   3.5.4. Key Configuration Recommendations
        *   **Security:**
            *   **Always** enable `security.jwt_enabled: true` in production or shared environments.
            *   If using a reverse proxy for SSL, set `security.https_redirect: true`.
            *   Configure `security.trusted_hosts` to your server's domain(s) instead of `["*"]` in production.
            *   Review default security headers in `security.headers` and customize if needed for your security policies (e.g., a stricter Content Security Policy).
        *   **Resource Management:**
            *   Adjust `crawler.pool.max_pages` based on your server's RAM and CPU. Too high can lead to instability; too low can underutilize resources.
            *   Set `app.workers` (Uvicorn workers) typically to `(2 * CPU_CORES) + 1` as a starting point, but benchmark for your specific workload.
            *   Tune `crawler.pool.idle_ttl_sec` to balance between keeping browser instances warm (lower TTL) and conserving resources (higher TTL).
        *   **Monitoring:**
            *   Ensure `observability.prometheus.enabled: true` if you use Prometheus.
            *   Integrate the `observability.health_check.endpoint` into your load balancer or container orchestrator health checks.
        *   **Performance:**
            *   For `rate_limiting`, use a Redis backend (`storage_uri: redis://...`) if you have multiple server instances behind a load balancer to share rate limit state. For a single instance, `memory://` is fine.
            *   Adjust `rate_limiting.default_limit` to a reasonable value that protects your server and downstream services without unduly restricting legitimate users.

*   3.6. Interacting with the Dockerized Crawl4ai Server
    *   3.6.1. The Playground Interface (`/playground`)
        *   **How-to:** Open your web browser and navigate to `http://localhost:11235/playground` (or your server's address and port).
        *   **Purpose:**
            *   Provides an interactive UI (Swagger/OpenAPI) to explore all available API endpoints.
            *   Allows you to test API calls directly from your browser.
            *   Shows request and response schemas, making it easy to understand payload structures.
            *   Helps in generating example request payloads for your own client applications.
        *   **Key features to explore:**
            *   Expand each endpoint to see its parameters, request body schema, and possible responses.
            *   Use the "Try it out" button to send test requests.
            *   Examine the "Schemas" section at the bottom to understand the structure of objects like `CrawlRequest`, `BrowserConfig`, `CrawlerRunConfig`, and `CrawlResult`.
    *   3.6.2. Using the Python SDK (`Crawl4aiDockerClient`)
        *   **How-to:**
            ```python
            from crawl4ai.docker_client import Crawl4aiDockerClient
            import asyncio

            client = Crawl4aiDockerClient(base_url="http://localhost:11235")

            async def run_crawl():
                # ... (define browser_config_dict and crawler_config_dict)
                # See "Constructing JSON Configuration Payloads" below for examples
                browser_config_dict = {"type": "BrowserConfig", "params": {"headless": True}}
                crawler_config_dict = {"type": "CrawlerRunConfig", "params": {"screenshot": True}}

                results = await client.crawl(
                    urls=["https://example.com"],
                    browser_config=browser_config_dict,
                    crawler_config=crawler_config_dict
                )
                for result in results:
                    if result.success:
                        print(f"Crawled {result.url}, screenshot available: {bool(result.screenshot)}")
                    else:
                        print(f"Failed {result.url}: {result.error_message}")

            # asyncio.run(run_crawl())
            ```
        *   **Authentication with the SDK when JWT is enabled:**
            *   If your server has `security.jwt_enabled: true`, you'll need to authenticate the client.
            *   *Code Example:*
                ```python
                # client = Crawl4aiDockerClient(base_url="http://localhost:11235")
                # await client.authenticate_with_email(email="user@example.com")
                # Now client will automatically include the token in subsequent requests.
                # Or, if you already have a token:
                # client.set_token("your_jwt_token_here")
                ```
                *Note: The `authenticate_with_email` method is a conceptual example. The actual SDK might require you to fetch the token separately and then use `client.set_token()`.*
        *   **Making `crawl()` requests:**
            *   **Non-streaming (default):**
                *   **When to use:** For a small number of URLs or when you need all results before proceeding.
                *   **How results are returned:** The `client.crawl()` call will block until all URLs are processed, then return a list of `CrawlResult` objects.
            *   **Streaming (`stream=True`):**
                *   **Benefits:** For long-running crawls involving many URLs or when processing time per URL is high. It allows you to process results incrementally as they become available, improving responsiveness and potentially reducing memory footprint if you process and discard results immediately.
                *   **How to process:** The `client.crawl(..., stream=True)` will return an async generator. You iterate over it using `async for`.
            *   *Code Example: Python snippet demonstrating both.*
                ```python
                from crawl4ai.docker_client import Crawl4aiDockerClient
                from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode
                import asyncio

                client = Crawl4aiDockerClient(base_url="http://localhost:11235")
                # Assume client is authenticated if JWT is enabled server-side

                browser_cfg_dict = BrowserConfig(headless=True).dump() # Use .dump() to get the serializable dict
                crawler_cfg_dict_base = CrawlerRunConfig(cache_mode=CacheMode.BYPASS).dump()

                urls_to_crawl = ["https://example.com", "https://crawl4ai.com"]

                async def non_streaming_example():
                    print("\n--- Non-Streaming Example ---")
                    results_list = await client.crawl(
                        urls=urls_to_crawl,
                        browser_config=browser_cfg_dict,
                        crawler_config=crawler_cfg_dict_base
                    )
                    for result_data in results_list: # result_data is a dict here
                        print(f"Non-Streamed: {result_data.get('url')} - Success: {result_data.get('success')}")

                async def streaming_example():
                    print("\n--- Streaming Example ---")
                    crawler_cfg_dict_stream = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=True).dump()
                    async for result_data in client.crawl( # result_data is a dict here
                        urls=urls_to_crawl,
                        browser_config=browser_cfg_dict,
                        crawler_config=crawler_cfg_dict_stream
                    ):
                        print(f"Streamed: {result_data.get('url')} - Success: {result_data.get('success')}")
                        # Process each result as it arrives
                        if result_data.get('status') == 'completed': # Check for final stream completion marker
                            print("Stream ended.")
                            break


                async def main_sdk():
                    # If JWT is enabled:
                    # success = await client.authenticate_with_email(email="user@example.com")
                    # if not success:
                    #     print("SDK Authentication failed.")
                    #     return

                    await non_streaming_example()
                    await streaming_example()

                # asyncio.run(main_sdk())
                ```
        *   **Fetching API Schema with `get_schema()`:**
            *   **How this helps:** The schema describes the structure of `BrowserConfig` and `CrawlerRunConfig`, including all available parameters, their types, and default values. This is useful for programmatically understanding configuration options or validating your payloads.
            *   *Code Example:*
                ```python
                # async def show_schema():
                #     schema = await client.get_schema()
                #     print("BrowserConfig Schema:", json.dumps(schema['browser'], indent=2))
                #     print("CrawlerRunConfig Schema:", json.dumps(schema['crawler'], indent=2))
                # asyncio.run(show_schema())
                ```
    *   3.6.3. Constructing JSON Configuration Payloads
        *   **Understanding the `{"type": "ClassName", "params": {...}}` pattern:**
            *   Crawl4ai uses this pattern for serializing and deserializing configuration objects that can have different underlying implementations (strategies).
            *   `"type"`: The Python class name (e.g., "BrowserConfig", "CrawlerRunConfig", "LLMExtractionStrategy").
            *   `"params"`: A dictionary of the parameters that would be passed to the class's `__init__` method.
            *   **Why this pattern?** It allows the server to dynamically instantiate the correct Python configuration objects from the JSON payload sent by the client.
        *   **How-to:** Translate Python class initializations to JSON:
            *   If you have `BrowserConfig(headless=False, browser_type="firefox")` in Python.
            *   The JSON equivalent is:
                ```json
                {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": false,
                        "browser_type": "firefox"
                    }
                }
                ```
            *   Python `BrowserConfig().dump()` or `CrawlerRunConfig().dump()` methods automatically generate this correct JSON-serializable dictionary structure.
        *   **Common pitfalls:**
            *   Forgetting the `"type"` field.
            *   Incorrectly nesting `"params"`.
            *   Using Python booleans (`True`) instead of JSON booleans (`true`). The `dump()` method handles this.
        *   *Example: JSON payload for a complex `CrawlRequest` for the `/crawl` endpoint.*
            ```json
            {
                "urls": ["https://example.com/news", "https://blog.example.com"],
                "browser_config": {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": true,
                        "user_agent": "MyCustomCrawler/1.0"
                    }
                },
                "crawler_config": {
                    "type": "CrawlerRunConfig",
                    "params": {
                        "screenshot": true,
                        "pdf": false,
                        "word_count_threshold": 50,
                        "cache_mode": "bypass"
                    }
                }
            }
            ```
    *   3.6.4. Direct REST API Usage
        *   **When to prefer direct HTTP requests:**
            *   When integrating Crawl4ai into applications written in languages other than Python.
            *   For simple, one-off requests where setting up the SDK might be overkill.
            *   When you need fine-grained control over HTTP headers or request timing not exposed by the SDK.
        *   **How-to:** Making POST requests to `/crawl` (non-streaming).
            *   *Example: `curl` snippet for `/crawl`.*
                ```bash
                # Ensure you have your JWT token if security is enabled
                # export C4AI_TOKEN="your_jwt_token_here"
                curl -X POST "http://localhost:11235/crawl" \
                     -H "Content-Type: application/json" \
                     -H "Authorization: Bearer $C4AI_TOKEN" \
                     -d '{
                           "urls": ["https://example.com"],
                           "browser_config": {"type": "BrowserConfig", "params": {"headless": true}},
                           "crawler_config": {"type": "CrawlerRunConfig", "params": {"screenshot": false}}
                         }'
                ```
            *   *Python `requests` snippet for `/crawl`.*
                ```python
                # import requests
                # import json
                #
                # headers = {"Content-Type": "application/json"}
                # # if jwt_token: headers["Authorization"] = f"Bearer {jwt_token}"
                #
                # payload = {
                #     "urls": ["https://example.com"],
                #     "browser_config": {"type": "BrowserConfig", "params": {"headless": True}},
                #     "crawler_config": {"type": "CrawlerRunConfig", "params": {"screenshot": False}}
                # }
                # response = requests.post("http://localhost:11235/crawl", headers=headers, json=payload)
                # if response.status_code == 200:
                #     print(json.dumps(response.json(), indent=2))
                # else:
                #     print(f"Error: {response.status_code} - {response.text}")
                ```
        *   **How-to:** Making POST requests to `/crawl/stream` (streaming).
            *   **Understanding NDJSON:** The server will stream back results as Newline Delimited JSON. Each line is a complete JSON object representing a `CrawlResult` for one URL, or a status update.
            *   *Example: `curl` for `/crawl/stream` (NDJSON output will print to terminal).*
                ```bash
                # curl -N -X POST "http://localhost:11235/crawl/stream" \
                #      -H "Content-Type: application/json" \
                #      -H "Authorization: Bearer $C4AI_TOKEN" \
                #      -d '{
                #            "urls": ["https://example.com", "https://crawl4ai.com"],
                #            "browser_config": {"type": "BrowserConfig", "params": {"headless": true}},
                #            "crawler_config": {"type": "CrawlerRunConfig", "params": {"stream": true}}
                #          }'
                ```
            *   *Python `requests` snippet for `/crawl/stream` and processing NDJSON.*
                ```python
                # import requests
                # import json
                #
                # headers = {"Content-Type": "application/json"}
                # # if jwt_token: headers["Authorization"] = f"Bearer {jwt_token}"
                #
                # payload = {
                #     "urls": ["https://example.com", "https://crawl4ai.com"],
                #     "browser_config": {"type": "BrowserConfig", "params": {"headless": True}},
                #     "crawler_config": {"type": "CrawlerRunConfig", "params": {"stream": True}} # stream implicitly handled by endpoint
                # }
                # with requests.post("http://localhost:11235/crawl/stream", headers=headers, json=payload, stream=True) as r:
                #     if r.status_code == 200:
                #         for line in r.iter_lines():
                #             if line:
                #                 result_data = json.loads(line.decode('utf-8'))
                #                 print(f"Streamed API: {result_data.get('url')} - Success: {result_data.get('success')}")
                #                 if result_data.get('status') == 'completed':
                #                     print("Stream ended via API.")
                #                     break
                #     else:
                #         print(f"Error: {r.status_code} - {r.text}")
                ```

*   3.7. Exploring Additional API Endpoints
    These endpoints provide targeted functionalities beyond general crawling.
    *   3.7.1. `/html`: Generating Preprocessed HTML
        *   **Purpose:** Use this when you need the HTML of a page after JavaScript execution and basic sanitization (e.g., removing scripts, styles), but *before* Crawl4ai's full Markdown conversion or complex filtering. It's ideal for feeding into custom HTML parsers or schema extraction tools that expect mostly clean, rendered HTML.
        *   **Request structure (`HTMLRequest`):**
            *   `url (str)`: The URL to fetch.
        *   **Response structure:**
            *   `html (str)`: The preprocessed HTML content.
            *   `url (str)`: The original URL requested.
            *   `success (bool)`: Indicates if the operation was successful.
        *   *Example: Use case: You have an external tool that extracts microdata from HTML. Use `/html` to get the rendered HTML for this tool.*
            ```bash
            # curl -X POST "http://localhost:11235/html" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://example.com/dynamic-page"}'
            ```
    *   3.7.2. `/screenshot`: Capturing Web Pages
        *   **Purpose:** To obtain a visual snapshot (PNG) of a web page as it's rendered by the browser. Useful for archiving, visual verification, or when textual content alone isn't sufficient.
        *   **Key parameters (`ScreenshotRequest`):**
            *   `url (str)`: The URL to capture.
            *   `screenshot_wait_for (Optional[float])`: Seconds to wait after page load *before* taking the screenshot.
                *   **How to use:** Essential for pages with animations, delayed content loading via JS, or elements that appear after a short interval. Set this to a few seconds (e.g., `2.0` or `5.0`) to allow such content to render.
            *   `output_path (Optional[str])`:
                *   If provided (e.g., `"./screenshots/page.png"`), the server saves the screenshot to this path *relative to the server's filesystem*. The response will contain the absolute path.
                *   If `null` or omitted, the screenshot is returned as a base64-encoded string in the JSON response.
                *   **Decision:** Use `output_path` if the server has persistent storage and you want files saved directly. Omit it if the client needs to receive and handle the image data.
        *   **Response structure:**
            *   `success (bool)`
            *   `path (str)`: (If `output_path` was provided) Absolute path to the saved screenshot on the server.
            *   `screenshot (str)`: (If `output_path` was *not* provided) Base64 encoded PNG data.
        *   *Example: Capturing a screenshot of a dynamic page after a 2-second delay and receiving it as base64.*
            ```bash
            # curl -X POST "http://localhost:11235/screenshot" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://example.com/animated-chart", "screenshot_wait_for": 2.0}'
            ```
    *   3.7.3. `/pdf`: Generating PDFs
        *   **Purpose:** To create a PDF document from a rendered web page. Useful for printable versions, archiving, or offline reading.
        *   **Key parameters (`PDFRequest`):**
            *   `url (str)`: The URL to convert to PDF.
            *   `output_path (Optional[str])`: Similar to `/screenshot`, if provided, saves the PDF to this server-side path. Otherwise, returns base64 PDF data.
        *   **Response structure:**
            *   `success (bool)`
            *   `path (str)`: (If `output_path` was provided) Absolute path to the saved PDF on the server.
            *   `pdf (str)`: (If `output_path` was *not* provided) Base64 encoded PDF data.
        *   *Example: Generating a PDF for documentation and saving it on the server.*
            ```bash
            # curl -X POST "http://localhost:11235/pdf" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://crawl4ai.com/docs", "output_path": "/app_data/pdfs/crawl4ai_docs.pdf"}'
            ```
    *   3.7.4. `/execute_js`: Running Custom JavaScript
        *   **Purpose:** This is a powerful endpoint for advanced page interactions. Use it when you need to:
            *   Click buttons, fill forms, or trigger other UI events programmatically.
            *   Extract data that is only available after certain JS execution (e.g., from dynamically generated DOM elements).
            *   Modify the page content or state before further processing or screenshotting.
        *   **Key parameters (`JSEndpointRequest`):**
            *   `url (str)`: The URL on which to execute the scripts.
            *   `scripts (List[str])`: A list of JavaScript code snippets to execute in order.
            *   **Best practices for JS snippets:**
                *   Each script in the list should be an **expression that returns a value**, or an IIFE (Immediately Invoked Function Expression).
                *   If a script is asynchronous (e.g., involves `fetch` or `setTimeout`), it **must** return a `Promise`. Crawl4ai will `await` this promise.
                *   Keep snippets focused. For complex logic, consider breaking it into multiple steps in the `scripts` list.
                *   Be mindful of the page's context. Your script runs within the browser's environment for that page.
                *   *Example of a good snippet:* `async () => { await new Promise(r => setTimeout(r, 1000)); return document.title; }`
                *   *Example of a snippet that might not work as expected if not returning a promise for async ops:* `setTimeout(() => { console.log('done'); }, 1000);` (Crawl4ai might not wait for this).
        *   **Response structure:** The full `CrawlResult` object (as a JSON serializable dictionary). The results of your JS executions will be in the `js_execution_result` field of the `CrawlResult`. This field will be a dictionary where keys are `script_0`, `script_1`, etc., and values are the return values of your corresponding scripts.
            *   **How to access:** `response_json['results'][0]['js_execution_result']['script_0']`
        *   *Example: Clicking a "Load More" button and then extracting the count of new items.*
            ```python
            # Python client example
            # js_scripts = [
            #     "document.querySelector('#load-more-btn').click();",
            #     "async () => { await new Promise(r => setTimeout(r, 2000)); return document.querySelectorAll('.item').length; }"
            # ]
            # payload = {"url": "https://example.com/infinite-scroll", "scripts": js_scripts}
            # # ... make request to /execute_js ...
            # # new_item_count = response_data['results'][0]['js_execution_result']['script_1']
            ```
    *   3.7.5. `/ask` (or `/library-context`): Retrieving Library Context for AI
        *   **Purpose:** This endpoint is designed to provide contextual information about the Crawl4ai library itself. It's intended to be used by AI assistants (like code generation copilots or RAG systems) to help them understand Crawl4ai's API, features, and documentation, enabling them to generate more accurate code snippets or provide better assistance.
        *   **Key parameters:**
            *   `context_type (str)`: `"code"`, `"doc"`, or `"all"`.
                *   **When to use:**
                    *   `"code"`: For fetching relevant code snippets (functions, classes).
                    *   `"doc"`: For fetching relevant documentation sections.
                    *   `"all"`: For fetching both.
            *   `query (Optional[str])`: A natural language query or keywords. The endpoint uses BM25 (a text retrieval algorithm) to find relevant chunks of code or documentation based on this query.
                *   **How to formulate:** Be specific. E.g., "how to set proxy in BrowserConfig", "CrawlerRunConfig screenshot options".
            *   `score_ratio (float, default: 0.5)`: A value between 0.0 and 1.0. It filters results based on their BM25 score relative to the maximum possible score for the query. A higher `score_ratio` means more stringent filtering (fewer, more relevant results).
                *   **Understanding its impact:** Start with the default. If you get too few results, lower it. If too many irrelevant results, increase it.
            *   `max_results (int, default: 20)`: The maximum number of code chunks or documentation sections to return.
        *   **Response structure:** A JSON object containing:
            *   `code_results (List[Dict])`: If `context_type` includes "code". Each dict has `"text"` (the code chunk) and `"score"`.
            *   `doc_results (List[Dict])`: If `context_type` includes "doc". Each dict has `"text"` (the documentation chunk) and `"score"`.
        *   *Example: Using `/ask` to get information about `BrowserConfig` for an AI assistant.*
            ```bash
            # curl -X GET "http://localhost:11235/ask?context_type=all&query=BrowserConfig%20proxy%20settings&max_results=3" \
            #      -H "Authorization: Bearer $C4AI_TOKEN"
            ```
            This would return code snippets and documentation sections related to proxy settings in `BrowserConfig`.

*   3.8. MCP (Model Context Protocol) Integration
    *   3.8.1. What is MCP and Why Use It?
        *   **Explanation:** MCP (Model Context Protocol) is a standardized way for AI models and development tools (like IDE extensions) to interact with external services and fetch context. Crawl4ai's MCP support allows AI tools that understand MCP (e.g., Anthropic's Claude Code extension for VS Code) to directly use Crawl4ai's functionalities.
        *   **Benefits:**
            *   **Seamless Tool Integration:** AI tools can discover and use Crawl4ai's capabilities without custom API integrations for each tool.
            *   **Contextual Awareness:** The AI model gets structured information about what a tool can do, its parameters, and how to interpret its output.
            *   **Enhanced AI Assistance:** Enables AI to, for example, suggest Crawl4ai code, execute crawls, or get information from web pages directly within the development environment.
    *   3.8.2. Connection Endpoints: `/mcp/sse` and `/mcp/ws`
        *   **SSE (Server-Sent Events - `/mcp/sse`):** A unidirectional stream from server to client. Simpler for many MCP use cases where the tool primarily sends a request and awaits a response or stream of updates.
        *   **WebSockets (`/mcp/ws`):** A bidirectional, persistent connection. More suitable for highly interactive tools or when continuous two-way communication is needed.
        *   **When to choose:** For most current MCP integrations (like with Claude Code), SSE is often sufficient and simpler to implement on the client-tool side.
    *   3.8.3. **How-to:** Integrating with Claude Code
        *   The `claude mcp add` command registers an MCP-compliant service with your Claude Code extension.
        *   *Example:*
            ```bash
            # Assuming your Crawl4ai server is running locally
            claude mcp add -t sse c4ai-mcp-service http://localhost:11235/mcp/sse
            ```
            *Replace `c4ai-mcp-service` with a name of your choice for this tool in Claude.*
        *   **Illustrative workflow:**
            1.  Add the Crawl4ai MCP service to Claude Code.
            2.  In your code editor, you might ask Claude: "@c4ai-mcp-service Get the Markdown for example.com".
            3.  Claude, understanding MCP, would interact with the `/mcp/sse` endpoint, invoke the appropriate Crawl4ai tool (likely the `md` or `crawl` tool), and return the result to you in the editor.
    *   3.8.4. Available MCP Tools and Their Use Cases
        *   The tools exposed via MCP largely mirror the additional API endpoints:
            *   `md`: Get Markdown content for a URL. **Use case:** Quickly summarize a page for an LLM.
            *   `html`: Get preprocessed HTML. **Use case:** Provide cleaner HTML to an AI for parsing.
            *   `screenshot`: Get a screenshot. **Use case:** Visual context for an AI, or for documentation.
            *   `pdf`: Get a PDF. **Use case:** Archival or providing document context.
            *   `execute_js`: Run JavaScript on a page. **Use case:** Interact with dynamic elements before an AI processes the page.
            *   `crawl`: Perform a full crawl operation. **Use case:** Comprehensive data gathering directed by an AI.
            *   `ask`: Query library context. **Use case:** AI asks Crawl4ai about its own capabilities to generate better code.
    *   3.8.5. Testing MCP Connections and Tool Usage
        *   **Simple methods:**
            *   Use `curl` with the `-N` (no-buffering) flag for SSE to see the event stream:
                ```bash
                # Example: Test list_tools via MCP/SSE
                # You'd typically send a JSON-RPC request in the first message after connection.
                # This is a simplified conceptual test.
                # curl -N -H "Content-Type: application/json" http://localhost:11235/mcp/sse
                # (Then send a JSON-RPC request for list_tools on the established connection if the tool supports it interactively)
                ```
            *   Use a WebSocket client (like `wscat` or a browser's developer console) to connect to `/mcp/ws` and send JSON-RPC messages.
            *   The best way to test is often through an MCP-compliant client tool like the Claude Code extension.
    *   3.8.6. Accessing MCP Schemas (`/mcp/schema`)
        *   **How this helps:** This endpoint returns a JSON schema describing all available MCP tools, their methods, parameters, and return types. This is how MCP client tools (like Claude Code) discover what Crawl4ai can do via MCP. It's crucial for the self-describing nature of MCP.

*   3.9. Monitoring Your Crawl4ai Server
    *   3.9.1. Health Checks with `/health`
        *   **Purpose:** A simple endpoint to verify that the Crawl4ai server is running and responsive. Commonly used by load balancers, container orchestrators (like Kubernetes), or uptime monitoring services.
        *   **Interpreting the response:**
            *   A `200 OK` response with JSON like `{"status": "ok", "timestamp": ..., "version": "..."}` indicates the server is healthy.
            *   Any other status code or an inability to connect suggests a problem.
    *   3.9.2. Prometheus Metrics with `/metrics`
        *   **How to integrate:** If `observability.prometheus.enabled: true` in `config.yml` (default is true), this endpoint exposes metrics in Prometheus format. Configure your Prometheus server to scrape this endpoint.
        *   **Overview of important metrics (inferred from `prometheus_fastapi_instrumentator` usage):**
            *   Request counts, latencies, and error rates for API endpoints.
            *   Python process information (CPU, memory - if default instrumentator collectors are active).
            *   Potentially custom metrics related to crawl queue length, active browser instances, etc. (though these might need explicit addition in `server.py`).
        *   **Why use:** Essential for understanding server load, performance bottlenecks, error trends, and for setting up alerts.

*   3.10. Understanding the Server's Inner Workings (High-Level for Users)
    Understanding these components can help you configure the server optimally and troubleshoot issues.
    *   3.10.1. FastAPI Application: The Core of the Server
        *   **Role:** FastAPI is a modern, fast web framework for building APIs with Python. It handles incoming HTTP requests, routing, request validation, and response serialization for all Crawl4ai API endpoints.
        *   **Why it's used:** Its performance, ease of use, and automatic data validation/serialization make it well-suited for building robust APIs like Crawl4ai's.
    *   3.10.2. Managing Browser Instances with `crawler_pool`
        *   **Role:** The `crawler_pool` (likely an instance of `BrowserManager` or a similar custom pool) is responsible for managing a collection of `AsyncWebCrawler` instances.
        *   `get_crawler`: When an API request needs a browser, this function provides an available (and potentially pre-warmed) `AsyncWebCrawler` instance from the pool. If all instances are busy, it might create a new one up to a limit, or wait.
        *   `close_all` and `janitor`: These are crucial for resource management.
            *   `close_all` is typically called on server shutdown to gracefully close all browser instances.
            *   The `janitor` task (referenced in `lifespan`) periodically checks for idle browser instances in the pool and closes them if they've exceeded their `idle_ttl_sec` (configured in `config.yml`).
        *   **Impact:** Proper pool management prevents resource leaks (e.g., too many zombie browser processes) and optimizes browser startup times by reusing instances.
    *   3.10.3. Capping Concurrent Pages with `GLOBAL_SEM`
        *   **Role:** `GLOBAL_SEM` (an `asyncio.Semaphore`) acts as a server-wide gatekeeper, limiting the total number of browser pages that can be concurrently active across all `AsyncWebCrawler` instances.
        *   **Why this is important:** Each browser page consumes significant memory and CPU. Without a cap, a high volume of requests could easily overwhelm the server, leading to crashes or extreme slowdowns.
        *   **How `crawler.pool.max_pages` in `config.yml` relates:** This configuration value directly sets the limit for `GLOBAL_SEM`.
        *   **Decision:** Adjust `max_pages` carefully based on your server's RAM. If you see `asyncio.TimeoutError` or tasks getting stuck waiting for the semaphore, you might have too many concurrent requests for your `max_pages` setting, or individual crawls are taking too long.
    *   3.10.4. Asynchronous Task Management (Job Router - `api.py` based)
        *   **Role:** For operations that can be time-consuming (like a crawl involving many URLs, or an LLM extraction that requires multiple API calls), Crawl4ai often offloads these to background tasks. This is especially true for non-streaming `/crawl` or `/llm/{url_or_task_id}` endpoints.
        *   The "job router" (conceptually, parts of `api.py` and `job.py`) handles:
            1.  Receiving the initial request.
            2.  Assigning a unique `task_id`.
            3.  Storing initial task metadata (URL, status: PENDING/PROCESSING) often in Redis.
            4.  Adding the actual work (e.g., `process_llm_extraction` or `handle_crawl_job`) to a FastAPI `BackgroundTasks` queue or a more robust Celery/RQ queue (if integrated).
            5.  Returning the `task_id` to the client immediately.
            6.  The client then polls a status endpoint (e.g., `/task/{task_id}`) to check progress.
            7.  Once the background task completes, it updates the task's status and result in Redis.
        *   **Role of Redis:**
            *   Stores task state (status, result, error).
            *   Can act as a message broker for task queues in more advanced setups.
        *   **User Interaction:** You submit a job, get a task ID, and then poll for completion. This prevents HTTP timeouts for long-running operations.
    *   3.10.5. Rate Limiting and Security Middleware
        *   **How `config.yml` settings are applied:** FastAPI allows "middleware" to process requests before they hit your main endpoint logic and before responses are sent.
            *   **Rate Limiting:** The `slowapi` library is used. Middleware intercepts each request, checks the client's IP (or token identity) against configured limits (e.g., "1000/minute" from `config.yml`) stored in memory or Redis. If limits are exceeded, it returns a `429 Too Many Requests` error.
            *   **Security:** Middleware like `HTTPSRedirectMiddleware` and `TrustedHostMiddleware` enforce security policies (redirecting HTTP to HTTPS, validating Host headers). Security headers are added to outgoing responses.
        *   **Protections offered:**
            *   Rate limiting: Prevents abuse and server overload.
            *   HTTPS redirect: Enforces secure connections.
            *   Trusted hosts: Mitigates host header injection attacks.
            *   Security headers: Protect against common web vulnerabilities like XSS, clickjacking.
    *   3.10.6. Mapping API Requests to `AsyncWebCrawler`
        *   1.  An HTTP request hits a FastAPI endpoint (e.g., `POST /crawl`).
        *   2.  FastAPI, using Pydantic, validates and parses the JSON request body into a `CrawlRequest` Pydantic model. This model contains `urls`, `browser_config` (as a dict), and `crawler_config` (as a dict).
        *   3.  The endpoint logic uses `BrowserConfig.load(browser_config_dict)` and `CrawlerRunConfig.load(crawler_config_dict)` to convert these dictionaries back into their respective Python configuration objects.
        *   4.  It then calls `await crawler_pool.get_crawler(browser_config_object)` to obtain an appropriate `AsyncWebCrawler` instance. The pool might reuse an existing compatible instance or create a new one.
        *   5.  Finally, `await crawler_instance.arun(url=..., config=crawler_run_config_object)` or `await crawler_instance.arun_many(...)` is called to perform the actual crawl.
        *   **Key takeaway:** The `{"type": ..., "params": ...}` JSON structure is crucial for the server to correctly deserialize configurations passed from clients into the Python objects `AsyncWebCrawler` expects. The `.dump()` methods on config objects are the Pythonic way to generate these serializable dicts.

## 4. Understanding Crawl4ai Versioning

Crawl4ai follows Semantic Versioning (SemVer) to help you manage updates and understand the implications of new releases.

*   4.1. Semantic Versioning (`MAJOR.MINOR.PATCH`)
    *   **`MAJOR` (e.g., `0.x.x` -> `1.x.x`):** Incremented for **incompatible API changes** (breaking changes). You will likely need to update your code when upgrading to a new major version.
        *   *Why it matters:* Pay close attention when a MAJOR version changes. Read release notes carefully.
    *   **`MINOR` (e.g., `0.5.x` -> `0.6.x`):** Incremented for **new functionality added in a backward-compatible manner**. Your existing code should continue to work.
        *   *Why it matters:* You can usually upgrade minor versions safely to get new features and improvements.
    *   **`PATCH` (e.g., `0.6.0` -> `0.6.1`):** Incremented for **backward-compatible bug fixes**.
        *   *Why it matters:* It's generally safe and recommended to apply patch updates.
    *   **Why this is important for users:** SemVer provides predictability. You can configure your dependency management (e.g., in `requirements.txt` or `pyproject.toml`) to allow automatic patch and minor updates (e.g., `crawl4ai~=0.6.0`) but require manual intervention for major updates.

*   4.2. Pre-release Suffixes
    Crawl4ai uses standard suffixes for pre-release versions, allowing users to test upcoming features.
    *   `dev` (e.g., `0.7.0.dev1`): **Development versions.** These are typically built automatically from the main development branch. They are the most cutting-edge but can be unstable and are not recommended for production.
    *   `a` (alpha, e.g., `0.7.0a1`): **Alpha releases.** Early previews of new major or minor versions. Features might be incomplete or buggy. Use for testing and providing early feedback.
    *   `b` (beta, e.g., `0.7.0b1`): **Beta releases.** Feature-set is largely complete, but the release is still undergoing testing and refinement. More stable than alpha but may still contain bugs.
    *   `rc` (release candidate, e.g., `0.7.0rc1`): **Release candidates.** Believed to be stable and ready for final release, pending final testing. Good for testing in staging environments.
    *   **Guidance on when to use pre-release versions:**
        *   Use `dev`, `a`, or `b` if you want to experiment with upcoming features or contribute to testing, but be prepared for instability.
        *   Use `rc` if you want to test the very latest potentially stable version before its official release.
        *   For production, always stick to stable releases (no suffix).
        *   To install pre-releases: `pip install crawl4ai --pre`.

## 5. Troubleshooting Common Deployment Issues

Here are some common issues you might encounter and how to approach them:

*   5.1. Library Installation Problems
    *   **Playwright browser download failures:**
        *   **Symptom:** `crawl4ai-setup` or `playwright install` fails with network errors or messages about not being able to download browsers.
        *   **Reasoning:** Often due to network connectivity issues, firewalls, or proxies blocking the download. Playwright needs to download browser binaries which can be large.
        *   **Solution:**
            *   Ensure stable internet connection.
            *   If behind a proxy, configure Playwright's proxy environment variables (`HTTP_PROXY`, `HTTPS_PROXY`).
            *   Try running `playwright install --with-deps chromium` (or your browser of choice) manually to see more detailed error messages.
            *   Check Playwright's documentation for troubleshooting browser downloads.
    *   **Dependency conflicts:**
        *   **Symptom:** `pip install crawl4ai` fails with messages about conflicting package versions.
        *   **Reasoning:** Your existing Python environment might have packages with versions incompatible with Crawl4ai's dependencies.
        *   **Solution:**
            *   **Best Practice:** Use a virtual environment (e.g., `venv`, `conda`) for your Crawl4ai projects to isolate dependencies.
            *   Examine the error messages to identify the conflicting packages and try to resolve them, perhaps by upgrading/downgrading other packages or installing Crawl4ai in a fresh environment.

*   5.2. Docker Deployment Issues
    *   **Port conflicts:**
        *   **Symptom:** `docker run` or `docker-compose up` fails with an error like "port is already allocated."
        *   **Reasoning:** The default port for Crawl4ai (11235) is already in use by another application on your host machine.
        *   **Solution:**
            *   Stop the other application using the port.
            *   Map Crawl4ai to a different host port: `docker run -p <new_host_port>:11235 ...` (e.g., `-p 11236:11235`). Remember to update your client to use the new host port.
    *   **Incorrect environment variable setup for LLM API keys:**
        *   **Symptom:** LLM-dependent features (like `LLMExtractionStrategy`) fail, often with authentication errors from the LLM provider.
        *   **Reasoning:** The Docker container doesn't have access to the necessary API keys.
        *   **Solution:** Ensure you are correctly passing the `.llm.env` file when running the container (`--env-file .llm.env`) or that environment variables are set through Docker Compose or your orchestration platform. Double-check the variable names in your `.llm.env` file match what `config.yml` expects (e.g., `OPENAI_API_KEY`).
    *   **Memory allocation issues (`--shm-size`):**
        *   **Symptom:** Browsers inside Docker crash, pages fail to load with cryptic errors, or the container itself becomes unresponsive, especially under load.
        *   **Reasoning:** Chromium-based browsers use `/dev/shm` (shared memory) extensively. The Docker default for `/dev/shm` (often 64MB) is usually too small for multiple or complex browser tabs.
        *   **Solution:** Always run your Crawl4ai Docker container with an increased shared memory size. Start with `--shm-size=1g`. If issues persist, try `2g`. The `docker-compose.yml` provided in the Crawl4ai repository typically includes a volume mount for `/dev/shm` which effectively does the same.
    *   **Problems building local Docker images:**
        *   **Symptom:** `docker build` or `docker-compose build` fails.
        *   **Reasoning:** Could be network issues during dependency downloads, incorrect build arguments, problems with the Dockerfile syntax (if modified), or insufficient disk space.
        *   **Solution:**
            *   Check your internet connection.
            *   Carefully review the build arguments you're passing (`INSTALL_TYPE`, `ENABLE_GPU`, etc.).
            *   Examine the Docker build output for specific error messages.
            *   Ensure you have enough disk space.

*   5.3. Server Configuration (`config.yml`) Errors
    *   **YAML syntax errors:**
        *   **Symptom:** Server fails to start, with errors related to parsing `config.yml`.
        *   **Reasoning:** Incorrect indentation, missing colons, or other YAML syntax issues.
        *   **Solution:** Use a YAML linter or validator to check your `config.yml` file. Pay close attention to indentation (spaces, not tabs).
    *   **Misconfigured JWT settings:**
        *   **Symptom:** If `jwt_enabled: true`, clients might get `401 Unauthorized` or `403 Forbidden` errors even with what seems like a correct token.
        *   **Reasoning:** Issues with secret key consistency (if applicable, though Crawl4ai uses a fixed default or one configurable via env var), token expiration, or incorrect algorithm settings (though Crawl4ai handles this internally).
        *   **Solution:** Ensure clients are sending the token correctly in the `Authorization: Bearer <token>` header. Regenerate tokens if they might have expired. For complex JWT issues, you might need to debug the token generation/validation logic if you've heavily customized the server.

*   5.4. API Interaction Problems
    *   **Authentication failures:**
        *   **Symptom:** Client receives `401` or `403` errors.
        *   **Reasoning:** JWT is enabled on the server, but the client is not sending a valid token, or the token has expired.
        *   **Solution:** Ensure your client correctly obtains a token from `/token` and includes it in the `Authorization` header for subsequent requests.
    *   **Incorrectly structured request payloads:**
        *   **Symptom:** Client receives `422 Unprocessable Entity` errors.
        *   **Reasoning:** The JSON payload sent to endpoints like `/crawl` does not match the expected Pydantic schema (e.g., missing required fields, incorrect data types, wrong `{"type": ..., "params": ...}` structure for configs).
        *   **Solution:** Refer to the `/playground` (Swagger UI) for the correct request schemas. Use the `dump()` method of `BrowserConfig` and `CrawlerRunConfig` if constructing payloads in Python to ensure correct serialization.
    *   **Understanding error responses from the API:**
        *   The API usually returns JSON error responses with a `detail` field explaining the issue. Pay attention to this field.
        *   HTTP status codes also provide clues (400 for bad request, 401/403 for auth, 404 for not found, 422 for validation, 500 for server errors).

*   5.5. When to Check Server Logs
    *   **How to access Docker container logs:**
        ```bash
        docker logs crawl4ai-server # Replace crawl4ai-server with your container name/ID
        docker logs -f crawl4ai-server # To follow logs in real-time
        ```
        If using Docker Compose:
        ```bash
        docker-compose logs crawl4ai # Assuming 'crawl4ai' is the service name in docker-compose.yml
        ```
    *   **What to look for:**
        *   Python tracebacks indicating exceptions within the server code.
        *   Log messages from `crawl4ai` itself (often prefixed with tags like `[CRAWLER]`, `[ERROR]`, `[CONFIG]`).
        *   Uvicorn/FastAPI startup messages and request logs.
        *   Any messages related to resource limits (memory, file descriptors).
        *   Playwright browser errors if they are not caught and handled by the application.

## 6. Best Practices for Deployment

*   6.1. **Choosing the Right Deployment Method:**
    *   **Library:** For quick scripts, Python-centric projects, or when direct integration is paramount.
    *   **Docker (Pre-built):** For ease of use, standard deployments, and quick server setup.
    *   **Docker Compose:** For managing Crawl4ai with other services (like Redis) or for simplified local builds with custom arguments.
    *   **Docker (Manual Build):** For full customization, development, or specific CI/CD needs.
*   6.2. **Security Considerations for Server Deployment:**
    *   **Always enable JWT (`security.jwt_enabled: true`)** if the server is accessible beyond your local machine.
    *   Use strong, unique secrets for JWT if you customize it (though Crawl4ai has a default mechanism).
    *   Configure `security.trusted_hosts` to specific domains in production.
    *   Use a reverse proxy (like Nginx or Traefik) to handle SSL/TLS termination and potentially add another layer of security (WAF, IP blocking).
    *   Keep API keys and sensitive configurations out of version control; use `.llm.env` or environment variables.
*   6.3. **Monitoring and Scaling Your Dockerized Server:**
    *   Utilize the `/health` endpoint for liveness/readiness probes in orchestrators.
    *   Integrate `/metrics` with Prometheus and Grafana for performance monitoring and alerting.
    *   Scale horizontally (more container instances) behind a load balancer for high availability and increased throughput.
    *   Adjust `crawler.pool.max_pages` and container resources (CPU, RAM, `--shm-size`) based on observed load and performance.
*   6.4. **Managing Dependencies and Upgrades:**
    *   For library usage, use virtual environments.
    *   For Docker, pin to specific image versions (e.g., `unclecode/crawl4ai:0.6.0`) in production to avoid unexpected updates.
    *   Read release notes carefully before upgrading `MAJOR` or `MINOR` versions.
*   6.5. **Leveraging Configuration for Optimal Performance and Cost-Effectiveness:**
    *   Use appropriate `CacheMode` settings in `CrawlerRunConfig` to avoid re-crawling unchanged content.
    *   Fine-tune `word_count_threshold` and content filters to process only relevant data, especially before sending to costly LLMs.
    *   If using LLM extraction, design efficient prompts and schemas. Consider if a simpler CSS/XPath extraction can achieve the same for some fields.
    *   Adjust `crawler.pool.idle_ttl_sec` to balance resource usage and browser startup latency.

## 7. Next Steps & Further Learning

With a solid understanding of deployment, you're ready to explore more advanced capabilities:

*   7.1. **Exploring Advanced Crawler Configuration (`CrawlerRunConfig`):** Dive into parameters like `js_code`, `wait_for`, various filters (`word_count_threshold`, `exclude_paths`), and media handling options.
*   7.2. **Diving Deeper into Extraction Strategies:** Learn about `LLMExtractionStrategy`, `JsomCssExtractionStrategy`, and how to build custom schemas for precise data extraction.
*   7.3. **Advanced Page Interaction Techniques:** Master the use of `js_code` for complex interactions, form submissions, and handling dynamic content that simple waits can't manage.
*   7.4. **Contributing to Crawl4ai:** If you're interested in improving Crawl4ai, check out the [contribution guidelines](https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTORS.md) and open issues/PRs.

This deployment guide should provide a strong foundation. Remember that the best configuration often comes from understanding your specific use case, experimenting, and monitoring performance. Happy Crawling!
```

---


## Deployment - Examples
Source: crawl4ai_deployment_examples_content.llm.md

```markdown
# Examples for `crawl4ai` - Deployment Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_deployment.md`
**Library Version Context:** 0.5.1-d1
**Outline Generation Date:** 2025-05-24
---

This document provides runnable code examples showcasing the diverse usage patterns and configurations of the `crawl4ai` deployment component. The examples primarily focus on interacting with the API provided by a deployed Crawl4ai instance.

## I. Introduction to Crawl4ai Deployment Examples

### 1.1. Overview of the API and common interaction patterns (e.g., using `requests` library).
The Crawl4ai deployment exposes a FastAPI backend. Most examples will use the `requests` library for synchronous calls and `httpx` for asynchronous calls to interact with these API endpoints. The base URL for a local deployment is typically `http://localhost:11235`.

```python
import requests
import httpx # For async examples later
import asyncio
import json
import time
import os
import base64

# Assume the Crawl4ai API is running locally
BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
API_TOKEN = os.environ.get("CRAWL4AI_API_TOKEN") # Set if your API requires auth

def get_headers():
    if API_TOKEN:
        return {"Authorization": f"Bearer {API_TOKEN}"}
    return {}

print(f"Crawl4AI API Base URL: {BASE_URL}")
if API_TOKEN:
    print("API Token will be used for authenticated requests.")
else:
    print("No API Token found in env; assuming API does not require authentication for these examples.")

# A simple synchronous GET request
try:
    response = requests.get(f"{BASE_URL}/health")
    response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)
    print(f"Health check successful: {response.json()}")
except requests.exceptions.RequestException as e:
    print(f"Error connecting to Crawl4AI API: {e}")
    print("Please ensure the Crawl4AI Docker container or server is running.")
```

### 1.2. Note on Authentication: Brief explanation of when and how to use API tokens.
If JWT authentication is enabled in `config.yml` (via `security.jwt_enabled: true`), most API endpoints will require an `Authorization: Bearer <YOUR_TOKEN>` header. You can obtain a token from the `/token` endpoint using a whitelisted email address. The `get_headers()` helper function in the examples will attempt to use `CRAWL4AI_API_TOKEN` if set.

---
## II. Docker and Docker-Compose

### 2.1. Building the Docker Image

#### 2.1.1. Example: Basic `docker build` command.
This command builds the default Docker image from the root of the `crawl4ai` repository.
```bash
# Navigate to the root of the crawl4ai repository
# cd /path/to/crawl4ai
docker build -t crawl4ai:latest .
```

#### 2.1.2. Example: Building with `INSTALL_TYPE=all` build argument.
This installs all optional dependencies, including those for advanced AI/ML features.
```bash
# Navigate to the root of the crawl4ai repository
# cd /path/to/crawl4ai
docker build --build-arg INSTALL_TYPE=all -t crawl4ai:all-features .
```

#### 2.1.3. Example: Building with `ENABLE_GPU=true` build argument (conceptual, as GPU usage is complex).
This attempts to include GPU support (e.g., CUDA toolkits) if the base image and host support it.
```bash
# Navigate to the root of the crawl4ai repository
# cd /path/to/crawl4ai
# Ensure your Docker daemon and host are configured for GPU passthrough
docker build --build-arg ENABLE_GPU=true --build-arg TARGETARCH=amd64 -t crawl4ai:gpu-amd64 .
# For ARM64 with GPU (e.g., NVIDIA Jetson), you might need specific base images or configurations.
# docker build --build-arg ENABLE_GPU=true --build-arg TARGETARCH=arm64 -t crawl4ai:gpu-arm64 .
```
**Note:** Full GPU support in Docker can be complex and depends on your host system, NVIDIA drivers, and Docker version. The `Dockerfile` provides a basic attempt.

---
### 2.2. Running with Docker Compose

#### 2.2.1. Example: Basic `docker-compose up` using the provided `docker-compose.yml`.
This starts the Crawl4ai service as defined in the `docker-compose.yml` file.
```bash
# Navigate to the directory containing docker-compose.yml
# cd /path/to/crawl4ai
docker-compose up -d
```

#### 2.2.2. Example: Overriding image tag in `docker-compose` via environment variable `TAG`.
You can specify a different image tag for the `crawl4ai` service.
```bash
# Example: Using a specific version tag
TAG=0.6.0 docker-compose up -d

# Example: Using a custom built tag
# TAG=my-custom-crawl4ai-build docker-compose up -d
```

#### 2.2.3. Example: Overriding `INSTALL_TYPE` in `docker-compose` via environment variable.
If your `docker-compose.yml` is set up to use build arguments from environment variables, you can override `INSTALL_TYPE`.
```bash
# Assuming docker-compose.yml uses INSTALL_TYPE from env for the build context:
# (The provided docker-compose.yml directly passes it as a build arg)
# If you modify docker-compose.yml to pick up an env var for INSTALL_TYPE:
# INSTALL_TYPE=all docker-compose up -d --build
```
**Note:** The provided `docker-compose.yml` directly sets `INSTALL_TYPE` in the `args` section. To make it environment-variable driven like `TAG`, you would modify the `docker-compose.yml`'s `build.args` section.

---
### 2.3. Configuration via Environment Variables & `.llm.env`

#### 2.3.1. Example: Setting `OPENAI_API_KEY` using an `.llm.env` file.
Create a `.llm.env` file in the same directory as `docker-compose.yml` or where you run the server.
```text
# Contents of .llm.env
OPENAI_API_KEY="sk-your_openai_api_key_here"
```
The `docker-compose.yml` (or server if run directly) will load this file.

#### 2.3.2. Example: Showing how to pass multiple LLM API keys via `.llm.env`.
You can add keys for various supported LLM providers.
```text
# Contents of .llm.env
OPENAI_API_KEY="sk-your_openai_api_key_here"
ANTHROPIC_API_KEY="sk-ant-your_anthropic_api_key_here"
GROQ_API_KEY="gsk_your_groq_api_key_here"
# ...and other keys supported by LiteLLM
```

---
### 2.4. Accessing the Deployed Service

#### 2.4.1. Example: Python script to perform a basic health check (`/health`) on the locally deployed service.
```python
import requests
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")

try:
    response = requests.get(f"{BASE_URL}/health")
    response.raise_for_status()
    data = response.json()
    print(f"Service is healthy. Version: {data.get('version')}, Timestamp: {data.get('timestamp')}")
except requests.exceptions.RequestException as e:
    print(f"Failed to connect or health check failed: {e}")
```

#### 2.4.2. Example: Accessing the API playground at `/playground`.
Open your web browser and navigate to `http://localhost:11235/playground` (or your deployed URL + `/playground`). This will show the FastAPI interactive API documentation.

---
### 2.5. Understanding Shared Memory

#### 2.5.1. Explanation: Importance of `/dev/shm` for Chromium performance and how it's configured in `docker-compose.yml`.
Chromium-based browsers (like Chrome, Edge) use `/dev/shm` (shared memory) extensively. If the default Docker limit for `/dev/shm` (often 64MB) is too small, browser instances can crash or perform poorly. The `docker-compose.yml` provided with Crawl4ai typically increases this:
```yaml
# Snippet from a typical docker-compose.yml for crawl4ai
# services:
#   crawl4ai:
#     # ... other configurations ...
#     shm_size: '1g' # Or '2g', depending on expected load
#     # Alternatively, for more flexibility but less security:
#     # volumes:
#     #   - /dev/shm:/dev/shm
```
Setting `shm_size` or mounting `/dev/shm` directly from the host provides more shared memory, preventing common browser crashes within Docker. The `Dockerfile` also sets `ENV DEBIAN_FRONTEND=noninteractive` and browser flags like `--disable-dev-shm-usage` to mitigate some issues, but adequate shared memory is still crucial.

---
## III. Interacting with the Crawl4ai API Endpoints

### A. Authentication (`/token`)

#### A.1. Example: Python script to obtain an API token using a valid email.
This example assumes JWT authentication is enabled and "user@example.com" is whitelisted (this is illustrative, actual whitelisting is not part of the default config).
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")

# This email domain would need to be configured as allowed in your security settings
# if verify_email_domain is used.
email_to_test = "user@example.com" # Replace with a valid email if your server uses domain verification

payload = {"email": email_to_test}
try:
    response = requests.post(f"{BASE_URL}/token", json=payload)
    if response.status_code == 200:
        token_data = response.json()
        print(f"Successfully obtained token for {email_to_test}:")
        print(json.dumps(token_data, indent=2))
        # Store this token for subsequent authenticated requests
        # API_TOKEN = token_data["access_token"]
    else:
        print(f"Failed to obtain token for {email_to_test}. Status: {response.status_code}, Response: {response.text}")
except requests.exceptions.RequestException as e:
    print(f"Error connecting to /token endpoint: {e}")
```
**Note:** The default `config.yml` has `security.jwt_enabled: false`. For this example to fully work, you would need to enable JWT and potentially configure allowed email domains.

#### A.2. Example: Python script attempting to obtain a token with an invalid email domain and handling the error.
```python
import requests
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")

# Assuming "invalid-domain.com" is not whitelisted.
# The default Crawl4AI config doesn't whitelist specific domains for /token,
# but if `verify_email_domain` were true in auth.py, this would be relevant.
# For now, this will likely succeed if jwt_enabled is false, or fail if jwt_enabled is true
# and no user exists, or pass if jwt_enabled is true and any email can get a token.
payload = {"email": "test@invalid-domain.com"}
try:
    response = requests.post(f"{BASE_URL}/token", json=payload)
    if response.status_code == 400 and "Invalid email domain" in response.text:
        print(f"Correctly failed to obtain token for invalid domain: {response.text}")
    elif response.status_code == 200:
        print(f"Obtained token (unexpected if domain verification is strict): {response.json()}")
    else:
        print(f"Token request status: {response.status_code}, Response: {response.text}")
except requests.exceptions.RequestException as e:
    print(f"Error connecting to /token endpoint: {e}")
```

#### A.3. Example: Python script making an authenticated request to a protected endpoint.
This example assumes an endpoint like `/md` is protected and requires a token.
```python
import requests
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
# First, obtain a token (replace with actual token for a real protected setup)
# For this example, we'll use a placeholder. If API_TOKEN is set in env, it will be used.
# If not, and the endpoint is truly protected, this will fail.
# API_TOKEN = "your_manually_obtained_token_or_from_previous_step"

headers = get_headers() # Uses API_TOKEN from environment if set

md_payload = {"url": "https://example.com"}
try:
    response = requests.post(f"{BASE_URL}/md", json=md_payload, headers=headers)
    if response.status_code == 200:
        print("Successfully accessed protected /md endpoint.")
        print(json.dumps(response.json(), indent=2, ensure_ascii=False)[:500] + "...")
    elif response.status_code == 401 or response.status_code == 403:
        print(f"Authentication/Authorization failed for /md: {response.status_code} - {response.text}")
        print("Ensure JWT is enabled and you have a valid token if this endpoint is protected.")
    else:
        print(f"Request to /md failed: {response.status_code} - {response.text}")
except requests.exceptions.RequestException as e:
    print(f"Error connecting to /md endpoint: {e}")
```
**Note:** By default, most Crawl4ai endpoints are not protected by JWT even if `jwt_enabled` is true, unless explicitly decorated with `Depends(token_dep)`.

---
### B. Core Crawling Endpoints

#### B.1. `/crawl` (Asynchronous Job-based Crawling via Redis)

The `/crawl` endpoint submits a job to a Redis queue. You then poll the `/task/{task_id}` endpoint to get the status and results.

##### B.1.1. Example: Submitting a single URL crawl job and getting a `task_id`.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com"],
    # browser_config and crawler_config are optional, defaults will be used
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Crawl job submitted successfully. Task ID: {task_id}")
        print(f"Poll status at: {BASE_URL}/task/{task_id}")
    else:
        print(f"Failed to submit job or get task_id: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting crawl job: {e}")
```

##### B.1.2. Example: Submitting multiple URLs as a single crawl job.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com", "https://www.python.org"],
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Multi-URL crawl job submitted. Task ID: {task_id}")
    else:
        print(f"Failed to submit job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting multi-URL crawl job: {e}")

```

##### B.1.3. Example: Submitting a crawl job with a custom `browser_config` (e.g., headless false).
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com"],
    "browser_config": {
        "headless": False, # Run browser in visible mode (if server environment supports UI)
        "viewport_width": 800,
        "viewport_height": 600
    }
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Crawl job with custom browser_config submitted. Task ID: {task_id}")
    else:
        print(f"Failed to submit job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting crawl job with custom browser_config: {e}")
```

##### B.1.4. Example: Submitting a crawl job with a custom `crawler_config` (e.g., specific `word_count_threshold`).
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com"],
    "crawler_config": {
        "word_count_threshold": 50, # Only process content blocks with more than 50 words
        "screenshot": True # Also take a screenshot
    }
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Crawl job with custom crawler_config submitted. Task ID: {task_id}")
    else:
        print(f"Failed to submit job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting crawl job with custom crawler_config: {e}")
```

##### B.1.5. Example: Submitting a job that uses a specific `CacheMode` (e.g., `BYPASS`).
`CacheMode` values are typically: "DISABLED", "ENABLED", "BYPASS", "READ_ONLY", "WRITE_ONLY".
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com"],
    "crawler_config": {
        "cache_mode": "BYPASS" # Force a fresh crawl, ignore existing cache, don't write to cache
    }
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Crawl job with CacheMode.BYPASS submitted. Task ID: {task_id}")
    else:
        print(f"Failed to submit job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting crawl job with CacheMode.BYPASS: {e}")
```

##### B.1.6. Example: Submitting a job to extract PDF content from a URL.
(This assumes the URL points directly to a PDF or the page leads to a PDF download that the crawler handles).
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# URL of a sample PDF file
pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

payload = {
    "urls": [pdf_url],
    "crawler_config": {
        # Crawl4ai should auto-detect PDF content type and use appropriate processor
        "pdf": True # Explicitly enabling PDF processing, though often auto-detected
    }
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"PDF crawl job submitted for {pdf_url}. Task ID: {task_id}")
        print(f"Poll status at: {BASE_URL}/task/{task_id}")
    else:
        print(f"Failed to submit PDF crawl job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting PDF crawl job: {e}")
```

##### B.1.7. Example: Submitting a job to take a screenshot from a URL.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "urls": ["https://example.com"],
    "crawler_config": {
        "screenshot": True,
        "screenshot_wait_for": 2 # wait 2 seconds after page load before screenshot
    }
}

try:
    response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
    response.raise_for_status()
    job_data = response.json()
    task_id = job_data.get("task_id")
    if task_id:
        print(f"Screenshot job submitted for example.com. Task ID: {task_id}")
        print(f"Poll status at: {BASE_URL}/task/{task_id}")
    else:
        print(f"Failed to submit screenshot job: {job_data}")
except requests.exceptions.RequestException as e:
    print(f"Error submitting screenshot job: {e}")
```

---
#### B.2. `/task/{task_id}` (Job Status and Results)

##### B.2.1. Example: Python script to poll the `/task/{task_id}` endpoint for PENDING status.
```python
import requests
import time
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# Assume task_id is obtained from a previous /crawl request
# For this example, we'll submit a quick job first
submit_payload = {"urls": ["http://example.com/nonexistent-page-for-quick-fail-or-processing"]}
task_id = None
try:
    submit_response = requests.post(f"{BASE_URL}/crawl", json=submit_payload, headers=headers)
    submit_response.raise_for_status()
    task_id = submit_response.json().get("task_id")
except requests.exceptions.RequestException as e:
    print(f"Failed to submit initial job for polling example: {e}")

if task_id:
    print(f"Polling for task: {task_id}")
    for _ in range(5): # Poll a few times
        try:
            status_response = requests.get(f"{BASE_URL}/task/{task_id}", headers=headers)
            status_response.raise_for_status()
            status_data = status_response.json()
            print(f"Current status: {status_data.get('status')}")
            if status_data.get('status') in ["COMPLETED", "FAILED"]:
                break
            time.sleep(2)
        except requests.exceptions.RequestException as e:
            print(f"Error polling task status: {e}")
            break
else:
    print("No task ID to poll.")
```

##### B.2.2. Example: Python script to retrieve results for a COMPLETED job.
```python
import requests
import time
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# Submit a job that should complete successfully
submit_payload = {"urls": ["https://example.com"]}
task_id = None
try:
    submit_response = requests.post(f"{BASE_URL}/crawl", json=submit_payload, headers=headers)
    submit_response.raise_for_status()
    task_id = submit_response.json().get("task_id")
except requests.exceptions.RequestException as e:
    print(f"Failed to submit job for result retrieval example: {e}")


if task_id:
    print(f"Waiting for task {task_id} to complete...")
    while True:
        try:
            status_response = requests.get(f"{BASE_URL}/task/{task_id}", headers=headers)
            status_response.raise_for_status()
            status_data = status_response.json()
            current_status = status_data.get('status')
            print(f"Task status: {current_status}")

            if current_status == "COMPLETED":
                print("\nJob COMPLETED. Results:")
                # The 'result' field contains the JSON string of the CrawlResult model(s)
                # For a single URL job, it's typically a dict. For multiple, a list of dicts.
                # The structure from api.py suggests `result` field in Redis is a JSON string
                # of a dictionary which itself contains a 'results' key (list of CrawlResult dicts).
                
                # This is based on how handle_crawl_job in api.py stores results
                # and how the /task/{task_id} endpoint decodes it.
                # The 'result' from /task/{task_id} should already be a parsed dict.
                
                crawl_results_wrapper = status_data.get("result")
                if crawl_results_wrapper and "results" in crawl_results_wrapper:
                    actual_results = crawl_results_wrapper["results"]
                    for i, res_item in enumerate(actual_results):
                        print(f"\n--- Result for URL {i+1} ({res_item.get('url', 'N/A')}) ---")
                        print(f"  Success: {res_item.get('success')}")
                        print(f"  Markdown (first 100 chars): {res_item.get('markdown', {}).get('raw_markdown', '')[:100]}...")
                        if res_item.get('screenshot'):
                             print("  Screenshot captured (base64 data not printed).")
                else:
                     print(f"Unexpected result structure: {crawl_results_wrapper}")
                break
            elif current_status == "FAILED":
                print(f"\nJob FAILED. Error: {status_data.get('error')}")
                break
            
            time.sleep(3) # Poll every 3 seconds
        except requests.exceptions.RequestException as e:
            print(f"Error polling task status: {e}")
            break
        except KeyboardInterrupt:
            print("\nPolling interrupted.")
            break
else:
    print("No task ID to retrieve results for.")

```

##### B.2.3. Example: Python script to get error details for a FAILED job.
```python
import requests
import time
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# Submit a job that is likely to fail (e.g., invalid URL or one that times out quickly)
submit_payload = {"urls": ["http://nonexistentdomain1234567890.com"]}
task_id = None
try:
    submit_response = requests.post(f"{BASE_URL}/crawl", json=submit_payload, headers=headers)
    submit_response.raise_for_status()
    task_id = submit_response.json().get("task_id")
except requests.exceptions.RequestException as e:
    print(f"Failed to submit job for failure example: {e}")

if task_id:
    print(f"Waiting for task {task_id} (expected to fail)...")
    while True:
        try:
            status_response = requests.get(f"{BASE_URL}/task/{task_id}", headers=headers)
            status_response.raise_for_status()
            status_data = status_response.json()
            current_status = status_data.get('status')
            print(f"Task status: {current_status}")

            if current_status == "FAILED":
                print("\nJob FAILED as expected.")
                error_message = status_data.get('error', 'No error message provided.')
                print(f"Error details: {error_message}")
                break
            elif current_status == "COMPLETED":
                print("\nJob COMPLETED unexpectedly.")
                break
            
            time.sleep(2)
        except requests.exceptions.RequestException as e:
            print(f"Error polling task status: {e}")
            break
        except KeyboardInterrupt:
            print("\nPolling interrupted.")
            break
else:
    print("No task ID to check for failure.")
```

##### B.2.4. Example: Full workflow - submit job, poll status, retrieve results or error.
This combines the above examples into a more complete client script.
```python
import requests
import time
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

def submit_and_poll(payload, timeout_seconds=60):
    task_id = None
    try:
        # Submit the job
        print(f"Submitting job with payload: {payload}")
        submit_response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
        submit_response.raise_for_status()
        task_id = submit_response.json().get("task_id")
        if not task_id:
            print("Error: No task_id received.")
            return None
        print(f"Job submitted. Task ID: {task_id}. Polling for completion...")

        # Poll for status
        start_time = time.time()
        while time.time() - start_time < timeout_seconds:
            status_response = requests.get(f"{BASE_URL}/task/{task_id}", headers=headers)
            status_response.raise_for_status()
            status_data = status_response.json()
            current_status = status_data.get('status')
            print(f"  Task {task_id} status: {current_status} (elapsed: {time.time() - start_time:.1f}s)")

            if current_status == "COMPLETED":
                print(f"Task {task_id} COMPLETED.")
                return status_data.get("result") # This should be the parsed JSON result
            elif current_status == "FAILED":
                print(f"Task {task_id} FAILED.")
                print(f"Error: {status_data.get('error')}")
                return None
            time.sleep(5) # Poll interval
        
        print(f"Task {task_id} timed out after {timeout_seconds} seconds.")
        return None

    except requests.exceptions.RequestException as e:
        print(f"API request error: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

if __name__ == "__main__":
    crawl_payload = {
        "urls": ["https://www.python.org/about/"],
        "crawler_config": {"screenshot": False}
    }
    results_data = submit_and_poll(crawl_payload)

    if results_data and "results" in results_data:
        for i, res_item in enumerate(results_data["results"]):
            print(f"\n--- Result for URL {res_item.get('url', 'N/A')} ---")
            print(f"  Success: {res_item.get('success')}")
            print(f"  Markdown (first 200 chars): {res_item.get('markdown', {}).get('raw_markdown', '')[:200]}...")
    elif results_data: # If result isn't in the expected wrapper structure
        print(f"\nReceived result data (unexpected structure):")
        print(json.dumps(results_data, indent=2, ensure_ascii=False))

```

---
#### B.3. `/crawl/stream` (Streaming Crawl Results)

##### B.3.1. Example: Python script to stream crawl results for a single URL and process NDJSON.
```python
import requests
import json
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()
headers['Accept'] = 'application/x-ndjson' # Important for streaming

payload = {
    "urls": ["https://example.com"],
    "crawler_config": {"stream": True} # Ensure stream is True in config
}

print(f"Streaming results for {payload['urls'][0]}...")
try:
    with requests.post(f"{BASE_URL}/crawl/stream", json=payload, headers=headers, stream=True) as response:
        response.raise_for_status()
        for line in response.iter_lines():
            if line:
                try:
                    result_chunk = json.loads(line.decode('utf-8'))
                    if "status" in result_chunk and result_chunk["status"] == "completed":
                        print("\nStream finished.")
                        break
                    print("\nReceived chunk:")
                    # Print some key info from the chunk
                    print(f"  URL: {result_chunk.get('url', 'N/A')}")
                    print(f"  Success: {result_chunk.get('success')}")
                    if 'markdown' in result_chunk and isinstance(result_chunk['markdown'], dict):
                         print(f"  Markdown (snippet): {result_chunk['markdown'].get('raw_markdown', '')[:100]}...")
                    else:
                         print(f"  Markdown (snippet): {str(result_chunk.get('markdown', ''))[:100]}...")
                    if result_chunk.get('error_message'):
                        print(f"  Error: {result_chunk.get('error_message')}")
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON line: {e} - Line: {line.decode('utf-8')}")
except requests.exceptions.RequestException as e:
    print(f"Error during streaming request: {e}")

```

##### B.3.2. Example: Python script to stream crawl results for multiple URLs.
```python
import requests
import json
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()
headers['Accept'] = 'application/x-ndjson'

payload = {
    "urls": ["https://example.com", "https://www.python.org/doc/"],
    "crawler_config": {"stream": True}
}

print(f"Streaming results for multiple URLs...")
try:
    with requests.post(f"{BASE_URL}/crawl/stream", json=payload, headers=headers, stream=True) as response:
        response.raise_for_status()
        for line in response.iter_lines():
            if line:
                try:
                    result_chunk = json.loads(line.decode('utf-8'))
                    if "status" in result_chunk and result_chunk["status"] == "completed":
                        print("\nStream finished for all URLs.")
                        break
                    print(f"\nChunk for URL: {result_chunk.get('url', 'N/A')}")
                    # Process or display part of the result
                    print(f"  Success: {result_chunk.get('success')}")
                    if 'markdown' in result_chunk and isinstance(result_chunk['markdown'], dict):
                         print(f"  Markdown (snippet): {result_chunk['markdown'].get('raw_markdown', '')[:70]}...")
                    else:
                         print(f"  Markdown (snippet): {str(result_chunk.get('markdown', ''))[:70]}...")

                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON line: {e} - Line: {line.decode('utf-8')}")
except requests.exceptions.RequestException as e:
    print(f"Error during streaming request: {e}")
```

##### B.3.3. Example: Streaming crawl results with custom `browser_config` and `crawler_config`.
```python
import requests
import json
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()
headers['Accept'] = 'application/x-ndjson'

payload = {
    "urls": ["https://example.com"],
    "browser_config": {
        "headless": True,
        "user_agent": "Crawl4AI-Stream-Tester/1.0"
    },
    "crawler_config": {
        "stream": True,
        "word_count_threshold": 10 # Lower threshold for this example
    }
}

print(f"Streaming results with custom configs for {payload['urls'][0]}...")
try:
    with requests.post(f"{BASE_URL}/crawl/stream", json=payload, headers=headers, stream=True) as response:
        response.raise_for_status()
        for line in response.iter_lines():
            if line:
                result_chunk = json.loads(line.decode('utf-8'))
                if "status" in result_chunk and result_chunk["status"] == "completed":
                    print("\nStream finished.")
                    break
                print("\nReceived chunk with custom config:")
                print(f"  URL: {result_chunk.get('url')}")
                print(f"  Word count threshold was: {payload['crawler_config']['word_count_threshold']}")
                if 'markdown' in result_chunk and isinstance(result_chunk['markdown'], dict):
                     print(f"  Markdown (snippet): {result_chunk['markdown'].get('raw_markdown', '')[:70]}...")
                else:
                     print(f"  Markdown (snippet): {str(result_chunk.get('markdown', ''))[:70]}...")
except requests.exceptions.RequestException as e:
    print(f"Error during streaming request: {e}")
```

##### B.3.4. Example: Handling connection closure or errors during streaming.
```python
import requests
import json
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()
headers['Accept'] = 'application/x-ndjson'

payload = {
    "urls": ["https://thissitedoesnotexist12345.com", "https://example.com"], # First URL will fail
    "crawler_config": {"stream": True}
}

print(f"Streaming with a URL expected to fail...")
try:
    with requests.post(f"{BASE_URL}/crawl/stream", json=payload, headers=headers, stream=True) as response:
        # We might not get a non-200 status code immediately if the connection itself is established
        # Errors for individual URLs will be part of the NDJSON stream
        for line in response.iter_lines():
            if line:
                try:
                    result_chunk = json.loads(line.decode('utf-8'))
                    print(f"\nReceived data: {result_chunk.get('url', 'N/A')}")
                    if "status" in result_chunk and result_chunk["status"] == "completed":
                        print("Stream finished.")
                        break
                    if result_chunk.get('error_message'):
                        print(f"  ERROR for {result_chunk.get('url')}: {result_chunk.get('error_message')}")
                    elif result_chunk.get('success'):
                        print(f"  SUCCESS for {result_chunk.get('url')}")
                except json.JSONDecodeError as e:
                    print(f"  Error decoding JSON line: {e}")
except requests.exceptions.ChunkedEncodingError:
    print("Connection closed unexpectedly by server during streaming (ChunkedEncodingError).")
except requests.exceptions.RequestException as e:
    print(f"General error during streaming request: {e}")
```

---
### C. Content Transformation & Utility Endpoints

#### C.1. `/md` (Markdown Generation)

##### C.1.1. Example: Getting raw Markdown for a URL (default filter).
The default filter is `FIT` if no filter is specified.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {"url": "https://example.com", "f": "RAW"} # 'f' is for filter_type
try:
    response = requests.post(f"{BASE_URL}/md", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    print("Markdown (RAW filter - first 300 chars):")
    print(data.get("markdown", "")[:300] + "...")
except requests.exceptions.RequestException as e:
    print(f"Error fetching Markdown: {e}")
```

##### C.1.2. Example: Getting Markdown using the `FIT` filter type.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {"url": "https://example.com", "f": "FIT"}
try:
    response = requests.post(f"{BASE_URL}/md", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    print("Markdown (FIT filter - first 300 chars):")
    print(data.get("markdown", "")[:300] + "...")
except requests.exceptions.RequestException as e:
    print(f"Error fetching Markdown: {e}")
```

##### C.1.3. Example: Getting Markdown using the `BM25` filter type with a specific query.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://en.wikipedia.org/wiki/Python_(programming_language)", 
    "f": "BM25",
    "q": "What are the key features of Python?" # Query for BM25 filtering
}
try:
    response = requests.post(f"{BASE_URL}/md", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    print(f"Markdown (BM25 filter, query='{payload['q']}' - first 300 chars):")
    print(data.get("markdown", "")[:300] + "...")
except requests.exceptions.RequestException as e:
    print(f"Error fetching Markdown: {e}")
```

##### C.1.4. Example: Getting Markdown using the `LLM` filter type with a query (conceptual, requires LLM setup).
This requires an LLM provider (like OpenAI) to be configured in `config.yml` or via environment variables loaded by the server.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "f": "LLM",
    "q": "Summarize the history of Python" # Query for LLM to focus on
}
print("Attempting LLM-filtered Markdown (this may take a moment and requires LLM config)...")
try:
    # LLM requests can take longer
    response = requests.post(f"{BASE_URL}/md", json=payload, headers=headers, timeout=120) 
    response.raise_for_status()
    data = response.json()
    print(f"Markdown (LLM filter, query='{payload['q']}' - first 300 chars):")
    print(data.get("markdown", "")[:300] + "...")
except requests.exceptions.RequestException as e:
    print(f"Error fetching LLM-filtered Markdown: {e}")
    print("Ensure your LLM provider (e.g., OPENAI_API_KEY) is configured for the server.")
```

##### C.1.5. Example: Demonstrating cache usage with the `/md` endpoint (`c` parameter).
The `c` parameter can be "0" (bypass write, read if available - effectively WRITE_ONLY for this endpoint if no cache exists), "1" (force refresh, write - effectively ENABLED for this endpoint), or other numbers for revision control (not shown here).
```python
import requests
import os
import json
import time

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()
test_url = "https://example.com"

# First call: cache miss, should fetch and write to cache
print("First call (cache_mode=ENABLED implied by 'c=1', or default if 'c' omitted)")
payload1 = {"url": test_url, "f": "RAW", "c": "1"} # c="1" forces refresh and writes
start_time = time.time()
response1 = requests.post(f"{BASE_URL}/md", json=payload1, headers=headers)
duration1 = time.time() - start_time
response1.raise_for_status()
print(f"First call duration: {duration1:.2f}s. Markdown length: {len(response1.json().get('markdown', ''))}")

# Second call: should be a cache hit if c="0" or c is omitted and cache is fresh
print("\nSecond call (cache_mode=READ_ONLY implied by 'c=0', or default if 'c' omitted and cache fresh)")
payload2 = {"url": test_url, "f": "RAW", "c": "0"} # c="0" attempts to read from cache
start_time = time.time()
response2 = requests.post(f"{BASE_URL}/md", json=payload2, headers=headers)
duration2 = time.time() - start_time
response2.raise_for_status()
print(f"Second call duration: {duration2:.2f}s. Markdown length: {len(response2.json().get('markdown', ''))}")

if duration2 < duration1 / 2 and duration1 > 0.1 : # Heuristic for cache hit
    print("Second call was significantly faster, likely a cache hit.")
else:
    print("Cache behavior inconclusive or first call was very fast.")
```

---
#### C.2. `/html` (Preprocessed HTML)

##### C.2.1. Example: Fetching preprocessed HTML for a URL suitable for schema extraction.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {"url": "https://example.com"}
try:
    response = requests.post(f"{BASE_URL}/html", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    print("Preprocessed HTML (first 500 chars):")
    print(data.get("html", "")[:500] + "...")
    print(f"\nOriginal URL: {data.get('url')}")
except requests.exceptions.RequestException as e:
    print(f"Error fetching preprocessed HTML: {e}")
```

---
#### C.3. `/screenshot`

##### C.3.1. Example: Generating a PNG screenshot for a URL and receiving base64 data.
```python
import requests
import os
import base64
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {"url": "https://example.com"}
try:
    response = requests.post(f"{BASE_URL}/screenshot", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    if data.get("screenshot"):
        print("Screenshot received (base64 data).")
        # To save the image:
        # image_data = base64.b64decode(data["screenshot"])
        # with open("example_screenshot.png", "wb") as f:
        #     f.write(image_data)
        # print("Screenshot saved as example_screenshot.png")
    else:
        print(f"Screenshot generation failed or no data returned: {data}")
except requests.exceptions.RequestException as e:
    print(f"Error generating screenshot: {e}")
```

##### C.3.2. Example: Generating a screenshot with a custom `screenshot_wait_for` delay.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://example.com",
    "screenshot_wait_for": 3  # Wait 3 seconds after page load
}
try:
    response = requests.post(f"{BASE_URL}/screenshot", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    if data.get("screenshot"):
        print(f"Screenshot with {payload['screenshot_wait_for']}s delay received.")
    else:
        print(f"Screenshot generation failed: {data}")
except requests.exceptions.RequestException as e:
    print(f"Error generating screenshot with delay: {e}")
```

##### C.3.3. Example: Saving screenshot to server-side path via `output_path`.
**Note:** This requires `output_path` to be a path accessible and writable by the server process. For Docker, this usually means a mounted volume.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# This path needs to be valid from the server's perspective
# e.g., if running in Docker, it might be a path inside the container
# that is mapped to a host volume.
server_side_path = "/app/screenshots/example_com.png" # Example path

payload = {
    "url": "https://example.com",
    "output_path": server_side_path
}
try:
    response = requests.post(f"{BASE_URL}/screenshot", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    if data.get("success") and data.get("path"):
        print(f"Screenshot successfully saved to server path: {data.get('path')}")
        print("Note: This file is on the server, not the client machine unless paths are mapped.")
    else:
        print(f"Failed to save screenshot to server: {data}")
except requests.exceptions.RequestException as e:
    print(f"Error saving screenshot to server: {e}")
```

---
#### C.4. `/pdf`

##### C.4.1. Example: Generating a PDF for a URL and receiving base64 data.
```python
import requests
import os
import base64
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {"url": "https://example.com"}
try:
    response = requests.post(f"{BASE_URL}/pdf", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    if data.get("pdf"):
        print("PDF received (base64 data).")
        # To save the PDF:
        # pdf_data = base64.b64decode(data["pdf"])
        # with open("example_page.pdf", "wb") as f:
        #     f.write(pdf_data)
        # print("PDF saved as example_page.pdf")
    else:
        print(f"PDF generation failed or no data returned: {data}")
except requests.exceptions.RequestException as e:
    print(f"Error generating PDF: {e}")
```

##### C.4.2. Example: Saving PDF to server-side path via `output_path`.
**Note:** Similar to screenshots, `output_path` must be server-accessible.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

server_side_path = "/app/pdfs/example_com.pdf" # Example path

payload = {
    "url": "https://example.com",
    "output_path": server_side_path
}
try:
    response = requests.post(f"{BASE_URL}/pdf", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    if data.get("success") and data.get("path"):
        print(f"PDF successfully saved to server path: {data.get('path')}")
    else:
        print(f"Failed to save PDF to server: {data}")
except requests.exceptions.RequestException as e:
    print(f"Error saving PDF to server: {e}")

```

---
#### C.5. `/execute_js`

##### C.5.1. Example: Executing a simple JavaScript snippet (e.g., `return document.title;`) on a page.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://example.com",
    "scripts": ["return document.title;"]
}
try:
    response = requests.post(f"{BASE_URL}/execute_js", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json() # This is the full CrawlResult model as JSON
    print("Full CrawlResult from /execute_js:")
    # print(json.dumps(data, indent=2, ensure_ascii=False)) # Can be very long
    
    js_results = data.get("js_execution_result")
    if js_results and js_results.get("script_0"):
        print(f"\nResult of script 0 (document.title): {js_results['script_0']}")
    else:
        print(f"\nCould not find JS execution result: {js_results}")

except requests.exceptions.RequestException as e:
    print(f"Error executing JS: {e}")
```

##### C.5.2. Example: Executing multiple JavaScript snippets sequentially.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://example.com",
    "scripts": [
        "return document.title;",
        "return document.querySelectorAll('p').length;",
        "() => { const h1 = document.querySelector('h1'); return h1 ? h1.innerText : 'No H1'; }()"
    ]
}
try:
    response = requests.post(f"{BASE_URL}/execute_js", json=payload, headers=headers)
    response.raise_for_status()
    data = response.json()
    
    js_results = data.get("js_execution_result")
    if js_results:
        print("\nResults of JS snippets:")
        print(f"  Script 0 (Title): {js_results.get('script_0')}")
        print(f"  Script 1 (Paragraph count): {js_results.get('script_1')}")
        print(f"  Script 2 (H1 text): {js_results.get('script_2')}")
    else:
        print(f"\nCould not find JS execution results: {js_results}")

except requests.exceptions.RequestException as e:
    print(f"Error executing multiple JS snippets: {e}")
```

##### C.5.3. Example: Demonstrating how the full `CrawlResult` (JSON of model) is returned.
The `/execute_js` endpoint returns the entire `CrawlResult` object, serialized to JSON. This includes HTML, Markdown, links, etc., in addition to the `js_execution_result`.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

payload = {
    "url": "https://example.com",
    "scripts": ["return window.location.href;"]
}
try:
    response = requests.post(f"{BASE_URL}/execute_js", json=payload, headers=headers)
    response.raise_for_status()
    crawl_result_data = response.json()
    
    print("Demonstrating full CrawlResult structure from /execute_js:")
    print(f"  URL crawled: {crawl_result_data.get('url')}")
    print(f"  Success: {crawl_result_data.get('success')}")
    print(f"  HTML (snippet): {crawl_result_data.get('html', '')[:100]}...")
    if isinstance(crawl_result_data.get('markdown'), dict):
        print(f"  Markdown (snippet): {crawl_result_data['markdown'].get('raw_markdown', '')[:100]}...")
    else:
        print(f"  Markdown (snippet): {str(crawl_result_data.get('markdown', ''))[:100]}...")

    js_result = crawl_result_data.get("js_execution_result", {}).get("script_0")
    print(f"  Result of JS (window.location.href): {js_result}")

except requests.exceptions.RequestException as e:
    print(f"Error demonstrating full CrawlResult: {e}")
```

---
### D. Contextual Endpoints

#### D.1. `/ask` (RAG-like Context Retrieval)
The `/ask` endpoint uses local Markdown files (`c4ai-code-context.md` and `c4ai-doc-context.md`, which should be in the same directory as `server.py`) for retrieval.

##### D.1.1. Example: Asking a general question to retrieve "code" context.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

params = {
    "context_type": "code",
    "query": "How to handle Playwright installation?" # General query
}
try:
    response = requests.get(f"{BASE_URL}/ask", params=params, headers=headers)
    response.raise_for_status()
    data = response.json()
    print("Retrieved 'code' context for 'How to handle Playwright installation?':")
    if "code_results" in data:
        for i, item in enumerate(data["code_results"][:2]): # Show first 2 results
            print(f"\n--- Code Result {i+1} (Score: {item.get('score', 'N/A'):.2f}) ---")
            print(item.get("text", "")[:300] + "...")
    else:
        print(json.dumps(data, indent=2))
except requests.exceptions.RequestException as e:
    print(f"Error asking for code context: {e}")
```

##### D.1.2. Example: Asking a general question to retrieve "doc" context.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

params = {
    "context_type": "doc",
    "query": "Explain Crawl4ai API endpoints"
}
try:
    response = requests.get(f"{BASE_URL}/ask", params=params, headers=headers)
    response.raise_for_status()
    data = response.json()
    print("Retrieved 'doc' context for 'Explain Crawl4ai API endpoints':")
    if "doc_results" in data:
        for i, item in enumerate(data["doc_results"][:2]):
            print(f"\n--- Doc Result {i+1} (Score: {item.get('score', 'N/A'):.2f}) ---")
            print(item.get("text", "")[:300] + "...")
    else:
        print(json.dumps(data, indent=2))
except requests.exceptions.RequestException as e:
    print(f"Error asking for doc context: {e}")
```

##### D.1.3. Example: Using the `query` parameter to filter context related to a specific function.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

params = {
    "context_type": "all", # Search both code and docs
    "query": "AsyncWebCrawler arun method"
}
try:
    response = requests.get(f"{BASE_URL}/ask", params=params, headers=headers)
    response.raise_for_status()
    data = response.json()
    print(f"Retrieved 'all' context for query: '{params['query']}'")
    if "code_results" in data:
        print(f"\nFound {len(data['code_results'])} code results.")
        # Optionally print snippets
    if "doc_results" in data:
        print(f"Found {len(data['doc_results'])} doc results.")
        # Optionally print snippets
    # print(json.dumps(data, indent=2, ensure_ascii=False)[:1000] + "...")
except requests.exceptions.RequestException as e:
    print(f"Error asking with specific query: {e}")

```

##### D.1.4. Example: Adjusting `score_ratio` to change result sensitivity.
A lower `score_ratio` (e.g., 0.1) will return more, less relevant results. A higher one (e.g., 0.8) will be more strict. Default is 0.5.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

params_strict = {
    "context_type": "code",
    "query": "Playwright browser installation",
    "score_ratio": 0.8 # Higher, more strict
}
params_loose = {
    "context_type": "code",
    "query": "Playwright browser installation",
    "score_ratio": 0.2 # Lower, less strict
}

try:
    response_strict = requests.get(f"{BASE_URL}/ask", params=params_strict, headers=headers)
    response_strict.raise_for_status()
    data_strict = response_strict.json()
    print(f"Results with score_ratio=0.8: {len(data_strict.get('code_results', []))}")

    response_loose = requests.get(f"{BASE_URL}/ask", params=params_loose, headers=headers)
    response_loose.raise_for_status()
    data_loose = response_loose.json()
    print(f"Results with score_ratio=0.2: {len(data_loose.get('code_results', []))}")

except requests.exceptions.RequestException as e:
    print(f"Error adjusting score_ratio: {e}")
```

##### D.1.5. Example: Limiting results with `max_results`.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

params = {
    "context_type": "doc",
    "query": "crawl4ai features",
    "max_results": 3 # Limit to top 3 results
}
try:
    response = requests.get(f"{BASE_URL}/ask", params=params, headers=headers)
    response.raise_for_status()
    data = response.json()
    print(f"Retrieved max {params['max_results']} doc_results for 'crawl4ai features':")
    if "doc_results" in data:
        print(f"Actual results returned: {len(data['doc_results'])}")
        for item in data["doc_results"]:
            print(f"  - Score: {item.get('score', 0):.2f}, Text (snippet): {item.get('text', '')[:50]}...")
    else:
        print("No doc_results found.")
except requests.exceptions.RequestException as e:
    print(f"Error limiting results: {e}")
```

---
### E. Server & Configuration Information

#### E.1. `/config/dump`

##### E.1.1. Example: Dumping a `CrawlerRunConfig` Python object representation to its JSON equivalent via the API.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# This is a Python-style string representation of a CrawlerRunConfig
# that the server's _safe_eval_config can parse.
config_string = "CrawlerRunConfig(word_count_threshold=50, screenshot=True, cache_mode=CacheMode.BYPASS)"

payload = {"code": config_string}
try:
    response = requests.post(f"{BASE_URL}/config/dump", json=payload, headers=headers)
    response.raise_for_status()
    dumped_json = response.json()
    print("Dumped CrawlerRunConfig JSON:")
    print(json.dumps(dumped_json, indent=2))
except requests.exceptions.RequestException as e:
    print(f"Error dumping CrawlerRunConfig: {e}")
```

##### E.1.2. Example: Dumping a `BrowserConfig` Python object representation to its JSON equivalent via the API.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

config_string = "BrowserConfig(headless=False, user_agent='MyTestAgent/1.0')"
payload = {"code": config_string}
try:
    response = requests.post(f"{BASE_URL}/config/dump", json=payload, headers=headers)
    response.raise_for_status()
    dumped_json = response.json()
    print("Dumped BrowserConfig JSON:")
    print(json.dumps(dumped_json, indent=2))
except requests.exceptions.RequestException as e:
    print(f"Error dumping BrowserConfig: {e}")
```

##### E.1.3. Example: Attempting to dump an invalid or non-serializable configuration string.
```python
import requests
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

# Invalid: not a recognized Crawl4AI config class
invalid_config_string = "MyCustomClass(param=1)"
payload = {"code": invalid_config_string}
try:
    response = requests.post(f"{BASE_URL}/config/dump", json=payload, headers=headers)
    if response.status_code == 400:
        print(f"Correctly failed to dump invalid config string. Server response: {response.json()}")
    else:
        print(f"Unexpected response for invalid config: {response.status_code} - {response.text}")
except requests.exceptions.RequestException as e:
    print(f"Error attempting to dump invalid config: {e}")

# Invalid: nested function call (security restriction)
unsafe_config_string = "CrawlerRunConfig(word_count_threshold=__import__('os').system('echo unsafe'))"
payload_unsafe = {"code": unsafe_config_string}
try:
    response_unsafe = requests.post(f"{BASE_URL}/config/dump", json=payload_unsafe, headers=headers)
    if response_unsafe.status_code == 400:
        print(f"Correctly failed to dump unsafe config string. Server response: {response_unsafe.json()}")
    else:
        print(f"Unexpected response for unsafe config: {response_unsafe.status_code} - {response_unsafe.text}")
except requests.exceptions.RequestException as e:
    print(f"Error attempting to dump unsafe config: {e}")
```

---
#### E.2. `/schema`

##### E.2.1. Example: Fetching the default JSON schemas for `BrowserConfig` and `CrawlerRunConfig`.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

try:
    response = requests.get(f"{BASE_URL}/schema", headers=headers)
    response.raise_for_status()
    schemas = response.json()
    
    print("BrowserConfig Schema (sample):")
    # print(json.dumps(schemas.get("browser"), indent=2)) # Full schema can be long
    if "browser" in schemas and "properties" in schemas["browser"]:
        print(f"  BrowserConfig has {len(schemas['browser']['properties'])} properties.")
        print(f"  Example property 'headless': {schemas['browser']['properties'].get('headless')}")

    print("\nCrawlerRunConfig Schema (sample):")
    # print(json.dumps(schemas.get("crawler"), indent=2))
    if "crawler" in schemas and "properties" in schemas["crawler"]:
        print(f"  CrawlerRunConfig has {len(schemas['crawler']['properties'])} properties.")
        print(f"  Example property 'word_count_threshold': {schemas['crawler']['properties'].get('word_count_threshold')}")

except requests.exceptions.RequestException as e:
    print(f"Error fetching schemas: {e}")
```

---
#### E.3. `/health` & `/metrics`

##### E.3.1. Example: Python script to programmatically check the `/health` endpoint.
(Similar to example 2.4.1, but reiterated here for completeness of this section)
```python
import requests
import os
import json
from datetime import datetime

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

try:
    response = requests.get(f"{BASE_URL}/health", headers=headers)
    response.raise_for_status()
    health_data = response.json()
    print("Health Check:")
    print(f"  Status: {health_data.get('status')}")
    print(f"  Version: {health_data.get('version')}")
    ts = health_data.get('timestamp')
    if ts:
        print(f"  Timestamp: {ts} (UTC: {datetime.utcfromtimestamp(ts).isoformat()})")
except requests.exceptions.RequestException as e:
    print(f"Error checking health: {e}")
```

##### E.3.2. Example: Accessing Prometheus metrics at `/metrics` (assuming Prometheus is enabled in `config.yml`).
This typically involves pointing a Prometheus scraper at the endpoint or manually fetching.
```python
import requests
import os

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
# Prometheus metrics are usually at /metrics, but the server.py config uses
# config["observability"]["prometheus"]["endpoint"] which defaults to "/metrics"
METRICS_ENDPOINT = "/metrics" # As per default config.yml
headers = get_headers()

try:
    # First, check if Prometheus is enabled in the server's config
    # This is a conceptual check, real check depends on your setup
    config_response = requests.get(f"{BASE_URL}/health", headers=headers) # Health often includes version
    # In a real scenario, you might have an endpoint to get active config or infer from behavior

    print(f"Attempting to fetch metrics from {BASE_URL}{METRICS_ENDPOINT}")
    response = requests.get(f"{BASE_URL}{METRICS_ENDPOINT}", headers=headers)
    if response.status_code == 200:
        print("Prometheus metrics response (first 500 chars):")
        print(response.text[:500] + "...")
    elif response.status_code == 404:
        print(f"Metrics endpoint {METRICS_ENDPOINT} not found. Ensure Prometheus is enabled in config.yml.")
    else:
        print(f"Error fetching metrics: {response.status_code} - {response.text}")

except requests.exceptions.RequestException as e:
    print(f"Error connecting to metrics endpoint: {e}")
```
**Note:** For this to work, `observability.prometheus.enabled` must be `true` in the server's `config.yml`.

---
## IV. Configuring the Deployment (via `config.yml`)

### 4.1. Note: These examples primarily show snippets of `config.yml` and describe their effect, rather than Python code to modify the live configuration.
The `config.yml` file is read by the server on startup. Changes typically require a server restart.

### 4.2. Rate Limiting Configuration

#### 4.2.1. Example `config.yml` snippet: Enabling rate limiting with a custom limit (e.g., "10/second").
```yaml
# In your config.yml
rate_limiting:
  enabled: true
  default_limit: "10/second" # Allows 10 requests per second per client IP
  # trusted_proxies: ["127.0.0.1"] # If behind a reverse proxy
```

#### 4.2.2. Example `config.yml` snippet: Using Redis as a storage backend for rate limiting.
This is recommended for production if you have multiple server instances.
```yaml
# In your config.yml
rate_limiting:
  enabled: true
  default_limit: "1000/minute"
  storage_uri: "redis://localhost:6379" # Or your Redis server URI
  # Ensure your Redis server is running and accessible
```

---
### 4.3. Security Settings Configuration

#### 4.3.1. Example `config.yml` snippet: Enabling JWT authentication.
```yaml
# In your config.yml
security:
  enabled: true
  jwt_enabled: true
  # jwt_secret_key: "YOUR_VERY_SECRET_KEY" # Auto-generated if not set
  # jwt_algorithm: "HS256"
  # jwt_access_token_expire_minutes: 30
  # jwt_allowed_email_domains: ["example.com", "another.org"] # Optional: Restrict token issuance
```
**Note:** Enabling `jwt_enabled` means endpoints decorated with the token dependency will require authentication.

#### 4.3.2. Example `config.yml` snippet: Enabling HTTPS redirect.
This is useful if your server is behind a reverse proxy that handles TLS termination.
```yaml
# In your config.yml
security:
  enabled: true
  https_redirect: true # Adds middleware to redirect HTTP to HTTPS
```

#### 4.3.3. Example `config.yml` snippet: Setting custom trusted hosts.
Restricts which `Host` headers are accepted. Use `["*"]` to allow all (less secure).
```yaml
# In your config.yml
security:
  enabled: true
  trusted_hosts: ["api.example.com", "localhost", "127.0.0.1"]
```

#### 4.3.4. Example `config.yml` snippet: Configuring custom HTTP security headers (CSP, X-Frame-Options).
```yaml
# In your config.yml
security:
  enabled: true
  headers:
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'; script-src 'self' 'unsafe-inline'; object-src 'none';"
    strict_transport_security: "max-age=31536000; includeSubDomains"
```

---
### 4.4. LLM Provider Configuration

#### 4.4.1. Example `config.yml` snippet: Setting the default LLM provider and API key env variable.
```yaml
# In your config.yml
llm:
  provider: "openai/gpt-4o-mini" # Default provider/model
  api_key_env: "OPENAI_API_KEY"  # Environment variable to read the API key from
```
The server will then expect the `OPENAI_API_KEY` environment variable to be set.

#### 4.4.2. Example `config.yml` snippet: Overriding the API key directly in the config (for testing/specific cases).
**Warning:** Not recommended for production due to security risks of hardcoding keys.
```yaml
# In your config.yml
llm:
  provider: "openai/gpt-3.5-turbo"
  api_key: "sk-this_is_a_test_key_do_not_use_in_prod" # Key directly in config
```

#### 4.4.3. Example `config.yml` snippet: Configuring for a different LiteLLM-supported provider (e.g., Groq).
```yaml
# In your config.yml
llm:
  provider: "groq/llama3-8b-8192"
  api_key_env: "GROQ_API_KEY" # Server will look for this env var
```

---
### 4.5. Default Crawler Settings
These settings in `config.yml` under the `crawler` key affect the default behavior if not overridden by specific `BrowserConfig` or `CrawlerRunConfig` in API requests.

#### 4.5.1. Example `config.yml` snippet: Modifying `crawler.base_config.simulate_user`.
```yaml
# In your config.yml
crawler:
  base_config:
    simulate_user: true # Enable user simulation features by default
```

#### 4.5.2. Example `config.yml` snippet: Adjusting `crawler.memory_threshold_percent`.
This is for the `MemoryAdaptiveDispatcher`.
```yaml
# In your config.yml
crawler:
  memory_threshold_percent: 85.0 # Pause new tasks if system memory usage exceeds 85%
```

#### 4.5.3. Example `config.yml` snippet: Configuring default `crawler.rate_limiter` parameters.
```yaml
# In your config.yml
crawler:
  rate_limiter:
    enabled: true
    base_delay: [0.5, 1.5] # Default delay between 0.5 and 1.5 seconds
```

#### 4.5.4. Example `config.yml` snippet: Adding default browser arguments to `crawler.browser.extra_args`.
```yaml
# In your config.yml
crawler:
  browser:
    # Default kwargs for BrowserConfig
    # headless: true
    # text_mode: false # etc.
    extra_args:
      - "--disable-gpu" # Already default, but shown for example
      - "--window-size=1920,1080"
      # Add other chromium flags as needed
```

#### 4.5.5. Example `config.yml` snippet: Changing `crawler.pool.max_pages` (global semaphore).
This controls the maximum number of concurrent browser pages globally for the server.
```yaml
# In your config.yml
crawler:
  pool:
    max_pages: 20 # Allow up to 20 concurrent browser pages
```

#### 4.5.6. Example `config.yml` snippet: Changing `crawler.pool.idle_ttl_sec` (janitor GC timeout).
This controls how long an idle browser instance in the pool will live before being closed.
```yaml
# In your config.yml
crawler:
  pool:
    idle_ttl_sec: 600 # Close idle browsers after 10 minutes (default is 30 min)
```

---
## V. Model-Controller-Presenter (MCP) Bridge Integration

### 5.1. Overview of MCP and its purpose with Crawl4ai.
The Model-Controller-Presenter (MCP) bridge allows AI tools and agents (like Claude Code, potentially others in the future) to interact with Crawl4ai's capabilities as "tools." Crawl4ai endpoints decorated with `@mcp_tool` become callable functions for these AI agents. This enables AIs to leverage web crawling and data extraction within their reasoning and task execution processes.

### 5.2. Accessing MCP Endpoints

#### 5.2.1. Example: Conceptual connection to the MCP WebSocket endpoint (`/mcp/ws`).
Connecting to `/mcp/ws` would typically be done by an MCP-compatible client library.
```python
# This is a conceptual Python example using a hypothetical MCP client library
# For actual MCP client usage, refer to the specific MCP tool's documentation.
# from mcp_client_library import MCPClient # Hypothetical library

# async def connect_mcp_ws():
#     mcp_url = f"{BASE_URL.replace('http', 'ws')}/mcp/ws"
#     async with MCPClient(mcp_url) as client:
#         print(f"Connected to MCP WebSocket at {mcp_url}")
#         # ... send/receive MCP messages ...
#         # e.g., await client.list_tools()
#         # e.g., await client.call_tool(tool_name="crawl", arguments={"urls": ["https://example.com"]})

# if __name__ == "__main__":
#     # asyncio.run(connect_mcp_ws()) # Uncomment if you have a client library
    print("MCP WebSocket conceptual connection. Real client library needed.")
```

#### 5.2.2. Example: Conceptual connection to the MCP SSE endpoint (`/mcp/sse`).
Server-Sent Events (SSE) is another transport for MCP.
```python
# Similar to WebSocket, an MCP-compatible SSE client would be used.
# from sseclient import SSEClient # A possible library for SSE

# def connect_mcp_sse():
#     mcp_sse_url = f"{BASE_URL}/mcp/sse"
#     print(f"Attempting to connect to MCP SSE at {mcp_sse_url} (conceptual)")
    # try:
    #     messages = SSEClient(mcp_sse_url) # This is synchronous, an async version would be better
    #     for msg in messages:
    #         print(f"MCP SSE Message: {msg.data}")
    #         if "some_condition_to_stop": # e.g. after init message
    #             break
    # except Exception as e:
    #     print(f"Error with MCP SSE: {e}")
    print("MCP SSE conceptual connection. Real client library needed.")

# if __name__ == "__main__":
    # connect_mcp_sse() # Uncomment if you have a client library
```

#### 5.2.3. Example: Fetching the MCP schema from `/mcp/schema` using `requests`.
This endpoint provides information about available MCP tools and resources.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

try:
    response = requests.get(f"{BASE_URL}/mcp/schema", headers=headers)
    response.raise_for_status()
    mcp_schema = response.json()
    print("MCP Schema:")
    # print(json.dumps(mcp_schema, indent=2)) # Can be verbose

    if "tools" in mcp_schema:
        print(f"\nAvailable MCP Tools ({len(mcp_schema['tools'])}):")
        for tool in mcp_schema["tools"][:3]: # Show first 3 tools
            print(f"  - Name: {tool.get('name')}, Description: {tool.get('description', '')[:50]}...")
    
    if "resources" in mcp_schema:
        print(f"\nAvailable MCP Resources ({len(mcp_schema['resources'])}):")
        for resource in mcp_schema["resources"][:3]: # Show first 3 resources
            print(f"  - Name: {resource.get('name')}, Description: {resource.get('description', '')[:50]}...")

except requests.exceptions.RequestException as e:
    print(f"Error fetching MCP schema: {e}")
```

### 5.3. Understanding MCP Tool Exposure

#### 5.3.1. Explanation: How endpoints decorated with `@mcp_tool` become available through the MCP bridge.
In `server.py`, FastAPI endpoints decorated with `@mcp_tool("tool_name")` are automatically registered with the MCP bridge. The MCP bridge then exposes these tools (like `/crawl`, `/md`, `/screenshot`, etc.) to connected MCP clients (e.g., AI agents). The arguments of the FastAPI endpoint function become the expected arguments for the MCP tool call.

#### 5.3.2. Example: Invoking a Crawl4ai tool (e.g., `/md`) through a simulated MCP client request structure (if simple enough to demonstrate with `requests`).
This is a conceptual illustration. A real MCP client would handle the JSON-RPC formatting for calls via WebSocket or SSE. The `/mcp/messages` endpoint is used by the SSE client to POST messages.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
# This requires a client_id which is usually established during SSE handshake
# For a simple test, if the server allows it, you might be able to send.
# However, this is highly dependent on the MCP server's transport implementation.

# This is a simplified, conceptual example of what an MCP call might look like
# if sent via a direct POST (which is how SSE clients send requests).
# A proper MCP client would handle session IDs and JSON-RPC framing.
mcp_tool_call_payload = {
    "jsonrpc": "2.0",
    "method": "call_tool",
    "params": {
        "name": "md", # The tool name, matches @mcp_tool("md")
        "arguments": { # These map to the FastAPI endpoint's Pydantic model or parameters
            "body": { # Matches the 'body: MarkdownRequest' in the get_markdown endpoint
                "url": "https://example.com",
                "f": "RAW"
            }
        }
    },
    "id": "some_unique_request_id"
}

# The SSE transport uses a client-specific POST endpoint, e.g., /mcp/messages/<client_id>
# This example cannot fully replicate that without a client_id.
# We'll try to hit a hypothetical endpoint or illustrate the payload.
print("Conceptual MCP tool call payload (actual call needs proper client/transport):")
print(json.dumps(mcp_tool_call_payload, indent=2))

# If you had a direct POST endpoint for tools (not standard MCP for SSE/WS):
# try:
#     # This is NOT how MCP typically works for SSE/WS, but for a hypothetical direct tool POST:
#     # response = requests.post(f"{BASE_URL}/mcp/call_tool_directly", json=mcp_tool_call_payload, headers=get_headers())
#     # response.raise_for_status()
#     # tool_result = response.json()
#     # print("\nResult from conceptual direct tool call:")
#     # print(json.dumps(tool_result, indent=2))
#     pass
# except requests.exceptions.RequestException as e:
#     print(f"Error in conceptual direct tool call: {e}")
```

---
## VI. Advanced Scenarios & Client-Side Best Practices

### 6.1. Chaining API Calls for Complex Workflows

#### 6.1.1. Example: Fetch preprocessed HTML using `/html`, then use this HTML as input to a local `crawl4ai` instance or another tool (conceptual).
```python
import requests
import os
import json
import asyncio
# Assuming crawl4ai is also installed as a library for local processing
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

async def chained_workflow():
    target_url = "https://example.com/article"
    
    # Step 1: Fetch preprocessed HTML from the API
    print(f"Step 1: Fetching preprocessed HTML for {target_url} via API...")
    html_payload = {"url": target_url}
    preprocessed_html = None
    try:
        response = requests.post(f"{BASE_URL}/html", json=html_payload, headers=headers)
        response.raise_for_status()
        data = response.json()
        preprocessed_html = data.get("html")
        if preprocessed_html:
            print(f"Successfully fetched preprocessed HTML (length: {len(preprocessed_html)}).")
        else:
            print("Failed to get preprocessed HTML from API.")
            return
    except requests.exceptions.RequestException as e:
        print(f"Error fetching preprocessed HTML: {e}")
        return

    # Step 2: Use this HTML with a local Crawl4AI instance for further processing
    # (e.g., applying a very specific local Markdown generator or extraction)
    if preprocessed_html:
        print("\nStep 2: Processing fetched HTML with a local Crawl4AI instance...")
        # Example: Generate Markdown using a specific local configuration
        custom_md_generator = DefaultMarkdownGenerator(
            # content_source="raw_html" because we are feeding it raw HTML
            content_source="raw_html", 
            options={"body_width": 0} # No line wrapping
        )
        local_run_config = CrawlerRunConfig(markdown_generator=custom_md_generator)
        
        async with AsyncWebCrawler() as local_crawler:
            # Use "raw:" prefix to tell the local crawler this is direct HTML content
            result = await local_crawler.arun(url=f"raw:{preprocessed_html}", config=local_run_config)
            if result.success and result.markdown:
                print("Markdown generated locally from API-fetched HTML (first 300 chars):")
                print(result.markdown.raw_markdown[:300] + "...")
            else:
                print(f"Local processing failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(chained_workflow())
```

---
### 6.2. API Error Handling

#### 6.2.1. Example: Python script showing robust error handling for common HTTP status codes (400, 401, 403, 404, 422, 500) when calling Crawl4ai API.
```python
import requests
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
# Use a token known to be invalid or expired if testing 401/403 with auth enabled
# invalid_headers = {"Authorization": "Bearer invalidtoken123"}
# For this example, we'll use the standard get_headers()
headers = get_headers()


def make_api_call(endpoint, method="GET", payload=None):
    url = f"{BASE_URL}/{endpoint.lstrip('/')}"
    try:
        if method.upper() == "GET":
            response = requests.get(url, params=payload, headers=headers, timeout=10)
        elif method.upper() == "POST":
            response = requests.post(url, json=payload, headers=headers, timeout=10)
        else:
            print(f"Unsupported method: {method}")
            return

        print(f"\n--- Testing {method} {url} with payload {payload} ---")
        print(f"Status Code: {response.status_code}")
        
        if response.ok: # status_code < 400
            print("Response JSON:")
            try:
                print(json.dumps(response.json(), indent=2, ensure_ascii=False)[:500] + "...")
            except json.JSONDecodeError:
                print("Response is not valid JSON.")
                print(f"Response Text (snippet): {response.text[:200]}...")
        else:
            print(f"Error Response Text: {response.text}")
            # Specific error handling based on status code
            if response.status_code == 400:
                print("Handling Bad Request (400)... Possible malformed payload.")
            elif response.status_code == 401:
                print("Handling Unauthorized (401)... API token might be missing or invalid.")
            elif response.status_code == 403:
                print("Handling Forbidden (403)... API token might lack permissions or IP restricted.")
            elif response.status_code == 404:
                print("Handling Not Found (404)... Endpoint or resource does not exist.")
            elif response.status_code == 422:
                print("Handling Unprocessable Entity (422)... Validation error with request data.")
                print(f"Details: {response.json().get('detail')}")
            elif response.status_code >= 500:
                print("Handling Server Error (5xx)... Problem on the server side.")
            
    except requests.exceptions.Timeout:
        print(f"Request to {url} timed out.")
    except requests.exceptions.ConnectionError:
        print(f"Could not connect to {url}. Is the server running?")
    except requests.exceptions.RequestException as e:
        print(f"An unexpected request error occurred for {url}: {e}")

if __name__ == "__main__":
    # Test a valid endpoint
    make_api_call("/health")
    
    # Test a non-existent endpoint (expected 404)
    make_api_call("/nonexistent_endpoint")

    # Test /md with missing URL (expected 422)
    make_api_call("/md", method="POST", payload={"f": "RAW"}) 

    # Test /token with invalid payload (expected 422 if email is missing)
    make_api_call("/token", method="POST", payload={"not_email": "test"})

    # If JWT is enabled, an unauthenticated call to a protected endpoint would give 401/403.
    # For this example, assume /admin is a hypothetical protected endpoint.
    # print("\nAttempting access to hypothetical protected /admin endpoint...")
    # make_api_call("/admin", headers={}) # No auth header
```

---
### 6.3. Client-Side Script for Long-Running Jobs

#### 6.3.1. Example: A Python client that submits a job to `/crawl`, polls `/task/{task_id}` with backoff, and retrieves results.
This is a more robust version of the polling mechanism shown earlier.
```python
import requests
import time
import os
import json

BASE_URL = os.environ.get("CRAWL4AI_BASE_URL", "http://localhost:11235")
headers = get_headers()

def submit_job_and_wait_with_backoff(payload, max_poll_time=300, initial_poll_interval=2, max_poll_interval=30, backoff_factor=1.5):
    try:
        # 1. Submit Job
        submit_response = requests.post(f"{BASE_URL}/crawl", json=payload, headers=headers)
        submit_response.raise_for_status()
        task_id = submit_response.json().get("task_id")
        if not task_id:
            print("Failed to get task_id from submission.")
            return None
        print(f"Job submitted. Task ID: {task_id}. Polling with backoff...")

        # 2. Poll with Exponential Backoff
        poll_interval = initial_poll_interval
        start_time = time.time()
        
        while time.time() - start_time < max_poll_time:
            status_response = requests.get(f"{BASE_URL}/task/{task_id}", headers=headers)
            status_response.raise_for_status()
            status_data = status_response.json()
            current_status = status_data.get("status")
            
            print(f"  Task {task_id} status: {current_status} (next poll in {poll_interval:.1f}s)")

            if current_status == "COMPLETED":
                print(f"Task {task_id} COMPLETED.")
                return status_data.get("result")
            elif current_status == "FAILED":
                print(f"Task {task_id} FAILED. Error: {status_data.get('error')}")
                return None
            
            time.sleep(poll_interval)
            poll_interval = min(poll_interval * backoff_factor, max_poll_interval)
            
        print(f"Task {task_id} polling timed out after {max_poll_time} seconds.")
        return None

    except requests.exceptions.RequestException as e:
        print(f"API Error: {e}")
        return None
    except Exception as e:
        print(f"Unexpected error: {e}")
        return None

if __name__ == "__main__":
    # Example of a potentially longer job (crawling a site known for being slow or large)
    long_job_payload = {
        "urls": ["https://archive.org/web/"], # A site that might take a bit longer
        "crawler_config": {"word_count_threshold": 500} # Higher threshold
    }
    
    print("\n--- Testing Long-Running Job Client ---")
    job_result = submit_job_and_wait_with_backoff(long_job_payload, max_poll_time=120) # 2 min timeout

    if job_result and "results" in job_result:
        for i, res_item in enumerate(job_result["results"]):
            print(f"\nResult for {res_item.get('url')}:")
            print(f"  Success: {res_item.get('success')}")
            if res_item.get('success'):
                 md_length = len(res_item.get('markdown', {}).get('raw_markdown', ''))
                 print(f"  Markdown Length: {md_length}")
    elif job_result:
         print(f"\nReceived result data (unexpected structure):")
         print(json.dumps(job_result, indent=2, ensure_ascii=False))
    else:
        print("\nJob did not complete successfully or timed out.")
```

---
### 6.4. Batching Requests to `/crawl/stream` vs. `/crawl`

#### 6.4.1. Discussion: When to use streaming for many URLs vs. submitting a single job with multiple URLs.

*   **`/crawl` (Job-based, polling):**
    *   **Pros:**
        *   Better for very large numbers of URLs where you don't need immediate feedback for each.
        *   Robust to client disconnections (job continues on server).
        *   Redis queue handles load and persistence of jobs.
        *   Server manages concurrency and resources more globally.
    *   **Cons:**
        *   Requires a polling mechanism on the client side.
        *   Results are only available once the entire batch (or individual URL within a multi-URL job if server processes them somewhat independently before final aggregation) is complete.
    *   **Use when:** You have hundreds or thousands of URLs, can tolerate some delay for results, and need a fire-and-forget submission style.

*   **`/crawl/stream` (Streaming):**
    *   **Pros:**
        *   Real-time feedback: results for each URL are streamed back as soon as they are processed.
        *   Simpler client logic if immediate processing of individual results is needed.
        *   Good for interactive applications or dashboards.
    *   **Cons:**
        *   Client must maintain an open connection. If it drops, the stream is lost.
        *   Can be less efficient for very large numbers of URLs if each URL is processed sequentially within the stream handler on the server (though `handle_stream_crawl_request` does process them concurrently up to server limits).
        *   The client needs to handle NDJSON parsing.
    *   **Use when:** You need results for URLs as they come in, are processing a moderate number of URLs, or building an interactive tool.

**General Guideline:**
*   For a few to a few dozen URLs where you want results quickly and can process them one-by-one: `/crawl/stream`.
*   For hundreds or thousands of URLs, or when you prefer to submit a batch and check back later: `/crawl` with polling.
*   If using `/crawl/stream` for many URLs, ensure your client-side processing of each streamed result is fast to avoid becoming a bottleneck. The server-side uses an `AsyncGenerator` which processes URLs concurrently up to its internal limits, so the client should be ready to consume these results efficiently.

```

---


## Data Extraction - Memory
Source: crawl4ai_extraction_memory_content.llm.md

Okay, I will now generate the "Foundational Memory" document for the `extraction` component of `crawl4ai`, based on the outline you provided and the information I've processed from the codebase and existing documentation.

```markdown
# Detailed Outline for crawl4ai - extraction Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_extraction.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Overview of Data Extraction in Crawl4ai

*   1.1. Purpose of the Extraction Component: The extraction component in Crawl4ai is responsible for parsing structured data from web content (HTML, text, Markdown) or PDF documents. It allows users to define how data should be identified and extracted, using various strategies ranging from rule-based (CSS, XPath, Regex) to LLM-powered approaches. Its goal is to transform raw crawled content into usable, structured information.
*   1.2. Core Concepts:
    *   1.2.1. `ExtractionStrategy`: This is an abstract base class (interface) that defines the contract for all specific extraction methods. Each strategy implements how data is extracted from the provided content.
    *   1.2.2. `ChunkingStrategy`: This is an abstract base class (interface) for strategies that preprocess content by splitting it into smaller, manageable chunks. This is particularly relevant for LLM-based extraction strategies that have token limits for their input.
    *   1.2.3. Schemas: Schemas define the structure of the data to be extracted. For non-LLM strategies like `JsonCssExtractionStrategy` or `JsonXPathExtractionStrategy`, schemas are typically dictionary-based, specifying selectors and field types. For `LLMExtractionStrategy`, schemas can be Pydantic models or JSON schema dictionaries that guide the LLM in structuring its output.
    *   1.2.4. `CrawlerRunConfig`: The `CrawlerRunConfig` object allows users to specify which `extraction_strategy` and `chunking_strategy` (if applicable) should be used for a particular crawl operation via its `arun()` method.

## 2. `ExtractionStrategy` Interface

*   2.1. Purpose: The `ExtractionStrategy` class, found in `crawl4ai.extraction_strategy`, serves as an abstract base class (ABC) defining the standard interface for all data extraction strategies within the Crawl4ai library. Implementations of this class provide specific methods for extracting structured data from content.
*   2.2. Key Abstract Methods:
    *   `extract(self, url: str, content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
        *   Description: Abstract method intended to extract meaningful blocks or chunks from the given content. Subclasses must implement this.
        *   Parameters:
            *   `url (str)`: The URL of the webpage.
            *   `content (str)`: The HTML, Markdown, or text content of the webpage.
            *   `*q`: Variable positional arguments.
            *   `**kwargs`: Variable keyword arguments.
        *   Returns: `List[Dict[str, Any]]` - A list of extracted blocks or chunks, typically as dictionaries.
    *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
        *   Description: Abstract method to process sections of text, often in parallel by default implementations in subclasses. Subclasses must implement this.
        *   Parameters:
            *   `url (str)`: The URL of the webpage.
            *   `sections (List[str])`: List of sections (strings) to process.
            *   `*q`: Variable positional arguments.
            *   `**kwargs`: Variable keyword arguments.
        *   Returns: `List[Dict[str, Any]]` - A list of processed JSON blocks.
*   2.3. Input Format Property:
    *   `input_format (str)`: [Read-only] - An attribute indicating the expected input format for the content to be processed by the strategy (e.g., "markdown", "html", "fit_html", "text"). Default is "markdown".

## 3. Non-LLM Based Extraction Strategies

*   ### 3.1. Class `NoExtractionStrategy`
    *   3.1.1. Purpose: A baseline `ExtractionStrategy` that performs no actual data extraction. It returns the input content as is, typically useful for scenarios where only raw or cleaned HTML/Markdown is needed without further structuring.
    *   3.1.2. Inheritance: `ExtractionStrategy`
    *   3.1.3. Initialization (`__init__`):
        *   3.1.3.1. Signature: `NoExtractionStrategy(**kwargs)`
        *   3.1.3.2. Parameters:
            *   `**kwargs`: Passed to the base `ExtractionStrategy` initializer.
    *   3.1.4. Key Public Methods:
        *   `extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Returns the provided `html` content wrapped in a list containing a single dictionary: `[{"index": 0, "content": html}]`.
        *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Returns a list where each input section is wrapped in a dictionary: `[{"index": i, "tags": [], "content": section} for i, section in enumerate(sections)]`.

*   ### 3.2. Class `JsonCssExtractionStrategy`
    *   3.2.1. Purpose: Extracts structured data from HTML content using a JSON schema that defines CSS selectors to locate and extract data for specified fields. It uses BeautifulSoup4 for parsing and selection.
    *   3.2.2. Inheritance: `JsonElementExtractionStrategy` (which inherits from `ExtractionStrategy`)
    *   3.2.3. Initialization (`__init__`):
        *   3.2.3.1. Signature: `JsonCssExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.2.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.2.4. Schema Definition for `JsonCssExtractionStrategy`:
        *   3.2.4.1. `name (str)`: A descriptive name for the schema (e.g., "ProductDetails").
        *   3.2.4.2. `baseSelector (str)`: The primary CSS selector that identifies each root element representing an item to be extracted (e.g., "div.product-item").
        *   3.2.4.3. `fields (List[Dict[str, Any]])`: A list of dictionaries, each defining a field to be extracted from within each `baseSelector` element.
            *   Each field dictionary:
                *   `name (str)`: The key for this field in the output JSON object.
                *   `selector (str)`: The CSS selector for this field, relative to its parent element (either the `baseSelector` or a parent "nested" field).
                *   `type (str)`: Specifies how to extract the data. Common values:
                    *   `"text"`: Extracts the text content of the selected element.
                    *   `"attribute"`: Extracts the value of a specified HTML attribute.
                    *   `"html"`: Extracts the raw inner HTML of the selected element.
                    *   `"list"`: Extracts a list of items. The `fields` sub-key then defines the structure of each item in the list (if objects) or the `selector` directly targets list elements for primitive values.
                    *   `"nested"`: Extracts a nested JSON object. The `fields` sub-key defines the structure of this nested object.
                *   `attribute (str, Optional)`: Required if `type` is "attribute". Specifies the name of the HTML attribute to extract (e.g., "href", "src").
                *   `fields (List[Dict[str, Any]], Optional)`: Required if `type` is "list" (for a list of objects) or "nested". Defines the structure of the nested object or list items.
                *   `transform (str, Optional)`: A string indicating a transformation to apply to the extracted value (e.g., "lowercase", "uppercase", "strip").
                *   `default (Any, Optional)`: A default value to use if the selector does not find an element or the attribute is missing.
    *   3.2.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` and applies the defined schema to extract structured data using CSS selectors.
    *   3.2.6. Features:
        *   3.2.6.1. Nested Extraction: Supports extracting complex, nested JSON objects by defining "nested" type fields within the schema.
        *   3.2.6.2. List Handling: Supports extracting lists of primitive values (e.g., list of strings from multiple `<li>` tags) or lists of structured objects (e.g., a list of product details, each with its own fields).

*   ### 3.3. Class `JsonXPathExtractionStrategy`
    *   3.3.1. Purpose: Extracts structured data from HTML/XML content using a JSON schema that defines XPath expressions to locate and extract data. It uses `lxml` for parsing and XPath evaluation.
    *   3.3.2. Inheritance: `JsonElementExtractionStrategy` (which inherits from `ExtractionStrategy`)
    *   3.3.3. Initialization (`__init__`):
        *   3.3.3.1. Signature: `JsonXPathExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.3.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules, where selectors are XPath expressions.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.3.4. Schema Definition: The schema structure is identical to `JsonCssExtractionStrategy` (see 3.2.4), but the `baseSelector` and field `selector` values must be valid XPath expressions.
    *   3.3.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` using `lxml` and applies the defined schema to extract structured data using XPath expressions.

*   ### 3.4. Class `JsonLxmlExtractionStrategy`
    *   3.4.1. Purpose: Provides an alternative CSS selector-based extraction strategy leveraging the `lxml` library for parsing and selection, which can offer performance benefits over BeautifulSoup4 in some cases.
    *   3.4.2. Inheritance: `JsonCssExtractionStrategy` (and thus `JsonElementExtractionStrategy`, `ExtractionStrategy`)
    *   3.4.3. Initialization (`__init__`):
        *   3.4.3.1. Signature: `JsonLxmlExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.4.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules, using CSS selectors.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.4.4. Schema Definition: Identical to `JsonCssExtractionStrategy` (see 3.2.4).
    *   3.4.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` using `lxml` and applies the defined schema to extract structured data using lxml's CSS selector capabilities (which often translates CSS to XPath internally).

*   ### 3.5. Class `RegexExtractionStrategy`
    *   3.5.1. Purpose: Extracts data from text content (HTML, Markdown, or plain text) using a collection of regular expression patterns. Each match is returned as a structured dictionary.
    *   3.5.2. Inheritance: `ExtractionStrategy`
    *   3.5.3. Initialization (`__init__`):
        *   3.5.3.1. Signature: `RegexExtractionStrategy(patterns: Union[Dict[str, str], List[Tuple[str, str]], "RegexExtractionStrategy._B"] = _B.NOTHING, input_format: str = "fit_html", **kwargs)`
        *   3.5.3.2. Parameters:
            *   `patterns (Union[Dict[str, str], List[Tuple[str, str]], "_B"], default: _B.NOTHING)`:
                *   Description: Defines the regex patterns to use.
                *   Can be a dictionary mapping labels to regex strings (e.g., `{"email": r"..."}`).
                *   Can be a list of (label, regex_string) tuples.
                *   Can be a bitwise OR combination of `RegexExtractionStrategy._B` enum members for using built-in patterns (e.g., `RegexExtractionStrategy.Email | RegexExtractionStrategy.Url`).
            *   `input_format (str, default: "fit_html")`: Specifies the input format for the content. Options: "html" (raw HTML), "markdown" (Markdown from HTML), "text" (plain text from HTML), "fit_html" (content filtered for relevance before regex application).
            *   `**kwargs`: Passed to the base `ExtractionStrategy`.
    *   3.5.4. Built-in Patterns (`RegexExtractionStrategy._B` Enum - an `IntFlag`):
        *   `EMAIL (auto())`: Matches email addresses. Example pattern: `r"[\\w.+-]+@[\\w-]+\\.[\\w.-]+"`
        *   `PHONE_INTL (auto())`: Matches international phone numbers. Example pattern: `r"\\+?\\d[\\d .()-]{7,}\\d"`
        *   `PHONE_US (auto())`: Matches US phone numbers. Example pattern: `r"\\(?\\d{3}\\)?[-. ]?\\d{3}[-. ]?\\d{4}"`
        *   `URL (auto())`: Matches URLs. Example pattern: `r"https?://[^\\s\\'\"<>]+"`
        *   `IPV4 (auto())`: Matches IPv4 addresses. Example pattern: `r"(?:\\d{1,3}\\.){3}\\d{1,3}"`
        *   `IPV6 (auto())`: Matches IPv6 addresses. Example pattern: `r"[A-F0-9]{1,4}(?::[A-F0-9]{1,4}){7}"`
        *   `UUID (auto())`: Matches UUIDs. Example pattern: `r"[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}"`
        *   `CURRENCY (auto())`: Matches currency amounts. Example pattern: `r"(?:USD|EUR|RM|\\$|€|¥|£)\\s?\\d+(?:[.,]\\d{2})?"`
        *   `PERCENTAGE (auto())`: Matches percentages. Example pattern: `r"\\d+(?:\\.\\d+)?%"`
        *   `NUMBER (auto())`: Matches numbers (integers, decimals). Example pattern: `r"\\b\\d{1,3}(?:[,.]?\\d{3})*(?:\\.\\d+)?\\b"`
        *   `DATE_ISO (auto())`: Matches ISO 8601 dates (YYYY-MM-DD). Example pattern: `r"\\d{4}-\\d{2}-\\d{2}"`
        *   `DATE_US (auto())`: Matches US-style dates (MM/DD/YYYY or MM/DD/YY). Example pattern: `r"\\d{1,2}/\\d{1,2}/\\d{2,4}"`
        *   `TIME_24H (auto())`: Matches 24-hour time formats (HH:MM or HH:MM:SS). Example pattern: `r"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?:[:.][0-5]\\d)?\\b"`
        *   `POSTAL_US (auto())`: Matches US postal codes (ZIP codes). Example pattern: `r"\\b\\d{5}(?:-\\d{4})?\\b"`
        *   `POSTAL_UK (auto())`: Matches UK postal codes. Example pattern: `r"\\b[A-Z]{1,2}\\d[A-Z\\d]? ?\\d[A-Z]{2}\\b"`
        *   `HTML_COLOR_HEX (auto())`: Matches HTML hex color codes. Example pattern: `r"#[0-9A-Fa-f]{6}\\b"`
        *   `TWITTER_HANDLE (auto())`: Matches Twitter handles. Example pattern: `r"@[\\w]{1,15}"`
        *   `HASHTAG (auto())`: Matches hashtags. Example pattern: `r"#[\\w-]+"`
        *   `MAC_ADDR (auto())`: Matches MAC addresses. Example pattern: `r"(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}"`
        *   `IBAN (auto())`: Matches IBANs. Example pattern: `r"[A-Z]{2}\\d{2}[A-Z0-9]{11,30}"`
        *   `CREDIT_CARD (auto())`: Matches common credit card numbers. Example pattern: `r"\\b(?:4\\d{12}(?:\\d{3})?|5[1-5]\\d{14}|3[47]\\d{13}|6(?:011|5\\d{2})\\d{12})\\b"`
        *   `ALL (_B(-1).value & ~_B.NOTHING.value)`: Includes all built-in patterns except `NOTHING`.
        *   `NOTHING (_B(0).value)`: Includes no built-in patterns.
    *   3.5.5. Key Public Methods:
        *   `extract(self, url: str, content: str, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Applies all configured regex patterns (built-in and custom) to the input `content`.
            *   Returns: `List[Dict[str, Any]]` - A list of dictionaries, where each dictionary represents a match and contains:
                *   `"url" (str)`: The source URL.
                *   `"label" (str)`: The label of the matching regex pattern.
                *   `"value" (str)`: The actual matched string.
                *   `"span" (Tuple[int, int])`: The start and end indices of the match within the content.
    *   3.5.6. Static Method: `generate_pattern`
        *   3.5.6.1. Signature: `staticmethod generate_pattern(label: str, html: str, query: Optional[str] = None, examples: Optional[List[str]] = None, llm_config: Optional[LLMConfig] = None, **kwargs) -> Dict[str, str]`
        *   3.5.6.2. Purpose: Uses an LLM to automatically generate a Python-compatible regular expression pattern for a given label, based on sample HTML content, an optional natural language query describing the target, and/or examples of desired matches.
        *   3.5.6.3. Parameters:
            *   `label (str)`: A descriptive label for the pattern to be generated (e.g., "product_price", "article_date").
            *   `html (str)`: The HTML content from which the pattern should be inferred.
            *   `query (Optional[str], default: None)`: A natural language description of what kind of data the regex should capture (e.g., "Extract the publication date", "Find all ISBN numbers").
            *   `examples (Optional[List[str]], default: None)`: A list of example strings that the generated regex should successfully match from the provided HTML.
            *   `llm_config (Optional[LLMConfig], default: None)`: Configuration for the LLM to be used. If `None`, uses default `LLMConfig`.
            *   `**kwargs`: Additional arguments passed to the LLM completion request (e.g., `temperature`, `max_tokens`).
        *   3.5.6.4. Returns: `Dict[str, str]` - A dictionary containing the generated pattern, in the format `{label: "regex_pattern_string"}`.

## 4. LLM-Based Extraction Strategies

*   ### 4.1. Class `LLMExtractionStrategy`
    *   4.1.1. Purpose: Employs Large Language Models (LLMs) to extract either structured data according to a schema or relevant blocks of text based on natural language instructions from various content formats (HTML, Markdown, text).
    *   4.1.2. Inheritance: `ExtractionStrategy`
    *   4.1.3. Initialization (`__init__`):
        *   4.1.3.1. Signature: `LLMExtractionStrategy(llm_config: Optional[LLMConfig] = None, instruction: Optional[str] = None, schema: Optional[Union[Dict[str, Any], "BaseModel"]] = None, extraction_type: str = "block", chunk_token_threshold: int = CHUNK_TOKEN_THRESHOLD, overlap_rate: float = OVERLAP_RATE, word_token_rate: float = WORD_TOKEN_RATE, apply_chunking: bool = True, force_json_response: bool = False, **kwargs)`
        *   4.1.3.2. Parameters:
            *   `llm_config (Optional[LLMConfig], default: None)`: Configuration for the LLM. If `None`, a default `LLMConfig` is created.
            *   `instruction (Optional[str], default: None)`: Natural language instructions to guide the LLM's extraction process (e.g., "Extract the main article content", "Summarize the key points").
            *   `schema (Optional[Union[Dict[str, Any], "BaseModel"]], default: None)`: A Pydantic model class or a dictionary representing a JSON schema. Used when `extraction_type` is "schema" to define the desired output structure.
            *   `extraction_type (str, default: "block")`: Determines the extraction mode.
                *   `"block"`: LLM identifies and extracts relevant blocks/chunks of text based on the `instruction`.
                *   `"schema"`: LLM attempts to populate the fields defined in `schema` from the content.
            *   `chunk_token_threshold (int, default: CHUNK_TOKEN_THRESHOLD)`: The target maximum number of tokens for each chunk of content sent to the LLM. `CHUNK_TOKEN_THRESHOLD` is defined in `crawl4ai.config` (default value: 10000).
            *   `overlap_rate (float, default: OVERLAP_RATE)`: The percentage of overlap between consecutive chunks to ensure context continuity. `OVERLAP_RATE` is defined in `crawl4ai.config` (default value: 0.1, i.e., 10%).
            *   `word_token_rate (float, default: WORD_TOKEN_RATE)`: An estimated ratio of words to tokens (e.g., 0.75 words per token). Used for approximating chunk boundaries. `WORD_TOKEN_RATE` is defined in `crawl4ai.config` (default value: 0.75).
            *   `apply_chunking (bool, default: True)`: If `True`, the input content is chunked before being sent to the LLM. If `False`, the entire content is sent (which might exceed token limits for large inputs).
            *   `force_json_response (bool, default: False)`: If `True` and `extraction_type` is "schema", instructs the LLM to strictly adhere to JSON output format.
            *   `**kwargs`: Passed to `ExtractionStrategy` and potentially to the underlying LLM API calls (e.g., `temperature`, `max_tokens` if not set in `llm_config`).
    *   4.1.4. Key Public Methods:
        *   `extract(self, url: str, content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Processes the input `content`. If `apply_chunking` is `True`, it first chunks the content using the specified `chunking_strategy` (or a default one if `LLMExtractionStrategy` manages it internally). Then, for each chunk (or the whole content if not chunked), it constructs a prompt based on `instruction` and/or `schema` and sends it to the configured LLM.
            *   Returns: `List[Dict[str, Any]]` - A list of dictionaries.
                *   If `extraction_type` is "block", each dictionary typically contains `{"index": int, "content": str, "tags": List[str]}`.
                *   If `extraction_type` is "schema", each dictionary is an instance of the extracted structured data, ideally conforming to the provided `schema`. If the LLM returns multiple JSON objects in a list, they are parsed and returned.
        *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Processes a list of content `sections` in parallel (using `ThreadPoolExecutor`). Each section is passed to the `extract` method logic.
            *   Returns: `List[Dict[str, Any]]` - Aggregated list of results from processing all sections.
    *   4.1.5. `TokenUsage` Tracking:
        *   `total_usage (TokenUsage)`: [Read-only Public Attribute] - An instance of `TokenUsage` that accumulates the token counts (prompt, completion, total) from all LLM API calls made by this `LLMExtractionStrategy` instance.
        *   `usages (List[TokenUsage])`: [Read-only Public Attribute] - A list containing individual `TokenUsage` objects for each separate LLM API call made during the extraction process. This allows for detailed tracking of token consumption per call.

## 5. `ChunkingStrategy` Interface and Implementations

*   ### 5.1. Interface `ChunkingStrategy`
    *   5.1.1. Purpose: The `ChunkingStrategy` class, found in `crawl4ai.chunking_strategy`, is an abstract base class (ABC) that defines the interface for different content chunking algorithms. Chunking is used to break down large pieces of text or HTML into smaller, manageable segments, often before feeding them to an LLM or other processing steps.
    *   5.1.2. Key Abstract Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Abstract method that must be implemented by subclasses to split the input `content` string into a list of string chunks.
            *   Parameters:
                *   `content (str)`: The content to be chunked.
            *   Returns: `List[str]` - A list of content chunks.

*   ### 5.2. Class `RegexChunking`
    *   5.2.1. Purpose: Implements `ChunkingStrategy` by splitting content based on a list of regular expression patterns. It can also attempt to merge smaller chunks to meet a target `chunk_size`.
    *   5.2.2. Inheritance: `ChunkingStrategy`
    *   5.2.3. Initialization (`__init__`):
        *   5.2.3.1. Signature: `RegexChunking(patterns: Optional[List[str]] = None, chunk_size: Optional[int] = None, overlap: Optional[int] = None, word_token_ratio: Optional[float] = WORD_TOKEN_RATE, **kwargs)`
        *   5.2.3.2. Parameters:
            *   `patterns (Optional[List[str]], default: None)`: A list of regex patterns used to split the text. If `None`, defaults to paragraph-based splitting (`["\\n\\n+"]`).
            *   `chunk_size (Optional[int], default: None)`: The target token size for each chunk. If specified, the strategy will try to merge smaller chunks created by regex splitting to approximate this size.
            *   `overlap (Optional[int], default: None)`: The target token overlap between consecutive chunks when `chunk_size` is active.
            *   `word_token_ratio (Optional[float], default: WORD_TOKEN_RATE)`: The estimated ratio of words to tokens, used if `chunk_size` or `overlap` are specified. `WORD_TOKEN_RATE` is defined in `crawl4ai.config` (default value: 0.75).
            *   `**kwargs`: Additional keyword arguments.
    *   5.2.4. Key Public Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Splits the input `content` using the configured regex patterns. If `chunk_size` is set, it then merges these initial chunks to meet the target size with the specified overlap.

*   ### 5.3. Class `IdentityChunking`
    *   5.3.1. Purpose: A `ChunkingStrategy` that does not perform any actual chunking. It returns the input content as a single chunk in a list.
    *   5.3.2. Inheritance: `ChunkingStrategy`
    *   5.3.3. Initialization (`__init__`):
        *   5.3.3.1. Signature: `IdentityChunking(**kwargs)`
        *   5.3.3.2. Parameters:
            *   `**kwargs`: Additional keyword arguments.
    *   5.3.4. Key Public Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Returns the input `content` as a single-element list: `[content]`.

## 6. Defining Schemas for Extraction

*   6.1. Purpose: Schemas provide a structured way to define what data needs to be extracted from content and how it should be organized. This allows for consistent and predictable output from the extraction process.
*   6.2. Schemas for CSS/XPath/LXML-based Extraction (`JsonCssExtractionStrategy`, etc.):
    *   6.2.1. Format: These strategies use a dictionary-based JSON-like schema.
    *   6.2.2. Key elements: As detailed in section 3.2.4 for `JsonCssExtractionStrategy`:
        *   `name (str)`: Name of the schema.
        *   `baseSelector (str)`: CSS selector (for CSS strategies) or XPath expression (for XPath strategy) identifying the repeating parent elements.
        *   `fields (List[Dict[str, Any]])`: A list defining each field to extract. Each field definition includes:
            *   `name (str)`: Output key for the field.
            *   `selector (str)`: CSS/XPath selector relative to the `baseSelector` or parent "nested" element.
            *   `type (str)`: "text", "attribute", "html", "list", "nested".
            *   `attribute (str, Optional)`: Name of HTML attribute (if type is "attribute").
            *   `fields (List[Dict], Optional)`: For "list" (of objects) or "nested" types.
            *   `transform (str, Optional)`: e.g., "lowercase".
            *   `default (Any, Optional)`: Default value if not found.
*   6.3. Schemas for LLM-based Extraction (`LLMExtractionStrategy`):
    *   6.3.1. Format: `LLMExtractionStrategy` accepts schemas in two main formats when `extraction_type="schema"`:
        *   Pydantic models: The Pydantic model class itself.
        *   Dictionary: A Python dictionary representing a valid JSON schema.
    *   6.3.2. Pydantic Models:
        *   Definition: Users can define a Pydantic `BaseModel` where each field represents a piece of data to be extracted. Field types and descriptions are automatically inferred.
        *   Conversion: `LLMExtractionStrategy` internally converts the Pydantic model to its JSON schema representation (`model_json_schema()`) to guide the LLM.
    *   6.3.3. Dictionary-based JSON Schema:
        *   Structure: Users can provide a dictionary that conforms to the JSON Schema specification. This typically includes a `type: "object"` at the root and a `properties` dictionary defining each field, its type (e.g., "string", "number", "array", "object"), and optionally a `description`.
        *   Usage: This schema is passed to the LLM to instruct it on the desired output format.

## 7. Configuration with `CrawlerRunConfig`

*   7.1. Purpose: The `CrawlerRunConfig` class (from `crawl4ai.async_configs`) is used to configure the behavior of a specific `arun()` or `arun_many()` call on an `AsyncWebCrawler` instance. It allows specifying various runtime parameters, including the extraction and chunking strategies.
*   7.2. Key Attributes:
    *   `extraction_strategy (Optional[ExtractionStrategy], default: None)`:
        *   Purpose: Specifies the `ExtractionStrategy` instance to be used for processing the content obtained during the crawl. If `None`, no structured extraction beyond basic Markdown generation occurs (unless a default is applied by the crawler).
        *   Type: An instance of a class inheriting from `ExtractionStrategy`.
    *   `chunking_strategy (Optional[ChunkingStrategy], default: RegexChunking())`:
        *   Purpose: Specifies the `ChunkingStrategy` instance to be used for breaking down content into smaller pieces before it's passed to an `ExtractionStrategy` (particularly `LLMExtractionStrategy`).
        *   Type: An instance of a class inheriting from `ChunkingStrategy`.
        *   Default: An instance of `RegexChunking()` with its default parameters (paragraph-based splitting).

## 8. LLM-Specific Configuration and Models

*   ### 8.1. Class `LLMConfig`
    *   8.1.1. Purpose: The `LLMConfig` class (from `crawl4ai.async_configs`) centralizes configuration parameters for interacting with Large Language Models (LLMs) through various providers.
    *   8.1.2. Initialization (`__init__`):
        *   8.1.2.1. Signature:
            ```python
            class LLMConfig:
                def __init__(
                    self,
                    provider: str = DEFAULT_PROVIDER,
                    api_token: Optional[str] = None,
                    base_url: Optional[str] = None,
                    temperature: Optional[float] = None,
                    max_tokens: Optional[int] = None,
                    top_p: Optional[float] = None,
                    frequency_penalty: Optional[float] = None,
                    presence_penalty: Optional[float] = None,
                    stop: Optional[List[str]] = None,
                    n: Optional[int] = None,
                ): ...
            ```
        *   8.1.2.2. Parameters:
            *   `provider (str, default: DEFAULT_PROVIDER)`: Specifies the LLM provider and model, e.g., "openai/gpt-4o-mini", "ollama/llama3.3". `DEFAULT_PROVIDER` is "openai/gpt-4o-mini".
            *   `api_token (Optional[str], default: None)`: API token for the LLM provider. If `None`, the system attempts to read it from environment variables (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `GROQ_API_KEY` based on provider). Can also be prefixed with "env:" (e.g., "env:MY_CUSTOM_LLM_KEY").
            *   `base_url (Optional[str], default: None)`: Custom base URL for the LLM API endpoint, for self-hosted or alternative provider endpoints.
            *   `temperature (Optional[float], default: None)`: Controls randomness in LLM generation. Higher values (e.g., 0.8) make output more random, lower (e.g., 0.2) more deterministic.
            *   `max_tokens (Optional[int], default: None)`: Maximum number of tokens the LLM should generate in its response.
            *   `top_p (Optional[float], default: None)`: Nucleus sampling parameter. An alternative to temperature; controls the cumulative probability mass of tokens considered for generation.
            *   `frequency_penalty (Optional[float], default: None)`: Penalizes new tokens based on their existing frequency in the text so far, decreasing repetition.
            *   `presence_penalty (Optional[float], default: None)`: Penalizes new tokens based on whether they have appeared in the text so far, encouraging new topics.
            *   `stop (Optional[List[str]], default: None)`: A list of sequences where the API will stop generating further tokens.
            *   `n (Optional[int], default: None)`: Number of completions to generate for each prompt.
    *   8.1.3. Helper Methods:
        *   `from_kwargs(kwargs: dict) -> LLMConfig`:
            *   Description: [Static method] Creates an `LLMConfig` instance from a dictionary of keyword arguments.
        *   `to_dict() -> dict`:
            *   Description: Converts the `LLMConfig` instance into a dictionary representation.
        *   `clone(**kwargs) -> LLMConfig`:
            *   Description: Creates a new `LLMConfig` instance as a copy of the current one, allowing specific attributes to be overridden with `kwargs`.

*   ### 8.2. Dataclass `TokenUsage`
    *   8.2.1. Purpose: The `TokenUsage` dataclass (from `crawl4ai.models`) is used to store information about the number of tokens consumed during an LLM API call.
    *   8.2.2. Fields:
        *   `completion_tokens (int, default: 0)`: The number of tokens generated by the LLM in the completion.
        *   `prompt_tokens (int, default: 0)`: The number of tokens in the prompt sent to the LLM.
        *   `total_tokens (int, default: 0)`: The sum of `completion_tokens` and `prompt_tokens`.
        *   `completion_tokens_details (Optional[dict], default: None)`: Provider-specific detailed breakdown of completion tokens, if available.
        *   `prompt_tokens_details (Optional[dict], default: None)`: Provider-specific detailed breakdown of prompt tokens, if available.

## 9. PDF Processing and Extraction

*   ### 9.1. Overview of PDF Processing
    *   9.1.1. Purpose: Crawl4ai provides specialized strategies to handle PDF documents, enabling the fetching of PDF content and subsequent extraction of text, images, and metadata. This allows PDFs to be treated as a primary content source similar to HTML web pages.
    *   9.1.2. Key Components:
        *   `PDFCrawlerStrategy`: For fetching/identifying PDF content.
        *   `PDFContentScrapingStrategy`: For processing PDF content using an underlying PDF processor.
        *   `NaivePDFProcessorStrategy`: The default logic for parsing PDF files.

*   ### 9.2. Class `PDFCrawlerStrategy`
    *   9.2.1. Purpose: An implementation of `AsyncCrawlerStrategy` specifically for handling PDF documents. It doesn't perform typical browser interactions but focuses on fetching PDF content and setting the appropriate response headers to indicate a PDF document, which then allows `PDFContentScrapingStrategy` to process it.
    *   9.2.2. Inheritance: `AsyncCrawlerStrategy` (from `crawl4ai.async_crawler_strategy`)
    *   9.2.3. Initialization (`__init__`):
        *   9.2.3.1. Signature: `PDFCrawlerStrategy(logger: Optional[AsyncLogger] = None)`
        *   9.2.3.2. Parameters:
            *   `logger (Optional[AsyncLogger], default: None)`: An optional logger instance for logging messages.
    *   9.2.4. Key Public Methods:
        *   `crawl(self, url: str, **kwargs) -> AsyncCrawlResponse`:
            *   Description: Fetches the content from the given `url`. If the content is identified as a PDF (either by URL extension or `Content-Type` header for remote URLs), it sets `response_headers={"Content-Type": "application/pdf"}` in the returned `AsyncCrawlResponse`. The `html` field of the response will contain a placeholder message as the actual PDF processing happens in the scraping strategy.
        *   `close(self) -> None`:
            *   Description: Placeholder for cleanup, typically does nothing in this strategy.
        *   `__aenter__(self) -> "PDFCrawlerStrategy"`:
            *   Description: Async context manager entry point.
        *   `__aexit__(self, exc_type, exc_val, exc_tb) -> None`:
            *   Description: Async context manager exit point, calls `close()`.

*   ### 9.3. Class `PDFContentScrapingStrategy`
    *   9.3.1. Purpose: An implementation of `ContentScrapingStrategy` designed to process PDF documents. It uses an underlying `PDFProcessorStrategy` (by default, `NaivePDFProcessorStrategy`) to extract text, images, and metadata from the PDF, then formats this information into a `ScrapingResult`.
    *   9.3.2. Inheritance: `ContentScrapingStrategy` (from `crawl4ai.content_scraping_strategy`)
    *   9.3.3. Initialization (`__init__`):
        *   9.3.3.1. Signature: `PDFContentScrapingStrategy(save_images_locally: bool = False, extract_images: bool = False, image_save_dir: Optional[str] = None, batch_size: int = 4, logger: Optional[AsyncLogger] = None)`
        *   9.3.3.2. Parameters:
            *   `save_images_locally (bool, default: False)`: If `True`, extracted images will be saved to the local filesystem.
            *   `extract_images (bool, default: False)`: If `True`, the strategy will attempt to extract images from the PDF.
            *   `image_save_dir (Optional[str], default: None)`: The directory where extracted images will be saved if `save_images_locally` is `True`. If `None`, a default or temporary directory might be used.
            *   `batch_size (int, default: 4)`: The number of PDF pages to process in parallel by the underlying `NaivePDFProcessorStrategy`.
            *   `logger (Optional[AsyncLogger], default: None)`: An optional logger instance.
    *   9.3.4. Key Attributes:
        *   `pdf_processor (NaivePDFProcessorStrategy)`: An instance of `NaivePDFProcessorStrategy` configured with the provided image and batch settings, used to do the actual PDF parsing.
    *   9.3.5. Key Public Methods:
        *   `scrape(self, url: str, html: str, **params) -> ScrapingResult`:
            *   Description: Takes a `url` (which can be a local file path or a remote HTTP/HTTPS URL pointing to a PDF) and processes it. The `html` parameter is typically a placeholder like "Scraper will handle the real work" as the content comes from the PDF file itself. It downloads remote PDFs to a temporary local file before processing.
            *   Returns: `ScrapingResult` containing the extracted PDF data, including `cleaned_html` (concatenated HTML of pages), `media` (extracted images), `links`, and `metadata`.
        *   `ascrape(self, url: str, html: str, **kwargs) -> ScrapingResult`:
            *   Description: Asynchronous version of `scrape`. Internally calls `scrape` using `asyncio.to_thread`.
    *   9.3.6. Internal Methods (Conceptual):
        *   `_get_pdf_path(self, url: str) -> str`:
            *   Description: If `url` is an HTTP/HTTPS URL, downloads the PDF to a temporary file and returns its path. If `url` starts with "file://", it strips the prefix and returns the local path. Otherwise, assumes `url` is already a local path. Handles download timeouts and errors.

*   ### 9.4. Class `NaivePDFProcessorStrategy`
    *   9.4.1. Purpose: The default implementation of `PDFProcessorStrategy` in Crawl4ai. It uses the PyPDF2 library (and Pillow for image processing) to parse PDF files, extract text content page by page, attempt to extract embedded images, and gather document metadata.
    *   9.4.2. Inheritance: `PDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`)
    *   9.4.3. Dependencies: Requires `PyPDF2` and `Pillow`. These are installed with the `crawl4ai[pdf]` extra.
    *   9.4.4. Initialization (`__init__`):
        *   9.4.4.1. Signature: `NaivePDFProcessorStrategy(image_dpi: int = 144, image_quality: int = 85, extract_images: bool = True, save_images_locally: bool = False, image_save_dir: Optional[Path] = None, batch_size: int = 4)`
        *   9.4.4.2. Parameters:
            *   `image_dpi (int, default: 144)`: DPI used when rendering PDF pages to images (if direct image extraction is not possible or disabled).
            *   `image_quality (int, default: 85)`: Quality setting (1-100) for images saved in lossy formats like JPEG.
            *   `extract_images (bool, default: True)`: If `True`, attempts to extract embedded images directly from the PDF's XObjects.
            *   `save_images_locally (bool, default: False)`: If `True`, extracted images are saved to disk. Otherwise, they are base64 encoded and returned in the `PDFPage.images` data.
            *   `image_save_dir (Optional[Path], default: None)`: If `save_images_locally` is True, this specifies the directory to save images. If `None`, a temporary directory (prefixed `pdf_images_`) is created and used.
            *   `batch_size (int, default: 4)`: The number of pages to process in parallel when using the `process_batch` method.
    *   9.4.5. Key Public Methods:
        *   `process(self, pdf_path: Path) -> PDFProcessResult`:
            *   Description: Processes the PDF specified by `pdf_path` page by page sequentially.
            *   Returns: `PDFProcessResult` containing metadata and a list of `PDFPage` objects.
        *   `process_batch(self, pdf_path: Path) -> PDFProcessResult`:
            *   Description: Processes the PDF specified by `pdf_path` by handling pages in parallel batches using a `ThreadPoolExecutor` with `max_workers` set to `batch_size`.
            *   Returns: `PDFProcessResult` containing metadata and a list of `PDFPage` objects, assembled in the correct page order.
    *   9.4.6. Internal Methods (Conceptual High-Level):
        *   `_process_page(self, page: PyPDF2PageObject, image_dir: Optional[Path]) -> PDFPage`: Extracts text, images (if `extract_images` is True), and links from a single PyPDF2 page object.
        *   `_extract_images(self, page: PyPDF2PageObject, image_dir: Optional[Path]) -> List[Dict]`: Iterates through XObjects on a page, identifies images, decodes them (handling FlateDecode, DCTDecode, CCITTFaxDecode, JPXDecode), and either saves them locally or base64 encodes them.
        *   `_extract_links(self, page: PyPDF2PageObject) -> List[str]`: Extracts URI actions from page annotations to get hyperlinks.
        *   `_extract_metadata(self, pdf_path: Path, reader: PyPDF2PdfReader) -> PDFMetadata`: Reads metadata from the PDF document information dictionary (e.g., /Title, /Author, /CreationDate).

*   ### 9.5. Data Models for PDF Processing
    *   9.5.1. Dataclass `PDFMetadata` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `title (Optional[str], default: None)`
            *   `author (Optional[str], default: None)`
            *   `producer (Optional[str], default: None)`
            *   `created (Optional[datetime], default: None)`
            *   `modified (Optional[datetime], default: None)`
            *   `pages (int, default: 0)`
            *   `encrypted (bool, default: False)`
            *   `file_size (Optional[int], default: None)`
    *   9.5.2. Dataclass `PDFPage` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `page_number (int)`
            *   `raw_text (str, default: "")`
            *   `markdown (str, default: "")`: Markdown representation of the page's text content, processed by `clean_pdf_text`.
            *   `html (str, default: "")`: HTML representation of the page's text content, processed by `clean_pdf_text_to_html`.
            *   `images (List[Dict], default_factory: list)`: List of image dictionaries. Each dictionary contains:
                *   `format (str)`: e.g., "png", "jpeg", "tiff", "jp2", "bin".
                *   `width (int)`
                *   `height (int)`
                *   `color_space (str)`: e.g., "/DeviceRGB", "/DeviceGray".
                *   `bits_per_component (int)`
                *   `path (str, Optional)`: If `save_images_locally` was True, path to the saved image file.
                *   `data (str, Optional)`: If `save_images_locally` was False, base64 encoded image data.
                *   `page (int)`: The page number this image was extracted from.
            *   `links (List[str], default_factory: list)`: List of hyperlink URLs found on the page.
            *   `layout (List[Dict], default_factory: list)`: List of dictionaries representing text layout elements, primarily: `{"type": "text", "text": str, "x": float, "y": float}`.
    *   9.5.3. Dataclass `PDFProcessResult` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `metadata (PDFMetadata)`
            *   `pages (List[PDFPage])`
            *   `processing_time (float, default: 0.0)`: Time in seconds taken to process the PDF.
            *   `version (str, default: "1.1")`: Version of the PDF processor strategy (e.g., "1.1" for current `NaivePDFProcessorStrategy`).

*   ### 9.6. Using PDF Strategies with `AsyncWebCrawler`
    *   9.6.1. Workflow:
        1.  Instantiate `AsyncWebCrawler`. The `crawler_strategy` parameter of `AsyncWebCrawler` should be set to an instance of `PDFCrawlerStrategy` if you intend to primarily crawl PDF URLs or local PDF files directly. If crawling mixed content where PDFs are discovered via links on HTML pages, the default `AsyncPlaywrightCrawlerStrategy` might be used initially, and then a PDF-specific scraping strategy would be applied when a PDF content type is detected.
        2.  In `CrawlerRunConfig`, set the `scraping_strategy` attribute to an instance of `PDFContentScrapingStrategy`. Configure this strategy with desired options like `extract_images`, `save_images_locally`, etc.
        3.  When `crawler.arun(url="path/to/document.pdf", config=run_config)` is called for a PDF URL or local file path:
            *   `PDFCrawlerStrategy` (if used) or the default crawler strategy fetches the file.
            *   `PDFContentScrapingStrategy.scrape()` is invoked. It uses its internal `NaivePDFProcessorStrategy` instance to parse the PDF.
            *   The extracted text, image data, and metadata are populated into the `CrawlResult` object (e.g., `result.markdown`, `result.media["images"]`, `result.metadata`).
    *   9.6.2. Example Snippet:
        ```python
        import asyncio
        from pathlib import Path
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, PDFCrawlerStrategy
        from crawl4ai.content_scraping_strategy import PDFContentScrapingStrategy
        from crawl4ai.processors.pdf import PDFContentScrapingStrategy # Corrected import path

        async def main():
            # Setup for PDF processing
            pdf_crawler_strategy = PDFCrawlerStrategy() # Use if directly targeting PDF URLs
            pdf_scraping_strategy = PDFContentScrapingStrategy(
                extract_images=True,
                save_images_locally=True,
                image_save_dir="./pdf_images_output" # Ensure this directory exists
            )
            Path("./pdf_images_output").mkdir(parents=True, exist_ok=True)

            # If crawling a website that links to PDFs, you might use the default crawler strategy
            # and rely on content-type detection to switch to PDFContentScrapingStrategy if needed.
            # For direct PDF URL:
            async with AsyncWebCrawler(crawler_strategy=pdf_crawler_strategy) as crawler:
                run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_strategy)
                # Example PDF URL (replace with a real one for testing)
                pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
                result = await crawler.arun(url=pdf_url, config=run_config)

                if result.success:
                    print(f"Successfully processed PDF: {result.url}")
                    if result.markdown:
                         print(f"Markdown content (first 500 chars): {result.markdown.raw_markdown[:500]}")
                    if result.media and result.media.images:
                        print(f"Extracted {len(result.media.images)} images.")
                        for img in result.media.images:
                            print(f"  - Image source/path: {img.src or img.path}, Page: {img.page}")
                    if result.metadata:
                        print(f"PDF Metadata: {result.metadata}")
                else:
                    print(f"Failed to process PDF: {result.url}, Error: {result.error_message}")

        # if __name__ == "__main__":
        #     asyncio.run(main())
        ```
```

---


## Data Extraction - Reasoning
Source: crawl4ai_extraction_reasoning_content.llm.md

Okay, I'm ready to generate the "Reasoning & Problem-Solving Framework" document for the `extraction` component, adhering strictly to the outline you provided and the principles for a "reasoning" document.

I will use the information from the previously shared code snippets and documentation to create illustrative examples, explain design rationale, and provide actionable advice.

Here is the `llm_reasoning_extraction.md` document:

```markdown
# Detailed Outline for crawl4ai - extraction Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_extraction.md`
**Library Version Context:** 0.6.0+ (based on provided code)
**Outline Generation Date:** 2024-05-24
---

# Mastering Data Extraction with Crawl4AI

## 1. Introduction: Why Structured Data Extraction Matters in Web Crawling
    * 1.1. The Value of Going Beyond Raw HTML: Turning Web Content into Actionable Data
        Web pages, in their raw HTML form, are designed for human consumption. While Crawl4AI excels at converting HTML to clean Markdown for LLMs, often the goal is to extract specific, structured pieces of information. This could be product prices, article headlines, author names, contact details, or any other data points that can be organized into a predictable format. Structured data is more readily usable for databases, APIs, analytics, training machine learning models, or feeding into other automated processes. Simply having the full HTML or Markdown isn't enough when you need to operate on discrete data fields.

    * 1.2. Common Challenges in Web Data Extraction (Dynamic content, varied structures, anti-scraping)
        Extracting data from the web isn't always straightforward. Common hurdles include:
        *   **Varied HTML Structures:** Websites change layouts, and even within a single site, different page types can have vastly different structures. A CSS selector that works today might break tomorrow.
        *   **Dynamic Content:** Much of the web's content is loaded via JavaScript after the initial HTML page. Extractors need to handle this, either by executing JS (as Crawl4AI's browser-based crawlers do) or by finding data in embedded JSON within `<script>` tags.
        *   **Anti-Scraping Measures:** Websites may employ techniques to deter or block automated scraping, requiring more sophisticated approaches.
        *   **Unstructured Data:** Sometimes, the data isn't neatly tagged. It might be buried in free-form text, requiring natural language understanding to identify and extract.
        *   **Scalability and Maintenance:** Writing and maintaining custom parsers for many sites can be a significant engineering effort.

    * 1.3. Crawl4AI's Approach: A Flexible, Strategy-Based Extraction Framework
        Crawl4AI tackles these challenges by offering a flexible and powerful extraction framework built around the concept of "strategies." This allows you to choose the best tool for the job, whether it's precise rule-based extraction or intelligent LLM-powered parsing.
        *   **`ExtractionStrategy` Interface:** This is the core. It defines a common contract for how extraction should happen. Crawl4AI provides several built-in strategies (CSS-based, XPath-based, Regex-based, LLM-based), and you can even implement your own for highly specialized needs. This promotes modularity – you can swap out extraction logic without changing your core crawling code.
        *   **`ChunkingStrategy` Interface:** Specifically for LLM-based extraction, this interface helps prepare content by breaking it into manageable pieces that fit within an LLM's context window. This is crucial for both performance and accuracy when dealing with large documents.
        *   **Balancing Rule-Based and LLM-Powered Extraction:** Crawl4AI doesn't force you into one paradigm. You can use fast and efficient CSS selectors for well-structured sites and then leverage the power of LLMs for complex, unstructured data, or even combine them in hybrid approaches. This flexibility is key to building robust and adaptable web data extraction pipelines.

## 2. Core Concepts in Crawl4AI Extraction
    * 2.1. The `ExtractionStrategy` Interface: Your Key to Custom Extraction
        *   2.1.1. Purpose: Why an interface? Promoting modularity and extensibility.
            The `ExtractionStrategy` interface (defined in `crawl4ai/extraction_strategy.py`) is a fundamental design choice in Crawl4AI. It establishes a common contract for all extraction methods. The primary benefit is **modularity**: your main crawling logic doesn't need to know the specifics of *how* data is extracted. It simply invokes the strategy, and the strategy handles the details. This makes your code cleaner and more maintainable.
            Furthermore, it promotes **extensibility**: if the built-in strategies don't fit your exact needs (e.g., you're dealing with a proprietary data format or a very unique web structure), you can create your own class that implements the `ExtractionStrategy` interface and plug it directly into Crawl4AI.

        *   2.1.2. Key Methods to Understand (Conceptual): `extract()` and `run()`.
            While you typically won't call these directly if using built-in strategies (Crawl4AI handles it), understanding their roles is important if you plan to create custom strategies:
            *   `extract(url: str, html_content: str, *args, **kwargs) -> List[Dict[str, Any]]`: This is the core method that every concrete strategy must implement. It takes the URL and HTML content (or pre-processed content like Markdown, depending on the `input_format` of the strategy) and returns a list of dictionaries, where each dictionary represents an extracted item.
            *   `run(url: str, sections: List[str], *args, **kwargs) -> List[Dict[str, Any]]`: This method is often used for strategies that process content in chunks (like `LLMExtractionStrategy`). It takes a list of content `sections` and typically calls `extract()` for each section, then aggregates the results. For simpler strategies that operate on the whole content at once, `run` might just call `extract` with the joined sections.

        *   2.1.3. When Would You Implement Your Own `ExtractionStrategy`?
            You'd consider creating a custom `ExtractionStrategy` in scenarios like:
            *   **Highly Specialized Data Sources:** If you're extracting data from a non-standard format (e.g., custom XML, binary files, or a very idiosyncratic HTML structure not well-suited for CSS/XPath/Regex).
            *   **Integrating Proprietary Extraction Logic:** If your organization has existing, specialized parsing libraries or algorithms you want to use within the Crawl4AI framework.
            *   **Advanced Performance Optimizations:** For extremely high-volume scraping of a specific site, you might develop a hyper-optimized parser that bypasses more general tools.
            *   **Unique Pre-processing or Post-processing:** If your extraction requires complex data transformations or enrichments beyond what the built-in strategies offer.

    * 2.2. The `ChunkingStrategy` Interface: Preparing Content for LLMs
        *   2.2.1. Why Chunking is Crucial for LLM-Based Extraction
            Large Language Models (LLMs) have a "context window" – a limit on the amount of text they can process at once (e.g., 4096, 8192, or even 128k+ tokens). If you feed an entire long webpage directly to an LLM for extraction:
            *   **Context Overflow:** The content might exceed the LLM's limit, leading to truncation and loss of information, or outright errors.
            *   **Reduced Accuracy:** Even if it fits, an LLM might struggle to find specific details in a very long, noisy document. Its attention can get diluted.
            *   **Higher Cost & Latency:** Processing more tokens means higher API costs (for paid models) and longer response times.
            Chunking addresses this by breaking down the input content into smaller, more focused segments, each of which can be processed by the LLM more effectively.

        *   2.2.2. How Chunking Strategies Work in Crawl4AI
            A `ChunkingStrategy` (defined in `crawl4ai/chunking_strategy.py`) is responsible for taking a single block of text (e.g., the Markdown content of a page) and dividing it into a list of smaller strings (chunks).
            *   The primary method is `chunk(document: str) -> List[str]`.
            *   The `LLMExtractionStrategy` then iterates over these chunks, sending each one (or a batch of them, depending on its internal logic) to the LLM for extraction. The results from each chunk are then typically aggregated.

        *   2.2.3. Overview of Built-in Chunking Strategies
            Crawl4AI provides a couple of ready-to-use chunking strategies:
            *   **`RegexChunking` (default for `LLMExtractionStrategy`):** This strategy (from `crawl4ai/chunking_strategy.py`) uses regular expressions to split text. By default, it might split by paragraphs or other common delimiters. It aims to create semantically meaningful chunks. This is often a good general-purpose choice.
                *   *When to use:* Good for text-heavy documents where paragraph or section breaks are meaningful.
            *   **`IdentityChunking`:** This strategy (from `crawl4ai/chunking_strategy.py`) doesn't actually do any chunking; it returns the input document as a single chunk.
                *   *When to use:*
                    *   When your input documents are already small enough to fit the LLM's context window.
                    *   When you have pre-processed your content into chunks *before* passing it to `LLMExtractionStrategy`.
                    *   When the LLM you're using has a very large context window and performs well on full documents for your specific task.

        *   2.2.4. When to Choose or Implement a Custom `ChunkingStrategy`.
            While the built-in chunkers are useful, you might need a custom `ChunkingStrategy` if:
            *   **Domain-Specific Document Structures:** Your content has unique structural elements that `RegexChunking` doesn't handle well (e.g., legal documents with numbered clauses, scripts with dialogue/scene breaks, log files).
            *   **Semantic Chunking Needs:** You require more sophisticated chunking based on semantic meaning rather than just regex patterns (though this can become complex and might involve NLP techniques within your custom chunker).
            *   **Fixed-Size Overlapping Chunks:** You want to implement a sliding window approach with precise control over chunk size and overlap, which might be beneficial for certain types_of information retrieval.
            *   **Table or List-Aware Chunking:** You need to ensure that tables or lists are not awkwardly split across chunks.

    * 2.3. Schema Definition: The Blueprint for Your Extracted Data
        *   2.3.1. Why a Well-Defined Schema is Essential
            A schema acts as a contract for your data. It defines:
            *   What pieces of information you expect to extract (the field names).
            *   The data type of each piece of information (e.g., string, integer, boolean, list, nested object).
            *   How to find each piece of information (e.g., CSS selector, XPath, or implied for LLM).
            Benefits include:
            *   **Consistency:** Ensures that extracted data always has the same structure, making it easier to process downstream.
            *   **Reliability:** Helps catch errors if a website's structure changes and a selector no longer works, or if an LLM fails to extract a required field.
            *   **Guidance:** For rule-based extractors, it provides the direct rules. For LLM-based extractors, it informs the LLM about the desired output structure, significantly improving the quality and predictability of results.
            *   **Validation:** Pydantic models, used with LLMs, offer automatic data validation.

        *   2.3.2. Defining Schemas for CSS/XPath/LXML Strategies (Dictionary-based)
            For strategies like `JsonCssExtractionStrategy`, `JsonXPathExtractionStrategy`, and `JsonLxmlExtractionStrategy`, the schema is a Python dictionary.
            *   **Structure:**
                ```python
                schema = {
                    "name": "MyExtractorName", # Optional: A name for your schema
                    "baseSelector": "div.product-item", # CSS selector for repeating items (e.g., products on a list page)
                    "fields": [
                        {
                            "name": "product_name",      # Name of the field in the output
                            "selector": "h2.product-title", # CSS/XPath selector relative to baseSelector (or page if no baseSelector)
                            "type": "text"             # "text", "attribute", "html", "nested", "list"
                        },
                        {
                            "name": "product_link",
                            "selector": "a.product-link",
                            "type": "attribute",
                            "attribute": "href"        # Name of the HTML attribute to extract (e.g., 'href' for links)
                        },
                        # ... more fields ...
                    ]
                }
                ```
            *   **Key Fields:**
                *   `baseSelector`: (Optional) If you're extracting a list of similar items (e.g., multiple products, articles), this selector targets the container element for each item. All field selectors will then be relative to this base element. If omitted, field selectors are relative to the whole document.
                *   `fields`: A list of dictionaries, each defining a field to extract.
                    *   `name`: The key for this field in the output JSON.
                    *   `selector`: The CSS selector or XPath expression to locate the data.
                    *   `type`:
                        *   `"text"`: Extracts the text content of the selected element.
                        *   `"attribute"`: Extracts the value of a specified HTML attribute (requires an additional `"attribute": "attr_name"` key).
                        *   `"html"`: Extracts the inner HTML of the selected element.
                        *   `"nested"`: Allows defining a sub-schema for extracting nested structured data (requires an additional `"fields": [...]` key, similar to the top-level fields).
                        *   `"list"`: Indicates that the selector is expected to return multiple elements, and the extraction logic (defined by sub-fields) should be applied to each. Often used with a nested `fields` definition.
            *   **Tips for Designing Dictionary-Based Schemas:**
                *   Be as specific as possible with your selectors to avoid ambiguity.
                *   Start with a simple schema and iteratively add more fields.
                *   Test your selectors in your browser's developer tools first.
                *   Use `baseSelector` for lists to keep field selectors concise and maintainable.
            *   **Example: Schema for extracting blog post titles and authors:**
                ```python
                blog_post_schema = {
                    "name": "BlogPostExtractor",
                    "baseSelector": "article.post",
                    "fields": [
                        {"name": "title", "selector": "h1.entry-title", "type": "text"},
                        {"name": "author", "selector": "span.author-name", "type": "text"},
                        {"name": "publication_date", "selector": "time.published-date", "type": "attribute", "attribute": "datetime"}
                    ]
                }
                ```

        *   2.3.3. Defining Schemas for `LLMExtractionStrategy` (Pydantic Models)
            When using `LLMExtractionStrategy` with `extraction_type="schema"` (the default), you provide a Pydantic model as the schema.
            *   **Advantages of Pydantic:**
                *   **Type Hints:** Clearly define the expected data type for each field.
                *   **Validation:** Pydantic automatically validates that the data extracted by the LLM conforms to your model's types and constraints. If not, it raises an error.
                *   **IDE Support:** Excellent autocompletion and type checking in modern IDEs.
                *   **Serialization:** Easy conversion to and from JSON.
            *   **How Pydantic Models Guide the LLM:** Crawl4AI internally converts your Pydantic model into a JSON schema representation, which is then included in the prompt to the LLM. This tells the LLM the exact structure and field names it should use in its JSON output.
            *   **Example: Pydantic model for product information:**
                ```python
                from pydantic import BaseModel, HttpUrl
                from typing import Optional, List

                class ProductInfo(BaseModel):
                    product_name: str
                    price: Optional[float]
                    description: str
                    image_urls: List[HttpUrl] = []
                    features: Optional[List[str]]
                ```
                When this model is used, the LLM will be instructed to return JSON objects that look like:
                ```json
                {
                  "product_name": "Awesome Laptop",
                  "price": 1299.99,
                  "description": "A very fast and light laptop.",
                  "image_urls": ["https://example.com/image1.jpg"],
                  "features": ["16GB RAM", "512GB SSD"]
                }
                ```

        *   2.3.4. Best Practices for Schema Design Across Strategy Types.
            *   **Be Specific with Field Names:** Use clear, descriptive names that reflect the data.
            *   **Start Simple:** Begin with a few key fields and expand as needed.
            *   **Handle Optional Data:** For fields that might not always be present, define them as optional in your Pydantic model (e.g., `Optional[str]`) or ensure your non-LLM logic handles missing elements gracefully (e.g., by providing default values or allowing `None`).
            *   **Consider Data Types:** Choose appropriate types (string, number, boolean, list, nested object) to ensure data integrity.
            *   **Test Iteratively:** Regularly test your schemas with real web content to catch issues early.

## 3. Non-LLM Based Extraction Strategies: Precision and Speed
    * 3.1. When to Choose Non-LLM Strategies
        Non-LLM (or rule-based) strategies are excellent choices when:
        *   **Website Structure is Consistent:** The target website has a stable and predictable HTML structure. Changes are infrequent.
        *   **Performance is Key:** These strategies are generally much faster and less resource-intensive than LLM-based approaches as they don't involve API calls to external services or loading large models.
        *   **Cost is a Major Factor:** Non-LLM strategies have no per-extraction operational cost beyond your own compute resources.
        *   **Data Points are Simple and Directly Targetable:** You need to extract clearly identifiable pieces of text, attributes, or simple lists.
        *   **You Have Expertise in CSS Selectors or XPath:** If you or your team are comfortable writing and maintaining these selectors.
        *   **No Semantic Interpretation Needed:** The data can be located purely by its position or tags in the HTML, without needing to understand the meaning of the surrounding text.

    * 3.2. Mastering `JsonCssExtractionStrategy`
        *   3.2.1. Understanding Its Strengths: Leveraging CSS Selectors
            `JsonCssExtractionStrategy` is often the first choice for non-LLM extraction due to the widespread familiarity with CSS selectors.
            *   **Strengths:**
                *   Relatively easy to learn and write.
                *   Well-supported by browsers' developer tools for testing.
                *   Efficient for most common extraction tasks.
            *   **Underlying Library:** Crawl4AI typically uses BeautifulSoup4 or LXML for parsing HTML and applying CSS selectors, providing robust and performant parsing.

        *   3.2.2. Workflow: Extracting Data with CSS
            *   **Step 1: Analyzing the Target HTML Structure:**
                *   Use your browser's developer tools (e.g., "Inspect Element") to examine the HTML of the page you want to scrape.
                *   Identify the HTML tags, classes, and IDs that uniquely contain the data you need.
                *   Example: If you want to extract an article's title, you might find it's always within an `<h1>` tag with class `article-title`.
            *   **Step 2: Crafting your Dictionary-Based Schema with CSS Selectors:**
                *   Define your schema as a Python dictionary, as described in section 2.3.2.
                *   Fill in the `selector` for each field with the appropriate CSS selector.
                ```python
                article_schema = {
                    "baseSelector": "article.post", # Target each article
                    "fields": [
                        {"name": "title", "selector": "h1.entry-title", "type": "text"},
                        {"name": "author_link", "selector": "a.author-url", "type": "attribute", "attribute": "href"}
                    ]
                }
                ```
            *   **Step 3: Configuring `CrawlerRunConfig` to use `JsonCssExtractionStrategy`:**
                ```python
                from crawl4ai import CrawlerRunConfig
                from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

                extraction_strategy = JsonCssExtractionStrategy(schema=article_schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                ```
            *   **Step 4: Interpreting the Results:**
                *   The `result.extracted_content` will be a JSON string containing a list of dictionaries, where each dictionary matches your schema.
                ```python
                import json
                # Assuming 'result' is the output from crawler.arun()
                if result.extracted_content:
                    data = json.loads(result.extracted_content)
                    for item in data:
                        print(f"Title: {item.get('title')}, Author Link: {item.get('author_link')}")
                ```

        *   3.2.3. Handling Nested Data Structures
            You can extract nested data by defining a field with `type: "nested"` and providing another `fields` list within it.
            *   **How to define:** The `selector` for the nested field targets the container of the nested data. The sub-fields' selectors are then relative to this nested container.
            *   **Example: Extracting comments and their authors:**
                ```python
                comment_schema = {
                    "baseSelector": "div.comment-thread",
                    "fields": [
                        {"name": "comment_id", "selector": "div.comment", "type": "attribute", "attribute": "data-comment-id"},
                        {
                            "name": "main_comment",
                            "selector": "div.comment-body", # Selector for the main comment container
                            "type": "nested",
                            "fields": [
                                {"name": "author", "selector": "span.comment-author", "type": "text"},
                                {"name": "text", "selector": "p.comment-text", "type": "text"}
                            ]
                        },
                        {
                            "name": "replies",
                            "selector": "div.reply", # Selector for each reply
                            "type": "list", # Indicates multiple replies
                            "fields": [ # Schema for each reply item
                                {"name": "reply_author", "selector": "span.reply-author", "type": "text"},
                                {"name": "reply_text", "selector": "p.reply-text", "type": "text"}
                            ]
                        }
                    ]
                }
                ```

        *   3.2.4. Extracting Lists of Items
            The `baseSelector` is key for extracting lists.
            *   **`baseSelector`:** Targets each individual item in the list (e.g., each `<li>` in a `<ul>`, each `div.product-card`).
            *   **Relative Field Selectors:** All selectors within the `fields` list are then evaluated *relative* to each element matched by `baseSelector`.
            *   **Example: Extracting a list of products from a category page:**
                ```python
                product_list_schema = {
                    "name": "ProductList",
                    "baseSelector": "div.product-listing div.product-item-container", # Each product card
                    "fields": [
                        {"name": "product_name", "selector": "h3.product-name a", "type": "text"},
                        {"name": "price", "selector": "span.price", "type": "text"},
                        {"name": "url", "selector": "h3.product-name a", "type": "attribute", "attribute": "href"}
                    ]
                }
                ```
                This would produce a list of product dictionaries.

        *   3.2.5. Code Example: Extracting News Headlines and Links from Hacker News (Illustrative)
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
            from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
            from crawl4ai.cache_manager import CacheMode

            async def extract_hn_news():
                hn_schema = {
                    "name": "HackerNewsFrontPage",
                    "baseSelector": "tr.athing", # Each story row in Hacker News
                    "fields": [
                        {
                            "name": "rank",
                            "selector": "span.rank",
                            "type": "text"
                        },
                        {
                            "name": "title",
                            "selector": "span.titleline > a", # Get the first 'a' tag within titleline
                            "type": "text"
                        },
                        {
                            "name": "url",
                            "selector": "span.titleline > a",
                            "type": "attribute",
                            "attribute": "href"
                        },
                        # Example for next row (subtext) data - shows using a more complex relative selector
                        {
                            "name": "points",
                            "selector": "xpath=./following-sibling::tr[1]/td[@class='subtext']/span[@class='score']",
                            "type": "text" # Note: Using XPath within CSS strategy for advanced relative selection
                                           # This is a conceptual example; pure CSS might be trickier for direct sibling access.
                                           # A more common CSS approach would be to have a slightly broader baseSelector
                                           # or separate extraction steps if nesting is too complex for pure CSS.
                        }
                    ]
                }

                extraction_strategy = JsonCssExtractionStrategy(schema=hn_schema)
                browser_config = BrowserConfig(headless=True)
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS # For fresh data in this example
                )

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    result = await crawler.arun(
                        url="https://news.ycombinator.com/",
                        config=run_config
                    )

                if result.success and result.extracted_content:
                    articles = json.loads(result.extracted_content)
                    print(f"Extracted {len(articles)} articles from Hacker News:")
                    for i, article in enumerate(articles[:5]): # Print first 5
                        print(f"  {i+1}. {article.get('title')} ({article.get('points', 'N/A points')}) - {article.get('url')}")
                else:
                    print(f"Failed to extract articles: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_hn_news())
            ```
            *Self-correction during thought process: The original `points` selector was a bit too complex for a pure CSS example within `JsonCssExtractionStrategy`. While some libraries might allow mixing, it's better to illustrate clear CSS or mention that for such relative sibling traversals, XPath might be more direct, or the schema/baseSelector might need restructuring.*

        *   3.2.6. Best Practices for Writing Robust CSS Selectors.
            *   **Prefer IDs if Stable:** `#unique-id` is usually the most robust if available and unique.
            *   **Use Specific but Not Overly Specific Classes:** `.meaningful-class` is good. Avoid overly long chains like `div.container > div.row > div.col-md-8 > article.post > h1` if `h1.post-title` is unique enough.
            *   **Attribute Selectors:** `input[name="email"]` can be very precise.
            *   **Avoid Relying on Order (unless necessary):** `:nth-child()` can be brittle if the page structure changes slightly. Use it sparingly.
            *   **Test Thoroughly:** Use browser dev tools to validate your selectors on various pages of the target site.

        *   3.2.7. Troubleshooting: Common Issues and Solutions
            *   **Selector Returning `None` or Empty List:**
                *   *Cause:* Selector is incorrect, element doesn't exist, or content is loaded dynamically *after* initial HTML.
                *   *Solution:* Double-check selector in dev tools. For dynamic content, ensure Crawl4AI's browser is rendering JS (default) or use `wait_for` in `CrawlerRunConfig`.
            *   **Handling Dynamic Class Names:**
                *   *Cause:* Sites using CSS-in-JS or frameworks might generate dynamic class names (e.g., `_header_a83hf8`).
                *   *Solution:* Look for stable parent elements or use attribute selectors that target parts of class names (e.g., `div[class*="header_"]`), or rely on structural selectors (e.g., `article > h1`). This is where XPath or LLM strategies might be more robust.
            *   **Extracting Incorrect Data:**
                *   *Cause:* Selector is too broad and matches multiple elements.
                *   *Solution:* Make your selector more specific. Use direct child `>` or adjacent sibling `+` combinators if appropriate.

    * 3.3. Leveraging `JsonXPathExtractionStrategy`
        *   3.3.1. When XPath Shines: Complex Selections and Navigating the DOM
            XPath (XML Path Language) is a powerful query language for selecting nodes from an XML or HTML document. It excels where CSS selectors might fall short:
            *   **Complex Relationships:** Selecting elements based on their ancestors, siblings, or preceding/following elements (e.g., "find the `div` that follows an `h2` with text 'Price'").
            *   **Text Content Matching:** Selecting elements based on their text content (e.g., `//button[contains(text(), 'Add to Cart')]`).
            *   **Navigating Up the DOM:** Easily selecting parent or ancestor elements.
            *   **Using Functions:** XPath has built-in functions for string manipulation, counting, etc.

        *   3.3.2. Key Differences from CSS Strategy (Syntax, capabilities).
            *   **Syntax:** XPath uses a path-like syntax (e.g., `/html/body/div[1]/h1`) whereas CSS uses selectors like `div.my-class > h1`.
            *   **Capabilities:** XPath is generally more powerful for traversing the DOM in complex ways. CSS is often simpler for common class/ID/tag selections.
            *   **Performance:** For simple selections, CSS can sometimes be faster. For complex traversals, a well-written XPath might be more efficient than a convoluted CSS equivalent. Crawl4AI uses LXML for XPath, which is highly performant.

        *   3.3.3. Workflow: Similar to CSS, but with XPath expressions.
            The workflow is identical to `JsonCssExtractionStrategy`, except your schema's `selector` fields will contain XPath expressions.
            *   **Step 1: Analyzing HTML:** Use browser developer tools. Many browsers allow you to right-click an element and "Copy XPath."
            *   **Step 2: Crafting your Dictionary-Based Schema with XPath:**
                ```python
                xpath_schema = {
                    "baseSelector": "//article[@class='blog-entry']", # XPath for each article
                    "fields": [
                        {"name": "title", "selector": ".//h1[contains(@class, 'title')]/text()", "type": "text"},
                        {"name": "author_url", "selector": ".//a[contains(@class, 'author-profile')]/@href", "type": "attribute"}
                        # Note: type "attribute" for XPath will get the attribute value if selector ends with /@attr
                        # type "text" will get text content. If selector selects an element, text() can be appended.
                    ]
                }
                ```
                *Important for XPath `type` handling:*
                *   If your XPath selector directly targets an attribute (e.g., `//a/@href`), `type: "attribute"` is redundant but harmless; the attribute value is returned.
                *   If your XPath selector targets an element and you want its text, use `type: "text"` (or append `/text()` to your XPath).
                *   If your XPath targets an element and you want an attribute of *that* element, you'd use `type: "attribute"` and specify the `attribute` key, e.g., `{"selector": "//img", "type": "attribute", "attribute": "src"}`.

            *   **Step 3: Configuration in `CrawlerRunConfig`:**
                ```python
                from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy
                extraction_strategy = JsonXPathExtractionStrategy(schema=xpath_schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                ```

        *   3.3.4. Code Example: Extracting Data Using XPath Functions (e.g., `contains()`, `text()`)
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
            from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy
            from crawl4ai.cache_manager import CacheMode

            async def extract_with_xpath():
                # Example HTML content
                sample_html = """
                <html><body>
                    <div class="product">
                        <h2>Product A</h2>
                        <span class="price">Price: $19.99</span>
                        <a href="/product/a" class="details-link">View Details</a>
                    </div>
                    <div class="product">
                        <h2>Product B</h2>
                        <span class="price">Price: $29.99</span>
                        <a href="/product/b" class="details-link">More Info</a>
                    </div>
                </body></html>
                """

                product_schema_xpath = {
                    "name": "ProductXPathExtractor",
                    "baseSelector": "//div[@class='product']",
                    "fields": [
                        {"name": "name", "selector": ".//h2/text()", "type": "text"},
                        # Extracts text after "Price: "
                        {"name": "price_value", "selector": "substring-after(.//span[contains(@class,'price')]/text(), 'Price: $')", "type": "text"},
                        {"name": "details_url", "selector": ".//a[contains(@class,'details-link') or contains(text(),'More Info')]/@href", "type": "attribute"}
                    ]
                }
                extraction_strategy = JsonXPathExtractionStrategy(schema=product_schema_xpath)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy, cache_mode=CacheMode.BYPASS)

                async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
                    # Using raw HTML input for this example
                    result = await crawler.arun(url=f"raw://{sample_html}", config=run_config)

                if result.success and result.extracted_content:
                    products = json.loads(result.extracted_content)
                    print("Extracted Products using XPath:")
                    for product in products:
                        print(product)
                else:
                    print(f"XPath extraction failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_with_xpath())
            ```

        *   3.3.5. Tips for Effective XPath Usage.
            *   **Start with `.` for relative paths:** Within a `baseSelector`, field selectors should usually start with `./` to be relative to the current base element.
            *   **Use `text()` to get text content:** `//div/text()` gets the direct text children. `//div//text()` gets all text within the div.
            *   **Select attributes with `/@attribute_name`:** `//img/@src`.
            *   **Leverage functions:** `contains()`, `starts-with()`, `substring-after()`, `normalize-space()` are very useful.
            *   **Be mindful of namespaces** if working with XML-heavy HTML or actual XML.

    * 3.4. Understanding `JsonLxmlExtractionStrategy`
        The `JsonLxmlExtractionStrategy` is essentially a specialized version of `JsonCssExtractionStrategy` that explicitly uses the LXML library for parsing and CSS selection.
        *   3.4.1. Potential Performance Gains: When to consider it.
            LXML is known for its speed. For very large HTML documents or high-throughput scraping scenarios where parsing speed is a bottleneck, `JsonLxmlExtractionStrategy` *might* offer better performance than the default BeautifulSoup-backed CSS selector engine (though BeautifulSoup itself can use LXML as a parser). The actual difference can vary.
        *   3.4.2. Usage and Configuration: Similarities and differences with `JsonCssExtractionStrategy`.
            Usage is identical to `JsonCssExtractionStrategy`. You provide the same dictionary-based schema with CSS selectors. Crawl4AI handles the backend difference.
            ```python
            from crawl4ai.extraction_strategy import JsonLxmlExtractionStrategy # Import this
            
            # Schema is the same as for JsonCssExtractionStrategy
            my_schema = { ... } 
            extraction_strategy = JsonLxmlExtractionStrategy(schema=my_schema)
            run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
            ```
        *   3.4.3. When to benchmark against `JsonCssExtractionStrategy`.
            If you suspect CSS selection is a performance bottleneck in your Crawl4AI application, and you're processing a large volume of pages or very large pages, it's worth benchmarking `JsonLxmlExtractionStrategy` against the default `JsonCssExtractionStrategy` to see if it provides a noticeable speedup in your specific environment and use case.

    * 3.5. Precise Targeting with `RegexExtractionStrategy`
        *   3.5.1. The Power of Regular Expressions: When Are They the Right Tool?
            Regular expressions are ideal when:
            *   **Data is in Unstructured or Semi-Structured Text:** The information isn't neatly tagged with specific HTML elements or classes (e.g., extracting an email address from a paragraph of text).
            *   **Targeting Specific Patterns:** You need to find data that conforms to a known pattern, like email addresses, phone numbers, dates, URLs, postal codes, UUIDs, product SKUs, etc.
            *   **HTML Structure is Unreliable:** If the HTML tags around the data change frequently, but the data itself has a consistent textual pattern.
            *   **Fallback or Augmentation:** Can be used to extract data that CSS/XPath selectors miss, or to clean/validate data extracted by other means.

        *   3.5.2. Utilizing Built-in Patterns
            `RegexExtractionStrategy` (from `crawl4ai.extraction_strategy`) comes with a handy `BuiltInPatterns` IntFlag enum. This allows you to easily enable common extraction patterns without writing the regex yourself.
            *   **Overview:** Refer to `RegexExtractionStrategy._B` (or `RegexExtractionStrategy.BuiltInPatterns` if aliased publicly) for the available flags like `EMAIL`, `PHONE_US`, `URL`, `IPV4`, `UUID`, `DATE_ISO`, `CURRENCY`, etc. Each flag corresponds to a pre-defined, tested regex pattern.
            *   **How to use:** You pass the bitwise OR of the desired patterns to the `pattern` argument of the `RegexExtractionStrategy` constructor.
            *   **Code Example: Extracting all email addresses and US phone numbers from a webpage's text:**
                ```python
                import asyncio
                import json
                from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
                from crawl4ai.extraction_strategy import RegexExtractionStrategy

                async def extract_contact_info():
                    # Combine built-in patterns
                    patterns_to_use = RegexExtractionStrategy.BuiltInPatterns.EMAIL | \
                                      RegexExtractionStrategy.BuiltInPatterns.PHONE_US
                    
                    extraction_strategy = RegexExtractionStrategy(pattern=patterns_to_use)
                    
                    # This strategy works best on plain text, so use 'markdown' or 'text' input_format
                    # if using with the standard crawler flow, or pass plain text directly.
                    run_config = CrawlerRunConfig(
                        extraction_strategy=extraction_strategy,
                        # input_format='text' # Alternative: let the strategy handle HTML to text
                    )

                    sample_text_content = """
                    Contact us at support@example.com or call (800) 555-1212.
                    Our sales team can be reached at sales@example.com.
                    For urgent matters, dial 1-800-555-1234.
                    Our website is https://example.com.
                    """

                    async with AsyncWebCrawler() as crawler:
                        # Here, we're directly using the 'extract' method for simplicity with raw text
                        # In a full crawl, you'd use crawler.arun() with the run_config
                        extracted_data = extraction_strategy.extract(
                            url="raw://text_content", # Dummy URL for raw content
                            html_content=sample_text_content # Provide text directly
                        )
                    
                    print("Extracted Contact Info:")
                    for item in extracted_data:
                        print(f"  Label: {item['label']}, Value: {item['value']}, Span: {item['span']}")

                if __name__ == "__main__":
                    asyncio.run(extract_contact_info())
                ```
                **Output structure for `RegexExtractionStrategy`:**
                Each extracted item is a dictionary:
                `{"url": "source_url", "label": "pattern_label", "value": "matched_string", "span": [start_index, end_index]}`

        *   3.5.3. Defining and Using Custom Regex Patterns
            If built-in patterns aren't sufficient, you can provide your own.
            *   **Passing a Dictionary:** Supply a dictionary where keys are labels (strings) for your patterns, and values are the regex pattern strings.
            *   **Tips for Writing Regex:**
                *   Use non-capturing groups `(?:...)` if you don't need to capture a part of the match.
                *   Be mindful of greediness (e.g., use `*?` or `+?` for non-greedy matches).
                *   Test your regex thoroughly with tools like regex101.com.
                *   Remember that regex patterns are raw strings in Python (e.g., `r"\b\d{5}\b"`).
            *   **Code Example: Extracting custom product SKUs (e.g., SKU-XXXX-YYYY):**
                ```python
                import asyncio
                import json
                from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
                from crawl4ai.extraction_strategy import RegexExtractionStrategy

                async def extract_skus():
                    custom_patterns = {
                        "product_sku": r"SKU-\d{4}-[A-Z]{4}"
                    }
                    extraction_strategy = RegexExtractionStrategy(custom=custom_patterns)
                    run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)

                    sample_text = "Product Alpha SKU-1234-ABCD and Product Beta SKU-5678-EFGH."
                    
                    # Direct usage for simplicity
                    extracted_data = extraction_strategy.extract(url="raw://text", html_content=sample_text)

                    print("Extracted SKUs:")
                    for item in extracted_data:
                        print(item)
                
                if __name__ == "__main__":
                    asyncio.run(extract_skus())
                ```

        *   3.5.4. Leveraging `generate_pattern()` for Dynamic Regex Creation
            The static method `RegexExtractionStrategy.generate_pattern(examples: List[str], labels: List[str] = None, llm_config: LLMConfig = None, **kwargs) -> str` (or Dict[str, str] if labels are provided) is a powerful utility that uses an LLM to generate a regex pattern for you based on examples.
            *   **How it Works:** You provide a list of example strings that you want to match. Optionally, you can provide corresponding labels if you want to generate multiple patterns for different types of data. The method then queries an LLM (configurable via `llm_config`) to infer a regex pattern that would capture those examples.
            *   **Use Cases:**
                *   You have a clear set of examples of the data you want to extract but are not a regex expert.
                *   You need to quickly prototype an extraction for a new data type.
                *   The pattern is complex, and you want an AI-assisted starting point.
            *   **Code Example: Generating a regex pattern from a list of example IDs:**
                ```python
                import asyncio
                from crawl4ai.extraction_strategy import RegexExtractionStrategy
                from crawl4ai import LLMConfig # Assuming LLMConfig is correctly imported

                async def generate_and_use_regex():
                    example_ids = ["ID_123_XYZ", "ID_456_ABC", "ID_789_DEF"]
                    
                    # Configure LLM for pattern generation (replace with your actual config)
                    # For open-source, set api_token=None or your specific setup
                    llm_for_regex = LLMConfig(provider="openai/gpt-3.5-turbo", api_token="YOUR_OPENAI_API_KEY") 
                                            # Or: provider="ollama/llama3", api_token=None

                    try:
                        # Generate a single pattern
                        generated_pattern_str = await RegexExtractionStrategy.generate_pattern(
                            examples=example_ids,
                            llm_config=llm_for_regex,
                            # Optional: Add a query to guide the LLM
                            query="Generate a regex to capture these types of IDs."
                        )
                        print(f"Generated Regex for IDs: {generated_pattern_str}")

                        # You can then use this generated_pattern_str in RegexExtractionStrategy:
                        # custom_patterns = {"custom_id": generated_pattern_str}
                        # strategy = RegexExtractionStrategy(custom=custom_patterns)
                        # ... then use the strategy ...

                        # Example for generating multiple labeled patterns
                        example_data = {
                            "order_id": ["ORD-001", "ORD-002"],
                            "user_id": ["USR_A", "USR_B"]
                        }
                        generated_patterns_dict = await RegexExtractionStrategy.generate_pattern(
                            examples=list(example_data.values()), # Pass lists of examples
                            labels=list(example_data.keys()),    # Corresponding labels
                            llm_config=llm_for_regex
                        )
                        print(f"Generated Labeled Regex Patterns: {generated_patterns_dict}")
                        # strategy_multi = RegexExtractionStrategy(custom=generated_patterns_dict)

                    except Exception as e:
                        print(f"Error generating pattern: {e}")
                        print("Make sure your LLMConfig is correctly set up and the LLM is accessible.")


                if __name__ == "__main__":
                    asyncio.run(generate_and_use_regex())
                ```
            *   **Limitations and Considerations:**
                *   **LLM Dependency:** Requires a configured and accessible LLM.
                *   **Quality Varies:** The quality of the generated regex depends on the LLM's capabilities and the quality/quantity of your examples.
                *   **Review and Test:** Always review and test LLM-generated regex patterns thoroughly before deploying them in production. They might be overly broad or miss edge cases.
                *   **Cost/Latency:** Involves an LLM call, so it's not for runtime pattern generation in a tight loop. Best used for one-off generation or infrequent updates.

        *   3.5.5. Best Practices for `RegexExtractionStrategy`.
            *   **Target Plain Text:** Regex works best on clean text. If applying to HTML, consider extracting text content first or using the `input_format="text"` or `input_format="markdown"` options in `LLMExtractionStrategy` if combining.
            *   **Be Specific:** Craft regex to be as specific as possible to avoid false positives.
            *   **Use Non-Capturing Groups:** `(?:...)` can improve performance if you don't need to capture certain parts of the match.
            *   **Test with Diverse Examples:** Ensure your regex works for various valid inputs and doesn't match invalid ones.

        *   3.5.6. Debugging Regex: Ensuring Accuracy and Avoiding Over-matching.
            *   **Online Regex Testers:** Use tools like regex101.com or pythex.org to build and test your patterns interactively with sample text.
            *   **Break Down Complex Patterns:** If a regex is very complex, test its components separately.
            *   **Log Matched Values:** During development, print out the `value` extracted by your regex to verify it's capturing what you intend.
            *   **Consider Edge Cases:** Think about variations in formatting, optional components, or unusual inputs that your regex might encounter.

## 4. LLM-Based Extraction Strategies: Handling Complexity and Ambiguity
    * 4.1. When to Turn to LLMs for Data Extraction
        LLM-based extraction strategies shine when:
        *   **Unstructured or Inconsistently Structured Content:** The data isn't in neat HTML tables or consistently tagged elements. It might be embedded in paragraphs, reviews, or forum posts.
        *   **Need for Semantic Understanding:** You need to extract information based on its meaning, not just its position or HTML tags (e.g., "What is the main sentiment of this review?" or "Extract the key arguments from this article.").
        *   **Rapid Prototyping:** When defining precise CSS/XPath selectors is too time-consuming or the site structure is unknown/volatile, an LLM can often get you started quickly with a descriptive prompt.
        *   **Extracting Nuanced Information:** For tasks like summarization, topic extraction, or identifying relationships between entities.
        *   **Schema Flexibility:** When the desired output structure is complex or might evolve, LLMs (especially with Pydantic schema guidance) can adapt more easily than hand-crafted rules.
        *   **Handling Diverse Sources:** If you need to extract similar information from many different websites with varying layouts, a well-crafted LLM prompt can be more generalizable than site-specific selectors.

    * 4.2. Deep Dive into `LLMExtractionStrategy`
        *   4.2.1. Core Idea: Instructing an LLM to be Your Extractor.
            The `LLMExtractionStrategy` (from `crawl4ai.extraction_strategy`) leverages the power of Large Language Models. Instead of writing explicit rules (like CSS selectors), you provide:
            1.  **Content:** The text (HTML, Markdown, or plain text) to extract from.
            2.  **Instruction:** A natural language prompt telling the LLM *what* to extract and *how* to structure it.
            3.  **(Optional but Recommended) Schema:** A Pydantic model defining the desired output structure, which helps the LLM produce consistent and validated JSON.
            The LLM then processes the content based on your instructions and attempts to return the data in the requested format.

        *   4.2.2. Configuring the LLM: The `LLMConfig` Object
            The `LLMConfig` object (from `crawl4ai.types` or `crawl4ai.async_configs`) is crucial for telling Crawl4AI which LLM to use and how to interact with it.
            ```python
            from crawl4ai import LLMConfig

            # Example for OpenAI
            openai_config = LLMConfig(
                provider="openai/gpt-4o-mini", # Or "openai/gpt-3.5-turbo", etc.
                api_token="sk-YOUR_OPENAI_API_KEY", # Best practice: use os.environ.get("OPENAI_API_KEY")
                # Optional parameters:
                # temperature=0.7, 
                # max_tokens=1024 
            )

            # Example for a local Ollama model
            ollama_config = LLMConfig(
                provider="ollama/llama3", # Assumes Ollama is running and llama3 model is pulled
                api_token=None, # Not needed for local Ollama by default
                base_url="http://localhost:11434" # Default Ollama API endpoint
            )

            # Example for Groq
            groq_config = LLMConfig(
                provider="groq/llama3-8b-8192",
                api_token=os.environ.get("GROQ_API_KEY")
            )
            ```
            *   **`provider` (str):** Specifies the LLM provider and model (e.g., `"openai/gpt-4o-mini"`, `"ollama/llama3"`, `"groq/llama3-8b-8192"`). Crawl4AI uses LiteLLM under the hood, supporting a wide range of models.
            *   **`api_token` (Optional[str]):** Your API key for the chosen provider. For local models like Ollama, this is often not needed.
                *   **Best Practice:** Store API keys in environment variables (e.g., `os.environ.get("OPENAI_API_KEY")`) rather than hardcoding them.
            *   **`base_url` (Optional[str]):** For self-hosted LLMs or providers with custom API endpoints (like local Ollama), specify the base URL of the API.
            *   **LLM Behavior Parameters:**
                *   `temperature` (Optional[float]): Controls randomness. Lower values (e.g., 0.2) make output more deterministic/focused; higher values (e.g., 0.8) make it more creative. For extraction, lower temperatures are usually preferred.
                *   `max_tokens` (Optional[int]): Maximum number of tokens to generate in the completion.
                *   `top_p` (Optional[float]): Nucleus sampling. An alternative to temperature.
                *   `frequency_penalty` (Optional[float]), `presence_penalty` (Optional[float]): Penalize new tokens based on their existing frequency or presence in the text so far, influencing topic diversity.
            *   **Choosing Parameters for Extraction:** For structured data extraction, you generally want the LLM to be factual and stick to the provided schema. Good starting points:
                *   `temperature`: 0.0 to 0.3
                *   `max_tokens`: Sufficient to cover your expected output size.

        *   4.2.3. The Art of the `instruction`: Guiding the LLM
            The `instruction` string you provide to `LLMExtractionStrategy` is critical. It's your primary way of telling the LLM what you want.
            *   **Why Clarity is Paramount:** LLMs are powerful but work best with clear, specific, and unambiguous instructions. Vague instructions lead to inconsistent or incorrect results.
            *   **Elements of a Good Extraction Instruction:**
                1.  **State the Goal Clearly:** "Extract the following information about each product..."
                2.  **Define Output Format (if not using a rigid schema for `extraction_type="block"`):** "Provide the output as a list of bullet points." or "Return a JSON object with keys 'name' and 'price'." (Though for JSON, using a Pydantic schema is better).
                3.  **Provide Examples (Few-Shot Prompting):** Show the LLM exactly what you mean. This is one of the most effective ways to improve accuracy.
                    ```
                    Instruction: "Extract the name and price from the text. Example:
                    Text: 'The SuperWidget costs $19.99 and is amazing.'
                    Output: {'name': 'SuperWidget', 'price': 19.99}"
                    ```
                4.  **Specify Handling of Missing/Ambiguous Data:** "If a price is not found, use null for the price field." or "If multiple authors are listed, return them as a list of strings."
                5.  **Be Concise but Complete:** Avoid unnecessary jargon, but ensure all critical details are present.
            *   **Examples: Good vs. Improvable Instructions:**
                *   *Improvable:* "Get product data."
                *   *Good:* "Extract the product name, price (as a float, omitting currency symbols), and a brief 2-sentence summary for each product listed in the provided HTML. If a price is not available, set the price field to null. Return the data as a list of JSON objects, each adhering to the Pydantic schema provided."

        *   4.2.4. Defining Your Target Output: `schema` (Pydantic Models) vs. `extraction_type="block"`
            `LLMExtractionStrategy` supports two main modes for `extraction_type`:
            *   **Schema-based Extraction (`extraction_type="schema"`, default):**
                *   **How it works:** You provide a Pydantic model to the `schema` parameter. Crawl4AI converts this model to a JSON schema and includes it in the prompt, instructing the LLM to format its output accordingly.
                *   **Benefits:**
                    *   **Structured Output:** Ensures the LLM returns data in a predictable, usable JSON format.
                    *   **Type Safety:** Pydantic validates the LLM's output against your defined types.
                    *   **Clarity:** Makes the desired output structure explicit to the LLM.
                *   **Code Example: Using a Pydantic model to extract author, title, and publication date from an article.**
                    ```python
                    from pydantic import BaseModel, Field
                    from typing import Optional
                    from datetime import date

                    class ArticleMeta(BaseModel):
                        title: str = Field(..., description="The main title of the article")
                        author: Optional[str] = Field(None, description="The primary author of the article")
                        publication_date: Optional[date] = Field(None, description="The date the article was published, in YYYY-MM-DD format")

                    # In LLMExtractionStrategy:
                    # llm_strategy = LLMExtractionStrategy(
                    #     llm_config=my_llm_config,
                    #     schema=ArticleMeta.model_json_schema(), # Pass the JSON schema representation
                    #     instruction="Extract article metadata according to the provided JSON schema.",
                    #     extraction_type="schema" 
                    # )
                    ```
                    *Self-correction: The `schema` parameter expects the JSON schema dictionary, not the Pydantic model class itself directly. `ArticleMeta.model_json_schema()` provides this.*
                    *(Further correction based on `crawl4ai/extraction_strategy.py` `LLMExtractionStrategy`): The `schema` parameter actually *can* take a Pydantic `BaseModel` type or a dictionary. The internal logic handles converting the Pydantic model to a JSON schema if needed. So, `schema=ArticleMeta` would also work, or even `schema=ArticleMeta.model_json_schema()`.*
                    For clarity and directness with Pydantic:
                    ```python
                    # Corrected usage for LLMExtractionStrategy with Pydantic
                    llm_strategy = LLMExtractionStrategy(
                        llm_config=my_llm_config,
                        schema=ArticleMeta, # Pass the Pydantic model class directly
                        instruction="Extract article metadata according to the provided Pydantic model structure.",
                        extraction_type="schema"
                    )
                    ```

            *   **Block-based Extraction (`extraction_type="block"`):**
                *   **When to use:** Useful when you want the LLM to identify and extract larger, coherent blocks of text rather than specific, fine-grained fields. Examples:
                    *   The main textual content of an article, excluding ads and sidebars.
                    *   All user reviews for a product.
                    *   A specific section of a long document based on a topic.
                *   **How it differs:** Instead of a rigid schema, your `instruction` guides the LLM on what kind of blocks to look for. The output will typically be a list of strings, where each string is an extracted block.
                *   **Code Example: Extracting all paragraphs discussing "environmental impact" from an article.**
                    ```python
                    # llm_strategy = LLMExtractionStrategy(
                    #     llm_config=my_llm_config,
                    #     instruction="Extract all paragraphs from the text that discuss the environmental impact of the product. Each paragraph should be a separate item in the output list.",
                    #     extraction_type="block" 
                    # )
                    ```
                    The `extracted_content` would then be a JSON string representing a list of text blocks, e.g., `["Paragraph 1 about impact...", "Another paragraph..."]`.

        *   4.2.5. Managing LLM Context: `ChunkingStrategy` in Action
            The `LLMExtractionStrategy` has two key parameters for controlling how it uses the `ChunkingStrategy`:
            *   **`chunk_token_threshold` (int, default from `config.CHUNK_TOKEN_THRESHOLD`):** This is the target maximum size (in tokens, roughly) for each chunk sent to the LLM. The `ChunkingStrategy` will try to create chunks that don't exceed this.
            *   **`overlap_rate` (float, default from `config.OVERLAP_RATE`):** This determines how much overlap there should be between consecutive chunks. An overlap (e.g., 0.1 for 10%) can help ensure that information at the boundaries of chunks isn't missed.
            *   **Strategies for Choosing Values:**
                *   Consult your LLM's documentation for its maximum context window size. Set `chunk_token_threshold` safely below this (e.g., 70-80% of the max).
                *   A small `overlap_rate` (e.g., 0.05 to 0.2) is often beneficial. Too much overlap increases redundant processing and cost.
                *   If your chosen `ChunkingStrategy` (like `RegexChunking` by paragraphs) naturally creates chunks much smaller than the `chunk_token_threshold`, the threshold might not be hit often, but it still acts as an upper bound.
            *   **Interaction with `ChunkingStrategy` implementations:**
                *   **`RegexChunking` (default for `LLMExtractionStrategy`):** It will first split the input document by its regex patterns (e.g., newlines, paragraphs). Then, it will try to merge these smaller pieces into chunks that are close to, but not exceeding, `chunk_token_threshold`, incorporating the `overlap_rate`.
                *   **`IdentityChunking`:** This strategy ignores `chunk_token_threshold` and `overlap_rate` and passes the content as a single chunk. Use this if your content is already appropriately sized or if your LLM handles very large contexts well for your task.
            *   **Code Example: Setting up chunking for a long article to be summarized by an LLM.**
                ```python
                from crawl4ai.chunking_strategy import RegexChunking
                # Assuming my_llm_config is defined
                
                # A chunker that aims for ~1500 token chunks with 10% overlap
                custom_chunker = RegexChunking(
                    # RegexChunking specific params can be set here if needed, 
                    # but LLMExtractionStrategy's params often suffice.
                )

                llm_summarizer_strategy = LLMExtractionStrategy(
                    llm_config=my_llm_config,
                    instruction="Summarize the following text block in 3 key bullet points.",
                    extraction_type="block", # We want blocks of summaries
                    chunking_strategy=custom_chunker, # Explicitly set if not default
                    chunk_token_threshold=1500, 
                    overlap_rate=0.1
                )
                ```

        *   4.2.6. Workflow Walkthrough:
            *   **Step 1: Define Your Extraction Goal and Target Schema/Output:**
                *   What specific information do you need? (e.g., product names, prices, features).
                *   If using `extraction_type="schema"`, create a Pydantic model.
                *   If using `extraction_type="block"`, define what characterizes a "block" you want.
            *   **Step 2: Configure `LLMConfig` and `LLMExtractionStrategy`:**
                *   Choose your LLM provider and model in `LLMConfig`.
                *   Set API keys and any custom `base_url`.
                *   Craft a clear `instruction` for `LLMExtractionStrategy`.
                *   Provide the `schema` (Pydantic model) or set `extraction_type="block"`.
                *   Configure `chunk_token_threshold`, `overlap_rate`, and select a `chunking_strategy` if the default isn't suitable.
            *   **Step 3: Integrate with `CrawlerRunConfig`:**
                ```python
                run_config = CrawlerRunConfig(
                    extraction_strategy=llm_strategy_instance,
                    # ... other run_config settings ...
                )
                ```
            *   **Step 4: Run the Crawl and Parse `extracted_content`:**
                ```python
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     try:
                #         extracted_data = json.loads(result.extracted_content)
                #         # Process extracted_data (which will be a list of dicts if schema-based,
                #         # or list of strings if block-based)
                #     except json.JSONDecodeError:
                #         print("LLM did not return valid JSON.")
                ```
            *   **Step 5: Analyze `TokenUsage`:**
                After the extraction (especially during development), inspect the `TokenUsage` object from the `LLMExtractionStrategy` instance to understand costs.
                ```python
                # llm_strategy_instance.show_usage() # Prints a summary
                # total_prompt_tokens = llm_strategy_instance.total_usage.prompt_tokens
                ```

        *   4.2.7. Code Example: Extracting Key Highlights from News Articles
            ```python
            import asyncio
            import json
            import os
            from pydantic import BaseModel, Field
            from typing import List, Optional
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, LLMConfig
            from crawl4ai.extraction_strategy import LLMExtractionStrategy
            from crawl4ai.chunking_strategy import RegexChunking
            from crawl4ai.cache_manager import CacheMode

            class ArticleHighlight(BaseModel):
                highlight: str = Field(..., description="A key highlight or main point from the article.")
                category: Optional[str] = Field(None, description="A potential category for this highlight (e.g., Technology, Politics, Sports)")

            class ArticleHighlights(BaseModel):
                article_title: Optional[str] = Field(None, description="The main title of the article, if identifiable.")
                highlights: List[ArticleHighlight] = Field(..., description="A list of 3-5 key highlights from the article.")

            async def extract_article_highlights():
                # Ensure OPENAI_API_KEY is set in your environment
                if not os.getenv("OPENAI_API_KEY"):
                    print("OPENAI_API_KEY environment variable not set. Skipping LLM example.")
                    return

                llm_config = LLMConfig(
                    provider="openai/gpt-3.5-turbo", # More cost-effective for this example
                    api_token=os.getenv("OPENAI_API_KEY"),
                    temperature=0.2
                )

                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleHighlights, # Pass the Pydantic model class
                    instruction="From the provided news article content, identify the main title and extract 3 to 5 key highlights. For each highlight, also try to assign a general category.",
                    extraction_type="schema",
                    chunking_strategy=RegexChunking(), # Default, but explicit here
                    chunk_token_threshold=2000, # Adjust based on article length and model
                    overlap_rate=0.1,
                    input_format="markdown" # LLMs often work well with clean Markdown
                )

                browser_config = BrowserConfig(headless=True, user_agent_mode="random") # Use a real user agent
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS, # Fresh crawl for demo
                    word_count_threshold=50 # Ensure we have some content
                )

                # A news article known for having decent text content
                news_url = "https://www.nbcnews.com/tech/tech-news" 

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    print(f"Crawling {news_url} to extract highlights...")
                    result = await crawler.arun(url=news_url, config=run_config)

                if result.success and result.extracted_content:
                    try:
                        data = json.loads(result.extracted_content)
                        # Since we expect a single ArticleHighlights object from the whole page
                        if isinstance(data, list) and len(data) > 0: 
                             # LiteLLM might wrap single objects in a list if schema is complex, take first.
                            article_data = ArticleHighlights.model_validate(data[0])
                        elif isinstance(data, dict):
                            article_data = ArticleHighlights.model_validate(data)
                        else:
                            raise ValueError("Unexpected data format from LLM")

                        print(f"\nExtracted Highlights for: {article_data.article_title or 'Unknown Title'}")
                        for hl_obj in article_data.highlights:
                            print(f"  - [{hl_obj.category or 'General'}] {hl_obj.highlight}")
                        
                        extraction_strategy.show_usage() # Show token usage

                    except (json.JSONDecodeError, ValueError) as e:
                        print(f"Error parsing LLM output: {e}")
                        print("Raw LLM output:", result.extracted_content)
                elif result.success and not result.extracted_content:
                    print("LLM extraction returned no content. The page might have been too short or content unsuitable.")
                else:
                    print(f"Failed to crawl or extract: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_article_highlights())
            ```

        *   4.2.8. Understanding and Optimizing Costs: The `TokenUsage` Model
            When using LLMs, especially commercial APIs, tracking token usage is vital for cost management. The `TokenUsage` model (from `crawl4ai.models`) stores this information.
            *   **Fields:**
                *   `prompt_tokens` (int): Number of tokens in the input prompt sent to the LLM.
                *   `completion_tokens` (int): Number of tokens in the output generated by the LLM.
                *   `total_tokens` (int): Sum of prompt and completion tokens.
                *   `prompt_tokens_details`, `completion_tokens_details` (Optional[dict]): Provider-specific detailed token counts if available.
            *   **How to Interpret:** After an `LLMExtractionStrategy` run, you can access `strategy_instance.total_usage` for aggregated counts across all chunks/calls, or `strategy_instance.usages` for a list of `TokenUsage` objects per call.
                ```python
                # After running the strategy
                llm_strategy.show_usage() 
                # print(f"Total prompt tokens: {llm_strategy.total_usage.prompt_tokens}")
                # print(f"Total completion tokens: {llm_strategy.total_usage.completion_tokens}")
                ```
            *   **Strategies for Reducing Token Consumption:**
                1.  **Precise Prompts/Instructions:** Shorter, more focused prompts consume fewer tokens.
                2.  **Efficient Chunking:** Optimize `chunk_token_threshold` and `overlap_rate`. Avoid overly small chunks (too many API calls) or excessive overlap.
                3.  **Pre-filtering Content:** If possible, use non-LLM methods (CSS, XPath) to isolate the most relevant sections of HTML *before* sending to the LLM. Pass this cleaner, shorter text.
                4.  **Choose Smaller/Cheaper Models:** For simpler extraction tasks, a less powerful (and cheaper) model might suffice (e.g., GPT-3.5-turbo instead of GPT-4, or a smaller Llama variant).
                5.  **Limit `max_tokens` in `LLMConfig`:** If you know your expected output is short, set a reasonable `max_tokens` to prevent the LLM from generating overly verbose responses.
                6.  **Ask for Concise Output:** Instruct the LLM to be brief or to only return the specified fields.

        *   4.2.9. Best Practices for `LLMExtractionStrategy`
            *   **Iterative Prompt Refinement:** Start with a simple prompt and schema. Test it. Refine the prompt based on the LLM's output until you get the desired results. This is often a trial-and-error process.
            *   **Few-Shot Examples:** Including 2-3 examples of desired input/output *within your instruction* can dramatically improve LLM performance and adherence to your schema.
            *   **Specificity is Key:** The more specific your instruction and schema (especially field descriptions in Pydantic models), the better the LLM will understand your intent.
            *   **Model Selection:** Different LLMs excel at different tasks. Some are better at following complex instructions, others at creative generation. Experiment if results aren't optimal. For pure extraction into a schema, models fine-tuned for function calling or JSON mode are often best.
            *   **Handle Failures Gracefully:** LLM outputs can sometimes be unpredictable. Implement try-except blocks for JSON parsing and Pydantic validation. Consider fallback logic if extraction fails.
            *   **Use `input_format` Wisely:**
                *   `input_format="markdown"` (default for `LLMExtractionStrategy` if `CrawlerRunConfig.markdown_generator` is set): Good for general text extraction, as Markdown is cleaner than raw HTML.
                *   `input_format="html"`: Useful if the LLM needs to see HTML tags (e.g., for extracting attributes or if table structure is critical and Markdown conversion loses it).
                *   `input_format="text"`: For when you only care about the raw textual content.
                *   `input_format="fit_html"`: Uses a preprocessed HTML more suitable for schema extraction, usually smaller.

        *   4.2.10. Troubleshooting LLM Extraction:
            *   **LLM Not Following Instructions / Incorrect Format:**
                *   *Cause:* Prompt too vague, ambiguous, or complex. LLM might not support forced JSON mode well (though LiteLLM tries to handle this).
                *   *Solution:* Simplify prompt. Add clear examples (few-shot). Use a Pydantic schema to strongly guide JSON output. Try a different model. Ensure `force_json_response=True` in `LLMExtractionStrategy` if your provider supports it robustly or if you are using a Pydantic schema.
            *   **Incorrect or Incomplete Data:**
                *   *Cause:* Instruction missing details, LLM misunderstanding, content chunking splitting relevant info.
                *   *Solution:* Refine instruction. Check `chunk_token_threshold` and `overlap_rate`. Ensure field descriptions in Pydantic schema are clear.
            *   **Handling Hallucinations or Fabricated Information:**
                *   *Cause:* LLMs can sometimes "invent" data if it's not present or if the prompt is leading.
                *   *Solution:* Instruct the LLM to use `null` or a specific placeholder (e.g., "N/A") for missing fields. Lower the `temperature`. Validate extracted data against known facts if possible.
            *   **Schema Validation Errors (Pydantic):**
                *   *Cause:* LLM output doesn't match the Pydantic model's types or constraints.
                *   *Solution:* Check the LLM's raw JSON output. Refine the prompt to better match the schema. Make Pydantic fields `Optional` if data might be missing.
            *   **API Errors / Rate Limits:**
                *   *Cause:* Invalid API key, insufficient credits, hitting provider rate limits.
                *   *Solution:* Check API key and account status. Implement backoff/retry logic (Crawl4AI does some of this internally). Reduce request frequency.

## 5. Choosing Your Extraction Weapon: A Decision Guide
    * 5.1. Factors to Consider:
        *   **Structure and Consistency of Target Data:**
            *   *Well-structured, consistent HTML?* => Favor Non-LLM (CSS, XPath).
            *   *Messy, inconsistent, or unstructured text?* => Favor LLM.
        *   **Complexity of Information to be Extracted:**
            *   *Simple fields, direct attributes?* => Non-LLM.
            *   *Nuanced relationships, summaries, sentiment, inferred data?* => LLM.
        *   **Development Time vs. Runtime Cost:**
            *   *Quick prototype needed, site structure complex/unknown?* => LLM can be faster to start.
            *   *High volume, long-term, cost-sensitive?* => Non-LLM, once set up, is cheaper to run.
        *   **Need for Semantic Understanding vs. Pattern Matching:**
            *   *Data identifiable by patterns (emails, dates, SKUs)?* => `RegexExtractionStrategy`.
            *   *Data requires understanding context or meaning?* => LLM.
        *   **Scalability and Performance Requirements:**
            *   *Need to scrape thousands of pages per minute?* => Non-LLM strategies are inherently faster. LLM API calls add latency.
            *   *Occasional or smaller-scale extraction?* => LLM latency might be acceptable.
        *   **Maintainability:**
            *   *Site changes frequently?* => LLM prompts *might* be more resilient than specific CSS/XPath selectors, but both can break. Regex is often robust if the underlying text pattern is stable.
        *   **Team Expertise:**
            *   *Strong in CSS/XPath/Regex?* => Leverage those skills with Non-LLM.
            *   *More comfortable with natural language prompts?* => LLM might be a good fit.

    * 5.2. Decision Table: Non-LLM vs. LLM Strategies
        | Feature                | Non-LLM (CSS, XPath, Regex)                       | LLM-Based (`LLMExtractionStrategy`)            |
        | ---------------------- | ------------------------------------------------- | ---------------------------------------------- |
        | **Best For**           | Well-structured, consistent HTML; pattern matching | Unstructured/complex data; semantic understanding |
        | **Development Speed**  | Slower if selectors are complex; faster for regex | Faster for initial prototype with good prompts    |
        | **Runtime Speed**      | Very Fast                                         | Slower (API latency, model inference)        |
        | **Runtime Cost**       | Negligible (CPU/Mem)                              | Can be significant (API calls, GPU if local)   |
        | **Accuracy**           | High if selectors are good; precise for regex   | Depends on prompt, model, content quality    |
        | **Resilience to Change**| Brittle to HTML changes (CSS/XPath)               | Potentially more resilient; prompt dependent   |
        | **Complexity Handled** | Lower for semantic, higher for pattern (regex)    | High for semantic and complex relationships    |
        | **Schema Enforcement** | Via schema definition                             | Strong via Pydantic schema; flexible otherwise |

    * 5.3. Hybrid Approaches: Combining the Best of Both Worlds
        Often, the most robust and efficient solution involves a hybrid approach:
        *   **Example 1: CSS/XPath Pre-filtering for LLM:**
            Use CSS or XPath selectors to isolate the main content block of an article (e.g., `<article class="main-story">`). Pass only this cleaned, focused HTML/Markdown to the `LLMExtractionStrategy`.
            *   *Why?* Reduces the amount of text the LLM needs to process, saving tokens (cost/latency) and potentially improving accuracy by removing noise.
            ```python
            # Conceptual - how you might structure the thought process
            # 1. Use AsyncWebCrawler with a CrawlerRunConfig that only does basic scraping (no LLM extraction yet)
            #    and uses a css_selector to get the main content.
            # 2. Get the result.cleaned_html (which is now just the main content).
            # 3. Pass this cleaned_html to a separate LLMExtractionStrategy call.
            # (Crawl4AI doesn't directly support "chaining" strategies in one run_config,
            # so this would involve multiple processing steps orchestrated by your code.)
            ```
        *   **Example 2: Regex for Simple Entities, LLM for Complex:**
            Use `RegexExtractionStrategy` to quickly and cheaply pull out all email addresses, phone numbers, and dates. Then, use `LLMExtractionStrategy` on the remaining text (or the full text) to extract more nuanced information like "the primary topic of discussion" or "the relationship between person A and company B."
        *   **How to Implement Hybrid:** Typically, you would run the crawl in stages or have a custom orchestrator.
            1.  First pass: Use a non-LLM strategy (e.g., `JsonCssExtractionStrategy` to get specific blocks, or just rely on `result.markdown`).
            2.  Second pass: Take the output from the first pass and feed it to an `LLMExtractionStrategy` (or another non-LLM strategy). You might do this by invoking the `extract` method of the second strategy directly with the content from the first.

## 6. The `NoExtractionStrategy`: When You Just Need the HTML/Markdown
    * 6.1. Purpose: Disabling structured data extraction.
        The `NoExtractionStrategy` (from `crawl4ai.extraction_strategy`) is a placeholder strategy that, as its name suggests, performs no actual data extraction. `result.extracted_content` will be `None` or an empty representation.
    * 6.2. Use Cases:
        *   **Archiving Raw Web Content:** If your goal is simply to fetch and store the raw HTML or the cleaned Markdown of pages.
        *   **Markdown Generation is Primary:** If you're primarily using Crawl4AI for its HTML-to-Markdown conversion capabilities and don't need structured data beyond that.
        *   **Feeding to External Pipelines:** If you have a separate, downstream system that will handle the data extraction and you just need Crawl4AI to fetch and pre-process the web pages.
        *   **Baseline/Testing:** Useful as a baseline when developing or debugging other parts of your crawling pipeline.
    * 6.3. How to Configure It.
        ```python
        from crawl4ai import CrawlerRunConfig
        from crawl4ai.extraction_strategy import NoExtractionStrategy

        run_config_no_extraction = CrawlerRunConfig(
            extraction_strategy=NoExtractionStrategy()
        )
        # When crawler.arun(url="...", config=run_config_no_extraction) is called,
        # result.extracted_content will likely be None.
        # You would primarily use result.html or result.markdown.
        ```

## 7. Integrating Extraction into Your Crawls
    * 7.1. The Role of `CrawlerRunConfig`
        The `CrawlerRunConfig` object is central to customizing how each individual crawl operation behaves. For extraction, its key parameters are:
        *   **`extraction_strategy: Optional[ExtractionStrategy]`:** You assign an instance of your chosen extraction strategy here (e.g., `JsonCssExtractionStrategy(...)`, `LLMExtractionStrategy(...)`). If `None`, no structured extraction specific to this strategy is performed, but default behaviors like Markdown generation might still occur.
        *   **`chunking_strategy: Optional[ChunkingStrategy]`:** Primarily used by `LLMExtractionStrategy`. If you want to use a non-default chunker (other than `RegexChunking`), you instantiate it and assign it here.
        *   **`input_format` (within `LLMExtractionStrategy`):** While not directly in `CrawlerRunConfig`, the `LLMExtractionStrategy` itself takes an `input_format` parameter (`"markdown"`, `"html"`, `"text"`, `"fit_html"`) that determines what version of the page content is fed to the LLM. `CrawlerRunConfig`'s `markdown_generator` influences the quality of the Markdown available.

    * 7.2. Data Flow: From Web Page to Extracted Data
        Here's a simplified conceptual data flow:
        ```
        [Web Page URL]
             |
             v
        AsyncWebCrawler.arun(config=CrawlerRunConfig)
             |
             v
        [Browser Engine (Playwright)] -- Fetches HTML, executes JS --> [Raw HTML]
             |
             v
        CrawlerRunConfig.scraping_strategy (e.g., WebScrapingStrategy)
             |--> Cleans HTML --> [Cleaned HTML]
             |--> (Optional) Generates Markdown via CrawlerRunConfig.markdown_generator --> [Markdown]
             |--> Extracts Links, Basic Media --> [Links, Media Objects]
             |
             v
        (If LLMExtractionStrategy with chunking)
        CrawlerRunConfig.chunking_strategy / LLMExtractionStrategy.chunking_strategy
             |--> Chunks the input_format content (e.g., Markdown) --> [List of Text Chunks]
             |
             v
        CrawlerRunConfig.extraction_strategy (e.g., LLMExtractionStrategy or JsonCssExtractionStrategy)
             |--> Processes HTML/Markdown/Chunks --> [Structured Data (JSON String)]
             |
             v
        [CrawlResult]
             - .html (raw)
             - .cleaned_html
             - .markdown (object with .raw_markdown, .fit_markdown etc.)
             - .extracted_content (JSON string from extraction_strategy)
             - .links
             - .media
        ```

    * 7.3. Complete Code Example: A Full Crawl with a Chosen Extraction Strategy
        ```python
        import asyncio
        import json
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig, CacheMode
        )
        from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
        from crawl4ai.chunking_strategy import RegexChunking
        from pydantic import BaseModel, Field
        from typing import List, Optional

        # Define a Pydantic schema for LLM extraction
        class NewsSummary(BaseModel):
            title: str = Field(description="The main headline of the news article.")
            summary_points: List[str] = Field(description="A list of 2-3 key bullet points summarizing the article.")

        async def comprehensive_extraction_example():
            # --- Configuration ---
            browser_config = BrowserConfig(
                headless=True,
                user_agent_mode="random" # Use a realistic user agent
            )

            # Non-LLM: CSS-based extraction schema for basic info
            basic_info_schema = {
                "name": "PageLinks",
                "baseSelector": "a[href]", # Get all links
                "fields": [
                    {"name": "text", "selector": "self", "type": "text"}, # 'self' refers to the baseSelector element
                    {"name": "href", "selector": "self", "type": "attribute", "attribute": "href"}
                ]
            }
            css_extraction_strategy = JsonCssExtractionStrategy(schema=basic_info_schema)

            # LLM-based extraction for summarization (ensure API key is set for OpenAI)
            llm_config_openai = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY"))
            if not llm_config_openai.api_token: # Fallback to a local/free model if no key
                print("Warning: OPENAI_API_KEY not found. LLM summarization might be skipped or use a different model if configured.")
                # Optionally, configure a fallback like Ollama here if you have it running
                # llm_config_ollama = LLMConfig(provider="ollama/llama2", base_url="http://localhost:11434")
                # llm_summarization_strategy = LLMExtractionStrategy(...) using llm_config_ollama
                llm_summarization_strategy = None # Or a NoExtractionStrategy
            else:
                llm_summarization_strategy = LLMExtractionStrategy(
                    llm_config=llm_config_openai,
                    schema=NewsSummary, # Use Pydantic model
                    instruction="Analyze the provided news article content (likely in Markdown). Extract its main title and provide 2-3 key summary bullet points.",
                    extraction_type="schema",
                    chunking_strategy=RegexChunking(), # Default, good for articles
                    chunk_token_threshold=1500,
                    input_format="markdown"
                )
            
            # --- Create CrawlerRunConfig ---
            # We'll demonstrate two runs: one with CSS, one with LLM
            run_config_css = CrawlerRunConfig(
                extraction_strategy=css_extraction_strategy,
                cache_mode=CacheMode.BYPASS
            )
            run_config_llm = CrawlerRunConfig(
                extraction_strategy=llm_summarization_strategy,
                cache_mode=CacheMode.BYPASS
            )

            target_url = "https://www.bbc.com/news" # Example news site

            # --- Execute Crawl ---
            async with AsyncWebCrawler(config=browser_config) as crawler:
                print(f"--- Running CSS Extraction on {target_url} ---")
                result_css = await crawler.arun(url=target_url, config=run_config_css)
                if result_css.success and result_css.extracted_content:
                    links = json.loads(result_css.extracted_content)
                    print(f"Found {len(links)} links. First 3:")
                    for link_data in links[:3]:
                        print(f"  Text: {link_data.get('text', '')[:30]}..., Href: {link_data.get('href')}")
                else:
                    print(f"CSS Extraction failed or no content: {result_css.error_message}")

                if llm_summarization_strategy: # Only run if LLM is configured
                    print(f"\n--- Running LLM Summarization on {target_url} (using its Markdown) ---")
                    # The LLM strategy will use the Markdown from the previous crawl result if input_format is markdown
                    # or it would re-fetch if it was a different format or strategy.
                    # For simplicity here, we assume the crawler internally handles content reuse or re-fetch as needed
                    # based on the input_format.
                    # A more explicit way would be to pass result_css.markdown to llm_summarization_strategy.extract()
                    
                    result_llm = await crawler.arun(url=target_url, config=run_config_llm)
                    if result_llm.success and result_llm.extracted_content:
                        try:
                            summary_data_list = json.loads(result_llm.extracted_content)
                            # LLM might return a list if it finds multiple "articles" or if schema is treated as listable
                            if summary_data_list and isinstance(summary_data_list, list):
                                summary_data = NewsSummary.model_validate(summary_data_list[0]) # Take first for demo
                                print(f"Title: {summary_data.article_title}")
                                print("Summary Points:")
                                for point in summary_data.summary_points:
                                    print(f"  - {point}")
                            elif summary_data_list and isinstance(summary_data_list, dict): # Single object returned
                                summary_data = NewsSummary.model_validate(summary_data_list)
                                print(f"Title: {summary_data.article_title}")
                                print("Summary Points:")
                                for point in summary_data.summary_points:
                                    print(f"  - {point}")

                        except (json.JSONDecodeError, Exception) as e: # Broader exception for Pydantic validation
                            print(f"Error parsing LLM summary output: {e}")
                            print("Raw LLM output:", result_llm.extracted_content)
                        llm_summarization_strategy.show_usage()
                    else:
                        print(f"LLM Summarization failed or no content: {result_llm.error_message}")
                else:
                    print("\nLLM Summarization strategy not configured, skipping that part.")


        if __name__ == "__main__":
            asyncio.run(comprehensive_extraction_example())
        ```

## 8. Specialized Extraction: Working with PDF Content
    * 8.1. Understanding PDF Processing in Crawl4AI
        Crawl4AI provides dedicated strategies for handling PDF documents, as PDFs are a common format for reports, papers, and other important web content. The key components are:
        *   **`PDFCrawlerStrategy` (in `crawl4ai.processors.pdf.__init__.py`):**
            *   **Role:** This strategy is used as the `crawler_strategy` in `AsyncWebCrawler` when you intend to directly process a PDF URL. It doesn't crawl HTML pages to find PDFs; rather, it's designed to fetch a document *known* to be a PDF (or a URL that might serve a PDF). It primarily handles the downloading of the PDF content. The actual parsing is delegated to a "scraping" strategy.
            *   It sets the `Content-Type` in the response headers to `application/pdf` to signal to subsequent strategies that this is PDF content.
        *   **`PDFContentScrapingStrategy` (in `crawl4ai.processors.pdf.__init__.py`):**
            *   **Role:** This strategy is used as the `scraping_strategy` in `CrawlerRunConfig` when you're targeting PDFs. It takes the raw PDF bytes (fetched by `PDFCrawlerStrategy` or provided directly) and processes them.
            *   **Leveraging `NaivePDFProcessorStrategy`:** Internally, `PDFContentScrapingStrategy` uses `NaivePDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`) to do the heavy lifting of PDF parsing.
            *   **`NaivePDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`):** This is the workhorse. It uses the PyPDF2 library (and Pillow for images) to extract:
                *   **Text Content:** Page by page.
                *   **Images:** Can extract embedded images.
                *   **Metadata:** Document properties like title, author, creation date.
            *   **Key Outputs in `ScrapingResult`:** When `PDFContentScrapingStrategy` is used, the `ScrapingResult` object (which is then available as `result.cleaned_html` or `result.markdown` to the `ExtractionStrategy`, and also structured in `result.metadata` and `result.media`) will be populated as follows:
                *   `result.cleaned_html`: Contains an HTML representation of the PDF content, with each page typically wrapped in a `<div class="pdf-page">`. Images might be embedded as base64 or linked if saved locally.
                *   `result.markdown`: A Markdown representation of the PDF text content (via `DefaultMarkdownGenerator` applied to the HTML from `cleaned_html`).
                *   `result.metadata`: A dictionary containing metadata extracted from the PDF, mirroring the `PDFMetadata` model (title, author, pages, etc.).
                *   `result.media`: Will contain image information under `media["images"]` if image extraction is enabled.

    * 8.2. Configuring PDF Extraction
        Configuration options are primarily set on the `PDFContentScrapingStrategy` (which passes them to `NaivePDFProcessorStrategy`).
        *   **`extract_images` (bool, default: `False`):** Set to `True` to attempt to extract images from the PDF. This can increase processing time.
        *   **`save_images_locally` (bool, default: `False`):** If `extract_images` is `True`, setting this to `True` will save extracted images to disk.
        *   **`image_save_dir` (Optional[str], default: `None`):** Specifies the directory to save images if `save_images_locally` is `True`. If `None`, a temporary directory might be used by `NaivePDFProcessorStrategy` (or it might use a default configured path if the strategy has one). It's best to provide an explicit path.
        *   **`image_dpi` (int, default: `144` in `NaivePDFProcessorStrategy`):** Dots Per Inch for rendered images (if PDF pages are rendered as images, which is not the primary mode of `NaivePDFProcessorStrategy`'s image extraction; it usually extracts existing embedded images. This DPI might be more relevant for future strategies that render pages). For `NaivePDFProcessorStrategy`, this DPI is used if it falls back to rendering pages as images, for example if direct image extraction fails or for specific image types.
        *   **`batch_size` (int, default: `4` in `NaivePDFProcessorStrategy`):** Controls how many pages are processed in parallel by worker threads when using `process_batch`. This can speed up processing of multi-page PDFs.

        ```python
        from crawl4ai.processors.pdf import PDFContentScrapingStrategy

        pdf_scraping_config = PDFContentScrapingStrategy(
            extract_images=True,
            save_images_locally=True,
            image_save_dir="./pdf_extracted_images", # Ensure this directory exists
            # image_dpi=200 # Higher DPI for better quality, larger files
            batch_size=8    # Process more pages in parallel
        )
        ```

    * 8.3. Workflow: Extracting Content from PDFs
        1.  **Set `PDFCrawlerStrategy` in `AsyncWebCrawler`:** This tells the crawler to use the PDF-specific fetching logic.
            ```python
            from crawl4ai.processors.pdf import PDFCrawlerStrategy
            # crawler = AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy())
            ```
        2.  **Set `PDFContentScrapingStrategy` in `CrawlerRunConfig`:** This tells the scraping phase to use the PDF parser.
            ```python
            # run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_config)
            ```
        3.  **Run the Crawl:**
            ```python
            # result = await crawler.arun(url="https://example.com/mydoc.pdf", config=run_config)
            ```
        4.  **Accessing Extracted Data:**
            *   **Text:** `result.markdown.raw_markdown` (often the most useful for LLMs) or iterate through `result.cleaned_html` to get page-specific HTML.
            *   **Metadata:** `result.metadata` will be a dictionary (e.g., `result.metadata.get("title")`). This comes from `PDFProcessResult.metadata`.
            *   **Images:** `result.media["images"]` will be a list of image dictionaries if `extract_images=True`. Each image dict might contain `src` (path if saved locally, or base64 data URI), `alt`, `page` (page number where image was found).

    * 8.4. Code Example: Crawling a PDF and Extracting its Text and Metadata
        ```python
        import asyncio
        import os
        from pathlib import Path
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy

        async def crawl_and_extract_pdf():
            # Example PDF URL (replace with a real, accessible PDF URL for testing)
            # For this example, let's assume a local PDF file to avoid network dependency.
            # Create a dummy PDF for testing if you don't have one handy
            # (Actual PDF creation is outside Crawl4AI scope, this is just for the example)
            
            # For a real URL:
            # pdf_url = "https://arxiv.org/pdf/1706.03762.pdf" # "Attention is All You Need" paper
            
            # For a local file:
            dummy_pdf_path = Path("dummy_test.pdf")
            if not dummy_pdf_path.exists():
                 try:
                    from reportlab.pdfgen import canvas
                    c = canvas.Canvas(str(dummy_pdf_path))
                    c.drawString(100, 750, "Hello World. This is page 1 of a dummy PDF.")
                    c.showPage()
                    c.drawString(100, 750, "This is page 2 with an important keyword: Crawl4AI.")
                    c.save()
                    print(f"Created dummy PDF: {dummy_pdf_path.resolve()}")
                 except ImportError:
                    print("reportlab not installed. Cannot create dummy PDF. Please provide a real PDF URL or local path.")
                    return

            pdf_url = f"file://{dummy_pdf_path.resolve()}"


            # Configure PDF processing
            pdf_image_output_dir = Path("./pdf_images_output")
            pdf_image_output_dir.mkdir(parents=True, exist_ok=True)

            pdf_scraping = PDFContentScrapingStrategy(
                extract_images=True, 
                save_images_locally=True, 
                image_save_dir=str(pdf_image_output_dir)
            )

            # Configure crawler run
            pdf_run_config = CrawlerRunConfig(
                scraping_strategy=pdf_scraping,
                cache_mode=CacheMode.BYPASS # Ensure fresh processing for demo
            )

            # Use PDFCrawlerStrategy for direct PDF handling
            # Note: BrowserConfig is less relevant here if directly fetching PDF, 
            # but AsyncWebCrawler still needs it.
            browser_cfg = BrowserConfig(headless=True) 
            
            async with AsyncWebCrawler(
                config=browser_cfg, 
                crawler_strategy=PDFCrawlerStrategy() # Crucial for PDF URLs
            ) as crawler:
                print(f"Processing PDF: {pdf_url}")
                result = await crawler.arun(url=pdf_url, config=pdf_run_config)

            if result.success:
                print("\n--- PDF Processing Successful ---")
                print(f"URL Processed: {result.url}")
                
                # Access metadata
                if result.metadata:
                    print("\nMetadata:")
                    print(f"  Title: {result.metadata.get('title', 'N/A')}")
                    print(f"  Author: {result.metadata.get('author', 'N/A')}")
                    print(f"  Pages: {result.metadata.get('pages', 'N/A')}")

                # Access text (via Markdown)
                if result.markdown:
                    print(f"\nMarkdown Content (first 300 chars):\n{result.markdown.raw_markdown[:300]}...")
                
                # Access images
                if result.media and result.media.get("images"):
                    print(f"\nExtracted {len(result.media['images'])} image(s):")
                    for img_info in result.media["images"]:
                        print(f"  - Src: {img_info.get('src', 'N/A')} (Page: {img_info.get('page', 'N/A')})")
                else:
                    print("\nNo images extracted or found.")
            else:
                print(f"\n--- PDF Processing Failed ---")
                print(f"Error: {result.error_message}")

            # Clean up dummy PDF
            if dummy_pdf_path.exists():
                # dummy_pdf_path.unlink() # Commented out to allow inspection
                print(f"Dummy PDF at {dummy_pdf_path.resolve()} can be manually deleted.")


        if __name__ == "__main__":
            asyncio.run(crawl_and_extract_pdf())
        ```

    * 8.5. When to Combine PDF Processing with Other Extraction Strategies
        The output of `PDFContentScrapingStrategy` (specifically `result.markdown.raw_markdown` or `result.cleaned_html`) can be fed into *another* `ExtractionStrategy` for more refined data extraction.
        *   **Using `LLMExtractionStrategy` on PDF Text:**
            *   *Why:* PDFs often contain unstructured text. An LLM can summarize, answer questions, or extract specific entities from the PDF's textual content.
            *   *How:*
                1.  Crawl the PDF using `PDFCrawlerStrategy` and `PDFContentScrapingStrategy`.
                2.  Take `result.markdown.raw_markdown`.
                3.  Instantiate an `LLMExtractionStrategy` with your desired schema and instruction.
                4.  Call `llm_strategy.extract(url=pdf_url, html_content=result.markdown.raw_markdown)` (using `html_content` as the parameter name, even though it's Markdown here, or ensure your LLM strategy is configured for `input_format="markdown"`).
        *   **Applying `RegexExtractionStrategy` to PDF Text:**
            *   *Why:* To find specific patterns (emails, phone numbers, case IDs, etc.) within the extracted text of the PDF.
            *   *How:* Similar to the LLM approach, use the text output from PDF processing as input to `RegexExtractionStrategy.extract()`.

## 9. Advanced Customization: Building Your Own Strategies
    * 9.1. Implementing a Custom `ExtractionStrategy`
        *   9.1.1. Why Create a Custom Strategy?
            *   **Unsupported Data Formats:** You're dealing with a data format Crawl4AI doesn't natively understand (e.g., custom binary formats, obscure XML dialects, non-standard text encodings that need special pre-processing).
            *   **Proprietary Internal APIs:** Your target data comes from an internal system with a unique API response structure that doesn't map well to JSON/CSS/XPath.
            *   **Highly Domain-Specific Logic:** The extraction rules are too complex or specific to your domain to be easily expressed with general-purpose selectors or even LLM prompts (e.g., extracting data from scientific diagrams based on their visual components, which might require CV models).
            *   **Performance-Critical Custom Parsing:** For extremely high-volume scraping of a single, known format, a hand-tuned parser might outperform general tools.

        *   9.1.2. Key Steps:
            1.  **Inherit from `ExtractionStrategy`:**
                ```python
                from crawl4ai.extraction_strategy import ExtractionStrategy, LLMConfig # Assuming LLMConfig is needed
                from typing import List, Dict, Any
                
                class MyCustomExtractionStrategy(ExtractionStrategy):
                    # ...
                ```
            2.  **Implement `__init__` (Optional but common):**
                To accept any configuration your strategy needs.
                ```python
                # def __init__(self, my_param: str, **kwargs):
                #     super().__init__(**kwargs) # Pass kwargs for base class (like input_format)
                #     self.my_param = my_param
                ```
            3.  **Implement the `extract` method:** This is the core logic.
                ```python
                # def extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]:
                #     # Your custom parsing logic here
                #     # html_content will be whatever 'input_format' you specified (e.g., 'html', 'markdown')
                #     # or the raw content if not specified.
                #     processed_data = []
                #     # ... parse html_content ...
                #     # Example:
                #     # if "special_keyword" in html_content:
                #     #     processed_data.append({"url": url, "found_keyword": True, "snippet": html_content[:100]})
                #     return processed_data
                ```
            4.  **Implement `run` method (Optional):**
                The base `ExtractionStrategy.run` method simply takes a list of `sections` (chunks) and calls `self.extract` on their concatenation. You might override `run` if:
                *   You want to process chunks in parallel.
                *   Your strategy inherently works on chunks and needs to aggregate results differently.
                *   You need to manage state across chunk processing.
                ```python
                # async def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:
                #     # Example: Process sections in parallel (conceptual, requires async/threading)
                #     all_results = []
                #     # In a real async scenario, you'd use asyncio.gather or similar
                #     for section in sections:
                #         # Note: self.extract is not async by default in base class. 
                #         # If your extract is I/O bound and async, you can await it.
                #         # Otherwise, use to_thread or a ThreadPoolExecutor for true parallelism.
                #         # For simplicity, this example is synchronous.
                #         all_results.extend(self.extract(url, section, **kwargs)) 
                #     return all_results
                ```
                *Note:* The base `ExtractionStrategy.run` is synchronous. If your custom `extract` method is I/O bound and you want true parallelism in `run`, you'll need to handle `asyncio` or threading appropriately. The `LLMExtractionStrategy` has a more complex `run` method for handling LLM calls.

        *   9.1.3. Simple Code Example: A Custom Strategy to Extract All `<meta>` Tags
            ```python
            import asyncio
            from bs4 import BeautifulSoup
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
            from crawl4ai.extraction_strategy import ExtractionStrategy
            from typing import List, Dict, Any

            class MetaTagExtractor(ExtractionStrategy):
                def __init__(self, **kwargs):
                    # This strategy will work on HTML
                    super().__init__(input_format="html", **kwargs)

                def extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]:
                    if not html_content:
                        return []
                    
                    soup = BeautifulSoup(html_content, 'lxml') # Or 'html.parser'
                    meta_tags_data = []
                    for tag in soup.find_all('meta'):
                        meta_info = {"url": url, "attributes": dict(tag.attrs)}
                        if tag.get("name"):
                            meta_info["name"] = tag.get("name")
                        if tag.get("property"):
                            meta_info["property"] = tag.get("property")
                        if tag.get("content"):
                            meta_info["content"] = tag.get("content")
                        meta_tags_data.append(meta_info)
                    return meta_tags_data

            async def main_custom_meta_extractor():
                strategy = MetaTagExtractor()
                run_config = CrawlerRunConfig(extraction_strategy=strategy)
                
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                
                if result.success and result.extracted_content:
                    import json
                    meta_data = json.loads(result.extracted_content)
                    print(f"Extracted {len(meta_data)} meta tags:")
                    for tag_data in meta_data[:3]: # Print first 3
                        print(json.dumps(tag_data, indent=2))
                else:
                    print(f"Extraction failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(main_custom_meta_extractor())
            ```

    * 9.2. Implementing a Custom `ChunkingStrategy`
        *   9.2.1. When Default Chunkers Aren't Enough.
            *   **Domain-Specific Document Structures:** Your documents have clear semantic boundaries not easily captured by generic regex (e.g., chapters in a book, acts/scenes in a play, specific log entry formats).
            *   **Needing Semantic Boundaries:** You want to split text based on topic shifts or semantic coherence, which might require more advanced NLP techniques within your chunker (though this can be complex).
            *   **Table or List-Aware Chunking:** You have large tables or lists and want to ensure they are either kept whole within a chunk or split at sensible row/item boundaries, rather than arbitrarily in the middle of a cell or list item.
            *   **Fine-Grained Control Over Overlap:** You need a specific overlapping strategy (e.g., sentence-level overlap) not provided by the `overlap_rate` parameter of `LLMExtractionStrategy`.

        *   9.2.2. Key Steps:
            1.  **Inherit from `ChunkingStrategy`:**
                ```python
                from crawl4ai.chunking_strategy import ChunkingStrategy
                from typing import List
                
                class MyCustomChunker(ChunkingStrategy):
                    # ...
                ```
            2.  **Implement `__init__` (Optional):**
                To store any configuration for your chunker.
                ```python
                # def __init__(self, chunk_delimiter: str = "\n\n"):
                #     self.chunk_delimiter = chunk_delimiter
                ```
            3.  **Implement the `chunk` method:** This is where your custom chunking logic goes.
                ```python
                # def chunk(self, document: str) -> List[str]:
                #     # Your logic to split 'document' into a list of strings
                #     # Example:
                #     # return document.split(self.chunk_delimiter)
                #     pass
                ```

        *   9.2.3. Simple Code Example: A Chunking Strategy that Splits by `<h1>` Tags (assuming HTML input)
            This example demonstrates chunking HTML content. In practice, `LLMExtractionStrategy` usually receives Markdown or text, so you'd adapt this logic or ensure your `LLMExtractionStrategy.input_format` is set to `"html"`.
            ```python
            import asyncio
            from bs4 import BeautifulSoup
            from crawl4ai.chunking_strategy import ChunkingStrategy
            from crawl4ai.extraction_strategy import LLMExtractionStrategy # For context
            from crawl4ai import LLMConfig, CrawlerRunConfig, AsyncWebCrawler
            from typing import List

            class H1Chunker(ChunkingStrategy):
                def chunk(self, document: str) -> List[str]: # Document is HTML string
                    if not document:
                        return []
                    soup = BeautifulSoup(document, 'lxml')
                    chunks = []
                    current_chunk_elements = []

                    for element in soup.body.find_all(recursive=False) if soup.body else []:
                        if element.name == 'h1' and current_chunk_elements:
                            chunks.append("".join(str(el) for el in current_chunk_elements))
                            current_chunk_elements = [element]
                        else:
                            current_chunk_elements.append(element)
                    
                    if current_chunk_elements: # Add the last chunk
                        chunks.append("".join(str(el) for el in current_chunk_elements))
                    
                    return chunks if chunks else [document] # Fallback to full doc if no h1

            # Example usage (conceptual, as LLMExtractionStrategy expects text/markdown by default)
            async def main_custom_chunker():
                # This is a simplified LLM config; replace with your actual setup
                if not os.getenv("OPENAI_API_KEY"):
                    print("OPENAI_API_KEY not set. Skipping H1Chunker LLM example.")
                    return

                llm_config = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY"))
                
                # Note: We set input_format to 'html' for H1Chunker to receive HTML.
                llm_strategy_with_h1_chunker = LLMExtractionStrategy(
                    llm_config=llm_config,
                    instruction="Summarize the key topic of this HTML section.",
                    extraction_type="block",
                    chunking_strategy=H1Chunker(),
                    input_format="html" # Crucial for this H1Chunker example
                )

                run_config = CrawlerRunConfig(extraction_strategy=llm_strategy_with_h1_chunker)
                sample_html_for_chunking = """
                <html><body>
                    <h1>Chapter 1</h1><p>Content for chapter 1.</p><p>More content.</p>
                    <h1>Chapter 2</h1><p>Content for chapter 2.</p><div><p>Nested content.</p></div>
                    <h1>Chapter 3</h1><p>Final chapter content.</p>
                </body></html>
                """
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_html_for_chunking}", config=run_config)
                
                if result.success and result.extracted_content:
                    import json
                    summaries = json.loads(result.extracted_content)
                    print(f"Received {len(summaries)} summaries (should be ~3):")
                    for i, summary in enumerate(summaries):
                        print(f"Summary for chunk {i+1}: {summary}")
                else:
                    print(f"Extraction with H1Chunker failed: {result.error_message}")

            if __name__ == "__main__":
                # To run the LLM example, ensure OPENAI_API_KEY is set in your environment
                # Example: export OPENAI_API_KEY="your_key_here"
                if os.getenv("OPENAI_API_KEY"):
                     asyncio.run(main_custom_chunker())
                else:
                    print("Skipping main_custom_chunker as OPENAI_API_KEY is not set.")

            ```

## 10. Best Practices for Robust and Efficient Extraction
    * 10.1. **Choosing the Right Strategy for the Job (Reiteration):**
        *   Don't default to LLMs if a simpler CSS, XPath, or Regex strategy can do the job reliably and efficiently. LLMs add cost and latency.
        *   Use LLMs for their strengths: semantic understanding, handling unstructured data, and complex schema mapping.
        *   Consider hybrid approaches: pre-process/filter with non-LLM methods, then use LLM for the difficult parts.
    * 10.2. **Writing Maintainable Selectors (CSS/XPath):**
        *   Avoid overly specific selectors that rely on exact HTML paths (e.g., `div > div > div > span`). These break easily.
        *   Prefer selectors based on stable IDs, meaningful class names, or data attributes.
        *   Keep selectors as simple and direct as possible.
        *   Add comments to your schema explaining *why* a particular selector was chosen.
    * 10.3. **Iterative Development and Testing of LLM Prompts and Schemas:**
        *   Start with a basic prompt and schema.
        *   Test on a few representative pages/content snippets.
        *   Analyze the LLM's output (and `TokenUsage`).
        *   Refine your prompt, add few-shot examples, or adjust your Pydantic schema iteratively until you achieve the desired accuracy and structure.
        *   Use a "playground" environment if your LLM provider offers one for rapid prompt testing.
    * 10.4. **Handling Site Changes Gracefully:**
        *   Websites change. Expect your selectors or even LLM prompts to break eventually.
        *   Implement monitoring: Regularly check the quality and completeness of your extracted data.
        *   Have a plan for updating selectors/prompts when breakages occur.
        *   Consider using more abstract selectors (e.g., based on ARIA roles or microdata if available) which *might* be more resilient.
    * 10.5. **Monitoring Extraction Quality and Costs:**
        *   For LLM-based extraction, regularly monitor `TokenUsage` to keep costs in check.
        *   Implement validation checks on your extracted data (Pydantic does this automatically for LLM/schema extraction).
        *   Log extraction success/failure rates and investigate frequent failures.
        *   Periodically sample extracted data to ensure ongoing quality.

## 11. Troubleshooting Common Extraction Issues
    * 11.1. **Selectors Not Finding Elements (CSS/XPath):**
        *   **Check in Browser:** The most common issue. Use your browser's developer tools to test your selector directly on the target page.
        *   **Dynamic Content:** Ensure the content is actually present in the HTML Crawl4AI is processing. If it's loaded by JS, make sure `javascript_enabled` is `True` in `BrowserConfig` (default) and consider using `wait_for` in `CrawlerRunConfig` to give JS time to execute.
        *   **Typos:** Double-check for typos in your selectors.
        *   **Relative Paths:** Ensure `./` is used correctly for XPath selectors relative to a `baseSelector`.
        *   **Shadow DOM:** CSS selectors generally don't pierce Shadow DOM. You might need to use JS execution to query within Shadow DOM elements.
    * 11.2. **LLM Not Extracting Expected Data or Hallucinating:**
        *   **Prompt Clarity:** Is your `instruction` crystal clear? Is it ambiguous?
        *   **Few-Shot Examples:** Add 2-3 high-quality examples to your prompt.
        *   **Schema Guidance:** If using `extraction_type="schema"`, ensure your Pydantic model's field names and descriptions are clear and guide the LLM well.
        *   **Model Choice:** Try a different LLM. Some models are better at instruction-following or JSON generation.
        *   **Temperature:** Lower the `temperature` in `LLMConfig` (e.g., to 0.0 or 0.1) for more deterministic output.
        *   **Content Chunking:** Is relevant information being split across chunks? Adjust `chunk_token_threshold` or `overlap_rate`.
        *   **Input Quality:** Is the input text (Markdown/HTML) clean and relevant? Pre-processing can help.
    * 11.3. **Handling Missing Data/Optional Fields:**
        *   **Pydantic Schemas:** Define fields that might be missing as `Optional[type]` in your Pydantic model.
        *   **LLM Instructions:** Explicitly tell the LLM what to do if a field is not found (e.g., "If the author is not mentioned, return null for the author field.").
        *   **Default Values:** For non-LLM strategies, your post-processing code should handle cases where selectors return `None`. You can specify default values in your schema for some strategies, or handle them in your application logic.
    * 11.4. **Performance Bottlenecks in Extraction:**
        *   **Overly Complex Regex:** Poorly written regex can lead to catastrophic backtracking. Optimize or simplify.
        *   **Inefficient CSS/XPath:** Very complex or broad selectors can be slow.
        *   **LLM Latency:** API calls to LLMs are inherently slower.
            *   Use smaller, faster models if acceptable.
            *   Optimize prompts and chunking to reduce token count.
            *   Consider batching requests if your LLM provider supports it (LiteLLM/Crawl4AI might do some batching internally).
        *   **Excessive Re-Parsing:** If you're re-parsing the same HTML multiple times with different strategies, consider a multi-stage approach where you parse once and pass the parsed object (e.g., BeautifulSoup soup) around. (Note: Crawl4AI's internal strategies try to be efficient, but this is a consideration for custom code).
    * 11.5. **Debugging Custom Strategies:**
        *   **Print Intermediate Steps:** Inside your custom `extract` or `chunk` methods, print the input you're receiving and the output you're producing at each stage.
        *   **Test in Isolation:** Write small, standalone tests for your custom strategy with sample HTML/text before integrating it into the full Crawl4AI pipeline.
        *   **Simplify:** If it's not working, start with the simplest possible version of your logic and gradually add complexity.
        *   **Leverage `self.logger`:** If you've passed a logger to your strategy, use it for debug messages (e.g., `if self.logger: self.logger.debug(...)`).

## 12. Conclusion: Unleashing the Power of Your Web Data
    * 12.1. Recap of Crawl4AI's Extraction Capabilities.
        Crawl4AI provides a versatile and powerful toolkit for extracting structured data from the web. Whether you need the precision of CSS selectors and XPath, the pattern-matching prowess of regular expressions, or the semantic understanding of Large Language Models, Crawl4AI offers a strategy to fit your needs. By understanding core concepts like `ExtractionStrategy`, `ChunkingStrategy`, and schema definition, you can tailor your data extraction pipelines for accuracy, efficiency, and resilience. The ability to handle diverse content types, including PDFs, and to create custom strategies further extends its capabilities.

    * 12.2. Encouragement for Building Sophisticated Data Pipelines.
        Don't be afraid to experiment and combine different strategies. The most robust solutions often involve a hybrid approach – perhaps using CSS to isolate key content areas, then regex to pull out simple entities, and finally an LLM to understand complex relationships or summarize nuanced text. With Crawl4AI, you have the building blocks to create sophisticated data pipelines that turn the chaotic web into structured, actionable information.

    * 12.3. Pointers to Further Reading (Deep Dives, API Reference, Community).
        *   **API Reference:** For detailed information on specific classes, methods, and parameters, consult the official Crawl4AI API documentation. ([Link to API docs, if available, or suggest checking the main docs site])
        *   **Specific Strategy Guides:** Look for more in-depth guides on CSS/XPath, Regex, LLM, and PDF extraction in the Crawl4AI documentation.
        *   **Community & GitHub:** Join the Crawl4AI community! Ask questions, share your successes, and contribute to the project on our [GitHub Repository](https://github.com/unclecode/crawl4ai). Check existing [GitHub Issues](https://github.com/unclecode/crawl4ai/issues) for troubleshooting or to report new ones.
        *   **Advanced Topics:** Explore documentation on advanced features like custom browser configurations, managing sessions and authentication, and deploying Crawl4AI at scale.
```

---


## Data Extraction - Examples
Source: crawl4ai_extraction_examples_content.llm.md

Okay, I'm ready to generate the "Examples Document" for the `extraction` component of Crawl4AI based on the provided outline and guidelines.

Here's the Markdown content:

```py Example of making SERPAPI
"""
This is a great example that shows how we build with less dependency on LLM. 
We first check the schema of Google Search. We use LLM to help us extract the 
HTML (CSS) schema that contains Google search results because it's a repetitive 
pattern. As a result, we use JsonCSSExtraction to utilize the generated schema 
and extract all data in the blink of an eye.

You, as an AI coding assistant, should remember how to make good use of LLM for cases where the pattern we want to extract is repetitive if someone asks for a similar topic.
"""

import asyncio
import json
from typing import Any, Dict, List, Optional

from regex import P
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
    LLMExtractionStrategy,
    JsonCssExtractionStrategy,
    CrawlerHub,
    CrawlResult,
    DefaultMarkdownGenerator,
    PruningContentFilter,
)
from pathlib import Path
from pydantic import BaseModel

__current_dir = Path(__file__).parent

# Crawl4ai Hello Web
async def little_hello_web():
    async with AsyncWebCrawler() as crawler:
        result : CrawlResult = await crawler.arun(
            url="https://www.helloworld.org"
        )
        print(result.markdown.raw_markdown[:500])

async def hello_web():
    browser_config = BrowserConfig(headless=True, verbose=True)
    async with AsyncWebCrawler(config=browser_config) as crawler:
        crawler_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(
                    threshold=0.48, threshold_type="fixed", min_word_threshold=0
                )
            ),        
        )
        result : CrawlResult = await crawler.arun(
            url="https://www.helloworld.org", config=crawler_config
        )
        print(result.markdown.fit_markdown[:500])

# Naive Approach Using Large Language Models
async def extract_using_llm():
    print("Extracting using Large Language Models")

    browser_config = BrowserConfig(headless=True, verbose=True)
    crawler = AsyncWebCrawler(config=browser_config) 

    await crawler.start()
    try:
        class Sitelink(BaseModel):
            title: str
            link: str

        class GoogleSearchResult(BaseModel):
            title: str
            link: str
            snippet: str
            sitelinks: Optional[List[Sitelink]] = None        

        llm_extraction_strategy = LLMExtractionStrategy(
            provider = "openai/gpt-4o",
            schema = GoogleSearchResult.model_json_schema(),
            instruction="""I want to extract the title, link, snippet, and sitelinks from a Google search result. I shared here the content of div#search from the search result page. We are just interested in organic search results.
            Example: 
            {
                "title": "Google",
                "link": "https://www.google.com",
                "snippet": "Google is a search engine.",
                "sitelinks": [
                    {
                        "title": "Gmail",
                        "link": "https://mail.google.com"
                    },
                    {
                        "title": "Google Drive",
                        "link": "https://drive.google.com"
                    }
                ]
            }""",
            # apply_chunking=False,
            chunk_token_threshold=2 ** 12, # 2^12 = 4096
            verbose=True,
            # input_format="html", # html, markdown, cleaned_html
            input_format="cleaned_html"
        )


        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            keep_attrs=["id", "class"],
            keep_data_attributes=True,
            delay_before_return_html=2,
            extraction_strategy=llm_extraction_strategy,
            css_selector="div#search",
        )

        result : CrawlResult = await crawler.arun(
            url="https://www.google.com/search?q=apple%20inc&start=0&num=10",
            config=crawl_config,
        )
    
        search_result = {}
        if result.success:
            search_result = json.loads(result.extracted_content)

            # save search result to file
            with open(__current_dir / "search_result_using_llm.json", "w") as f:
                f.write(json.dumps(search_result, indent=4))
            print(json.dumps(search_result, indent=4)) 

    finally:
        await crawler.close()

# Example of using CrawlerHub
async def schema_generator():
    print("Generating schema")
    html = ""

    # Load html from file
    with open(__current_dir / "google_search_item.html", "r") as f:
        html = f.read()
    
    organic_schema = JsonCssExtractionStrategy.generate_schema(
            html=html,
            target_json_example="""{
                "title": "...",
                "link": "...",
                "snippet": "...",
                "date": "1 hour ago",
                "sitelinks": [
                    {
                        "title": "...",
                        "link": "..."
                    }
                ]
            }""",
            query="""The given HTML is the crawled HTML from the Google search result, which refers to one HTML element representing one organic Google search result. Please find the schema for the organic search item based on the given HTML. I am interested in the title, link, snippet text, sitelinks, and date.""",
        )
    
    print(json.dumps(organic_schema, indent=4))    
    pass

# Golden Standard
async def build_schema(html:str, force: bool = False) -> Dict[str, Any]:
    print("Building schema")
    schemas = {}
    if (__current_dir / "organic_schema.json").exists() and not force:
        with open(__current_dir / "organic_schema.json", "r") as f:
            schemas["organic"] = json.loads(f.read())
    else:        
        # Extract schema from html
        organic_schema = JsonCssExtractionStrategy.generate_schema(
            html=html,
            target_json_example="""{
                "title": "...",
                "link": "...",
                "snippet": "...",
                "date": "1 hour ago",
                "sitelinks": [
                    {
                        "title": "...",
                        "link": "..."
                    }
                ]
            }""",
            query="""The given html is the crawled html from Google search result. Please find the schema for organic search item in the given html, I am interested in title, link, snippet text, sitelinks and date. Usually they are all inside a div#search.""",
        )

        # Save schema to file current_dir/organic_schema.json
        with open(__current_dir / "organic_schema.json", "w") as f:
            f.write(json.dumps(organic_schema, indent=4))
        
        schemas["organic"] = organic_schema    

    # Repeat the same for top_stories_schema
    if (__current_dir / "top_stories_schema.json").exists():
        with open(__current_dir / "top_stories_schema.json", "r") as f:
            schemas["top_stories"] = json.loads(f.read())
    else:
        top_stories_schema = JsonCssExtractionStrategy.generate_schema(
            html=html,
            target_json_example="""{
            "title": "...",
            "link": "...",
            "source": "Insider Monkey",
            "date": "1 hour ago",
        }""",
            query="""The given HTML is the crawled HTML from the Google search result. Please find the schema for the Top Stories item in the given HTML. I am interested in the title, link, source, and date.""",
        )

        with open(__current_dir / "top_stories_schema.json", "w") as f:
            f.write(json.dumps(top_stories_schema, indent=4))
        
        schemas["top_stories"] = top_stories_schema

    # Repeat the same for suggested_queries_schema
    if (__current_dir / "suggested_queries_schema.json").exists():
        with open(__current_dir / "suggested_queries_schema.json", "r") as f:
            schemas["suggested_queries"] = json.loads(f.read())
    else:
        suggested_queries_schema = JsonCssExtractionStrategy.generate_schema(
            html=html,
            target_json_example="""{
            "query": "A for Apple",
        }""",
            query="""The given HTML contains the crawled HTML from Google search results. Please find the schema for each suggested query in the section "relatedSearches" at the bottom of the page. I am interested in the queries only.""",
        )

        with open(__current_dir / "suggested_queries_schema.json", "w") as f:
            f.write(json.dumps(suggested_queries_schema, indent=4))
        
        schemas["suggested_queries"] = suggested_queries_schema
    
    return schemas

async def search(q: str = "apple inc") -> Dict[str, Any]:
    print("Searching for:", q)

    browser_config = BrowserConfig(headless=True, verbose=True)
    crawler = AsyncWebCrawler(config=browser_config)
    search_result: Dict[str, List[Dict[str, Any]]] = {} 

    await crawler.start()
    try:
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            keep_attrs=["id", "class"],
            keep_data_attributes=True,
            delay_before_return_html=2,
        )
        from urllib.parse import quote
        result: CrawlResult = await crawler.arun(
            f"https://www.google.com/search?q={quote(q)}&start=0&num=10",
            config=crawl_config
        )

        if result.success:
            schemas : Dict[str, Any] = await build_schema(result.html)

            for schema in schemas.values():
                schema_key = schema["name"].lower().replace(' ', '_')
                search_result[schema_key] = JsonCssExtractionStrategy(
                    schema=schema
                ).run(
                    url="",
                    sections=[result.html],
                )

            # save search result to file
            with open(__current_dir / "search_result.json", "w") as f:
                f.write(json.dumps(search_result, indent=4))
            print(json.dumps(search_result, indent=4))        

    finally:
        await crawler.close()

    return search_result

# Example of using CrawlerHub
async def hub_example(query: str = "apple inc"):
    print("Using CrawlerHub")
    crawler_cls = CrawlerHub.get("google_search")
    crawler = crawler_cls()

    # Text search
    text_results = await crawler.run(
        query=query,
        search_type="text",  
        schema_cache_path="/Users/unclecode/.crawl4ai"
    )
    # Save search result to file
    with open(__current_dir / "search_result_using_hub.json", "w") as f:
        f.write(json.dumps(json.loads(text_results), indent=4))

    print(json.dumps(json.loads(text_results), indent=4))


async def demo():
    # Step 1: Introduction & Overview 
    await little_hello_web()
    await hello_web()

    # Step 2: Demo end result, using hub
     await hub_example()

    # Step 3: Using LLm for extraction
     await extract_using_llm()

    # Step 4: GEt familiar with schema generation
     await schema_generator()

    # Step 5: Golden Standard
     await search()


if __name__ == "__main__":
    asyncio.run(demo())
````



```markdown
# Examples for crawl4ai - `extraction` Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_extraction.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

This document provides a collection of runnable code examples demonstrating various features and configurations of the `extraction` component in the `crawl4ai` library.

## 1. Introduction to Extraction Strategies

### 1.1. Overview: Purpose of Extraction Strategies in Crawl4ai.

Extraction strategies in Crawl4ai are responsible for taking raw or processed content (like HTML or Markdown) and extracting structured data or specific blocks of information from it. This is crucial for transforming web content into a more usable format, often for feeding into Large Language Models (LLMs) or other data processing pipelines.

### 1.2. Example: Basic `CrawlerRunConfig` Setup with an `extraction_strategy`.

This example shows how to integrate an extraction strategy (here, `NoExtractionStrategy` for simplicity) into the `AsyncWebCrawler` workflow using `CrawlerRunConfig`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import NoExtractionStrategy

async def basic_config_with_extraction_strategy():
    # Initialize a simple extraction strategy
    no_extraction = NoExtractionStrategy()

    # Configure the crawler run to use this strategy
    run_config = CrawlerRunConfig(
        extraction_strategy=no_extraction
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="http://example.com",
            config=run_config
        )

        if result.success:
            print("Crawl successful.")
            # For NoExtractionStrategy, extracted_content will likely be None or empty
            print(f"Extracted Content: {result.extracted_content}")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(basic_config_with_extraction_strategy())
```
---

## 2. `NoExtractionStrategy`: Baseline (No Extraction)

The `NoExtractionStrategy` is a pass-through strategy. It doesn't perform any actual data extraction, meaning `result.extracted_content` will typically be `None` or an empty representation. It's useful as a baseline or when you only need the raw/cleaned HTML or Markdown.

### 2.1. Example: Using `NoExtractionStrategy` to demonstrate no structured data is extracted.

#### 2.1.1. Scenario: `AsyncWebCrawler` with `NoExtractionStrategy`.

This example demonstrates how `AsyncWebCrawler` behaves when `NoExtractionStrategy` is employed.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import NoExtractionStrategy
from crawl4ai.utils import HEADERS

async def no_extraction_with_crawler():
    no_extraction_strat = NoExtractionStrategy()
    
    # Provide a basic user agent
    browser_config = {"headers": HEADERS}
    
    run_config = CrawlerRunConfig(
        extraction_strategy=no_extraction_strat
    )

    async with AsyncWebCrawler(browser_config=browser_config) as crawler:
        result = await crawler.arun(
            url="http://example.com",
            config=run_config
        )

        if result.success:
            print(f"Crawled URL: {result.url}")
            print(f"Markdown content (first 100 chars): {result.markdown.raw_markdown[:100]}...")
            # Extracted content should be None or an empty representation
            print(f"Extracted Content: {result.extracted_content}") 
            assert result.extracted_content is None or len(result.extracted_content) == 0, \
                "Extracted content should be empty with NoExtractionStrategy"
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(no_extraction_with_crawler())
```

#### 2.1.2. Scenario: Direct call to `NoExtractionStrategy.extract()`.

You can also use extraction strategies directly if you have the content.

```python
from crawl4ai.extraction_strategy import NoExtractionStrategy

def direct_no_extraction():
    strategy = NoExtractionStrategy()
    sample_html = "<html><body><h1>Title</h1><p>Some text.</p></body></html>"
    
    # The 'extract' method might expect certain parameters like url, even if not used by this strategy
    extracted_data = strategy.extract(url="http://dummy.com", html_content=sample_html)
    
    print(f"Direct call to NoExtractionStrategy.extract() returned: {extracted_data}")
    # Expected: A list containing a dictionary with the original content, or similar passthrough
    # For NoExtractionStrategy, the behavior is to return a list of one block with the original content
    # if it's a simple string input. The actual structure might vary slightly based on internal logic.
    # The key is that no "structured" extraction happens.
    # Based on current implementation, it returns [{'index': 0, 'content': sample_html}]
    assert isinstance(extracted_data, list)
    assert len(extracted_data) == 1
    assert extracted_data[0]['content'] == sample_html


if __name__ == "__main__":
    direct_no_extraction()
```
---

## 3. `LLMExtractionStrategy`: LLM-Powered Structured Data Extraction

This is the primary strategy for extracting structured data using Large Language Models (LLMs). It allows you to define schemas (using Pydantic models or dictionaries) or provide natural language instructions to guide the LLM in extracting the desired information.

*Note: For the following examples, actual LLM calls are often mocked for brevity and to avoid requiring API keys for every example. In a real application, you would configure your LLM provider and API key.*

### 3.1. Core Concepts and Basic Usage

#### 3.1.1. Example: Basic initialization of `LLMExtractionStrategy` with default parameters.
This example shows how to initialize `LLMExtractionStrategy`. By default, it might use OpenAI if `OPENAI_API_KEY` is set. For this example, we'll assume mocking or a local LLM setup if no API key is found.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os

# Basic initialization - defaults to OpenAI if OPENAI_API_KEY is set,
# or you can specify a provider like Ollama.
try:
    # Attempt to use OpenAI if key is available
    llm_config = LLMConfig(api_token=os.environ.get("OPENAI_API_KEY"))
    if not llm_config.api_token:
        raise ValueError("OpenAI API key not found, using Ollama for example.")
    strategy = LLMExtractionStrategy(llm_config=llm_config)
    print("Initialized LLMExtractionStrategy with default provider (likely OpenAI).")
except Exception as e:
    print(f"OpenAI init failed ({e}), trying Ollama (make sure Ollama is running with a model like 'llama3').")
    try:
        # Fallback to Ollama if OpenAI key is not set or fails
        # Ensure Ollama is running and has a model like 'llama3'
        ollama_config = LLMConfig(provider="ollama/llama3", api_token="ollama") 
        strategy = LLMExtractionStrategy(llm_config=ollama_config)
        print("Initialized LLMExtractionStrategy with Ollama (llama3).")
    except Exception as e_ollama:
        print(f"Ollama init also failed: {e_ollama}")
        print("Please set up an LLM (OpenAI API key or local Ollama) for these examples.")
        strategy = None

if strategy:
    print(f"Strategy initialized. Provider: {strategy.llm_config.provider}")
    # You can now use this 'strategy' object for extraction.
    # For a basic initialization, we won't run an extraction here to keep it simple.
```

#### 3.1.2. Example: Direct usage of `LLMExtractionStrategy.extract()` with simple Markdown content.
This shows how to use the strategy directly with some Markdown text. We'll mock the LLM call.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mocking the LLM call
mock_llm_response_block = MagicMock()
mock_llm_response_block.choices = [MagicMock()]
mock_llm_response_block.choices[0].message.content = """
<blocks>
  <block>
    <content>This is the main title.</content>
    <tags><tag>title</tag></tags>
  </block>
  <block>
    <content>An introductory paragraph about the topic.</content>
    <tags><tag>introduction</tag></tags>
  </block>
</blocks>
"""
mock_llm_response_block.usage = MagicMock()
mock_llm_response_block.usage.completion_tokens = 20
mock_llm_response_block.usage.prompt_tokens = 50
mock_llm_response_block.usage.total_tokens = 70
mock_llm_response_block.usage.completion_tokens_details = {}
mock_llm_response_block.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_block)
def direct_markdown_extraction(mock_perform_completion):
    # For this example, we assume Ollama is running or an API key is set for another provider
    try:
        strategy = LLMExtractionStrategy(llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"))
    except:
        print("Ollama not available, skipping direct markdown extraction test. Ensure Ollama is running.")
        return

    sample_markdown = """
# Main Title
An introductory paragraph about the topic.
## Subheading
More details here.
"""
    # Default extraction_type is "block"
    extracted_data = strategy.extract(url="http://dummy.com/markdown", html_content=sample_markdown)
    
    print("Direct Markdown Extraction (mocked LLM):")
    print(json.dumps(extracted_data, indent=2))
    
    # Verify mock was called
    assert mock_perform_completion.called

if __name__ == "__main__":
    direct_markdown_extraction()
```

#### 3.1.3. Example: Direct usage of `LLMExtractionStrategy.extract()` with simple HTML content (`input_format="html"`).
This example demonstrates processing HTML content by specifying `input_format="html"`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mocking the LLM call (similar to above)
mock_llm_response_html_block = MagicMock()
mock_llm_response_html_block.choices = [MagicMock()]
mock_llm_response_html_block.choices[0].message.content = """
<blocks>
  <block>
    <content>HTML Title</content>
    <tags><tag>h1</tag><tag>title</tag></tags>
  </block>
  <block>
    <content>This is paragraph text from HTML.</content>
    <tags><tag>p</tag><tag>content</tag></tags>
  </block>
</blocks>
"""
mock_llm_response_html_block.usage = MagicMock() # Assuming same usage structure
mock_llm_response_html_block.usage.completion_tokens = 25
mock_llm_response_html_block.usage.prompt_tokens = 60
mock_llm_response_html_block.usage.total_tokens = 85
mock_llm_response_html_block.usage.completion_tokens_details = {}
mock_llm_response_html_block.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_html_block)
def direct_html_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="html"  # Specify that the input is HTML
        )
    except:
        print("Ollama not available, skipping direct HTML extraction test.")
        return

    sample_html = "<html><body><h1>HTML Title</h1><p>This is paragraph text from HTML.</p><div><p>Another paragraph.</p></div></body></html>"
    
    extracted_data = strategy.extract(url="http://dummy.com/html", html_content=sample_html)
    
    print("Direct HTML Extraction (mocked LLM, input_format='html'):")
    print(json.dumps(extracted_data, indent=2))
    assert mock_perform_completion.called

if __name__ == "__main__":
    direct_html_extraction()
```
---

### 3.2. Schema Definition for Extraction

#### 3.2.1. **Using Pydantic Models for Schema:**

##### 3.2.1.1. Example: Defining a simple Pydantic model and extracting data matching it (`extraction_type="schema"`).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json

class SimpleItem(BaseModel):
    name: str
    description: str

# Mock LLM response to return JSON matching SimpleItem
mock_llm_response_simple_schema = MagicMock()
mock_llm_response_simple_schema.choices = [MagicMock()]
mock_llm_response_simple_schema.choices[0].message.content = json.dumps({
    "name": "My Item",
    "description": "A simple description."
})
mock_llm_response_simple_schema.usage = MagicMock() # Populate usage as needed
mock_llm_response_simple_schema.usage.completion_tokens = 15
mock_llm_response_simple_schema.usage.prompt_tokens = 70
mock_llm_response_simple_schema.usage.total_tokens = 85
mock_llm_response_simple_schema.usage.completion_tokens_details = {}
mock_llm_response_simple_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_simple_schema)
def pydantic_simple_schema_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=SimpleItem.model_json_schema(), # Pass the Pydantic schema
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic simple schema test.")
        return

    sample_content = "The item is called My Item. It has a simple description."
    # For schema extraction, html_content is passed as the context to the LLM
    extracted_json_string = strategy.extract(url="http://dummy.com/item", html_content=sample_content)
    
    print("Pydantic Simple Schema Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string) # The result is a JSON string
        print(json.dumps(extracted_data, indent=2))
        # Validate with Pydantic model
        item_instance = SimpleItem(**extracted_data)
        print(f"Validated Pydantic instance: {item_instance}")
    else:
        print("No data extracted.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    pydantic_simple_schema_extraction()
```

##### 3.2.1.2. Example: Pydantic model with various field types (str, int, bool, List).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from typing import List
from unittest.mock import patch, MagicMock
import json

class ComplexItem(BaseModel):
    name: str
    count: int
    is_active: bool
    tags: List[str]

# Mock LLM response
mock_llm_response_complex_schema = MagicMock()
mock_llm_response_complex_schema.choices = [MagicMock()]
mock_llm_response_complex_schema.choices[0].message.content = json.dumps({
    "name": "Complex Gadget",
    "count": 10,
    "is_active": True,
    "tags": ["tech", "gadget", "new"]
})
# ... (mock usage as before)
mock_llm_response_complex_schema.usage = MagicMock()
mock_llm_response_complex_schema.usage.completion_tokens = 30; mock_llm_response_complex_schema.usage.prompt_tokens = 100; mock_llm_response_complex_schema.usage.total_tokens = 130
mock_llm_response_complex_schema.usage.completion_tokens_details = {}; mock_llm_response_complex_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_complex_schema)
def pydantic_complex_schema_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=ComplexItem.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic complex schema test.")
        return

    sample_content = "Product: Complex Gadget. Stock: 10 units. Status: Active. Categories: tech, gadget, new."
    extracted_json_string = strategy.extract(url="http://dummy.com/gadget", html_content=sample_content)
    
    print("Pydantic Complex Schema Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        item_instance = ComplexItem(**extracted_data)
        print(f"Validated Pydantic instance: {item_instance}")
    else:
        print("No data extracted.")

if __name__ == "__main__":
    pydantic_complex_schema_extraction()
```

##### 3.2.1.3. Example: Pydantic model with `Optional` fields.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from typing import Optional
from unittest.mock import patch, MagicMock
import json

class OptionalItem(BaseModel):
    name: str
    description: Optional[str] = None # description is optional
    price: float

# Mock LLM response - sometimes description is present, sometimes not
mock_llm_response_optional_schema_1 = MagicMock()
mock_llm_response_optional_schema_1.choices = [MagicMock()]
mock_llm_response_optional_schema_1.choices[0].message.content = json.dumps({
    "name": "Basic Widget",
    "price": 9.99
    # description is omitted
})
# ... (mock usage)
mock_llm_response_optional_schema_1.usage = MagicMock()
mock_llm_response_optional_schema_1.usage.completion_tokens = 10; mock_llm_response_optional_schema_1.usage.prompt_tokens = 60; mock_llm_response_optional_schema_1.usage.total_tokens = 70
mock_llm_response_optional_schema_1.usage.completion_tokens_details = {}; mock_llm_response_optional_schema_1.usage.prompt_tokens_details = {}


mock_llm_response_optional_schema_2 = MagicMock()
mock_llm_response_optional_schema_2.choices = [MagicMock()]
mock_llm_response_optional_schema_2.choices[0].message.content = json.dumps({
    "name": "Advanced Widget",
    "description": "This one has all the bells and whistles.",
    "price": 29.99
})
# ... (mock usage)
mock_llm_response_optional_schema_2.usage = MagicMock()
mock_llm_response_optional_schema_2.usage.completion_tokens = 20; mock_llm_response_optional_schema_2.usage.prompt_tokens = 70; mock_llm_response_optional_schema_2.usage.total_tokens = 90
mock_llm_response_optional_schema_2.usage.completion_tokens_details = {}; mock_llm_response_optional_schema_2.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff')
def pydantic_optional_schema_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=OptionalItem.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic optional schema test.")
        return

    sample_content_1 = "Item: Basic Widget, Price: $9.99."
    sample_content_2 = "Item: Advanced Widget, Price: $29.99. Description: This one has all the bells and whistles."

    # Test case 1: Description missing
    mock_perform_completion.return_value = mock_llm_response_optional_schema_1
    extracted_json_string_1 = strategy.extract(url="http://dummy.com/widget1", html_content=sample_content_1)
    print("Pydantic Optional Schema (description missing, mocked LLM):")
    if extracted_json_string_1:
        extracted_data_1 = json.loads(extracted_json_string_1)
        print(json.dumps(extracted_data_1, indent=2))
        item_instance_1 = OptionalItem(**extracted_data_1)
        print(f"Validated Pydantic instance 1: {item_instance_1}")
        assert item_instance_1.description is None

    # Test case 2: Description present
    mock_perform_completion.return_value = mock_llm_response_optional_schema_2
    extracted_json_string_2 = strategy.extract(url="http://dummy.com/widget2", html_content=sample_content_2)
    print("\nPydantic Optional Schema (description present, mocked LLM):")
    if extracted_json_string_2:
        extracted_data_2 = json.loads(extracted_json_string_2)
        print(json.dumps(extracted_data_2, indent=2))
        item_instance_2 = OptionalItem(**extracted_data_2)
        print(f"Validated Pydantic instance 2: {item_instance_2}")
        assert item_instance_2.description is not None

if __name__ == "__main__":
    pydantic_optional_schema_extraction()
```

##### 3.2.1.4. Example: Pydantic model with default values for fields.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from typing import Optional
from unittest.mock import patch, MagicMock
import json

class ItemWithDefaults(BaseModel):
    name: str
    status: str = "available" # Default value
    notes: Optional[str] = None

# Mock LLM - status might be omitted by LLM, Pydantic should use default
mock_llm_response_default_schema = MagicMock()
mock_llm_response_default_schema.choices = [MagicMock()]
mock_llm_response_default_schema.choices[0].message.content = json.dumps({
    "name": "Standard Item"
    # status is omitted, notes is omitted
})
# ... (mock usage)
mock_llm_response_default_schema.usage = MagicMock()
mock_llm_response_default_schema.usage.completion_tokens = 5; mock_llm_response_default_schema.usage.prompt_tokens = 50; mock_llm_response_default_schema.usage.total_tokens = 55
mock_llm_response_default_schema.usage.completion_tokens_details = {}; mock_llm_response_default_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_default_schema)
def pydantic_default_value_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=ItemWithDefaults.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic default value test.")
        return

    sample_content = "Product Name: Standard Item. Available for immediate shipping."
    extracted_json_string = strategy.extract(url="http://dummy.com/standard", html_content=sample_content)
    
    print("Pydantic Default Value Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(f"Raw LLM output (JSON): {json.dumps(extracted_data, indent=2)}")
        
        # Pydantic applies defaults during model instantiation
        item_instance = ItemWithDefaults(**extracted_data)
        print(f"Validated Pydantic instance: {item_instance}")
        print(f"Instance status (should be default): {item_instance.status}")
        assert item_instance.status == "available"

if __name__ == "__main__":
    pydantic_default_value_extraction()
```

#### 3.2.2. **Using Dictionaries for Schema:**

##### 3.2.2.1. Example: Defining a schema as a Python dictionary and extracting data (`extraction_type="schema"`).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Define schema as a dictionary (JSON Schema format)
product_schema_dict = {
    "type": "object",
    "properties": {
        "product_name": {"type": "string", "description": "Name of the product"},
        "price": {"type": "number", "description": "Price of the product"}
    },
    "required": ["product_name", "price"]
}

# Mock LLM response
mock_llm_response_dict_schema = MagicMock()
mock_llm_response_dict_schema.choices = [MagicMock()]
mock_llm_response_dict_schema.choices[0].message.content = json.dumps({
    "product_name": "Dictionary Product",
    "price": 49.95
})
# ... (mock usage)
mock_llm_response_dict_schema.usage = MagicMock()
mock_llm_response_dict_schema.usage.completion_tokens = 18; mock_llm_response_dict_schema.usage.prompt_tokens = 80; mock_llm_response_dict_schema.usage.total_tokens = 98
mock_llm_response_dict_schema.usage.completion_tokens_details = {}; mock_llm_response_dict_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_dict_schema)
def dictionary_schema_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=product_schema_dict, # Pass the dictionary schema
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping dictionary schema test.")
        return

    sample_content = "Check out the Dictionary Product, only $49.95!"
    extracted_json_string = strategy.extract(url="http://dummy.com/dictprod", html_content=sample_content)
    
    print("Dictionary Schema Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        assert "product_name" in extracted_data
        assert "price" in extracted_data
    else:
        print("No data extracted.")

if __name__ == "__main__":
    dictionary_schema_extraction()
```

#### 3.2.3. **Nested Schemas:**

##### 3.2.3.1. Example: Using a Pydantic model with nested Pydantic models as fields.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from typing import List
from unittest.mock import patch, MagicMock
import json

class Author(BaseModel):
    name: str
    email: Optional[str] = None

class Article(BaseModel):
    title: str
    author_details: Author # Nested Pydantic model
    tags: List[str]

# Mock LLM response
mock_llm_response_nested_schema = MagicMock()
mock_llm_response_nested_schema.choices = [MagicMock()]
mock_llm_response_nested_schema.choices[0].message.content = json.dumps({
    "title": "The Future of AI",
    "author_details": {"name": "Dr. AI Expert", "email": "ai@example.com"},
    "tags": ["AI", "ML", "Future"]
})
# ... (mock usage)
mock_llm_response_nested_schema.usage = MagicMock()
mock_llm_response_nested_schema.usage.completion_tokens = 40; mock_llm_response_nested_schema.usage.prompt_tokens = 120; mock_llm_response_nested_schema.usage.total_tokens = 160
mock_llm_response_nested_schema.usage.completion_tokens_details = {}; mock_llm_response_nested_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_nested_schema)
def pydantic_nested_schema_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=Article.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic nested schema test.")
        return

    sample_content = "Article: The Future of AI by Dr. AI Expert (ai@example.com). Tags: AI, ML, Future."
    extracted_json_string = strategy.extract(url="http://dummy.com/article", html_content=sample_content)
    
    print("Pydantic Nested Schema Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        article_instance = Article(**extracted_data)
        print(f"Validated Pydantic instance: {article_instance}")
        assert article_instance.author_details.name == "Dr. AI Expert"
    else:
        print("No data extracted.")

if __name__ == "__main__":
    pydantic_nested_schema_extraction()
```

##### 3.2.3.2. Example: Extracting a list of Pydantic model instances (e.g., list of products).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel, Field
from typing import List
from unittest.mock import patch, MagicMock
import json

class Product(BaseModel):
    name: str
    price: float

class ProductList(BaseModel):
    products: List[Product] = Field(description="A list of products found on the page")


# Mock LLM response
mock_llm_response_list_schema = MagicMock()
mock_llm_response_list_schema.choices = [MagicMock()]
mock_llm_response_list_schema.choices[0].message.content = json.dumps({
    "products": [
        {"name": "Laptop Pro", "price": 1200.00},
        {"name": "Wireless Mouse", "price": 25.00},
        {"name": "Keyboard", "price": 75.00}
    ]
})
# ... (mock usage)
mock_llm_response_list_schema.usage = MagicMock()
mock_llm_response_list_schema.usage.completion_tokens = 50; mock_llm_response_list_schema.usage.prompt_tokens = 150; mock_llm_response_list_schema.usage.total_tokens = 200
mock_llm_response_list_schema.usage.completion_tokens_details = {}; mock_llm_response_list_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_list_schema)
def pydantic_list_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=ProductList.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping Pydantic list extraction test.")
        return

    sample_content = """
    Available products:
    1. Laptop Pro - $1200.00
    2. Wireless Mouse - $25.00
    3. Mechanical Keyboard - $75.00
    """
    extracted_json_string = strategy.extract(url="http://dummy.com/products", html_content=sample_content)
    
    print("Pydantic List Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        product_list_instance = ProductList(**extracted_data)
        print(f"Validated Pydantic instance: {product_list_instance}")
        assert len(product_list_instance.products) == 3
        assert product_list_instance.products[0].name == "Laptop Pro"
    else:
        print("No data extracted.")

if __name__ == "__main__":
    pydantic_list_extraction()
```

#### 3.2.4. **Dynamic Schema Generation (Advanced):**

##### 3.2.4.1. Example: Programmatically generating a Pydantic model for the schema at runtime.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import create_model, BaseModel
from typing import List, Type
from unittest.mock import patch, MagicMock
import json

# Mock LLM response
mock_llm_response_dynamic_schema = MagicMock()
mock_llm_response_dynamic_schema.choices = [MagicMock()]
mock_llm_response_dynamic_schema.choices[0].message.content = json.dumps({
    "user_id": "user123",
    "username": "john_doe",
    "is_premium_member": True
})
# ... (mock usage)
mock_llm_response_dynamic_schema.usage = MagicMock()
mock_llm_response_dynamic_schema.usage.completion_tokens = 20; mock_llm_response_dynamic_schema.usage.prompt_tokens = 90; mock_llm_response_dynamic_schema.usage.total_tokens = 110
mock_llm_response_dynamic_schema.usage.completion_tokens_details = {}; mock_llm_response_dynamic_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_dynamic_schema)
def dynamic_schema_generation_extraction(mock_perform_completion):
    # Define fields dynamically
    fields_to_extract = {
        "user_id": (str, ...),  # '...' means required
        "username": (str, ...),
        "is_premium_member": (bool, False) # Optional with default
    }
    
    # Create Pydantic model dynamically
    DynamicUserModel: Type[BaseModel] = create_model(
        'DynamicUserModel',
        **fields_to_extract
    )

    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=DynamicUserModel.model_json_schema(),
            extraction_type="schema"
        )
    except:
        print("Ollama not available, skipping dynamic schema test.")
        return

    sample_content = "User ID: user123, Username: john_doe, Premium: Yes"
    extracted_json_string = strategy.extract(url="http://dummy.com/userprofile", html_content=sample_content)
    
    print("Dynamic Schema Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        user_instance = DynamicUserModel(**extracted_data)
        print(f"Validated Dynamic Pydantic instance: {user_instance}")
        assert user_instance.username == "john_doe"
    else:
        print("No data extracted.")

if __name__ == "__main__":
    dynamic_schema_generation_extraction()
```
---

### 3.3. Instruction Customization

#### 3.3.1. Example: Using `extraction_type="block"` with a custom `instruction` to guide block extraction (e.g., "Extract main paragraphs about AI").

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM response
mock_llm_response_block_instr = MagicMock()
mock_llm_response_block_instr.choices = [MagicMock()]
mock_llm_response_block_instr.choices[0].message.content = """
<blocks>
  <block>
    <content>Artificial intelligence is rapidly evolving.</content>
    <tags><tag>AI_paragraph</tag></tags>
  </block>
  <block>
    <content>It impacts various industries.</content>
    <tags><tag>AI_paragraph</tag></tags>
  </block>
</blocks>
"""
# ... (mock usage)
mock_llm_response_block_instr.usage = MagicMock()
mock_llm_response_block_instr.usage.completion_tokens = 30; mock_llm_response_block_instr.usage.prompt_tokens = 100; mock_llm_response_block_instr.usage.total_tokens = 130
mock_llm_response_block_instr.usage.completion_tokens_details = {}; mock_llm_response_block_instr.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_block_instr)
def block_extraction_with_instruction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="block",
            instruction="Extract only the main paragraphs discussing Artificial Intelligence. Ignore other sections."
        )
    except:
        print("Ollama not available, skipping block extraction with instruction test.")
        return

    sample_content = """
# The Future of Computing
This is an intro.
## Artificial Intelligence
Artificial intelligence is rapidly evolving. It impacts various industries.
## unrelated section
This is not about AI.
"""
    extracted_data = strategy.extract(url="http://dummy.com/ai_article", html_content=sample_content)
    
    print("Block Extraction with Custom Instruction (mocked LLM):")
    print(json.dumps(extracted_data, indent=2))
    if extracted_data:
        assert len(extracted_data) == 2
        assert "Artificial intelligence" in extracted_data[0]["content"]

if __name__ == "__main__":
    block_extraction_with_instruction()
```

#### 3.3.2. Example: Using `extraction_type="schema"` with a Pydantic schema and a guiding `instruction` (e.g., "Extract product details according to the schema. Focus on electronics.").

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json

class ProductSchema(BaseModel):
    productName: str
    category: str
    price: Optional[float] = None

# Mock LLM response
mock_llm_response_schema_instr = MagicMock()
mock_llm_response_schema_instr.choices = [MagicMock()]
mock_llm_response_schema_instr.choices[0].message.content = json.dumps({
    "productName": "Smart TV",
    "category": "Electronics",
    "price": 499.99
})
# ... (mock usage)
mock_llm_response_schema_instr.usage = MagicMock()
mock_llm_response_schema_instr.usage.completion_tokens = 25; mock_llm_response_schema_instr.usage.prompt_tokens = 110; mock_llm_response_schema_instr.usage.total_tokens = 135
mock_llm_response_schema_instr.usage.completion_tokens_details = {}; mock_llm_response_schema_instr.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_schema_instr)
def schema_extraction_with_instruction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=ProductSchema.model_json_schema(),
            extraction_type="schema",
            instruction="Extract product details. Prioritize items in the 'Electronics' category if multiple products are mentioned."
        )
    except:
        print("Ollama not available, skipping schema extraction with instruction test.")
        return
        
    sample_content = "We sell books, clothes, and a Smart TV for $499.99. The Smart TV is an electronic device."
    extracted_json_string = strategy.extract(url="http://dummy.com/store", html_content=sample_content)
    
    print("Schema Extraction with Instruction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        product_instance = ProductSchema(**extracted_data)
        assert product_instance.category == "Electronics"
    else:
        print("No data extracted.")

if __name__ == "__main__":
    schema_extraction_with_instruction()
```

#### 3.3.3. Example: Using `extraction_type="schema_from_instruction"` where the LLM infers the schema from a detailed `instruction` (e.g., "Extract the title, author, and publication date of the article.").
This powerful feature lets the LLM decide the schema based on your textual request.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM response - LLM invents the schema and fills it
mock_llm_response_infer_schema = MagicMock()
mock_llm_response_infer_schema.choices = [MagicMock()]
mock_llm_response_infer_schema.choices[0].message.content = json.dumps({
    "title": "Adventures in AI",
    "author": "Jane Coder",
    "publication_date": "2024-05-15"
})
# ... (mock usage)
mock_llm_response_infer_schema.usage = MagicMock()
mock_llm_response_infer_schema.usage.completion_tokens = 30; mock_llm_response_infer_schema.usage.prompt_tokens = 90; mock_llm_response_infer_schema.usage.total_tokens = 120
mock_llm_response_infer_schema.usage.completion_tokens_details = {}; mock_llm_response_infer_schema.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_infer_schema)
def schema_from_instruction_extraction(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="schema_from_instruction",
            instruction="Please extract the title, author, and publication date (YYYY-MM-DD) of this news article."
        )
    except:
        print("Ollama not available, skipping schema_from_instruction test.")
        return

    sample_content = """
    # Adventures in AI
    By Jane Coder, Published on May 15, 2024.
    This article explores the latest trends...
    """
    extracted_json_string = strategy.extract(url="http://dummy.com/news_article", html_content=sample_content)
    
    print("Schema from Instruction Extraction (mocked LLM):")
    if extracted_json_string:
        extracted_data = json.loads(extracted_json_string)
        print(json.dumps(extracted_data, indent=2))
        assert "title" in extracted_data and "author" in extracted_data and "publication_date" in extracted_data
    else:
        print("No data extracted.")

if __name__ == "__main__":
    schema_from_instruction_extraction()
```

#### 3.3.4. Example: Comparing outputs with and without a specific `instruction` when using `extraction_type="schema"`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json

class Event(BaseModel):
    eventName: str
    location: str
    date: str

# Mock LLM responses
mock_llm_no_instr = MagicMock()
mock_llm_no_instr.choices = [MagicMock()]
mock_llm_no_instr.choices[0].message.content = json.dumps({"eventName": "Tech Meetup", "location": "Online", "date": "2024-06-01"})
mock_llm_no_instr.usage = MagicMock(); mock_llm_no_instr.usage.completion_tokens = 20; mock_llm_no_instr.usage.prompt_tokens = 80; mock_llm_no_instr.usage.total_tokens = 100
mock_llm_no_instr.usage.completion_tokens_details = {}; mock_llm_no_instr.usage.prompt_tokens_details = {}


mock_llm_with_instr = MagicMock()
mock_llm_with_instr.choices = [MagicMock()]
mock_llm_with_instr.choices[0].message.content = json.dumps({"eventName": "AI Conference", "location": "San Francisco", "date": "2024-07-20"})
mock_llm_with_instr.usage = MagicMock(); mock_llm_with_instr.usage.completion_tokens = 22; mock_llm_with_instr.usage.prompt_tokens = 95; mock_llm_with_instr.usage.total_tokens = 117
mock_llm_with_instr.usage.completion_tokens_details = {}; mock_llm_with_instr.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff')
def schema_extraction_instruction_comparison(mock_perform_completion):
    try:
        llm_conf = LLMConfig(provider="ollama/llama3", api_token="ollama")
    except:
        print("Ollama not available, skipping schema instruction comparison test.")
        return

    sample_content = """
    Upcoming Events:
    - Tech Meetup, Online, June 1st, 2024
    - AI Conference, San Francisco, July 20th, 2024
    - Local Bake Sale, Town Hall, June 5th, 2024
    """

    # Case 1: No specific instruction
    mock_perform_completion.return_value = mock_llm_no_instr
    strategy_no_instr = LLMExtractionStrategy(
        llm_config=llm_conf,
        schema=Event.model_json_schema(),
        extraction_type="schema"
    )
    result_no_instr_json = strategy_no_instr.extract(url="http://dummy.com/events", html_content=sample_content)
    result_no_instr = json.loads(result_no_instr_json) if result_no_instr_json else {}
    print(f"Without specific instruction: {result_no_instr}")

    # Case 2: With instruction to focus
    mock_perform_completion.return_value = mock_llm_with_instr
    strategy_with_instr = LLMExtractionStrategy(
        llm_config=llm_conf,
        schema=Event.model_json_schema(),
        extraction_type="schema",
        instruction="Focus on extracting details for the 'AI Conference'."
    )
    result_with_instr_json = strategy_with_instr.extract(url="http://dummy.com/events", html_content=sample_content)
    result_with_instr = json.loads(result_with_instr_json) if result_with_instr_json else {}
    print(f"With instruction to focus on AI Conference: {result_with_instr}")

    assert result_no_instr.get("eventName") == "Tech Meetup" # Mock returns first one
    assert result_with_instr.get("eventName") == "AI Conference" # Mock returns AI conf due to instruction

if __name__ == "__main__":
    schema_extraction_instruction_comparison()
```
---

### 3.4. Controlling `extraction_type`

#### 3.4.1. Example: Demonstrating `extraction_type="block"` - output structure (list of blocks with content and tags).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM response for block extraction
mock_llm_block_output = MagicMock()
mock_llm_block_output.choices = [MagicMock()]
mock_llm_block_output.choices[0].message.content = """
<blocks>
  <block>
    <content>First important point.</content>
    <tags><tag>key_takeaway</tag></tags>
  </block>
  <block>
    <content>Second supporting detail.</content>
    <tags><tag>detail</tag><tag>supporting_info</tag></tags>
  </block>
</blocks>
"""
mock_llm_block_output.usage = MagicMock(); mock_llm_block_output.usage.completion_tokens=30; mock_llm_block_output.usage.prompt_tokens=70; mock_llm_block_output.usage.total_tokens=100
mock_llm_block_output.usage.completion_tokens_details = {}; mock_llm_block_output.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_block_output)
def demonstrate_block_extraction_type(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="block" # Explicitly set to block
        )
    except:
        print("Ollama not available, skipping block extraction type demo.")
        return

    sample_content = "Some text with a First important point and then a Second supporting detail."
    extracted_data = strategy.extract(url="http://dummy.com/blocks", html_content=sample_content)
    
    print("Block Extraction Output Structure (mocked LLM):")
    print(json.dumps(extracted_data, indent=2))
    
    # Expected output is a list of dictionaries (blocks)
    assert isinstance(extracted_data, list)
    if extracted_data:
        assert "content" in extracted_data[0]
        assert "tags" in extracted_data[0]
        assert isinstance(extracted_data[0]["tags"], list)

if __name__ == "__main__":
    demonstrate_block_extraction_type()
```

#### 3.4.2. Example: Demonstrating `extraction_type="schema"` - output structure (JSON string matching the schema).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json

class MyData(BaseModel):
    field1: str
    field2: int

# Mock LLM response
mock_llm_schema_output = MagicMock()
mock_llm_schema_output.choices = [MagicMock()]
mock_llm_schema_output.choices[0].message.content = json.dumps({"field1": "value1", "field2": 123})
mock_llm_schema_output.usage = MagicMock(); mock_llm_schema_output.usage.completion_tokens=15; mock_llm_schema_output.usage.prompt_tokens=60; mock_llm_schema_output.usage.total_tokens=75
mock_llm_schema_output.usage.completion_tokens_details = {}; mock_llm_schema_output.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_schema_output)
def demonstrate_schema_extraction_type(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            schema=MyData.model_json_schema(),
            extraction_type="schema" # Explicitly set to schema
        )
    except:
        print("Ollama not available, skipping schema extraction type demo.")
        return

    sample_content = "Field one is value1 and field two is 123."
    extracted_json_string = strategy.extract(url="http://dummy.com/schema_data", html_content=sample_content)
    
    print("Schema Extraction Output Structure (mocked LLM):")
    print(f"Raw JSON string from LLM: {extracted_json_string}")
    
    # Expected output is a JSON string that can be parsed into the schema
    if extracted_json_string:
        data = json.loads(extracted_json_string)
        print(f"Parsed data: {data}")
        instance = MyData(**data) # Validate with Pydantic
        assert instance.field1 == "value1"

if __name__ == "__main__":
    demonstrate_schema_extraction_type()
```

#### 3.4.3. Example: Demonstrating `extraction_type="schema_from_instruction"` - output structure (JSON string based on inferred schema).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM response - LLM infers schema and provides data
mock_llm_infer_schema_output = MagicMock()
mock_llm_infer_schema_output.choices = [MagicMock()]
mock_llm_infer_schema_output.choices[0].message.content = json.dumps({
    "book_title": "The LLM Handbook",
    "pages": 300
})
mock_llm_infer_schema_output.usage = MagicMock(); mock_llm_infer_schema_output.usage.completion_tokens=20; mock_llm_infer_schema_output.usage.prompt_tokens=70; mock_llm_infer_schema_output.usage.total_tokens=90
mock_llm_infer_schema_output.usage.completion_tokens_details = {}; mock_llm_infer_schema_output.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_infer_schema_output)
def demonstrate_schema_from_instruction_type(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="schema_from_instruction",
            instruction="Extract the book title and number of pages."
        )
    except:
        print("Ollama not available, skipping schema_from_instruction type demo.")
        return

    sample_content = "The book 'The LLM Handbook' contains 300 pages of valuable insights."
    extracted_json_string = strategy.extract(url="http://dummy.com/book_info", html_content=sample_content)
    
    print("Schema from Instruction Output Structure (mocked LLM):")
    print(f"Raw JSON string from LLM: {extracted_json_string}")
    
    if extracted_json_string:
        data = json.loads(extracted_json_string)
        print(f"Parsed data: {data}")
        assert "book_title" in data and "pages" in data

if __name__ == "__main__":
    demonstrate_schema_from_instruction_type()
```
---

### 3.5. LLM Configuration (`llm_config`)

#### 3.5.1. Example: Using the default LLM provider and model.
This assumes `OPENAI_API_KEY` is set in the environment, as OpenAI is often a default. If not, it will error or fallback if a global default is set elsewhere (less common for `LLMExtractionStrategy` without explicit config).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig # For explicit configuration if needed
import os

# Note: For this example to run successfully without explicit llm_config, 
# the environment variable OPENAI_API_KEY should be set, or another
# global default LLM provider must be configured for LiteLLM.
# If neither is true, LLMExtractionStrategy() might raise an error.

try:
    # This will try to use the default provider (often OpenAI if key is set)
    # or another globally configured default for LiteLLM.
    strategy_default = LLMExtractionStrategy() 
    print(f"Successfully initialized LLMExtractionStrategy with default provider: {strategy_default.llm_config.provider}")
    # To make this example runnable without a real API call:
    print("Note: Actual extraction would require a configured LLM and API key.")
except Exception as e:
    print(f"Failed to initialize with default LLM provider: {e}")
    print("This example requires a default LLM (e.g., OPENAI_API_KEY set) or LiteLLM global config.")
    print("Alternatively, provide an explicit LLMConfig to LLMExtractionStrategy.")

# To show an explicit (but still default-targeting) configuration:
# if os.getenv("OPENAI_API_KEY"):
#     llm_config_openai = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY"))
#     strategy_explicit_openai = LLMExtractionStrategy(llm_config=llm_config_openai)
#     print(f"Initialized explicitly with OpenAI: {strategy_explicit_openai.llm_config.provider}")
# else:
#     print("OPENAI_API_KEY not set, cannot show explicit OpenAI example.")
```

#### 3.5.2. Example: Configuring `LLMExtractionStrategy` with a specific OpenAI model via `LLMConfig` (e.g., `openai/gpt-4o-mini`).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os

# Ensure you have your OPENAI_API_KEY set in your environment variables
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    print("OPENAI_API_KEY not found in environment. Skipping OpenAI example.")
    print("To run this, set your OPENAI_API_KEY.")
else:
    llm_config_openai = LLMConfig(
        provider="openai/gpt-4o-mini", # Specify the OpenAI model
        api_token=openai_api_key
    )
    strategy_openai = LLMExtractionStrategy(llm_config=llm_config_openai)
    print(f"Initialized LLMExtractionStrategy with OpenAI provider: {strategy_openai.llm_config.provider}")
    # To test, you would call strategy_openai.extract(...)
    # For this example, we'll just show initialization.
    print("Strategy ready to use OpenAI gpt-4o-mini.")
```

#### 3.5.3. Example: Configuring `LLMExtractionStrategy` with a specific Ollama model via `LLMConfig` (e.g., `ollama/llama3`).
This requires Ollama to be running locally and the specified model (e.g., `llama3`) to be pulled.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig

# Assumes Ollama is running locally and 'llama3' model is available
# 'api_token' for Ollama is typically 'ollama' or can be omitted if not needed by your setup.
try:
    llm_config_ollama = LLMConfig(
        provider="ollama/llama3", 
        api_token="ollama", # Often 'ollama' or can be None if not required by local setup
        base_url="http://localhost:11434" # Default Ollama API URL
    )
    strategy_ollama = LLMExtractionStrategy(llm_config=llm_config_ollama)
    print(f"Initialized LLMExtractionStrategy with Ollama provider: {strategy_ollama.llm_config.provider}")
    print("Strategy ready to use Ollama with llama3.")
    print("Note: For actual extraction, ensure Ollama server is running and has the 'llama3' model pulled.")
except Exception as e:
    print(f"Failed to initialize Ollama strategy: {e}")
    print("Ensure Ollama is running (e.g., `ollama serve`) and you have pulled the model (e.g., `ollama pull llama3`).")

# Example of a test call (would require Ollama to be active)
# from unittest.mock import patch, MagicMock
# import json
# mock_response = MagicMock()
# mock_response.choices = [MagicMock()]
# mock_response.choices[0].message.content = json.dumps({"info": "extracted by ollama"})
# mock_response.usage = MagicMock(completion_tokens=5, prompt_tokens=10, total_tokens=15)
# mock_response.usage.completion_tokens_details = {}; mock_response.usage.prompt_tokens_details = {}
# @patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_response)
# def _test_ollama_call(mock_call, strategy):
#     if strategy:
#         result = strategy.extract("url", "content", extraction_type="schema_from_instruction", instruction="get info")
#         print(f"Ollama mock call result: {result}")
# _test_ollama_call(None, strategy_ollama if 'strategy_ollama' in locals() else None)
```

#### 3.5.4. Example: Configuring `LLMExtractionStrategy` with a specific Gemini model via `LLMConfig`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os

gemini_api_key = os.getenv("GEMINI_API_KEY")

if not gemini_api_key:
    print("GEMINI_API_KEY not found in environment. Skipping Gemini example.")
    print("To run this, set your GEMINI_API_KEY.")
else:
    llm_config_gemini = LLMConfig(
        provider="gemini/gemini-1.5-pro-latest", # Or another Gemini model
        api_token=gemini_api_key
    )
    strategy_gemini = LLMExtractionStrategy(llm_config=llm_config_gemini)
    print(f"Initialized LLMExtractionStrategy with Gemini provider: {strategy_gemini.llm_config.provider}")
    print("Strategy ready to use Gemini.")
```

#### 3.5.5. Example: Configuring `LLMExtractionStrategy` with a specific Anthropic (Claude) model via `LLMConfig`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os

anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

if not anthropic_api_key:
    print("ANTHROPIC_API_KEY not found in environment. Skipping Anthropic Claude example.")
    print("To run this, set your ANTHROPIC_API_KEY.")
else:
    llm_config_claude = LLMConfig(
        provider="anthropic/claude-3-opus-20240229", # Or another Claude model
        api_token=anthropic_api_key
    )
    strategy_claude = LLMExtractionStrategy(llm_config=llm_config_claude)
    print(f"Initialized LLMExtractionStrategy with Anthropic provider: {strategy_claude.llm_config.provider}")
    print("Strategy ready to use Claude.")

```

#### 3.5.6. Example: Overriding LLM parameters like `temperature` and `max_tokens` using `LLMConfig`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os

openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    print("OPENAI_API_KEY not found. This example shows parameter override with OpenAI.")
else:
    llm_config_custom_params = LLMConfig(
        provider="openai/gpt-3.5-turbo", # Using a common model for this example
        api_token=openai_api_key,
        temperature=0.2,       # Lower temperature for more deterministic output
        max_tokens=150         # Limit the maximum number of tokens in the response
        # You can add other provider-specific parameters here in extra_args if needed
        # extra_args={"top_p": 0.9} 
    )
    
    strategy_custom_params = LLMExtractionStrategy(
        llm_config=llm_config_custom_params,
        instruction="Extract the main point." # A simple instruction
    )
    
    print(f"Initialized LLMExtractionStrategy with custom LLM parameters for provider: {strategy_custom_params.llm_config.provider}")
    print(f"  Temperature: {strategy_custom_params.llm_config.temperature}")
    print(f"  Max Tokens: {strategy_custom_params.llm_config.max_tokens}")
    # print(f"  Extra Args: {strategy_custom_params.extra_args}") # Note: extra_args on LLMExtractionStrategy, not LLMConfig for this
    
    # A mock test to show parameters would be passed to perform_completion_with_backoff
    # from unittest.mock import patch, MagicMock
    # mock_response = MagicMock() # ... (setup mock_response) ...
    # @patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_response)
    # def _test_params(mock_call):
    #     strategy_custom_params.extract("url", "content")
    #     called_args = mock_call.call_args
    #     assert called_args is not None
    #     assert called_args.kwargs.get('temperature') == 0.2
    #     assert called_args.kwargs.get('max_tokens') == 150
    #     print("LLM call would have used temperature=0.2 and max_tokens=150.")
    # _test_params()
```

#### 3.5.7. Example: Using `LLMConfig` to specify a custom `base_url` for a self-hosted LLM.
This is useful for local LLMs like Ollama (already shown), vLLM, or other self-hosted OpenAI-compatible endpoints.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig

# Example for a generic OpenAI-compatible API running locally
# The provider name might need to be "openai/custom-model-name" or just "custom/model-name"
# depending on how LiteLLM handles it. For Ollama, it's typically "ollama/model-name".
custom_llm_config = LLMConfig(
    provider="custom/my-local-model", # Or "openai/my-local-model"
    api_token="no_key_needed_for_local", # Or your actual local key
    base_url="http://localhost:8000/v1" # Adjust to your local LLM API endpoint
)

try:
    strategy_custom_endpoint = LLMExtractionStrategy(llm_config=custom_llm_config)
    print(f"Initialized LLMExtractionStrategy with custom endpoint:")
    print(f"  Provider: {strategy_custom_endpoint.llm_config.provider}")
    print(f"  Base URL: {strategy_custom_endpoint.llm_config.base_url}")
    print("Note: This example assumes an OpenAI-compatible API is running at the specified base_url.")
except Exception as e:
    print(f"Failed to initialize custom endpoint strategy: {e}")

# Example for Ollama (already covered more specifically, but fits here too)
ollama_local_config = LLMConfig(
    provider="ollama/mistral", # Assuming mistral model is pulled
    base_url="http://localhost:11434", # Default Ollama
    api_token="ollama" # Usually 'ollama' or None
)
try:
    strategy_ollama_local = LLMExtractionStrategy(llm_config=ollama_local_config)
    print(f"\nInitialized LLMExtractionStrategy with local Ollama endpoint:")
    print(f"  Provider: {strategy_ollama_local.llm_config.provider}")
    print(f"  Base URL: {strategy_ollama_local.llm_config.base_url}")
except Exception as e:
    print(f"Failed to initialize local Ollama strategy via custom base_url example: {e}")
```
---

### 3.6. Chunking Configuration (`apply_chunking` and related parameters)

#### 3.6.1. Example: Default chunking behavior (`apply_chunking=True`).
When content is long, `LLMExtractionStrategy` automatically chunks it.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM to be called multiple times if chunking happens
mock_responses = []
for i in range(3): # Simulate 3 chunks
    mock_resp = MagicMock()
    mock_resp.choices = [MagicMock()]
    mock_resp.choices[0].message.content = json.dumps({"chunk_data": f"Data from chunk {i+1}"})
    mock_resp.usage = MagicMock(completion_tokens=10, prompt_tokens=50, total_tokens=60)
    mock_resp.usage.completion_tokens_details = {}; mock_resp.usage.prompt_tokens_details = {}
    mock_responses.append(mock_resp)

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', side_effect=mock_responses)
def default_chunking_behavior(mock_perform_completion):
    try:
        # Use a small chunk_token_threshold to force chunking for the example
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="schema_from_instruction",
            instruction="Extract data from this chunk.",
            chunk_token_threshold=10, # Very small to ensure chunking
            word_token_rate=0.75, # Approx tokens per word
            apply_chunking=True # Default
        )
    except:
        print("Ollama not available, skipping default chunking test.")
        return

    # Create content long enough to be chunked based on threshold and word_token_rate
    # (10 tokens / 0.75 tokens/word) approx 13 words for threshold.
    # Let's use 50 words.
    long_content = " ".join(["word"] * 50) 
    
    extracted_data_json = strategy.extract(url="http://dummy.com/long_content", html_content=long_content)
    
    print("Default Chunking Behavior (mocked LLM):")
    # The strategy internally merges results from chunks if schema-based.
    # For "schema_from_instruction", it might return a list of JSON strings or a merged JSON.
    # Current LLMExtractionStrategy for schema type returns a single JSON string, implying merging.
    # If it's block extraction, it would be a list of blocks.
    # Let's assume for schema extraction, it returns a list of dicts before final JSON dump in the example
    
    # The mock setup implies multiple calls. LLMExtractionStrategy aggregates results.
    # If the LLM returns a list for each chunk, results would be concatenated.
    # If it returns a dict, they'd be in a list.
    # The current mock returns a dict per chunk, so we expect a list of dicts if not merged by strategy.
    # However, the `extract` method's return is a single JSON string if schema-based.
    # This means the strategy handles merging internally or expects LLM to handle it.
    # For this test, we'll check if the LLM was called multiple times.
    
    print(f"LLM called {mock_perform_completion.call_count} times.")
    assert mock_perform_completion.call_count > 1, "Chunking should have occurred, LLM expected to be called multiple times."
    print(f"Final extracted JSON string: {extracted_data_json}")
    # Final result depends on how LLM merges/formats if it gets multiple chunk results.
    # Assuming the mock's structure (list of chunk_data) would be presented as a list by the LLM.
    if extracted_data_json:
        final_data = json.loads(extracted_data_json)
        print(json.dumps(final_data, indent=2))
        # Depending on LLM's aggregation logic, this might be a list or a single dict.
        # For this example, if our mock returns individual dicts, and the LLM is asked to provide a final JSON,
        # it might wrap them in a list. Let's assume it does.
        # For this particular mock structure and schema_from_instruction, the LLM would typically be
        # instructed to return a list of items if the content implies multiple items.
        # Here, the mock returns individual dicts, and the strategy just returns the last one.
        # To properly test chunk aggregation, a more sophisticated mock or real LLM is needed.
        # For now, verifying multiple calls is the main goal.

if __name__ == "__main__":
    default_chunking_behavior()
```

#### 3.6.2. Example: Disabling chunking (`apply_chunking=False`) for short content.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

mock_llm_no_chunking = MagicMock()
mock_llm_no_chunking.choices = [MagicMock()]
mock_llm_no_chunking.choices[0].message.content = json.dumps({"summary": "This is short content."})
mock_llm_no_chunking.usage = MagicMock(completion_tokens=5, prompt_tokens=20, total_tokens=25)
mock_llm_no_chunking.usage.completion_tokens_details = {}; mock_llm_no_chunking.usage.prompt_tokens_details = {}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_no_chunking)
def disable_chunking_behavior(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            apply_chunking=False, # Explicitly disable chunking
            extraction_type="schema_from_instruction",
            instruction="Summarize this."
        )
    except:
        print("Ollama not available, skipping disable chunking test.")
        return

    # Content is short enough that chunking wouldn't happen anyway, but this forces it.
    short_content = "This is a piece of short content." 
    extracted_data_json = strategy.extract(url="http://dummy.com/short", html_content=short_content)
    
    print("Disabled Chunking Behavior (mocked LLM):")
    print(f"LLM called {mock_perform_completion.call_count} time(s).")
    assert mock_perform_completion.call_count == 1, "Chunking was disabled, LLM should be called once."
    if extracted_data_json:
        print(json.dumps(json.loads(extracted_data_json), indent=2))


if __name__ == "__main__":
    disable_chunking_behavior()
```

#### 3.6.3. Example: Customizing `chunk_token_threshold` for smaller/larger chunks.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock setup
mock_llm_response_chunk_size = MagicMock()
mock_llm_response_chunk_size.choices = [MagicMock()]
mock_llm_response_chunk_size.choices[0].message.content = json.dumps({"info": "some data"})
mock_llm_response_chunk_size.usage = MagicMock(completion_tokens=5, prompt_tokens=10, total_tokens=15)
mock_llm_response_chunk_size.usage.completion_tokens_details = {}; mock_llm_response_chunk_size.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_response_chunk_size)
def customize_chunk_token_threshold(mock_perform_completion):
    long_text = " ".join(["word_for_testing_chunk_size"] * 100) # Approx 100 words
    
    # Scenario 1: Small threshold, more chunks
    try:
        strategy_small_chunks = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            chunk_token_threshold=20, # Small threshold, e.g., ~26 words
            word_token_rate=0.75,
            extraction_type="schema_from_instruction", instruction="get info"
        )
    except:
        print("Ollama not available, cannot run small chunk test.")
        strategy_small_chunks = None

    if strategy_small_chunks:
        strategy_small_chunks.extract(url="http://dummy.com/small_chunks", html_content=long_text)
        print(f"Small chunk_token_threshold (20): LLM called {mock_perform_completion.call_count} times.")
        small_chunk_calls = mock_perform_completion.call_count
        mock_perform_completion.reset_mock() # Reset for next call
    else:
        small_chunk_calls = 0

    # Scenario 2: Larger threshold, fewer chunks
    try:
        strategy_large_chunks = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            chunk_token_threshold=80, # Larger threshold, e.g., ~106 words
            word_token_rate=0.75,
            extraction_type="schema_from_instruction", instruction="get info"
        )
    except:
        print("Ollama not available, cannot run large chunk test.")
        strategy_large_chunks = None

    if strategy_large_chunks:
        strategy_large_chunks.extract(url="http://dummy.com/large_chunks", html_content=long_text)
        print(f"Large chunk_token_threshold (80): LLM called {mock_perform_completion.call_count} times.")
        large_chunk_calls = mock_perform_completion.call_count
    else:
        large_chunk_calls = 0
        
    if strategy_small_chunks and strategy_large_chunks:
        assert small_chunk_calls > large_chunk_calls, "Smaller threshold should result in more LLM calls."

if __name__ == "__main__":
    customize_chunk_token_threshold()
```

#### 3.6.4. Example: Customizing `overlap_rate` between chunks.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock setup - we'll primarily observe the number of calls or potentially the prompt content
mock_llm_overlap = MagicMock()
mock_llm_overlap.choices = [MagicMock()]
mock_llm_overlap.choices[0].message.content = json.dumps({"data_point": "value"})
mock_llm_overlap.usage = MagicMock(completion_tokens=5, prompt_tokens=10, total_tokens=15)
mock_llm_overlap.usage.completion_tokens_details = {}; mock_llm_overlap.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.LLMExtractionStrategy._create_llm_query_tasks') # Patching internal method to inspect chunks
def customize_overlap_rate(mock_create_tasks):
    # The _create_llm_query_tasks method is a good place to see the generated chunks.
    # It returns a list of partials. We can inspect the 'content' arg of the partials.
    
    long_text = "This is a moderately long text to demonstrate the effect of chunk overlap. " * 5
    # word_token_rate (default 0.75) means ~1 token per 1.33 words.
    # chunk_token_threshold (default 2048) is large.
    # Let's use a smaller threshold for demonstration.
    # Threshold 30 tokens => ~40 words.
    # Overlap 0.1 => 3 token overlap => ~4 words.
    # Overlap 0.5 => 15 token overlap => ~20 words.

    # Scenario 1: Small overlap
    try:
        strategy_small_overlap = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            chunk_token_threshold=30,
            overlap_rate=0.1, # 10% overlap
            extraction_type="block" # Block type is easier to see chunk content
        )
    except:
        print("Ollama not available. Skipping overlap rate test.")
        return

    strategy_small_overlap.extract(url="http://dummy.com/overlap1", html_content=long_text)
    chunks_small_overlap = [task.args[1] for task in mock_create_tasks.call_args[0][0]] # (tasks_list, url)
    print(f"Small overlap_rate (0.1) generated {len(chunks_small_overlap)} chunks.")
    if len(chunks_small_overlap) > 1:
        print(f"  Chunk 1 (end): ...{chunks_small_overlap[0][-30:]}")
        print(f"  Chunk 2 (start): {chunks_small_overlap[1][:30]}...")
    mock_create_tasks.reset_mock()

    # Scenario 2: Larger overlap
    strategy_large_overlap = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
        chunk_token_threshold=30,
        overlap_rate=0.5, # 50% overlap
        extraction_type="block"
    )
    strategy_large_overlap.extract(url="http://dummy.com/overlap2", html_content=long_text)
    chunks_large_overlap = [task.args[1] for task in mock_create_tasks.call_args[0][0]]
    print(f"Large overlap_rate (0.5) generated {len(chunks_large_overlap)} chunks.")
    if len(chunks_large_overlap) > 1:
        print(f"  Chunk 1 (end): ...{chunks_large_overlap[0][-30:]}")
        print(f"  Chunk 2 (start): {chunks_large_overlap[1][:30]}...")
    
    # With more overlap, for the same content and threshold, you might get more chunks,
    # or similar number of chunks but with more redundant content.
    # This example primarily shows the parameter being used.
    # Actual number of chunks can be complex to predict without exact tokenization.

if __name__ == "__main__":
    customize_overlap_rate()
```

#### 3.6.5. Example: Demonstrating the effect of different `word_token_rate` values.
The `word_token_rate` helps estimate token count from word count for chunking.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock

# We'll patch the internal chunking function to see how many chunks are made.
# Or, more simply, observe the number of LLM calls if apply_chunking=True.

mock_llm_wtr = MagicMock() # Generic mock for counting calls
mock_llm_wtr.choices = [MagicMock(message=MagicMock(content="{}"))]
mock_llm_wtr.usage = MagicMock(completion_tokens=1, prompt_tokens=1, total_tokens=2)
mock_llm_wtr.usage.completion_tokens_details = {}; mock_llm_wtr.usage.prompt_tokens_details = {}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_wtr)
def customize_word_token_rate(mock_perform_completion):
    # Approx 50 words.
    # word_token_rate helps estimate token length for chunking.
    # chunk_token_threshold default is large (2048), so let's use a smaller one.
    test_content = "This is a test sentence. It has ten words precisely. " * 5 

    try:
        # Scenario 1: Lower word_token_rate (means more words per token, so fewer tokens for same text)
        # Should result in fewer chunks if text length is near threshold.
        strategy_low_wtr = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            chunk_token_threshold=30, # e.g. needs 30 tokens
            word_token_rate=0.5, # Estimates 0.5 tokens per word (i.e., 2 words/token)
                                 # So 50 words -> ~25 tokens. Should be 1 chunk.
            extraction_type="block"
        )
        strategy_low_wtr.extract("url", test_content)
        calls_low_wtr = mock_perform_completion.call_count
        print(f"word_token_rate=0.5 (estimates fewer tokens): LLM calls = {calls_low_wtr}")
        mock_perform_completion.reset_mock()

        # Scenario 2: Higher word_token_rate (means fewer words per token, so more tokens for same text)
        # Should result in more chunks if text length is near threshold.
        strategy_high_wtr = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            chunk_token_threshold=30,
            word_token_rate=1.0, # Estimates 1 token per word
                                 # So 50 words -> ~50 tokens. Should be >1 chunk.
            extraction_type="block"
        )
        strategy_high_wtr.extract("url", test_content)
        calls_high_wtr = mock_perform_completion.call_count
        print(f"word_token_rate=1.0 (estimates more tokens): LLM calls = {calls_high_wtr}")

        assert calls_high_wtr >= calls_low_wtr, \
            "Higher word_token_rate should lead to more or equal chunks for the same content and token threshold."

    except Exception as e:
        print(f"Ollama not available or other error, skipping word_token_rate test: {e}")


if __name__ == "__main__":
    customize_word_token_rate()
```

#### 3.6.6. Example: Processing very large content that requires multiple chunks.
This is similar to 3.6.1, emphasizing that the system handles large inputs by breaking them down.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Simulate multiple LLM calls for multiple chunks
mock_responses_large_content = []
for i in range(5): # Expecting around 5 chunks for this example
    mock_resp = MagicMock()
    mock_resp.choices = [MagicMock()]
    # Simulate LLM returning structured data for each chunk
    mock_resp.choices[0].message.content = json.dumps({"document_part": f"Content from part {i+1} of the large document."})
    mock_resp.usage = MagicMock(completion_tokens=15, prompt_tokens=100, total_tokens=115) # Example usage
    mock_resp.usage.completion_tokens_details = {}; mock_resp.usage.prompt_tokens_details = {}
    mock_responses_large_content.append(mock_resp)


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', side_effect=mock_responses_large_content)
def process_very_large_content(mock_perform_completion):
    # Content designed to be split into several chunks
    # Assuming chunk_token_threshold=50 and word_token_rate=0.75 (~66 words/chunk)
    # This content has 300 words. 300 / (50/0.75) = 300 / 66.6 = ~4.5 chunks
    very_large_content = ("This is a segment of a very large document that needs to be processed. " * 60) # 300 words

    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            extraction_type="schema_from_instruction",
            instruction="Extract key information from this segment of the document.",
            chunk_token_threshold=50, # Smaller threshold to ensure multiple chunks
            word_token_rate=0.75,      # Estimate tokens per word
            apply_chunking=True
        )
    except:
        print("Ollama not available, skipping large content chunking test.")
        return

    print("Processing very large content (mocked LLM calls per chunk)...")
    # The extract method will handle the chunking and aggregation if extraction_type is schema-based.
    # The final extracted_data_json should ideally be a merged/structured result.
    # For this mock, the LLM is assumed to return a list of objects if instruction implies it.
    # Here, we'll get the last chunk's result if schema_from_instruction unless LLM aggregates.
    # To truly show aggregation, a more complex mocking or real LLM is needed.
    # Focus here is on the multiple calls due to chunking.
    
    extracted_data_json_string = strategy.extract(url="http://dummy.com/very_large_doc", html_content=very_large_content)
    
    print(f"LLM was called {mock_perform_completion.call_count} times due to chunking.")
    assert mock_perform_completion.call_count > 1, "Expected multiple LLM calls for large content."

    print("Final Extracted Data (structure depends on LLM's handling of chunked results):")
    if extracted_data_json_string:
        print(json.dumps(json.loads(extracted_data_json_string), indent=2))
    else:
        print("No data extracted or an error occurred.")
        
if __name__ == "__main__":
    process_very_large_content()
```
---

### 3.7. Input Format Selection (`input_format`)

#### 3.7.1. Example: Extracting from Markdown content (`input_format="markdown"`, default).
This is the default behavior if `input_format` is not specified.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

mock_llm_md_input = MagicMock() # Setup mock as before
mock_llm_md_input.choices = [MagicMock(message=MagicMock(content=json.dumps({"title": "Markdown Test"})))]
mock_llm_md_input.usage = MagicMock(completion_tokens=5, prompt_tokens=30, total_tokens=35)
mock_llm_md_input.usage.completion_tokens_details = {}; mock_llm_md_input.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_md_input)
def extract_from_markdown(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="markdown", # Explicitly set, though it's the default
            extraction_type="schema_from_instruction",
            instruction="Extract the main title."
        )
    except:
        print("Ollama not available, skipping markdown input test.")
        return

    sample_markdown = "# Markdown Test\nThis is some **bold** text."
    extracted_json = strategy.extract(url="http://dummy.com/md_page", html_content=sample_markdown)
    
    print("Extraction from Markdown (mocked LLM):")
    if extracted_json:
        print(json.dumps(json.loads(extracted_json), indent=2))

if __name__ == "__main__":
    extract_from_markdown()
```

#### 3.7.2. Example: Extracting directly from raw HTML (`input_format="html"`).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

mock_llm_html_input = MagicMock() # Setup mock
mock_llm_html_input.choices = [MagicMock(message=MagicMock(content=json.dumps({"page_heading": "HTML Document"})))]
mock_llm_html_input.usage = MagicMock(completion_tokens=6, prompt_tokens=40, total_tokens=46)
mock_llm_html_input.usage.completion_tokens_details = {}; mock_llm_html_input.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_html_input)
def extract_from_raw_html(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="html",
            extraction_type="schema_from_instruction",
            instruction="Extract the main heading (h1)."
        )
    except:
        print("Ollama not available, skipping raw HTML input test.")
        return

    sample_html = "<html><head><title>Test</title></head><body><h1>HTML Document</h1><p>Content</p></body></html>"
    extracted_json = strategy.extract(url="http://dummy.com/html_page", html_content=sample_html)
    
    print("Extraction from Raw HTML (mocked LLM):")
    if extracted_json:
        print(json.dumps(json.loads(extracted_json), indent=2))

if __name__ == "__main__":
    extract_from_raw_html()
```

#### 3.7.3. Example: Extracting from filtered HTML (`input_format="fit_html"`) after `MarkdownGenerator` with a `ContentFilterStrategy` has run.
This example shows a two-step process: first filtering HTML using `MarkdownGenerator` and a `ContentFilterStrategy`, then feeding its `fit_html` output to `LLMExtractionStrategy`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.content_filter_strategy import PruningContentFilter # Example filter
from unittest.mock import patch, MagicMock
import json

# Mock for the LLMExtractionStrategy part
mock_llm_fit_html = MagicMock()
mock_llm_fit_html.choices = [MagicMock(message=MagicMock(content=json.dumps({"main_content_summary": "Summary of pruned content."})))]
mock_llm_fit_html.usage = MagicMock(completion_tokens=10, prompt_tokens=50, total_tokens=60)
mock_llm_fit_html.usage.completion_tokens_details = {}; mock_llm_fit_html.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_fit_html)
async def extract_from_fit_html(mock_perform_completion):
    # Step 1: Setup MarkdownGenerator with a content filter to produce fit_html
    # For this example, we'll use PruningContentFilter.
    # In a real scenario, you might need an LLM for more advanced filters.
    # We'll use a simple mock HTML for this part.
    
    sample_raw_html = """
    <html><body>
        <header>Site Navigation</header>
        <nav>Links...</nav>
        <main>
            <h1>Main Article Title</h1>
            <p>This is the core content we want to keep.</p>
            <p>Another paragraph of important stuff.</p>
        </main>
        <aside>Related links</aside>
        <footer>Copyright info</footer>
    </body></html>
    """

    # Simulate getting fit_html (normally from crawler.arun() and result.markdown.fit_html)
    # Here we manually instantiate and run the filter's logic conceptually
    # Note: MarkdownGenerator itself creates fit_html when a filter is active
    # For simplicity, let's assume PruningContentFilter directly gives us usable HTML for LLM
    
    # A more accurate simulation would involve creating a MarkdownGenerator
    # and getting its `fit_html`. PruningContentFilter directly manipulates soup.
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(sample_raw_html, "lxml")
    pruning_filter = PruningContentFilter() 
    # PruningContentFilter.filter_content modifies soup in-place and returns string list
    # We will simulate its effect by just taking the main content for this test
    main_content_element = soup.find("main")
    fit_html_content = str(main_content_element) if main_content_element else "<p>Filtered content.</p>"
    
    print(f"--- Simulated Fit HTML (for LLM input) ---\n{fit_html_content}\n--------------------------------------")

    # Step 2: Use LLMExtractionStrategy with input_format="fit_html" (or just "html" if it's valid HTML)
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="html", # fit_html is still HTML, or use "fit_html" if specific handling is added
            extraction_type="schema_from_instruction",
            instruction="Summarize the main content provided."
        )
    except:
        print("Ollama not available, skipping fit_html extraction test.")
        return

    extracted_json = strategy.extract(url="http://dummy.com/filtered_page", html_content=fit_html_content)
    
    print("\nExtraction from Fit HTML (mocked LLM):")
    if extracted_json:
        print(json.dumps(json.loads(extracted_json), indent=2))
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_from_fit_html())
```

#### 3.7.4. Example: Extracting from plain text content (`input_format="text"`).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

mock_llm_text_input = MagicMock() # Setup mock
mock_llm_text_input.choices = [MagicMock(message=MagicMock(content=json.dumps({"sentiment": "positive"})))]
mock_llm_text_input.usage = MagicMock(completion_tokens=3, prompt_tokens=25, total_tokens=28)
mock_llm_text_input.usage.completion_tokens_details = {}; mock_llm_text_input.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_text_input)
def extract_from_plain_text(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="text",
            extraction_type="schema_from_instruction",
            instruction="Determine the sentiment of this text."
        )
    except:
        print("Ollama not available, skipping plain text input test.")
        return

    sample_text = "Crawl4ai is an amazing library for web scraping and data extraction!"
    extracted_json = strategy.extract(url="http://dummy.com/text_page", html_content=sample_text) 
    # html_content parameter is used for any text-based input, despite its name
    
    print("Extraction from Plain Text (mocked LLM):")
    if extracted_json:
        print(json.dumps(json.loads(extracted_json), indent=2))

if __name__ == "__main__":
    extract_from_plain_text()
```
---

### 3.8. Forcing JSON Response (`force_json_response`)

#### 3.8.1. Example: Using `force_json_response=True` with `extraction_type="schema"` or `"schema_from_instruction"`.
This is particularly useful with LLMs that might not strictly adhere to JSON output, or when using providers that support JSON mode.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json

class UserProfile(BaseModel):
    username: str
    email: str

# Mock LLM: simulate it trying to return JSON but maybe with extra text
# if force_json_response was False. With True, it should ensure clean JSON.
mock_llm_force_json = MagicMock()
mock_llm_force_json.choices = [MagicMock()]
# LiteLLM's JSON mode (which force_json_response=True often enables)
# typically ensures the LLM's output is directly the JSON object string.
mock_llm_force_json.choices[0].message.content = json.dumps(
    {"username": "testuser", "email": "test@example.com"}
)
mock_llm_force_json.usage = MagicMock(completion_tokens=15, prompt_tokens=70, total_tokens=85)
mock_llm_force_json.usage.completion_tokens_details = {}; mock_llm_force_json.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_force_json)
def force_json_response_example(mock_perform_completion):
    try:
        # Note: Some providers/models have better native JSON mode support.
        # OpenAI models often benefit from this.
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY","mock_key")), # Using OpenAI example
            schema=UserProfile.model_json_schema(),
            extraction_type="schema",
            force_json_response=True # Enable JSON mode
        )
        if not os.getenv("OPENAI_API_KEY"):
            print("Warning: OPENAI_API_KEY not set. Mocking will proceed, but real behavior might differ.")

    except:
        print("LLM provider not available, skipping force_json_response test.")
        return

    sample_content = "User: testuser, Email: test@example.com"
    extracted_json_string = strategy.extract(url="http://dummy.com/user", html_content=sample_content)
    
    print("Force JSON Response Example (mocked LLM):")
    if extracted_json_string:
        print(f"Raw output from LLM (should be clean JSON string): {extracted_json_string}")
        try:
            extracted_data = json.loads(extracted_json_string)
            print("Parsed data:", json.dumps(extracted_data, indent=2))
            UserProfile(**extracted_data) # Validate
            print("Successfully parsed and validated JSON.")
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON even with force_json_response: {e}")
            print("This might indicate an issue with the LLM's JSON mode or the mock setup.")
    else:
        print("No data extracted.")
    
    # Check if the 'response_format' was passed to litellm
    # This depends on the internal implementation detail of how force_json_response is passed.
    # Assuming it sets 'response_format': {'type': 'json_object'} in extra_args for litellm.
    # mock_perform_completion.assert_called_once()
    # call_kwargs = mock_perform_completion.call_args.kwargs
    # assert call_kwargs.get("extra_args", {}).get("response_format") == {"type": "json_object"}
    # print("LLM call included JSON response format.")


if __name__ == "__main__":
    force_json_response_example()
```

#### 3.8.2. Example: Comparing LLM output with and without `force_json_response=True` to show its effect on non-JSON-compliant LLMs.
This example requires an LLM that is known to sometimes produce non-JSON output or a more sophisticated mock.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json
import os

class SimpleData(BaseModel):
    key: str

# Mock 1: LLM returns non-JSON compliant string
mock_llm_non_json = MagicMock()
mock_llm_non_json.choices = [MagicMock()]
mock_llm_non_json.choices[0].message.content = "Here is the JSON you asked for: ```json\n{\"key\": \"value_one\"}\n``` Some extra text."
mock_llm_non_json.usage = MagicMock(completion_tokens=30, prompt_tokens=80, total_tokens=110)
mock_llm_non_json.usage.completion_tokens_details = {}; mock_llm_non_json.usage.prompt_tokens_details = {}


# Mock 2: LLM returns clean JSON (as if force_json_response worked)
mock_llm_forced_json = MagicMock()
mock_llm_forced_json.choices = [MagicMock()]
mock_llm_forced_json.choices[0].message.content = json.dumps({"key": "value_one"})
mock_llm_forced_json.usage = MagicMock(completion_tokens=10, prompt_tokens=80, total_tokens=90)
mock_llm_forced_json.usage.completion_tokens_details = {}; mock_llm_forced_json.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff')
def compare_force_json_response(mock_perform_completion):
    sample_content = "The key is value_one."
    schema_def = SimpleData.model_json_schema()
    
    try:
        # Using a provider that might benefit from force_json_response
        llm_config_for_comparison = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY","mock_key_compare"))
        if not os.getenv("OPENAI_API_KEY"):
            print("Warning: OPENAI_API_KEY not set for comparison. Mocking will show intended difference.")
    except:
        print("LLM provider not available, skipping force_json comparison test.")
        return

    # Case 1: force_json_response = False (default)
    mock_perform_completion.return_value = mock_llm_non_json
    strategy_no_force = LLMExtractionStrategy(
        llm_config=llm_config_for_comparison,
        schema=schema_def,
        extraction_type="schema",
        force_json_response=False
    )
    print("--- Without force_json_response ---")
    result_no_force_json_str = strategy_no_force.extract("url", sample_content)
    print(f"Raw output: {result_no_force_json_str}")
    try:
        data_no_force = json.loads(result_no_force_json_str) # This would likely fail with the mock
        print(f"Parsed data: {data_no_force}")
    except json.JSONDecodeError as e:
        print(f"Failed to parse as JSON (expected for this mock): {e}")

    # Case 2: force_json_response = True
    mock_perform_completion.return_value = mock_llm_forced_json
    strategy_with_force = LLMExtractionStrategy(
        llm_config=llm_config_for_comparison,
        schema=schema_def,
        extraction_type="schema",
        force_json_response=True
    )
    print("\n--- With force_json_response = True ---")
    result_with_force_json_str = strategy_with_force.extract("url", sample_content)
    print(f"Raw output: {result_with_force_json_str}")
    try:
        data_with_force = json.loads(result_with_force_json_str)
        SimpleData(**data_with_force) # Validate
        print(f"Parsed data: {data_with_force} (Successfully parsed and validated)")
    except json.JSONDecodeError as e:
        print(f"Failed to parse as JSON (unexpected with good JSON mode): {e}")

if __name__ == "__main__":
    compare_force_json_response()
```
---

### 3.9. Verbosity and Logging

#### 3.9.1. Example: Using `verbose=True` to see detailed LLM interaction logs.
Setting `verbose=True` in `LLMExtractionStrategy` enables detailed logging of prompts sent to and responses received from the LLM.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig, DefaultLogger
from unittest.mock import patch, MagicMock
import json
import io
import sys

# Mock LLM response
mock_llm_verbose = MagicMock()
mock_llm_verbose.choices = [MagicMock(message=MagicMock(content=json.dumps({"data": "verbose example"})))]
mock_llm_verbose.usage = MagicMock(completion_tokens=5, prompt_tokens=10, total_tokens=15)
mock_llm_verbose.usage.completion_tokens_details = {}; mock_llm_verbose.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_verbose)
def verbose_logging_example(mock_perform_completion):
    # Capture stdout to check for verbose logs
    old_stdout = sys.stdout
    sys.stdout = captured_output = io.StringIO()

    try:
        # Use a simple logger for this example that prints to stdout
        logger = DefaultLogger(verbose=True) 
        
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            verbose=True, # Enable verbose logging in the strategy
            logger=logger,  # Pass the logger
            extraction_type="schema_from_instruction",
            instruction="Extract something."
        )
    except: # Fallback if ollama/logger setup fails for some reason in test env
        sys.stdout = old_stdout
        print("Ollama/Logger not available, skipping verbose logging test.")
        return


    strategy.extract(url="http://dummy.com/verbose", html_content="Some sample content.")
    
    sys.stdout = old_stdout # Restore stdout
    output_log = captured_output.getvalue()
    
    print("\n--- Captured Verbose Log Output (should contain LLM prompt/response details) ---")
    print(output_log)

    # Check for typical verbose log messages (actual messages might vary)
    assert "LLM Request" in output_log or "Prompt for LLM" in output_log
    assert "LLM Response" in output_log or "Response from LLM" in output_log
    print("\nVerbose logging appeared to work.")

if __name__ == "__main__":
    verbose_logging_example()
```

#### 3.9.2. Example: Providing a custom `logger` instance to `LLMExtractionStrategy`.
You can integrate `LLMExtractionStrategy` with your existing logging setup.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig, DefaultLogger
import logging
import io

# Setup a custom Python logger
custom_logger = logging.getLogger("MyCustomExtractorLogger")
custom_logger.setLevel(logging.INFO)
log_capture_string = io.StringIO()
ch = logging.StreamHandler(log_capture_string)
ch.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
custom_logger.addHandler(ch)
custom_logger.propagate = False # Prevent duplicate logs if root logger also has a handler

# For LLMExtractionStrategy, we need to wrap this in a Crawl4ai compatible logger
class CustomCrawl4aiLogger(DefaultLogger):
    def __init__(self, py_logger, verbose=False):
        super().__init__(verbose=verbose)
        self.py_logger = py_logger

    def _log(self, level_str, message, tag=None, params=None, colors=None):
        # You can customize how messages are formatted and logged here
        log_message = f"[{tag or 'C4AI'}] {message}"
        if params:
            log_message = log_message.format(**params)
        
        if level_str.lower() == "info":
            self.py_logger.info(log_message)
        elif level_str.lower() == "error":
            self.py_logger.error(log_message)
        elif level_str.lower() == "warning":
            self.py_logger.warning(log_message)
        elif self.verbose and level_str.lower() == "debug": # Only log debug if verbose
             self.py_logger.debug(log_message)


# Mock the LLM call for this example to focus on logging
from unittest.mock import patch, MagicMock
import json
mock_llm_custom_log = MagicMock()
mock_llm_custom_log.choices = [MagicMock(message=MagicMock(content=json.dumps({"info":"logged"})))]
mock_llm_custom_log.usage = MagicMock(completion_tokens=3, prompt_tokens=10, total_tokens=13)
mock_llm_custom_log.usage.completion_tokens_details = {}; mock_llm_custom_log.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_custom_log)
def custom_logger_example(mock_perform_completion):
    crawl4ai_custom_logger = CustomCrawl4aiLogger(custom_logger, verbose=True)
    
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            logger=crawl4ai_custom_logger, # Pass the custom logger instance
            verbose=True, # Ensure strategy attempts to log debug messages too
            extraction_type="schema_from_instruction",
            instruction="Log this."
        )
    except:
        print("Ollama not available, skipping custom logger test.")
        return

    strategy.extract(url="http://dummy.com/custom_log", html_content="Content for custom logger.")
    
    log_contents = log_capture_string.getvalue()
    print("\n--- Captured Log Output (via custom Python logger) ---")
    print(log_contents)
    
    assert "MyCustomExtractorLogger" in log_contents # Check if our logger's name is in output
    assert "[LLM_REQ]" in log_contents or "[LLM_RESP]" in log_contents # Check for common strategy tags

if __name__ == "__main__":
    custom_logger_example()
```
---

### 3.10. Practical Extraction Scenarios
These examples use `AsyncWebCrawler` and might require actual internet access and potentially API keys for the LLMs. They will be mocked for consistency in testing, but the setup shows real-world usage.

#### 3.10.1. Example: Extracting product names, prices, and descriptions from an e-commerce page.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from typing import List, Optional
from unittest.mock import patch, MagicMock
import json
import os

class ProductInfo(BaseModel):
    name: str = Field(..., description="The name of the product")
    price: Optional[float] = Field(None, description="The price of the product, as a float")
    description_snippet: Optional[str] = Field(None, description="A short snippet of the product description")

class ProductPageExtract(BaseModel):
    products: List[ProductInfo] = Field(description="List of products found on the page")

# Mock the LLM call
mock_ecommerce_response = MagicMock()
mock_ecommerce_response.choices = [MagicMock()]
mock_ecommerce_response.choices[0].message.content = json.dumps({
    "products": [
        {"name": "Super Widget X1000", "price": 99.99, "description_snippet": "The best widget ever."},
        {"name": "Basic Widget B50", "price": 19.99, "description_snippet": "A simple, reliable widget."}
    ]
})
mock_ecommerce_response.usage = MagicMock(completion_tokens=50, prompt_tokens=300, total_tokens=350)
mock_ecommerce_response.usage.completion_tokens_details = {}; mock_ecommerce_response.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_ecommerce_response)
async def extract_ecommerce_products(mock_perform_completion):
    # This URL is a placeholder; a real e-commerce page would be used.
    # For CI/testing, we use a simple example.com which won't have products.
    # The key is to show the setup.
    ecommerce_url = "http://example.com" 
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY", "mock_key_ecommerce"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")
        
        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=ProductPageExtract.model_json_schema(),
            extraction_type="schema",
            instruction="Extract all product names, their prices, and a short description snippet from the page content."
        )
    except Exception as e:
        print(f"LLM setup failed for e-commerce example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strat,
        # word_count_threshold=5 # Lower for example.com if testing live
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=ecommerce_url, config=run_config)

    print(f"--- Extraction from E-commerce like page ({ecommerce_url}) ---")
    if result.success and result.extracted_content:
        extracted_data = json.loads(result.extracted_content)
        print(json.dumps(extracted_data, indent=2))
        
        # Validate with Pydantic
        page_data = ProductPageExtract(**extracted_data)
        for product in page_data.products:
            print(f"Product: {product.name}, Price: {product.price}")
    elif not result.success:
        print(f"Crawl failed: {result.error_message}")
    else:
        print("No structured data extracted or extraction failed.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_ecommerce_products())
```

#### 3.10.2. Example: Extracting article headlines, authors, and publication dates from a news site.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from typing import Optional
from unittest.mock import patch, MagicMock
import json
import os

class NewsArticle(BaseModel):
    headline: str = Field(..., description="The main headline of the news article")
    author: Optional[str] = Field(None, description="The author(s) of the article")
    publication_date: Optional[str] = Field(None, description="The date the article was published (e.g., YYYY-MM-DD)")

# Mock the LLM call
mock_news_response = MagicMock()
mock_news_response.choices = [MagicMock()]
mock_news_response.choices[0].message.content = json.dumps({
    "headline": "AI Breakthrough Announced", 
    "author": "Reporter Bot", 
    "publication_date": "2024-05-24"
})
mock_news_response.usage = MagicMock(completion_tokens=30, prompt_tokens=250, total_tokens=280)
mock_news_response.usage.completion_tokens_details = {}; mock_news_response.usage.prompt_tokens_details = {}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_news_response)
async def extract_news_article_details(mock_perform_completion):
    # Using Wikipedia for a stable, public news-like article structure
    news_url = "https://en.wikipedia.org/wiki/Artificial_intelligence" 
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY", "mock_key_news"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")

        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=NewsArticle.model_json_schema(),
            extraction_type="schema",
            instruction="From the provided news article content, extract the main headline, the author(s), and the publication date."
        )
    except Exception as e:
        print(f"LLM setup failed for news example: {e}. Skipping.")
        return


    run_config = CrawlerRunConfig(extraction_strategy=extraction_strat)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=news_url, config=run_config)

    print(f"--- Extraction from News Article ({news_url}) ---")
    if result.success and result.extracted_content:
        extracted_data = json.loads(result.extracted_content)
        print(json.dumps(extracted_data, indent=2))
        article_data = NewsArticle(**extracted_data)
        print(f"Headline: {article_data.headline}")
    elif not result.success:
        print(f"Crawl failed: {result.error_message}")
    else:
        print("No structured data extracted or extraction failed.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_news_article_details())
```

#### 3.10.3. Example: Extracting frequently asked questions (FAQs) and their answers from a support page.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from typing import List
from unittest.mock import patch, MagicMock
import json
import os

class FAQItem(BaseModel):
    question: str
    answer: str

class FAQPage(BaseModel):
    faqs: List[FAQItem]

# Mock the LLM call
mock_faq_response = MagicMock()
mock_faq_response.choices = [MagicMock()]
mock_faq_response.choices[0].message.content = json.dumps({
    "faqs": [
        {"question": "What is Crawl4ai?", "answer": "An awesome web crawler."},
        {"question": "How to install?", "answer": "pip install crawl4ai"}
    ]
})
mock_faq_response.usage = MagicMock(completion_tokens=60, prompt_tokens=300, total_tokens=360)
mock_faq_response.usage.completion_tokens_details = {}; mock_faq_response.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_faq_response)
async def extract_faqs(mock_perform_completion):
    # Placeholder URL - a real FAQ page would be used
    faq_url = "http://example.com/faq" 

    try:
        llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY", "mock_key_faq"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")
        
        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=FAQPage.model_json_schema(),
            extraction_type="schema",
            instruction="Extract all question and answer pairs from the FAQ section of this page."
        )
    except Exception as e:
        print(f"LLM setup failed for FAQ example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(extraction_strategy=extraction_strat)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=faq_url, config=run_config)

    print(f"--- Extraction from FAQ Page ({faq_url}) ---")
    if result.success and result.extracted_content:
        extracted_data = json.loads(result.extracted_content)
        print(json.dumps(extracted_data, indent=2))
        faq_page_data = FAQPage(**extracted_data)
        for faq_item in faq_page_data.faqs:
            print(f"Q: {faq_item.question}\nA: {faq_item.answer}\n")
    elif not result.success:
        print(f"Crawl failed: {result.error_message}")
    else:
        print("No structured data extracted or extraction failed.")

    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_faqs())
```

#### 3.10.4. Example: Extracting contact information (email, phone, address) from a company's "Contact Us" page.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from typing import Optional
from unittest.mock import patch, MagicMock
import json
import os

class ContactInfo(BaseModel):
    email: Optional[str] = Field(None, description="Company contact email address")
    phone: Optional[str] = Field(None, description="Company contact phone number")
    address: Optional[str] = Field(None, description="Company physical address")

# Mock the LLM call
mock_contact_response = MagicMock()
mock_contact_response.choices = [MagicMock()]
mock_contact_response.choices[0].message.content = json.dumps({
    "email": "support@example.com", 
    "phone": "1-800-555-1234", 
    "address": "123 Main St, Anytown, USA"
})
mock_contact_response.usage = MagicMock(completion_tokens=40, prompt_tokens=200, total_tokens=240)
mock_contact_response.usage.completion_tokens_details = {}; mock_contact_response.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_contact_response)
async def extract_contact_info(mock_perform_completion):
    contact_url = "http://example.com/contact" 

    try:
        llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY", "mock_key_contact"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")
        
        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=ContactInfo.model_json_schema(),
            extraction_type="schema",
            instruction="Extract the primary email, phone number, and physical address from this contact page."
        )
    except Exception as e:
        print(f"LLM setup failed for contact info example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(extraction_strategy=extraction_strat)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=contact_url, config=run_config)

    print(f"--- Extraction from Contact Page ({contact_url}) ---")
    if result.success and result.extracted_content:
        extracted_data = json.loads(result.extracted_content)
        print(json.dumps(extracted_data, indent=2))
        contact_data = ContactInfo(**extracted_data)
        print(f"Email: {contact_data.email}, Phone: {contact_data.phone}")
    elif not result.success:
        print(f"Crawl failed: {result.error_message}")
    else:
        print("No structured data extracted or extraction failed.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_contact_info())
```

#### 3.10.5. Example: Extracting key entities (people, organizations, locations) from a block of text using `extraction_type="block"` and a specific instruction.
This uses "block" extraction but with an instruction to guide the LLM to tag specific entities.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from unittest.mock import patch, MagicMock
import json
import os

# Mock LLM response - for block extraction with entity tagging
mock_entity_response = MagicMock()
mock_entity_response.choices = [MagicMock()]
mock_entity_response.choices[0].message.content = """
<blocks>
  <block>
    <content>Apple Inc. is headquartered in Cupertino.</content>
    <tags><tag>sentence</tag><tag>ORG:Apple Inc.</tag><tag>LOC:Cupertino</tag></tags>
  </block>
  <block>
    <content>Tim Cook is the CEO.</content>
    <tags><tag>sentence</tag><tag>PER:Tim Cook</tag></tags>
  </block>
</blocks>
"""
mock_entity_response.usage = MagicMock(completion_tokens=50, prompt_tokens=150, total_tokens=200)
mock_entity_response.usage.completion_tokens_details = {}; mock_entity_response.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_entity_response)
async def extract_entities_with_block(mock_perform_completion):
    entity_url = "http://example.com/about-us" # Placeholder
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY", "mock_key_entities"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")
        
        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            extraction_type="block",
            instruction="Extract each sentence as a block. For each block, identify and add tags for People (PER:Name), Organizations (ORG:Name), and Locations (LOC:Name) found within that sentence."
        )
    except Exception as e:
        print(f"LLM setup failed for entity extraction example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(extraction_strategy=extraction_strat)

    # For this demo, we'll use direct content instead of crawling a URL
    sample_text_for_entities = "Apple Inc. is headquartered in Cupertino. Tim Cook is the CEO."

    # Normally you'd use crawler.arun(url=..., config=...).
    # Here, we call the strategy directly for simplicity with local text.
    extracted_blocks = extraction_strat.extract(url=entity_url, html_content=sample_text_for_entities)


    print(f"--- Entity Extraction using extraction_type='block' ---")
    if extracted_blocks:
        print(json.dumps(extracted_blocks, indent=2))
        for block in extracted_blocks:
            print(f"Content: {block['content']}")
            print(f"  Tags: {block['tags']}")
    else:
        print("No blocks extracted or extraction failed.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(extract_entities_with_block())
```
---

## 4. Integration with `AsyncWebCrawler`

#### 4.1. Example: Basic `AsyncWebCrawler` run with `LLMExtractionStrategy` configured in `CrawlerRunConfig`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json
import os

class PageSummary(BaseModel):
    summary: str
    keywords: List[str]

mock_llm_crawler_run = MagicMock()
mock_llm_crawler_run.choices = [MagicMock()]
mock_llm_crawler_run.choices[0].message.content = json.dumps({
    "summary": "Example.com is a domain for use in illustrative examples.",
    "keywords": ["example", "domain", "documentation"]
})
mock_llm_crawler_run.usage = MagicMock(completion_tokens=30, prompt_tokens=100, total_tokens=130)
mock_llm_crawler_run.usage.completion_tokens_details = {}; mock_llm_crawler_run.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_crawler_run)
async def crawler_with_llm_extraction(mock_perform_completion):
    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_crawler"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")

        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=PageSummary.model_json_schema(),
            extraction_type="schema",
            instruction="Provide a one-sentence summary of the page and list up to 3 main keywords."
        )
    except Exception as e:
        print(f"LLM setup failed for crawler integration example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strat,
        word_count_threshold=5 # Ensure example.com content is processed
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="http://example.com", config=run_config)

    print(f"--- Crawler run with LLMExtractionStrategy ---")
    if result.success and result.extracted_content:
        extracted_data = json.loads(result.extracted_content)
        print("Extracted Data:")
        print(json.dumps(extracted_data, indent=2))
        
        summary_instance = PageSummary(**extracted_data)
        print(f"\nValidated Summary: {summary_instance.summary}")
        print(f"Keywords: {summary_instance.keywords}")
    elif not result.success:
        print(f"Crawl failed: {result.error_message}")
    else:
        print("No structured data extracted.")
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    asyncio.run(crawler_with_llm_extraction())
```

#### 4.2. Example: `AsyncWebCrawler` processing multiple URLs, each with the same `LLMExtractionStrategy`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json
import os

class SiteInfo(BaseModel):
    site_name: str
    main_purpose: str

# Mock LLM to return different info based on URL (simplified by just returning same mock structure)
mock_llm_multi_url = MagicMock()
mock_llm_multi_url.choices = [MagicMock()]
# We'll have the mock return slightly different content for each call
responses_for_multi_url = [
    json.dumps({"site_name": "Example Domain", "main_purpose": "Illustrative examples"}),
    json.dumps({"site_name": "IANA", "main_purpose": "Managing global IP addressing"})
]
call_count_multi = 0
def side_effect_multi_url(*args, **kwargs):
    global call_count_multi
    mock_llm_multi_url.choices[0].message.content = responses_for_multi_url[call_count_multi % len(responses_for_multi_url)]
    call_count_multi +=1
    return mock_llm_multi_url

mock_llm_multi_url.usage = MagicMock(completion_tokens=20, prompt_tokens=80, total_tokens=100) # Generic usage
mock_llm_multi_url.usage.completion_tokens_details = {}; mock_llm_multi_url.usage.prompt_tokens_details = {}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', side_effect=side_effect_multi_url)
async def crawler_many_with_llm_extraction(mock_perform_completion):
    urls_to_crawl = [
        "http://example.com",
        "https://www.iana.org/domains/reserved" # Another simple, stable page
    ]
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_many"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")

        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=SiteInfo.model_json_schema(),
            extraction_type="schema",
            instruction="Identify the site name and its main purpose."
        )
    except Exception as e:
        print(f"LLM setup failed for arun_many example: {e}. Skipping.")
        return

    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strat,
        word_count_threshold=10 # Adjust as needed for the test URLs
    )

    async with AsyncWebCrawler() as crawler:
        # arun_many returns an async generator if stream=True, or list if stream=False (default)
        results = await crawler.arun_many(urls=urls_to_crawl, config=run_config) 

    print(f"--- Crawler arun_many with LLMExtractionStrategy ---")
    for result in results:
        if result.success and result.extracted_content:
            extracted_data = json.loads(result.extracted_content)
            print(f"\nURL: {result.url}")
            print("Extracted Data:")
            print(json.dumps(extracted_data, indent=2))
        elif not result.success:
            print(f"\nCrawl for {result.url} failed: {result.error_message}")
        else:
            print(f"\nNo structured data extracted for {result.url}.")
    
    assert mock_perform_completion.call_count == len(urls_to_crawl)

if __name__ == "__main__":
    asyncio.run(crawler_many_with_llm_extraction())
```

#### 4.3. Example: `AsyncWebCrawler` where `CrawlerRunConfig` is dynamically changed per URL to use different extraction schemas or instructions.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from unittest.mock import patch, MagicMock
import json
import os

class TechArticleInfo(BaseModel):
    title: str
    primary_topic: str

class CompanyInfo(BaseModel):
    company_name: str
    services_offered: List[str]

# Mocks for different schemas
mock_tech_article_response = MagicMock()
mock_tech_article_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"title": "Intro to Crawling", "primary_topic": "Web Scraping"})))]
mock_tech_article_response.usage = MagicMock(completion_tokens=15, prompt_tokens=70, total_tokens=85)
mock_tech_article_response.usage.completion_tokens_details = {}; mock_tech_article_response.usage.prompt_tokens_details = {}


mock_company_response = MagicMock()
mock_company_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"company_name": "Example Corp", "services_offered": ["Web Hosting", "Domain Registration"]})))]
mock_company_response.usage = MagicMock(completion_tokens=20, prompt_tokens=90, total_tokens=110)
mock_company_response.usage.completion_tokens_details = {}; mock_company_response.usage.prompt_tokens_details = {}


# Side effect function to return different mocks based on schema/instruction
def dynamic_llm_side_effect(*args, **kwargs):
    # Heuristic: check prompt content for clues about which schema is expected
    prompt_content = args[1] # The prompt string is the second argument to perform_completion_with_backoff
    if "TechArticleInfo" in prompt_content or "primary_topic" in prompt_content:
        return mock_tech_article_response
    elif "CompanyInfo" in prompt_content or "services_offered" in prompt_content:
        return mock_company_response
    return MagicMock() # Default generic mock if no match

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', side_effect=dynamic_llm_side_effect)
async def crawler_dynamic_configs(mock_perform_completion):
    urls_and_configs: List[Dict[str, Any]] = [
        {
            "url": "https://en.wikipedia.org/wiki/Web_scraping", # Tech article like
            "schema_model": TechArticleInfo,
            "instruction": "Extract title and primary topic of this technical article."
        },
        {
            "url": "http://example.com", # Generic company like
            "schema_model": CompanyInfo,
            "instruction": "Identify the company name and list its main services."
        }
    ]
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_dynamic"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set. Mock will be used.")
    except Exception as e:
        print(f"LLM setup failed for dynamic config example: {e}. Skipping.")
        return

    all_results_data = []

    async with AsyncWebCrawler() as crawler:
        for item in urls_and_configs:
            current_url = item["url"]
            CurrentSchemaModel = item["schema_model"]
            current_instruction = item["instruction"]

            extraction_strat = LLMExtractionStrategy(
                llm_config=llm_conf,
                schema=CurrentSchemaModel.model_json_schema(),
                extraction_type="schema",
                instruction=current_instruction
            )
            run_config = CrawlerRunConfig(
                extraction_strategy=extraction_strat, 
                word_count_threshold=10
            )
            
            print(f"\nCrawling {current_url} with schema {CurrentSchemaModel.__name__}...")
            result = await crawler.arun(url=current_url, config=run_config)
            
            if result.success and result.extracted_content:
                extracted_data = json.loads(result.extracted_content)
                print(f"Extracted for {current_url}:")
                print(json.dumps(extracted_data, indent=2))
                all_results_data.append(extracted_data)
            else:
                print(f"Failed or no extraction for {current_url}: {result.error_message}")
                all_results_data.append({"error": result.error_message, "url": current_url})
                
    assert mock_perform_completion.call_count == len(urls_and_configs)
    assert all_results_data[0].get("primary_topic") == "Web Scraping"
    assert "Web Hosting" in all_results_data[1].get("services_offered", [])


if __name__ == "__main__":
    asyncio.run(crawler_dynamic_configs())
```
---

## 5. Combining Extraction with Other Crawl4ai Features

### 5.1. **Extraction from PDF Content:**

#### 5.1.1. Example: Using `PDFCrawerStrategy` and `PDFContentScrapingStrategy` to get HTML/text from a PDF, then using `LLMExtractionStrategy` on that content.
This example requires `crawl4ai[pdf]` to be installed. We'll use a public PDF URL.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig, BrowserConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.content_scraping_strategy import PDFContentScrapingStrategy # For PDF processing
from crawl4ai.async_crawler_strategy import PDFCrawlerStrategy # To handle PDF URLs
from pydantic import BaseModel, Field
from typing import List, Optional
from unittest.mock import patch, MagicMock
import json
import os

# Note: This example requires PyPDF2. Install with: pip install crawl4ai[pdf]

class PDFSummary(BaseModel):
    title: Optional[str] = Field(None, description="The title of the PDF document, if discernible.")
    first_paragraph_summary: str = Field(..., description="A brief summary of the first main paragraph of text.")

# Mock for LLM
mock_llm_pdf_extract = MagicMock()
mock_llm_pdf_extract.choices = [MagicMock(message=MagicMock(content=json.dumps({
    "title": "Sample PDF Document",
    "first_paragraph_summary": "This PDF discusses important topics related to data."
})))]
mock_llm_pdf_extract.usage = MagicMock(completion_tokens=20, prompt_tokens=150, total_tokens=170)
mock_llm_pdf_extract.usage.completion_tokens_details = {}; mock_llm_pdf_extract.usage.prompt_tokens_details = {}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_pdf_extract)
async def extract_from_pdf_content(mock_perform_completion):
    # A public, simple PDF for testing. (e.g., a small, text-based PDF)
    # Using a known, stable PDF URL: an RFC document (plain text heavy)
    pdf_url = "https://www.rfc-editor.org/rfc/rfc2616.txt.pdf" # This is a PDF version of an RFC text file
    # pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf" # A very simple dummy PDF

    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_pdf"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set for PDF example. Mock will be used.")

        extraction_strat = LLMExtractionStrategy(
            llm_config=llm_conf,
            schema=PDFSummary.model_json_schema(),
            extraction_type="schema",
            instruction="From the provided PDF text, extract the document title if available, and summarize the first main paragraph.",
            input_format="text" # PDFContentScrapingStrategy will provide text
        )
    except Exception as e:
        print(f"LLM setup failed for PDF extraction example: {e}. Skipping.")
        return

    # BrowserConfig might be needed if the PDF is rendered in-browser and not a direct link
    browser_cfg = BrowserConfig() 

    # CrawlerRunConfig for PDF processing and then LLM extraction
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strat,
        # The PDFContentScrapingStrategy will be used by PDFCrawlerStrategy
        # to convert PDF to text/HTML for the LLMExtractionStrategy.
        # LLMExtractionStrategy expects text if input_format="text".
        scraping_strategy=PDFContentScrapingStrategy(output_format="text") # Ensure text output for LLM
    )
    
    # Use PDFCrawlerStrategy to handle the PDF URL
    # Note: For PDFCrawlerStrategy, the 'browser_config' of AsyncWebCrawler is not directly used for PDF fetching.
    # PDFCrawlerStrategy uses requests library directly.
    async with AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy(), browser_config=browser_cfg) as crawler:
        print(f"Crawling PDF: {pdf_url}")
        result = await crawler.arun(url=pdf_url, config=run_config)

    print(f"--- Extraction from PDF Content ({pdf_url}) ---")
    if result.success:
        if result.extracted_content:
            extracted_data = json.loads(result.extracted_content)
            print("Extracted Data from PDF:")
            print(json.dumps(extracted_data, indent=2))
            pdf_summary_instance = PDFSummary(**extracted_data)
            print(f"\nValidated Title: {pdf_summary_instance.title}")
            print(f"Summary: {pdf_summary_instance.first_paragraph_summary}")
        else:
            print("PDF content processed, but no structured data extracted by LLM.")
            print(f"Raw Markdown/Text from PDF (first 300 chars): {result.markdown.raw_markdown[:300] if result.markdown else 'N/A'}...")
    else:
        print(f"Crawl/Processing of PDF failed: {result.error_message}")
    
    if extraction_strat.llm_config.api_token != "mock_key_pdf": # Only assert if not fully mocked
         assert mock_perform_completion.called

if __name__ == "__main__":
    # This example needs `pip install crawl4ai[pdf]`
    try:
        import PyPDF2 # Check if PyPDF2 is installed
        asyncio.run(extract_from_pdf_content())
    except ImportError:
        print("PyPDF2 not found. Please install it with `pip install crawl4ai[pdf]` to run this example.")
    except Exception as e:
        print(f"An error occurred: {e}")
```

### 5.2. **Extraction After Content Filtering (Illustrative):**

#### 5.2.1. Example: Manually running a `ContentFilterStrategy` on HTML, then passing the filtered HTML to `LLMExtractionStrategy.extract(input_format="html")`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.content_filter_strategy import PruningContentFilter # Example filter
from crawl4ai.utils import LLMConfig
from bs4 import BeautifulSoup
from unittest.mock import patch, MagicMock
import json

# Mock for LLM
mock_llm_filtered_html = MagicMock()
mock_llm_filtered_html.choices = [MagicMock(message=MagicMock(content=json.dumps({"main_idea": "The core idea is about X."})))]
mock_llm_filtered_html.usage = MagicMock(completion_tokens=10, prompt_tokens=40, total_tokens=50)
mock_llm_filtered_html.usage.completion_tokens_details = {}; mock_llm_filtered_html.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_filtered_html)
def extract_after_manual_filter(mock_perform_completion):
    sample_raw_html = """
    <html><head><title>Test Page</title></head><body>
        <header>Site Navigation Links...</header>
        <main id='content'>
            <h1>Article Title</h1>
            <p>This is the first important paragraph.</p>
            <div class='ad-banner'>Advertisement here</div>
            <p>This is the second important paragraph after an ad.</p>
        </main>
        <footer>Copyright 2024. All rights reserved.</footer>
    </body></html>
    """
    
    # Step 1: Manually apply a content filter
    # PruningContentFilter modifies the soup in-place
    soup = BeautifulSoup(sample_raw_html, "lxml")
    
    # Example of direct filter usage (conceptual, PruningContentFilter works on soup)
    # PruningContentFilter might typically be used within MarkdownGenerator
    # For this example, let's simulate its effect by selecting main content
    # and removing a known ad-like element.
    
    # Simulate pruning effect:
    header = soup.find("header")
    if header: header.decompose()
    footer = soup.find("footer")
    if footer: footer.decompose()
    ad_banner = soup.find("div", class_="ad-banner")
    if ad_banner: ad_banner.decompose()
        
    filtered_html_content = str(soup.find("main")) # Get HTML of the main content after pruning
    
    print(f"--- Filtered HTML (simulated) ---\n{filtered_html_content}\n---------------------------------")

    # Step 2: Pass filtered HTML to LLMExtractionStrategy
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"),
            input_format="html", # The filtered content is still HTML
            extraction_type="schema_from_instruction",
            instruction="What is the main idea of this content?"
        )
    except:
        print("Ollama not available, skipping manual filter extraction test.")
        return

    extracted_json = strategy.extract(url="http://dummy.com/filtered", html_content=filtered_html_content)
    
    print("\nExtraction from Manually Filtered HTML (mocked LLM):")
    if extracted_json:
        print(json.dumps(json.loads(extracted_json), indent=2))
    
    assert mock_perform_completion.called

if __name__ == "__main__":
    extract_after_manual_filter()
```
---

## 6. Advanced Techniques and Edge Cases

#### 6.1. Example: Handling cases where the LLM fails to extract data or returns an unexpected format (and how `force_json_response` might help).

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json
import os

class SimpleSchema(BaseModel):
    name: str

# Mock 1: LLM returns malformed JSON (e.g., with extra text, missing comma)
mock_malformed_json_response = MagicMock()
mock_malformed_json_response.choices = [MagicMock(message=MagicMock(content='Sure, here is the JSON: {"name": "Test" // oops, a comment'))]
mock_malformed_json_response.usage = MagicMock(completion_tokens=10, prompt_tokens=50, total_tokens=60)
mock_malformed_json_response.usage.completion_tokens_details = {}; mock_malformed_json_response.usage.prompt_tokens_details = {}


# Mock 2: LLM returns clean JSON (simulating successful force_json_response)
mock_clean_json_response = MagicMock()
mock_clean_json_response.choices = [MagicMock(message=MagicMock(content=json.dumps({"name": "Test"})))]
mock_clean_json_response.usage = MagicMock(completion_tokens=8, prompt_tokens=50, total_tokens=58)
mock_clean_json_response.usage.completion_tokens_details = {}; mock_clean_json_response.usage.prompt_tokens_details = {}


@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff')
def handle_llm_failures(mock_perform_completion):
    sample_content = "The name is Test."
    schema_def = SimpleSchema.model_json_schema()
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_failure"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set for failure handling example.")
    except:
        print("LLM provider setup failed. Skipping LLM failure handling test.")
        return

    # Scenario 1: LLM returns malformed JSON, force_json_response=False
    print("--- Scenario 1: Malformed JSON, force_json_response=False ---")
    mock_perform_completion.return_value = mock_malformed_json_response
    strategy_no_force = LLMExtractionStrategy(
        llm_config=llm_conf, schema=schema_def, extraction_type="schema", force_json_response=False
    )
    result_str_no_force = strategy_no_force.extract("url", sample_content)
    print(f"LLM raw output: {result_str_no_force}")
    try:
        # This should ideally fail or be handled by LiteLLM's built-in JSON parsing attempts
        parsed = json.loads(result_str_no_force) 
        SimpleSchema(**parsed) # Validate
        print(f"Parsed (unexpectedly successful for this mock): {parsed}")
    except Exception as e:
        print(f"Expected parsing/validation error: {e}")

    # Scenario 2: LLM returns malformed JSON, force_json_response=True
    # The mock for this scenario simulates that force_json_response helped the LLM return clean JSON
    print("\n--- Scenario 2: Malformed JSON (conceptually), force_json_response=True ---")
    mock_perform_completion.return_value = mock_clean_json_response 
    strategy_with_force = LLMExtractionStrategy(
        llm_config=llm_conf, schema=schema_def, extraction_type="schema", force_json_response=True
    )
    result_str_with_force = strategy_with_force.extract("url", sample_content)
    print(f"LLM raw output (should be clean JSON): {result_str_with_force}")
    try:
        parsed_forced = json.loads(result_str_with_force)
        SimpleSchema(**parsed_forced) # Validate
        print(f"Parsed successfully with force_json_response: {parsed_forced}")
    except Exception as e:
        print(f"Error parsing/validating even with force_json_response: {e}")

if __name__ == "__main__":
    handle_llm_failures()
```

#### 6.2. Example: Strategies for dealing with very long content that might exceed single LLM context windows even after chunking (e.g., iterative extraction or summarization prior to extraction).
This is a conceptual example. Real implementation would be more complex.
We'll show a simplified version where we first "summarize" chunks then extract from summaries.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from pydantic import BaseModel
from unittest.mock import patch, MagicMock
import json
import os

class DocumentExtract(BaseModel):
    overall_summary: str
    key_points: List[str]

# Mocks
# 1. For summarizing chunks
mock_summarize_chunk = MagicMock()
mock_summarize_chunk.choices = [MagicMock()]
mock_summarize_chunk.usage = MagicMock(completion_tokens=20, prompt_tokens=50, total_tokens=70)
mock_summarize_chunk.usage.completion_tokens_details = {}; mock_summarize_chunk.usage.prompt_tokens_details = {}


# 2. For final extraction from combined summaries
mock_final_extract = MagicMock()
mock_final_extract.choices = [MagicMock()]
mock_final_extract.choices[0].message.content = json.dumps({
    "overall_summary": "The document discusses AI advancements and their societal impact.",
    "key_points": ["AI is evolving fast.", "Ethics are important.", "Future is exciting."]
})
mock_final_extract.usage = MagicMock(completion_tokens=40, prompt_tokens=100, total_tokens=140)
mock_final_extract.usage.completion_tokens_details = {}; mock_final_extract.usage.prompt_tokens_details = {}


def mock_llm_router(*args, **kwargs):
    # Simplistic router based on instruction
    instruction = kwargs.get('instruction', '')
    if "summarize this chunk" in instruction.lower():
        # The content of the mock is less important here than the call itself
        mock_summarize_chunk.choices[0].message.content = json.dumps({"summary_of_chunk": "Chunk summary text."})
        return mock_summarize_chunk
    elif "overall document summary" in instruction.lower():
        return mock_final_extract
    return MagicMock() # Default

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', side_effect=mock_llm_router)
def iterative_extraction_for_long_content(mock_perform_completion):
    very_long_document_content = ("This is part of a very long document. " * 100) # Simulate long content
    
    try:
        llm_conf = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY", "mock_key_iterative"))
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set for iterative example.")
    except:
        print("LLM setup failed. Skipping iterative extraction test.")
        return

    # Step 1: Create strategy for summarizing chunks (block extraction for simplicity)
    summarizer_strategy = LLMExtractionStrategy(
        llm_config=llm_conf,
        extraction_type="block", # Could be schema { "summary": "..." }
        instruction="Summarize this chunk of text concisely.",
        chunk_token_threshold=60, # Smaller chunks for summarization
        word_token_rate=0.75,
        apply_chunking=True
    )

    print("--- Step 1: Summarizing chunks of the long document ---")
    # LLMExtractionStrategy.extract returns list of blocks if extraction_type="block"
    chunk_summaries_blocks = summarizer_strategy.extract("url", very_long_document_content) 
    
    # We'd expect chunk_summaries_blocks to be a list of dicts like [{'content': 'summary1', 'tags':[]}, ...]
    # For this mock, we'll just take the mocked 'content'
    chunk_summaries = [block.get("content", "") for block in chunk_summaries_blocks if isinstance(block, dict)]
    combined_summary_text = "\n".join(chunk_summaries)
    
    print(f"Summarized {len(chunk_summaries_blocks)} chunks into combined text of length {len(combined_summary_text)}.")
    print(f"Combined summary (first 100 chars): {combined_summary_text[:100]}...")

    # Step 2: Create strategy for final extraction from combined summaries
    final_extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_conf,
        schema=DocumentExtract.model_json_schema(),
        extraction_type="schema",
        instruction="From the provided combined summaries, create an overall document summary and list key points."
    )

    print("\n--- Step 2: Final extraction from combined summaries ---")
    final_extracted_json_str = final_extraction_strategy.extract("url_summary", combined_summary_text)

    if final_extracted_json_str:
        final_data = json.loads(final_extracted_json_str)
        print(json.dumps(final_data, indent=2))
        doc_extract_instance = DocumentExtract(**final_data)
        assert "AI is evolving fast" in doc_extract_instance.key_points
    else:
        print("Final extraction failed.")

if __name__ == "__main__":
    iterative_extraction_for_long_content()
```

#### 6.3. Example: Showing how to access `TokenUsage` statistics after an LLM extraction.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
from unittest.mock import patch, MagicMock
import json

# Mock LLM response with usage data
mock_llm_usage = MagicMock()
mock_llm_usage.choices = [MagicMock(message=MagicMock(content=json.dumps({"extracted": "data"})))]
mock_llm_usage.usage = MagicMock()
mock_llm_usage.usage.completion_tokens = 50
mock_llm_usage.usage.prompt_tokens = 150
mock_llm_usage.usage.total_tokens = 200
# Example of detailed token usage (if provider supports it)
mock_llm_usage.usage.completion_tokens_details = {"gpt-3.5-turbo": 50} 
mock_llm_usage.usage.prompt_tokens_details = {"gpt-3.5-turbo": 150}

@patch('crawl4ai.extraction_strategy.perform_completion_with_backoff', return_value=mock_llm_usage)
def show_token_usage(mock_perform_completion):
    try:
        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="ollama/llama3", api_token="ollama"), # Or any provider
            extraction_type="schema_from_instruction",
            instruction="Extract data."
        )
    except:
        print("Ollama not available, skipping token usage test.")
        return

    strategy.extract(url="http://dummy.com/usage_test", html_content="Some content to extract from.")
    
    print("--- Token Usage Statistics ---")
    
    # Total usage accumulated by the strategy instance across all its .extract() calls
    print(f"Total Accumulated Usage for this strategy instance:")
    print(f"  Completion Tokens: {strategy.total_usage.completion_tokens}")
    print(f"  Prompt Tokens: {strategy.total_usage.prompt_tokens}")
    print(f"  Total Tokens: {strategy.total_usage.total_tokens}")

    # Usage for the last .extract() call (or list of calls if chunking happened)
    if strategy.usages:
        print(f"\nUsage for the last .extract() operation (may include multiple LLM calls if chunked):")
        for i, usage_info in enumerate(strategy.usages[-mock_perform_completion.call_count:]): # Show for calls made in this run
            print(f"  LLM Call {i+1}:")
            print(f"    Completion Tokens: {usage_info.completion_tokens}")
            print(f"    Prompt Tokens: {usage_info.prompt_tokens}")
            print(f"    Total Tokens: {usage_info.total_tokens}")
            if usage_info.completion_tokens_details:
                 print(f"    Completion Details: {usage_info.completion_tokens_details}")
            if usage_info.prompt_tokens_details:
                 print(f"    Prompt Details: {usage_info.prompt_tokens_details}")
    else:
        print("No usage data recorded for the last operation (might be an issue or first run).")
        
    assert strategy.total_usage.total_tokens == 200 # Based on mock

if __name__ == "__main__":
    show_token_usage()
```
---

## 7. Deprecated Parameter Usage (for backward compatibility reference, if necessary)

#### 7.1. Example: Initializing `LLMExtractionStrategy` using deprecated `provider`, `api_token`, `base_url` and showing equivalence with `llm_config`. (Mark as deprecated in example comments).
This example shows the old way of passing LLM configuration directly to `LLMExtractionStrategy` and the new, recommended way using `LLMConfig`.

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.utils import LLMConfig
import os
import warnings

# Suppress deprecation warnings for this specific example
warnings.filterwarnings("ignore", category=DeprecationWarning, module="crawl4ai.extraction_strategy")


print("--- Demonstrating LLM Configuration: Deprecated vs. LLMConfig ---")

# Parameters for the example
provider_name = "openai/gpt-3.5-turbo"
api_key = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_PLACEHOLDER")
# base_api_url = "https://api.example.com/v1" # Example custom base URL

# --- Deprecated Way (for illustration) ---
# Note: This will raise DeprecationWarnings if not suppressed.
# It's included here to show users how to migrate.
print("\nAttempting initialization with DEPRECATED direct parameters:")
try:
    strategy_deprecated = LLMExtractionStrategy(
        provider=provider_name,
        api_token=api_key,
        # base_url=base_api_url, # If you had a custom base_url
        instruction="This is a test." # Need some instruction for it to init provider
    )
    print(f"  Deprecated way: Provider='{strategy_deprecated.llm_config.provider}', Token (first 5)='{strategy_deprecated.llm_config.api_token[:5] if strategy_deprecated.llm_config.api_token else None}', BaseURL='{strategy_deprecated.llm_config.base_url}'")
    assert strategy_deprecated.llm_config.provider == provider_name
except Exception as e:
    print(f"  Error with deprecated init (as expected if params fully removed or strict checks): {e}")


# --- New Recommended Way (using LLMConfig) ---
print("\nAttempting initialization with NEW LLMConfig object:")
llm_configuration = LLMConfig(
    provider=provider_name,
    api_token=api_key,
    # base_url=base_api_url # If you have a custom base_url
)
strategy_new = LLMExtractionStrategy(
    llm_config=llm_configuration,
    instruction="This is a test." # Need some instruction
)
print(f"  New way (LLMConfig): Provider='{strategy_new.llm_config.provider}', Token (first 5)='{strategy_new.llm_config.api_token[:5] if strategy_new.llm_config.api_token else None}', BaseURL='{strategy_new.llm_config.base_url}'")
assert strategy_new.llm_config.provider == provider_name

print("\nComparison:")
if 'strategy_deprecated' in locals() and strategy_deprecated.llm_config.provider == strategy_new.llm_config.provider:
    print("Both methods (deprecated and new) resulted in the same LLM provider configuration (provider name matches).")
else:
    print("There was a difference or the deprecated method failed as expected.")

# Restore default warning behavior if needed for other tests
warnings.resetwarnings()
```
---

**Note:** Many examples involving `LLMExtractionStrategy` use mocked LLM calls for simplicity and to avoid API key dependencies during automated testing or casual runs. When adapting these examples for real use, ensure you have a valid `LLMConfig` with appropriate provider details and API tokens, and remove or adapt the `@patch` decorators. For local testing, consider using Ollama with a model like `ollama/llama3`.

```

---


## Markdown Generation - Memory
Source: crawl4ai_markdown_memory_content.llm.md

Okay, I have processed the instructions and the provided information. I will now generate the detailed "Foundational Memory" document for the `markdown` component of `crawl4ai`, adhering strictly to the outline you've provided and focusing on factual API details.

```markdown
# Detailed Outline for crawl4ai - markdown Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_markdown.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Markdown Generation in Crawl4ai

*   1.1. Purpose: This section outlines the `markdown` component of the `crawl4ai` library. Its primary role is to convert HTML content, obtained during web crawling, into various Markdown formats. These formats are designed to be suitable for consumption by Large Language Models (LLMs), as well as for other applications requiring structured text from web pages.
*   1.2. Key Abstractions:
    *   `MarkdownGenerationStrategy`: An abstract base class that defines the interface for different markdown generation algorithms and approaches. This allows for customizable Markdown conversion processes.
    *   `DefaultMarkdownGenerator`: The standard, out-of-the-box implementation of `MarkdownGenerationStrategy`. It handles the conversion of HTML to Markdown, including features like link-to-citation conversion and integration with content filtering.
    *   `MarkdownGenerationResult`: A Pydantic data model that encapsulates the various outputs of the markdown generation process, such as raw markdown, markdown with citations, and markdown derived from filtered content.
    *   `CrawlerRunConfig.markdown_generator`: An attribute within the `CrawlerRunConfig` class that allows users to specify which instance of a `MarkdownGenerationStrategy` should be used for a particular crawl operation.
*   1.3. Relationship with Content Filtering: The markdown generation process can be integrated with `RelevantContentFilter` strategies. When a content filter is applied, it first refines the input HTML, and then this filtered HTML is used to produce a `fit_markdown` output, providing a more focused version of the content.

## 2. Core Interface: `MarkdownGenerationStrategy`

*   2.1. Purpose: The `MarkdownGenerationStrategy` class is an abstract base class (ABC) that defines the contract for all markdown generation strategies within `crawl4ai`. It ensures that any custom markdown generator will adhere to a common interface, making them pluggable into the crawling process.
*   2.2. Source File: `crawl4ai/markdown_generation_strategy.py`
*   2.3. Initialization (`__init__`)
    *   2.3.1. Signature:
        ```python
        class MarkdownGenerationStrategy(ABC):
            def __init__(
                self,
                content_filter: Optional[RelevantContentFilter] = None,
                options: Optional[Dict[str, Any]] = None,
                verbose: bool = False,
                content_source: str = "cleaned_html",
            ):
                # ...
        ```
    *   2.3.2. Parameters:
        *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional `RelevantContentFilter` instance. If provided, this filter will be used to process the HTML before generating the `fit_markdown` and `fit_html` outputs in the `MarkdownGenerationResult`.
        *   `options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary for strategy-specific custom options. This allows subclasses to receive additional configuration parameters. Defaults to an empty dictionary if `None`.
        *   `verbose (bool`, default: `False`)`: If `True`, enables verbose logging for the markdown generation process.
        *   `content_source (str`, default: `"cleaned_html"`)`: A string indicating the source of HTML to use for Markdown generation. Common values might include `"raw_html"` (original HTML from the page), `"cleaned_html"` (HTML after initial cleaning by the scraping strategy), or `"fit_html"` (HTML after being processed by `content_filter`). The actual available sources depend on the `ScrapingResult` provided to the markdown generator.
*   2.4. Abstract Methods:
    *   2.4.1. `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`
        *   Purpose: This abstract method must be implemented by concrete subclasses. It is responsible for taking an HTML string and converting it into various Markdown representations, encapsulated within a `MarkdownGenerationResult` object.
        *   Parameters:
            *   `input_html (str)`: The HTML string content to be converted to Markdown.
            *   `base_url (str`, default: `""`)`: The base URL of the crawled page. This is crucial for resolving relative URLs, especially when converting links to citations.
            *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary of options to be passed to the underlying HTML-to-text conversion engine (e.g., `CustomHTML2Text`).
            *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional `RelevantContentFilter` instance. If provided, this filter is used to generate `fit_markdown` and `fit_html`. This parameter overrides any filter set during the strategy's initialization for this specific call.
            *   `citations (bool`, default: `True`)`: A boolean flag indicating whether to convert Markdown links into a citation format (e.g., `[text]^[1]^`) with a corresponding reference list.
            *   `**kwargs`: Additional keyword arguments to allow for future extensions or strategy-specific parameters.
        *   Returns: (`MarkdownGenerationResult`) An object containing the results of the Markdown generation, including `raw_markdown`, `markdown_with_citations`, `references_markdown`, and potentially `fit_markdown` and `fit_html`.

## 3. Default Implementation: `DefaultMarkdownGenerator`

*   3.1. Purpose: `DefaultMarkdownGenerator` is the standard concrete implementation of `MarkdownGenerationStrategy`. It provides a robust mechanism for converting HTML to Markdown, featuring link-to-citation conversion and the ability to integrate with `RelevantContentFilter` strategies for focused content output.
*   3.2. Source File: `crawl4ai/markdown_generation_strategy.py`
*   3.3. Inheritance: Inherits from `MarkdownGenerationStrategy`.
*   3.4. Initialization (`__init__`)
    *   3.4.1. Signature:
        ```python
        class DefaultMarkdownGenerator(MarkdownGenerationStrategy):
            def __init__(
                self,
                content_filter: Optional[RelevantContentFilter] = None,
                options: Optional[Dict[str, Any]] = None,
                # content_source parameter from parent is available
                # verbose parameter from parent is available
            ):
                super().__init__(content_filter, options, content_source=kwargs.get("content_source", "cleaned_html"), verbose=kwargs.get("verbose", False))
        ```
        *(Note: The provided code snippet for `DefaultMarkdownGenerator.__init__` does not explicitly list `verbose` and `content_source`, but they are passed to `super().__init__` through `**kwargs` in the actual library code, so their effective signature matches the parent.)*
    *   3.4.2. Parameters:
        *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: As defined in `MarkdownGenerationStrategy`.
        *   `options (Optional[Dict[str, Any]]`, default: `None`)`: As defined in `MarkdownGenerationStrategy`.
        *   `verbose (bool`, default: `False`)`: (Passed via `kwargs` to parent) As defined in `MarkdownGenerationStrategy`.
        *   `content_source (str`, default: `"cleaned_html"`)`: (Passed via `kwargs` to parent) As defined in `MarkdownGenerationStrategy`.
*   3.5. Key Class Attributes:
    *   3.5.1. `LINK_PATTERN (re.Pattern)`: A compiled regular expression pattern used to find Markdown links. The pattern is `r'!\[(.[^\]]*)\]\(([^)]*?)(?:\s*\"(.*)\")?\)'`.
*   3.6. Key Public Methods:
    *   3.6.1. `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`
        *   Purpose: Implements the conversion of HTML to Markdown. It uses `CustomHTML2Text` for the base conversion, handles link-to-citation transformation, and integrates with an optional `RelevantContentFilter` to produce `fit_markdown`.
        *   Parameters:
            *   `input_html (str)`: The HTML content to convert.
            *   `base_url (str`, default: `""`)`: Base URL for resolving relative links.
            *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: Options for the `CustomHTML2Text` converter. If not provided, it uses `self.options`.
            *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: Overrides the instance's `content_filter` for this call.
            *   `citations (bool`, default: `True`)`: Whether to convert links to citations.
            *   `**kwargs`: Additional arguments (not currently used by this specific implementation beyond parent class).
        *   Core Logic:
            1.  Instantiates `CustomHTML2Text` using `base_url` and the resolved `html2text_options` (merged from method arg, `self.options`, and defaults).
            2.  Converts `input_html` to `raw_markdown` using the `CustomHTML2Text` instance.
            3.  If `citations` is `True`, calls `self.convert_links_to_citations(raw_markdown, base_url)` to get `markdown_with_citations` and `references_markdown`.
            4.  If `citations` is `False`, `markdown_with_citations` is set to `raw_markdown`, and `references_markdown` is an empty string.
            5.  Determines the active `content_filter` (parameter or instance's `self.content_filter`).
            6.  If an active `content_filter` exists:
                *   Calls `active_filter.filter_content(input_html)` to get a list of filtered HTML strings.
                *   Joins these strings with `\n` and wraps them in `<div>` tags to form `fit_html`.
                *   Uses a new `CustomHTML2Text` instance to convert `fit_html` into `fit_markdown`.
            7.  Otherwise, `fit_html` and `fit_markdown` are set to `None` (or empty strings based on implementation details).
            8.  Constructs and returns a `MarkdownGenerationResult` object with all generated Markdown variants.
    *   3.6.2. `convert_links_to_citations(self, markdown: str, base_url: str = "") -> Tuple[str, str]`
        *   Purpose: Transforms standard Markdown links within the input `markdown` string into a citation format (e.g., `[Link Text]^[1]^`) and generates a corresponding numbered list of references.
        *   Parameters:
            *   `markdown (str)`: The input Markdown string.
            *   `base_url (str`, default: `""`)`: The base URL used to resolve relative link URLs before they are added to the reference list.
        *   Returns: (`Tuple[str, str]`) A tuple where the first element is the Markdown string with links converted to citations, and the second element is a string containing the formatted list of references.
        *   Internal Logic:
            *   Uses the `LINK_PATTERN` regex to find all Markdown links.
            *   For each link, it resolves the URL using `fast_urljoin(base, url)` if `base_url` is provided and the link is relative.
            *   Assigns a unique citation number to each unique URL.
            *   Replaces the original link markup with the citation format (e.g., `[Text]^[Number]^`).
            *   Constructs a Markdown formatted reference list string.
*   3.7. Role of `CustomHTML2Text`:
    *   `CustomHTML2Text` is a customized version of an HTML-to-Markdown converter, likely based on the `html2text` library.
    *   It's instantiated by `DefaultMarkdownGenerator` to perform the core HTML to plain Markdown conversion.
    *   Its behavior is controlled by options passed via `html2text_options` in `generate_markdown` or `self.options` of the `DefaultMarkdownGenerator`. These options can include `body_width`, `ignore_links`, `ignore_images`, etc., influencing the final Markdown output. (Refer to `crawl4ai/html2text.py` for specific options).

## 4. Output Data Model: `MarkdownGenerationResult`

*   4.1. Purpose: `MarkdownGenerationResult` is a Pydantic `BaseModel` designed to structure and encapsulate the various Markdown outputs generated by any `MarkdownGenerationStrategy`. It provides a consistent way to access different versions of the converted content.
*   4.2. Source File: `crawl4ai/models.py`
*   4.3. Fields:
    *   4.3.1. `raw_markdown (str)`: The direct result of converting the input HTML to Markdown, before any citation processing or specific content filtering (by the generator itself) is applied. This represents the most basic Markdown version of the content.
    *   4.3.2. `markdown_with_citations (str)`: Markdown content where hyperlinks have been converted into a citation style (e.g., `[Link Text]^[1]^`). This is typically derived from `raw_markdown`.
    *   4.3.3. `references_markdown (str)`: A string containing a formatted list of references (e.g., numbered list of URLs) corresponding to the citations found in `markdown_with_citations`.
    *   4.3.4. `fit_markdown (Optional[str]`, default: `None`)`: Markdown content generated from HTML that has been processed by a `RelevantContentFilter`. This version is intended to be more concise or focused on relevant parts of the original content. It is `None` if no content filter was applied or if the filter resulted in no content.
    *   4.3.5. `fit_html (Optional[str]`, default: `None`)`: The HTML content that remains after being processed by a `RelevantContentFilter`. `fit_markdown` is generated from this `fit_html`. It is `None` if no content filter was applied or if the filter resulted in no content.
*   4.4. Methods:
    *   4.4.1. `__str__(self) -> str`:
        *   Purpose: Defines the string representation of a `MarkdownGenerationResult` object.
        *   Signature: `__str__(self) -> str`
        *   Returns: (`str`) The content of the `raw_markdown` field.

## 5. Integration with Content Filtering (`RelevantContentFilter`)

*   5.1. Purpose of Integration: `DefaultMarkdownGenerator` allows integration with `RelevantContentFilter` strategies to produce a `fit_markdown` output. This enables generating Markdown from a version of the HTML that has been refined or focused based on relevance criteria defined by the filter (e.g., keywords, semantic similarity, or LLM-based assessment).
*   5.2. Mechanism:
    *   A `RelevantContentFilter` instance can be passed to `DefaultMarkdownGenerator` either during its initialization (via the `content_filter` parameter) or directly to its `generate_markdown` method. The filter passed to `generate_markdown` takes precedence if both are provided.
    *   When an active filter is present, `DefaultMarkdownGenerator.generate_markdown` calls the filter's `filter_content(input_html)` method. This method is expected to return a list of HTML string chunks deemed relevant.
    *   These chunks are then joined (typically with `\n` and wrapped in `<div>` tags) to form the `fit_html` string.
    *   This `fit_html` is then converted to Markdown using `CustomHTML2Text`, and the result is stored as `fit_markdown`.
*   5.3. Impact on `MarkdownGenerationResult`:
    *   If a `RelevantContentFilter` is successfully used:
        *   `MarkdownGenerationResult.fit_markdown` will contain the Markdown derived from the filtered HTML.
        *   `MarkdownGenerationResult.fit_html` will contain the actual filtered HTML string.
    *   If no filter is used, or if the filter returns an empty list of chunks (indicating no content passed the filter), `fit_markdown` and `fit_html` will be `None` (or potentially empty strings, depending on the exact implementation details of joining an empty list).
*   5.4. Supported Filter Types (High-Level Mention):
    *   `PruningContentFilter`: A filter that likely removes irrelevant HTML sections based on predefined rules or structural analysis (e.g., removing common boilerplate like headers, footers, navbars).
    *   `BM25ContentFilter`: A filter that uses the BM25 ranking algorithm to score and select HTML chunks based on their relevance to a user-provided query.
    *   `LLMContentFilter`: A filter that leverages a Large Language Model to assess the relevance of HTML chunks, potentially based on a user query or a general understanding of content importance.
    *   *Note: Detailed descriptions and usage of each filter strategy are covered in their respective documentation sections.*

## 6. Configuration via `CrawlerRunConfig`

*   6.1. `CrawlerRunConfig.markdown_generator`
    *   Purpose: This attribute of the `CrawlerRunConfig` class allows a user to specify a custom `MarkdownGenerationStrategy` instance to be used for the markdown conversion phase of a crawl. This provides flexibility in how HTML content is transformed into Markdown.
    *   Type: `MarkdownGenerationStrategy` (accepts any concrete implementation of this ABC).
    *   Default Value: If not specified, an instance of `DefaultMarkdownGenerator()` is used by default within the `AsyncWebCrawler`'s `aprocess_html` method when `config.markdown_generator` is `None`.
    *   Usage Example:
        ```python
        from crawl4ai import CrawlerRunConfig, DefaultMarkdownGenerator, AsyncWebCrawler
        from crawl4ai.content_filter_strategy import BM25ContentFilter
        import asyncio

        # Example: Configure a markdown generator with a BM25 filter
        bm25_filter = BM25ContentFilter(user_query="Python programming language")
        custom_md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)

        run_config_with_custom_md = CrawlerRunConfig(
            markdown_generator=custom_md_generator,
            # Other run configurations...
        )

        async def example_crawl():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(
                    url="https://en.wikipedia.org/wiki/Python_(programming_language)",
                    config=run_config_with_custom_md
                )
                if result.success and result.markdown:
                    print("Raw Markdown (snippet):", result.markdown.raw_markdown[:200])
                    if result.markdown.fit_markdown:
                        print("Fit Markdown (snippet):", result.markdown.fit_markdown[:200])
        
        # asyncio.run(example_crawl())
        ```

## 7. Influencing Markdown Output for LLM Consumption

*   7.1. Role of `DefaultMarkdownGenerator.options` and `html2text_options`:
    *   The `options` parameter in `DefaultMarkdownGenerator.__init__` and the `html2text_options` parameter in its `generate_markdown` method are used to pass configuration settings directly to the underlying `CustomHTML2Text` instance.
    *   `html2text_options` provided to `generate_markdown` will take precedence over `self.options` set during initialization.
    *   These options control various aspects of the HTML-to-Markdown conversion, such as line wrapping, handling of links, images, and emphasis, which can be crucial for preparing text for LLMs.
*   7.2. Key `CustomHTML2Text` Options (via `html2text_options` or `DefaultMarkdownGenerator.options`):
    *   `bodywidth (int`, default: `0` when `DefaultMarkdownGenerator` calls `CustomHTML2Text` for `raw_markdown` and `fit_markdown` if not otherwise specified): Determines the width for wrapping lines. A value of `0` disables line wrapping, which is often preferred for LLM processing as it preserves sentence structure across lines.
    *   `ignore_links (bool`, default: `False` in `CustomHTML2Text`): If `True`, all hyperlinks (`<a>` tags) are removed from the output, leaving only their anchor text.
    *   `ignore_images (bool`, default: `False` in `CustomHTML2Text`): If `True`, all image tags (`<img>`) are removed from the output.
    *   `ignore_emphasis (bool`, default: `False` in `CustomHTML2Text`): If `True`, emphasized text (e.g., `<em>`, `<strong>`) is rendered as plain text without Markdown emphasis characters (like `*` or `_`).
    *   `bypass_tables (bool`, default: `False` in `CustomHTML2Text`): If `True`, tables are not formatted as Markdown tables but are rendered as a series of paragraphs, which might be easier for some LLMs to process.
    *   `default_image_alt (str`, default: `""` in `CustomHTML2Text`): Specifies a default alt text for images that do not have an `alt` attribute.
    *   `protect_links (bool`, default: `False` in `CustomHTML2Text`): If `True`, URLs in links are not processed or modified.
    *   `single_line_break (bool`, default: `True` in `CustomHTML2Text`): If `True`, single newlines in HTML are converted to Markdown line breaks (two spaces then a newline). This can help preserve some formatting.
    *   `mark_code (bool`, default: `True` in `CustomHTML2Text`): If `True`, `<code>` and `<pre>` blocks are appropriately marked in Markdown.
    *   `escape_snob (bool`, default: `False` in `CustomHTML2Text`): If `True`, more aggressive escaping of special Markdown characters is performed.
    *   *Note: This list is based on common `html2text` options; refer to `crawl4ai/html2text.py` for the exact implementation and default behaviors within `CustomHTML2Text`.*
*   7.3. Impact of `citations (bool)` in `generate_markdown`:
    *   When `citations=True` (default in `DefaultMarkdownGenerator.generate_markdown`):
        *   Standard Markdown links `[text](url)` are converted to `[text]^[citation_number]^`.
        *   A `references_markdown` string is generated, listing all unique URLs with their corresponding citation numbers. This helps LLMs trace information back to its source and can reduce token count if URLs are long or repetitive.
    *   When `citations=False`:
        *   Links remain in their original Markdown format `[text](url)`.
        *   `references_markdown` will be an empty string.
        *   This might be preferred if the LLM needs to directly process the URLs or if the citation format is not desired.
*   7.4. Role of `content_source` in `MarkdownGenerationStrategy`:
    *   This parameter (defaulting to `"cleaned_html"` in `DefaultMarkdownGenerator`) specifies which HTML version is used as the primary input for the `generate_markdown` method.
    *   `"cleaned_html"`: Typically refers to HTML that has undergone initial processing by the `ContentScrapingStrategy` (e.g., removal of scripts, styles, and potentially some boilerplate based on the scraping strategy's rules). This is usually the recommended source for general Markdown conversion.
    *   `"raw_html"`: The original, unmodified HTML content fetched from the web page. Using this source would bypass any initial cleaning done by the scraping strategy.
    *   `"fit_html"`: This source is relevant when a `RelevantContentFilter` is used. `fit_html` is the HTML output *after* the `RelevantContentFilter` has processed the `input_html` (which itself is determined by `content_source`). If `content_source` is, for example, `"cleaned_html"`, then `fit_html` is the result of filtering that cleaned HTML. `fit_markdown` is then generated from this `fit_html`.
*   7.5. `fit_markdown` vs. `raw_markdown`/`markdown_with_citations`:
    *   `raw_markdown` (or `markdown_with_citations` if `citations=True`) is generated from the HTML specified by `content_source` (e.g., `"cleaned_html"`). It represents a general conversion of that source.
    *   `fit_markdown` is generated *only if* a `RelevantContentFilter` is active (either set in `DefaultMarkdownGenerator` or passed to `generate_markdown`). It is derived from the `fit_html` (the output of the content filter).
    *   **Choosing which to use for LLMs:**
        *   Use `fit_markdown` when you need a concise, highly relevant subset of the page's content tailored to a specific query or set of criteria defined by the filter. This can reduce noise and token count for the LLM.
        *   Use `raw_markdown` or `markdown_with_citations` when you need a more comprehensive representation of the page's textual content, or when no specific filtering criteria are applied.
```

---


## Markdown Generation - Reasoning
Source: crawl4ai_markdown_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - markdown Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_markdown_generation.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Markdown Generation in Crawl4AI

*   1.1. **Why Markdown Generation Matters for LLMs**
    *   1.1.1. The role of clean, structured text for Large Language Model consumption.
        *   **Explanation:** LLMs perform significantly better when input data is well-structured and free of irrelevant noise (like HTML tags, scripts, or complex layouts not meant for textual understanding). Markdown, with its simple syntax, provides a human-readable and machine-parseable format that captures essential semantic structure (headings, lists, paragraphs, code blocks, tables) without the clutter of full HTML. This makes it easier for LLMs to understand the content's hierarchy, identify key information, and perform tasks like summarization, question-answering, or RAG (Retrieval Augmented Generation) more accurately and efficiently.
    *   1.1.2. Benefits of Markdown: readability, structure preservation, common format.
        *   **Explanation:**
            *   **Readability:** Markdown is designed to be easily readable in its raw form, making it simple for developers and users to inspect and understand the crawled content.
            *   **Structure Preservation:** It effectively preserves the semantic structure of the original HTML (headings, lists, emphasis, etc.), which is crucial context for LLMs.
            *   **Common Format:** Markdown is a widely adopted standard, ensuring compatibility with a vast ecosystem of tools, editors, and LLM input pipelines.
    *   1.1.3. How Crawl4AI's Markdown generation facilitates RAG and other LLM applications.
        *   **Explanation:** For RAG, Crawl4AI's Markdown output, especially when combined with content filtering, provides clean, relevant text chunks that can be easily embedded and indexed. This improves the quality of retrieved context for LLM prompts. For fine-tuning or direct prompting, the structured Markdown helps the LLM focus on the core content, leading to better quality responses and reducing token consumption by eliminating HTML overhead.

*   1.2. **Overview of Crawl4AI's Markdown Generation Pipeline**
    *   1.2.1. High-level flow: HTML -> (Optional Filtering) -> Markdown Conversion -> (Optional Citation Handling).
        *   **Explanation:**
            1.  **Input HTML:** The process starts with either raw HTML from the crawled page or a cleaned/selected HTML segment.
            2.  **Optional Content Filtering:** Before Markdown conversion, a `RelevantContentFilter` can be applied to the HTML. This step aims to remove boilerplate, ads, or irrelevant sections, resulting in `fit_html`. This is crucial for generating `fit_markdown`.
            3.  **Markdown Conversion:** The selected HTML (either the original, cleaned, or filtered `fit_html`) is converted into Markdown using an underlying `html2text` library, specifically `CustomHTML2Text` in Crawl4AI for enhanced control.
            4.  **Optional Citation Handling:** If enabled, inline links in the generated Markdown are converted to a citation format (e.g., `text [^1^]`), and a separate list of references is created.
    *   1.2.2. Key components involved: `MarkdownGenerationStrategy`, `DefaultMarkdownGenerator`, `CustomHTML2Text`, `RelevantContentFilter`.
        *   **Explanation:**
            *   **`MarkdownGenerationStrategy`:** An interface defining how Markdown should be generated. Allows for custom implementations.
            *   **`DefaultMarkdownGenerator`:** The standard implementation of `MarkdownGenerationStrategy`, using `CustomHTML2Text`. It orchestrates filtering (if provided) and citation handling.
            *   **`CustomHTML2Text`:** An enhanced version of the `html2text` library, providing fine-grained control over the HTML-to-Markdown conversion.
            *   **`RelevantContentFilter`:** An interface for strategies that filter HTML content before it's converted to Markdown, producing `fit_html` and consequently `fit_markdown`.
    *   1.2.3. How `CrawlerRunConfig` ties these components together.
        *   **Explanation:** The `CrawlerRunConfig` object allows you to specify which `MarkdownGenerationStrategy` (and by extension, which filters and `CustomHTML2Text` options) should be used for a particular crawl run via its `markdown_generator` parameter. This provides run-specific control over the Markdown output.

*   1.3. **Goals of this Guide**
    *   1.3.1. Understanding how to configure and customize Markdown output.
        *   **Explanation:** This guide will walk you through the various configuration options available, from choosing HTML sources and content filters to fine-tuning the `html2text` conversion itself.
    *   1.3.2. Best practices for generating LLM-friendly Markdown.
        *   **Explanation:** We'll discuss tips and techniques to produce Markdown that is optimally structured and cleaned for consumption by Large Language Models.
    *   1.3.3. Troubleshooting common Markdown generation issues.
        *   **Explanation:** We'll cover common problems encountered during Markdown generation (e.g., noisy output, missing content) and provide strategies for diagnosing and resolving them.

## 2. Core Concepts in Markdown Generation

*   2.1. **The `MarkdownGenerationStrategy` Interface**
    *   2.1.1. **Purpose and Design Rationale:**
        *   Why use a strategy pattern for Markdown generation? (Flexibility, extensibility).
            *   **Explanation:** The strategy pattern allows Crawl4AI to define a common interface for Markdown generation while enabling different concrete implementations. This means users can easily swap out the default Markdown generator for a custom one without altering the core crawler logic. It promotes flexibility and makes the system extensible for future Markdown conversion needs or integration with other libraries.
        *   Core problem it solves: Decoupling Markdown generation logic from the crawler.
            *   **Explanation:** By abstracting Markdown generation into a strategy, the `AsyncWebCrawler` itself doesn't need to know the specifics of *how* Markdown is created. It simply delegates the task to the configured strategy. This separation of concerns makes the codebase cleaner and easier to maintain.
    *   2.1.2. **When to Implement a Custom `MarkdownGenerationStrategy`:**
        *   Scenarios requiring completely different Markdown conversion logic.
            *   **Example:** If you need to convert HTML to a very specific dialect of Markdown not supported by `html2text`, or if you want to use a different underlying conversion library entirely.
        *   Integrating third-party Markdown conversion libraries.
            *   **Example:** If you prefer to use a library like `turndown` or `mistune` for its specific features or output style.
        *   Advanced pre/post-processing of Markdown.
            *   **Example:** If you need to perform complex transformations on the Markdown *after* initial generation, such as custom table formatting, complex footnote handling beyond standard citations, or domain-specific semantic tagging within the Markdown.
    *   2.1.3. **How to Implement a Custom `MarkdownGenerationStrategy`:**
        *   Key methods to override (`generate_markdown`).
            *   **Explanation:** The primary method to implement is `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`. This method will receive the HTML (based on `content_source`), and it's responsible for returning a `MarkdownGenerationResult` object.
        *   Input parameters and expected output (`MarkdownGenerationResult`).
            *   **Explanation:** Your custom strategy will receive the `input_html`, the `base_url` (for resolving relative links if needed), `html2text_options` (which you can choose to use or ignore), an optional `content_filter`, and a `citations` flag. It must return an instance of `MarkdownGenerationResult` populated with the relevant Markdown strings.
        *   *Code Example:*
            ```python
            from crawl4ai import MarkdownGenerationStrategy, MarkdownGenerationResult, RelevantContentFilter
            from typing import Optional, Dict, Any

            class MyCustomMarkdownStrategy(MarkdownGenerationStrategy):
                def __init__(self, content_source: str = "cleaned_html", **kwargs):
                    super().__init__(content_source=content_source, **kwargs)
                    # Initialize any custom resources if needed

                def generate_markdown(
                    self,
                    input_html: str,
                    base_url: str = "",
                    html2text_options: Optional[Dict[str, Any]] = None, # You can use or ignore these
                    content_filter: Optional[RelevantContentFilter] = None,
                    citations: bool = True, # You can decide how to handle this
                    **kwargs
                ) -> MarkdownGenerationResult:
                    
                    # 1. Apply content filter if provided and desired
                    fit_html_output = ""
                    if content_filter:
                        # Assuming content_filter.filter_content returns a list of HTML strings
                        filtered_html_blocks = content_filter.filter_content(input_html) 
                        fit_html_output = "\n".join(filtered_html_blocks)
                    
                    # 2. Your custom HTML to Markdown conversion logic
                    # This is where you'd use your preferred library or custom logic
                    raw_markdown_text = f"# Custom Markdown for {base_url}\n\n{input_html[:200]}..." # Placeholder
                    
                    markdown_with_citations_text = raw_markdown_text # Placeholder for citation logic
                    references_markdown_text = "" # Placeholder for references

                    # If you used a filter, also generate fit_markdown
                    fit_markdown_text = ""
                    if fit_html_output:
                        fit_markdown_text = f"# Custom Filtered Markdown\n\n{fit_html_output[:200]}..." # Placeholder

                    return MarkdownGenerationResult(
                        raw_markdown=raw_markdown_text,
                        markdown_with_citations=markdown_with_citations_text,
                        references_markdown=references_markdown_text,
                        fit_markdown=fit_markdown_text,
                        fit_html=fit_html_output
                    )

            # Usage:
            # custom_md_generator = MyCustomMarkdownStrategy()
            # run_config = CrawlerRunConfig(markdown_generator=custom_md_generator)
            ```
        *   Common pitfalls when creating custom strategies.
            *   **Explanation:**
                *   Forgetting to handle all fields in `MarkdownGenerationResult` (even if some are empty strings).
                *   Incorrectly managing `base_url` for relative links if your custom converter doesn't handle it.
                *   Performance bottlenecks if your custom logic is inefficient.
                *   Not properly integrating with the `content_filter` if one is provided.
    *   2.1.4. **Understanding `content_source` in `MarkdownGenerationStrategy`**
        *   2.1.4.1. Purpose: What HTML source should be used for Markdown generation?
            *   **Explanation:** The `content_source` attribute of a `MarkdownGenerationStrategy` (including `DefaultMarkdownGenerator`) tells the strategy which version of the HTML to use as the primary input for generating `raw_markdown` and `markdown_with_citations`.
        *   2.1.4.2. Available options: `"cleaned_html"`, `"raw_html"`, `"fit_html"`.
            *   **`"cleaned_html"` (Default):** This is the HTML after Crawl4AI's internal `ContentScrapingStrategy` (e.g., `WebScrapingStrategy` or `LXMLWebScrapingStrategy`) has processed it. This usually involves removing scripts, styles, and applying structural cleaning or selection based on `target_elements` or `css_selector` in `CrawlerRunConfig`.
            *   **`"raw_html"`:** The original, unmodified HTML fetched from the page. This is useful if you want to apply your own complete cleaning and Markdown conversion pipeline.
            *   **`"fit_html"`:** The HTML *after* a `RelevantContentFilter` (if provided to the `MarkdownGenerationStrategy`) has processed the input HTML (which would be `cleaned_html` or `raw_html` depending on the initial source). This option is powerful when you want Markdown generated *only* from the most relevant parts of the page.
        *   2.1.4.3. **Decision Guide: Choosing the Right `content_source`**:
            *   **When to use `"cleaned_html"`:** This is the recommended default for most LLM use cases. It provides a good balance of structured content without excessive noise, as common boilerplate is often removed by the scraping strategy.
            *   **When to use `"raw_html"`:** Choose this if you need absolute control over the HTML input for your Markdown converter, or if Crawl4AI's default cleaning removes elements you wish to keep. Be aware that this might result in noisier Markdown.
            *   **When to use `"fit_html"`:** Opt for this when you are using a `RelevantContentFilter` with your `MarkdownGenerationStrategy` and you want the `raw_markdown` and `markdown_with_citations` to be based *only* on the filtered content. This is distinct from just using the `fit_markdown` field in the result, as it makes the filtered content the *primary* source for all main Markdown outputs.
            *   **Impact on performance and output quality:**
                *   `"raw_html"` might be slightly faster if Crawl4AI's cleaning is complex, but could lead to lower quality Markdown due to more noise.
                *   `"cleaned_html"` offers a good trade-off.
                *   `"fit_html"` depends on the performance of the `RelevantContentFilter` itself.
        *   2.1.4.4. *Example Scenarios:*
            *   **General Summarization:** `"cleaned_html"` is usually best.
            *   **Highly Specific Q&A on a Section:** Use a `RelevantContentFilter` to produce `fit_html`, then set `content_source="fit_html"` (or just use the `fit_markdown` from the result if `raw_markdown` from `"cleaned_html"` is also desired).
            *   **Archiving Raw Structure:** `"raw_html"` might be chosen if the goal is to convert the entire, unmodified page structure to Markdown, perhaps for later, more nuanced processing.

*   2.2. **The `MarkdownGenerationResult` Model**
    *   2.2.1. **Understanding its Purpose:** Why a structured result object?
        *   **Explanation:** A structured object like `MarkdownGenerationResult` is used instead of a single Markdown string to provide different views or versions of the generated Markdown, catering to various use cases. This allows users to pick the representation that best suits their needs (e.g., with or without citations, raw vs. filtered) without re-processing. It also clearly separates the main content from metadata like references or the intermediate `fit_html`.
    *   2.2.2. **Deep Dive into `MarkdownGenerationResult` Fields:**
        *   `raw_markdown`:
            *   **What it is:** This is the direct, primary Markdown output generated from the `content_source` (e.g., `cleaned_html`) defined in the `MarkdownGenerationStrategy`. It does *not* have inline links converted to citation format.
            *   **How to use it:** Use this when you need the most "vanilla" Markdown, perhaps for LLMs that are sensitive to citation formats or if you plan to implement your own link/reference handling.
            *   **When it's useful:** For direct input to LLMs that don't require source attribution within the text, or as a base for further custom Markdown processing.
        *   `markdown_with_citations`:
            *   **What it is:** This takes the `raw_markdown` and converts its inline links (e.g., `[link text](http://example.com)`) into a citation format (e.g., `link text [^1^]`).
            *   **How it's generated:** The `DefaultMarkdownGenerator` (via `CustomHTML2Text`) scans `raw_markdown` for links, assigns unique numerical IDs to each unique URL, replaces the inline link with the text and citation marker, and populates `references_markdown`.
            *   **How to use it:** This is often the most useful Markdown for LLM tasks requiring RAG or for generating human-readable documents where sources are important. Combine it with `references_markdown`.
            *   *Example:*
                ```html
                <!-- Input HTML fragment -->
                <p>Crawl4AI is an <a href="https://github.com/unclecode/crawl4ai">open-source</a> library.</p>
                ```
                ```markdown
                // Resulting markdown_with_citations (simplified)
                Crawl4AI is an open-source [^1^] library.
                ```
        *   `references_markdown`:
            *   **What it is:** A separate Markdown string that lists all unique URLs found and converted to citations, formatted typically as a numbered list.
            *   **How to use it:** Append this string to the end of `markdown_with_citations` to create a complete document with a bibliography or reference section.
            *   **Why it's separate:** This provides flexibility. You can choose to display references at the end, in a sidebar, or not at all.
            *   *Example:*
                ```markdown
                ## References

                [^1^]: https://github.com/unclecode/crawl4ai
                ```
        *   `fit_markdown`:
            *   **What it is:** This is Markdown generated *exclusively* from the `fit_html`. `fit_html` itself is the output of a `RelevantContentFilter` if one was provided to the `MarkdownGenerationStrategy`. If no filter was used, `fit_markdown` will likely be empty or reflect the `raw_markdown`.
            *   **How to use it:** When your primary goal is to feed an LLM with the most relevant, filtered content. This is excellent for tasks like generating concise summaries or providing highly focused context for RAG.
            *   **Relationship with `raw_markdown`:** If a filter is active, `fit_markdown` is based on a *subset* or *transformed version* of the HTML that `raw_markdown` was based on (assuming `content_source` wasn't `"fit_html"`). If `content_source` *was* `"fit_html"`, then `raw_markdown` and `fit_markdown` would be derived from the same filtered HTML, but `fit_markdown` might still undergo different processing if the strategy handles it distinctly.
            *   *Example:* Imagine a news article page. `raw_markdown` might contain the article, comments, ads, and navigation. If a `BM25ContentFilter` is used with a query about "stock market impact", `fit_markdown` would ideally only contain paragraphs related to that topic, stripped of other page elements.
        *   `fit_html`:
            *   **What it is:** The actual HTML string *after* a `RelevantContentFilter` (like `PruningContentFilter` or `LLMContentFilter`) has processed the input HTML. If no filter is applied, this field will be empty.
            *   **How to use it:** Primarily for debugging your content filters. You can inspect `fit_html` to see exactly what HTML content was deemed "relevant" by your filter before it was converted to `fit_markdown`. It can also be useful if you need this filtered HTML for purposes other than Markdown generation.
            *   **Why it's included:** It provides transparency into the filtering process and allows advanced users to work with the intermediate filtered HTML directly.

## 3. The `DefaultMarkdownGenerator` - Your Go-To Solution

*   3.1. **Understanding the `DefaultMarkdownGenerator`**
    *   3.1.1. **Purpose and Design:** The `DefaultMarkdownGenerator` is Crawl4AI's standard, out-of-the-box mechanism for converting HTML content into various Markdown representations. It's designed to be a robust and generally applicable solution for most common use cases, especially when targeting LLM consumption.
    *   3.1.2. Core Functionality: Its primary task is to orchestrate the HTML-to-Markdown conversion. It internally uses an instance of `CustomHTML2Text` (Crawl4AI's enhanced `html2text` wrapper) to perform the actual conversion.
    *   3.1.3. How it handles citations and references by default.
        *   **Explanation:** If the `citations` parameter in its `generate_markdown` method is `True` (which it is by default), `DefaultMarkdownGenerator` will post-process the initially generated Markdown to convert inline links into citation markers (e.g., `[^1^]`) and generate a corresponding `references_markdown` block. This is done by its internal `CustomHTML2Text` instance.

*   3.2. **Configuring `DefaultMarkdownGenerator`**
    *   3.2.1. **Initialization Options:**
        *   `content_filter (Optional[RelevantContentFilter])`:
            *   **Why use it:** To refine the HTML *before* it's converted to Markdown. This is essential if you want `fit_markdown` (and consequently `fit_html`) to contain only the most relevant parts of the page, leading to a more focused Markdown output.
            *   **How it integrates:** When `generate_markdown` is called, if a `content_filter` is present, `DefaultMarkdownGenerator` first passes the `input_html` (determined by `content_source`) to this filter. The filter returns a list of HTML strings (or a single string if merged). This filtered HTML becomes the `fit_html`. Then, `fit_markdown` is generated from this `fit_html`. The `raw_markdown` and `markdown_with_citations` are still generated from the original `content_source` unless `content_source` itself is set to `"fit_html"`.
            *   *Impact:* Directly influences `fit_markdown` and `fit_html` fields in `MarkdownGenerationResult`. Can significantly reduce the noise and improve the relevance of the final Markdown for LLMs.
            *   *Code Example:*
                ```python
                from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig
                from crawl4ai.content_filter_strategy import PruningContentFilter

                # Initialize a filter
                pruning_filter = PruningContentFilter(threshold_type="fixed", threshold=0.5)
                
                # Initialize DefaultMarkdownGenerator with the filter
                md_generator_with_filter = DefaultMarkdownGenerator(content_filter=pruning_filter)

                # This generator will now produce 'fit_markdown' based on pruning.
                # run_config = CrawlerRunConfig(markdown_generator=md_generator_with_filter)
                # result = await crawler.arun(url="...", config=run_config)
                # print(result.markdown.fit_markdown) 
                ```
        *   `options (Optional[Dict[str, Any]])`:
            *   **What it is:** This dictionary allows you to pass configuration options directly to the underlying `CustomHTML2Text` instance. These options control the specifics of the HTML-to-Markdown conversion process.
            *   **How to use it:** Provide a dictionary where keys are `html2text` option names (e.g., `body_width`, `ignore_links`) and values are their desired settings.
            *   *See Section 6: Mastering `CustomHTML2Text` for detailed options.*
        *   `content_source (str)`:
            *   **Reiteration:** As discussed in section 2.1.4, this determines the primary HTML input for `raw_markdown` and `markdown_with_citations`.
            *   **How it interacts with `content_filter`:**
                *   If `content_source` is, for example, `"cleaned_html"` and a `content_filter` is also provided, the `content_filter` will process this `"cleaned_html"` to produce `fit_html`. The `fit_markdown` field in `MarkdownGenerationResult` will be based on this `fit_html`.
                *   However, `raw_markdown` and `markdown_with_citations` will still be based on the original `"cleaned_html"` (unless `content_source` was explicitly set to `"fit_html"`). This allows you to have both a "fuller" Markdown and a "filtered" Markdown from a single generation step.

*   3.3. **Common Workflows with `DefaultMarkdownGenerator`**
    *   3.3.1. **Workflow: Generating Basic Markdown with Citations**
        *   Steps: Instantiate `DefaultMarkdownGenerator` (or use the crawler's default). The crawler calls its `generate_markdown` method. Access `result.markdown.markdown_with_citations` and `result.markdown.references_markdown`.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

            async def basic_markdown_workflow():
                # DefaultMarkdownGenerator is used implicitly if none is specified in CrawlerRunConfig
                # Or explicitly:
                md_generator = DefaultMarkdownGenerator() 
                run_config = CrawlerRunConfig(markdown_generator=md_generator)

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                    if result.success:
                        print("--- Markdown with Citations ---")
                        print(result.markdown.markdown_with_citations[:500]) # Show first 500 chars
                        print("\n--- References ---")
                        print(result.markdown.references_markdown)
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
    *   3.3.2. **Workflow: Generating Focused Markdown using a Content Filter**
        *   Steps:
            1.  Choose and instantiate a `RelevantContentFilter` (e.g., `BM25ContentFilter`).
            2.  Instantiate `DefaultMarkdownGenerator`, passing the filter to its `content_filter` parameter.
            3.  Set this `DefaultMarkdownGenerator` instance in `CrawlerRunConfig.markdown_generator`.
            4.  After crawling, access `result.markdown.fit_markdown`.
        *   Key configuration considerations for the filter and generator:
            *   For `BM25ContentFilter`, ensure you provide a relevant `user_query`.
            *   Adjust filter thresholds (e.g., `bm25_threshold`) as needed.
            *   The `content_source` for `DefaultMarkdownGenerator` will be the input to the filter.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
            from crawl4ai.content_filter_strategy import BM25ContentFilter

            async def filtered_markdown_workflow():
                user_query = "information about Crawl4AI library"
                bm25_filter = BM25ContentFilter(user_query=user_query, bm25_threshold=0.1)
                
                md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
                
                run_config = CrawlerRunConfig(
                    markdown_generator=md_generator,
                    cache_mode=CacheMode.BYPASS # For consistent demo results
                )

                async with AsyncWebCrawler() as crawler:
                    # Using a page that hopefully has content related to the query
                    result = await crawler.arun(url="https://github.com/unclecode/crawl4ai", config=run_config) 
                    if result.success:
                        print("--- Fit Markdown (BM25 Filtered) ---")
                        print(result.markdown.fit_markdown) # This is the key output
                        # You can also inspect fit_html to see what the filter selected
                        # print("\n--- Fit HTML ---")
                        # print(result.markdown.fit_html[:500])
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
    *   3.3.3. **Workflow: Customizing Markdown Style via `html2text_options`**
        *   Steps: Instantiate `DefaultMarkdownGenerator` passing a dictionary of `html2text` options to its `options` parameter.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

            async def custom_style_markdown_workflow():
                # Example: Disable line wrapping and ignore images
                html2text_opts = {
                    "body_width": 0,      # Disable line wrapping
                    "ignore_images": True # Don't include image markdown ![alt](src)
                }
                md_generator = DefaultMarkdownGenerator(options=html2text_opts)
                
                run_config = CrawlerRunConfig(markdown_generator=md_generator)

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                    if result.success:
                        print("--- Custom Styled Markdown (No Wrap, No Images) ---")
                        print(result.markdown.raw_markdown[:500]) # raw_markdown will reflect these options
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
*   3.4. **Best Practices for `DefaultMarkdownGenerator`**
    *   **When to use `DefaultMarkdownGenerator` vs. a custom strategy:**
        *   Use `DefaultMarkdownGenerator` for most cases. It's robust and highly configurable through `content_filter` and `html2text_options`.
        *   Opt for a custom strategy only if you need fundamentally different conversion logic or integration with external Markdown libraries that `CustomHTML2Text` doesn't cover.
    *   **Tips for choosing the right `content_source` and `content_filter`:**
        *   Start with `content_source="cleaned_html"` (default) and no filter.
        *   If the output is too noisy, introduce a `RelevantContentFilter`. `PruningContentFilter` is a good first step for general boilerplate. Use `BM25ContentFilter` or `LLMContentFilter` for more targeted filtering based on semantic relevance.
        *   If your filter is very effective and you *only* want Markdown from the filtered content, consider setting `content_source="fit_html"` in your `DefaultMarkdownGenerator` instance.
    *   **How to leverage `MarkdownGenerationResult` effectively:**
        *   For LLM input where source attribution is important, use `markdown_with_citations` + `references_markdown`.
        *   For tasks needing maximum conciseness based on relevance, use `fit_markdown` (after configuring a `content_filter`).
        *   Use `raw_markdown` if you need the "purest" Markdown conversion without citation processing.
        *   Inspect `fit_html` to debug your content filters.

## 4. Integrating Content Filters for Smarter Markdown (`fit_markdown`)

*   4.1. **The "Why": Purpose of Content Filtering Before Markdown Generation**
    *   4.1.1. Reducing noise and improving relevance for LLMs.
        *   **Explanation:** Web pages often contain much more than just the main article content (e.g., navigation, ads, footers, related articles). These can be detrimental to LLM performance, increasing token count, processing time, and potentially confusing the model. Content filters aim to isolate the core, relevant information.
    *   4.1.2. Generating more concise and focused Markdown (`fit_markdown`).
        *   **Explanation:** By filtering the HTML *before* converting it to Markdown, the resulting `fit_markdown` is inherently more concise and focused on what the filter deemed important. This is ideal for tasks where brevity and relevance are key.
    *   4.1.3. How `fit_html` is generated and its role.
        *   **Explanation:** When a `RelevantContentFilter` is used with a `MarkdownGenerationStrategy`, the strategy first passes the input HTML (e.g., `cleaned_html`) to the filter's `filter_content` method. This method returns a list of HTML strings (or a single merged string). This output is stored as `fit_html` in the `MarkdownGenerationResult`. `fit_markdown` is then generated by converting this `fit_html` to Markdown.

*   4.2. **Overview of `RelevantContentFilter` Strategies**
    *   4.2.1. **`PruningContentFilter`**:
        *   **How it works:** Applies heuristic rules to remove common boilerplate. For example, it might remove elements with very short text content, elements with a high link-to-text ratio, or elements matching common boilerplate CSS classes/IDs (like "footer", "nav", "sidebar").
        *   **When to use it:** A good first-pass filter for general-purpose cleaning. It's fast and doesn't require LLM calls or complex configuration.
        *   **Impact on `fit_markdown`:** Typically good at removing obvious non-content sections, resulting in a cleaner, more article-focused Markdown.
    *   4.2.2. **`BM25ContentFilter`**:
        *   **How it works:** This filter uses the BM25 algorithm, a classical information retrieval technique. It tokenizes the HTML content into chunks and scores each chunk's relevance against a `user_query`. Chunks exceeding a `bm25_threshold` are kept.
        *   **When to use it:** When you want to extract content specifically related to a user's query from a larger page. Excellent for targeted information retrieval.
        *   **Impact on `fit_markdown`:** The output will be highly tailored to the query. If the query is "Tell me about Crawl4AI's caching", `fit_markdown` should primarily contain sections discussing caching.
    *   4.2.3. **`LLMContentFilter`**:
        *   **How it works:** This is the most powerful and flexible filter. It chunks the input HTML and sends each chunk (or a summary) to an LLM with specific `instructions` (e.g., "Extract only the paragraphs discussing financial results"). The LLM decides which chunks are relevant.
        *   **When to use it:** For complex filtering criteria that are hard to express with rules or keywords, or when nuanced understanding of content is required.
        *   **Impact on `fit_markdown`:** Can produce very precise and contextually relevant Markdown. However, it's generally slower and can be more expensive due to LLM API calls.
*   4.3. **Decision Guide: Choosing the Right `RelevantContentFilter`**
    *   *Table:*
        | Filter                | Speed      | Cost (LLM API) | Accuracy/Nuance | Use Case Examples                                  | Configuration Complexity |
        |-----------------------|------------|----------------|-----------------|----------------------------------------------------|--------------------------|
        | `PruningContentFilter`| Very Fast  | None           | Low-Medium      | General boilerplate removal, quick cleaning.       | Low                      |
        | `BM25ContentFilter`   | Fast       | None           | Medium          | Query-focused extraction, finding relevant sections. | Medium (query, threshold)|
        | `LLMContentFilter`    | Slow       | Potentially High| High            | Complex criteria, nuanced extraction, summarization. | High (prompt engineering)  |
    *   Factors to consider:
        *   **Desired Output Quality:** For the highest semantic relevance, `LLMContentFilter` is often best, but at a cost.
        *   **Performance Constraints:** If speed is critical, `PruningContentFilter` or `BM25ContentFilter` are preferred.
        *   **Nature of the HTML Content:** For well-structured articles, `PruningContentFilter` might be sufficient. For diverse content or Q&A, `BM25ContentFilter` or `LLMContentFilter` might be better.
        *   **Specificity of Task:** If you have a clear query, `BM25ContentFilter` excels. If you have complex instructions, `LLMContentFilter` is suitable.
*   4.4. **Code Examples: Combining Filters with `DefaultMarkdownGenerator`**
    *   4.4.1. *Example:* [Using `PruningContentFilter` to generate `fit_markdown`].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, CacheMode
        from crawl4ai.content_filter_strategy import PruningContentFilter

        async def pruning_filter_example():
            pruning_filter = PruningContentFilter(threshold=0.4, threshold_type="fixed") # Adjust threshold as needed
            md_generator = DefaultMarkdownGenerator(content_filter=pruning_filter)
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print("--- Fit Markdown (Pruned) ---")
                    print(result.markdown.fit_markdown[:1000]) # Show first 1000 chars
                    # print("\n--- Original Raw Markdown (for comparison) ---")
                    # print(result.markdown.raw_markdown[:1000])
        ```
    *   4.4.2. *Example:* [Using `BM25ContentFilter` with a query to generate query-focused `fit_markdown`].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, CacheMode
        from crawl4ai.content_filter_strategy import BM25ContentFilter

        async def bm25_filter_example():
            user_query = "Python syntax and semantics"
            bm25_filter = BM25ContentFilter(user_query=user_query, bm25_threshold=0.1)
            md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)
            
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print(f"--- Fit Markdown (BM25 Filtered for query: '{user_query}') ---")
                    print(result.markdown.fit_markdown)
        ```
    *   4.4.3. *Example:* [Using `LLMContentFilter` for nuanced content selection before Markdown generation].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, LLMConfig, CacheMode
        from crawl4ai.content_filter_strategy import LLMContentFilter
        import os

        async def llm_filter_example():
            # Ensure OPENAI_API_KEY is set in your environment
            if not os.getenv("OPENAI_API_KEY"):
                print("OPENAI_API_KEY not set. Skipping LLMContentFilter example.")
                return

            llm_config_obj = LLMConfig(provider="openai/gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
            
            instruction = "Extract only the sections that discuss Python's history and its creator."
            llm_filter = LLMContentFilter(
                llm_config=llm_config_obj,
                instruction=instruction,
                # chunk_token_threshold=1000 # Adjust as needed
            )
            
            md_generator = DefaultMarkdownGenerator(content_filter=llm_filter, content_source="cleaned_html")
            
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print(f"--- Fit Markdown (LLM Filtered with instruction: '{instruction}') ---")
                    print(result.markdown.fit_markdown)
                    llm_filter.show_usage() # Display token usage
                else:
                    print(f"Crawl failed: {result.error_message}")
        ```
*   4.5. **Best Practices for Content Filtering for Markdown**
    *   **Start Simple:** Begin with `PruningContentFilter` for general cleanup. It's fast and often effective for removing common boilerplate.
    *   **Query-Specific Tasks:** If your goal is to extract information relevant to a specific query, `BM25ContentFilter` is a great, cost-effective choice.
    *   **Nuanced Selection:** Reserve `LLMContentFilter` for tasks requiring deeper semantic understanding or complex filtering logic that rules-based or keyword-based approaches can't handle. Be mindful of its cost and latency.
    *   **Iterate and Test:** Content filtering is often an iterative process. Test your filter configurations on various pages to ensure they behave as expected. Inspect `fit_html` to understand what the filter is selecting/discarding.
    *   **Combine with `content_source`:** Remember that `fit_markdown` is derived from the output of the filter. If you also need Markdown from the pre-filtered content, ensure your `MarkdownGenerationStrategy`'s `content_source` is set appropriately (e.g., `"cleaned_html"`) so that `raw_markdown` reflects that, while `fit_markdown` reflects the filtered version.

## 5. Customizing Markdown Output via `CrawlerRunConfig`

*   5.1. **The Role of `CrawlerRunConfig.markdown_generator`**
    *   5.1.1. How it allows specifying a custom Markdown generation strategy for a crawl run.
        *   **Explanation:** The `markdown_generator` parameter within the `CrawlerRunConfig` object is the primary way to control how Markdown is generated for a specific crawl operation (i.e., a call to `crawler.arun()` or tasks within `crawler.arun_many()`). You can assign an instance of any class that adheres to the `MarkdownGenerationStrategy` interface to it.
    *   5.1.2. Overriding the default Markdown generation behavior.
        *   **Explanation:** If `CrawlerRunConfig.markdown_generator` is not set (i.e., it's `None`), Crawl4AI will use a default instance of `DefaultMarkdownGenerator` with its standard settings. By providing your own `MarkdownGenerationStrategy` instance (be it a configured `DefaultMarkdownGenerator` or a custom class), you override this default behavior for that particular run.

*   5.2. **Scenarios for Using `CrawlerRunConfig.markdown_generator`**
    *   5.2.1. Applying a pre-configured `DefaultMarkdownGenerator` with specific filters or options.
        *   **Why:** You might want different filtering logic or `html2text` options for different URLs or types of content you're crawling, even within the same `AsyncWebCrawler` instance.
    *   5.2.2. Plugging in a completely custom `MarkdownGenerationStrategy`.
        *   **Why:** As discussed in section 2.1.2, if you have unique Markdown requirements or want to use a different conversion library.
    *   5.2.3. Disabling Markdown generation entirely by setting it to `None` (if applicable, or by using a "NoOp" strategy).
        *   **Why:** If, for a specific crawl, you only need the HTML or extracted structured data and don't require Markdown output, you can pass `markdown_generator=None` (or a strategy that does nothing) to save processing time.
        *   *Note:* To truly disable Markdown generation and its associated `CustomHTML2Text` processing, you might need a "NoOpMarkdownGenerator". If `markdown_generator` is `None`, the crawler might still fall back to a default. A NoOp strategy would explicitly do nothing.
            ```python
            # class NoOpMarkdownGenerator(MarkdownGenerationStrategy):
            #     def generate_markdown(self, input_html: str, **kwargs) -> MarkdownGenerationResult:
            #         return MarkdownGenerationResult(raw_markdown="", markdown_with_citations="", references_markdown="")
            # run_config = CrawlerRunConfig(markdown_generator=NoOpMarkdownGenerator())
            ```

*   5.3. **Code Examples:**
    *   5.3.1. *Example:* [Setting a `DefaultMarkdownGenerator` with a `PruningContentFilter` in `CrawlerRunConfig`].
        ```python
        from crawl4ai import (
            AsyncWebCrawler, 
            CrawlerRunConfig, 
            DefaultMarkdownGenerator, 
            CacheMode
        )
        from crawl4ai.content_filter_strategy import PruningContentFilter

        async def run_with_specific_md_generator():
            # Configure a specific markdown generator
            pruning_filter = PruningContentFilter(threshold=0.6)
            specific_md_generator = DefaultMarkdownGenerator(
                content_filter=pruning_filter,
                options={"body_width": 0, "ignore_links": True} 
            )

            # Configure the crawl run to use this generator
            run_config = CrawlerRunConfig(
                markdown_generator=specific_md_generator,
                cache_mode=CacheMode.BYPASS
            )

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://example.com/article1", config=run_config)
                if result.success:
                    print("--- Markdown from Article 1 (Pruned, No Links, No Wrap) ---")
                    print(result.markdown.fit_markdown[:500]) 
                    # raw_markdown would also reflect no-wrap and no-links from html2text_options

                # For another URL, you could use a different (or default) generator
                # default_run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                # result2 = await crawler.arun(url="https://example.com/article2", config=default_run_config)

        # asyncio.run(run_with_specific_md_generator())
        ```
    *   5.3.2. *Example:* [Setting a custom `MyMarkdownStrategy` in `CrawlerRunConfig` (assuming `MyCustomMarkdownStrategy` from 2.1.3)].
        ```python
        # Assuming MyCustomMarkdownStrategy is defined as in section 2.1.3
        # from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
        # from your_module import MyCustomMarkdownStrategy # If it's in another file

        # async def run_with_custom_md_strategy():
        #     custom_strategy = MyCustomMarkdownStrategy(content_source="raw_html")
        #     run_config_custom = CrawlerRunConfig(
        #         markdown_generator=custom_strategy,
        #         cache_mode=CacheMode.BYPASS
        #     )

        #     async with AsyncWebCrawler() as crawler:
        #         result = await crawler.arun(url="https://example.com", config=run_config_custom)
        #         if result.success:
        #             print("--- Markdown from Custom Strategy ---")
        #             print(result.markdown.raw_markdown) # Or other fields your strategy populates
        
        # asyncio.run(run_with_custom_md_strategy())
        ```
*   5.4. **Interaction with Global vs. Run-Specific Configurations**
    *   **Explanation:** `AsyncWebCrawler` itself does not have a global `markdown_generator` setting during its initialization. Markdown generation is configured *per run* via `CrawlerRunConfig`. This design choice provides maximum flexibility, allowing different Markdown strategies for different URLs or tasks within the same crawler instance lifecycle. If `CrawlerRunConfig.markdown_generator` is not provided, a default `DefaultMarkdownGenerator` instance is used for that specific run.

## 6. Mastering `CustomHTML2Text` for Fine-Grained Control

*   6.1. **Understanding `CustomHTML2Text`**
    *   6.1.1. **Purpose:** Why Crawl4AI includes its own `html2text` extension.
        *   **Enhanced control:** `CustomHTML2Text` is a subclass of the standard `html2text.HTML2Text` library. Crawl4AI uses this custom version to gain more precise control over the HTML-to-Markdown conversion process, particularly to make the output more suitable for LLMs.
        *   **Specific adaptations:** It includes logic for handling Crawl4AI's citation and reference generation (`convert_links_to_citations`), and potentially other tweaks that improve the quality and utility of the Markdown output for AI applications.
    *   6.1.2. **How it's used by `DefaultMarkdownGenerator`**.
        *   **Explanation:** `DefaultMarkdownGenerator` instantiates `CustomHTML2Text` internally. When you pass `options` to `DefaultMarkdownGenerator`, these are ultimately used to configure this `CustomHTML2Text` instance. The `handle()` method of `CustomHTML2Text` is what performs the core HTML to Markdown conversion.

*   6.2. **Key `html2text_options` and Their Impact**
    *   (These options are passed via `DefaultMarkdownGenerator(options=...)`)
    *   6.2.1. `body_width`:
        *   **What it does:** Controls the maximum width of lines in the generated Markdown before wrapping.
        *   **Why configure it:** For LLM consumption, it's often best to disable automatic line wrapping to allow the LLM to process text based on natural paragraph breaks. Setting `body_width=0` achieves this.
        *   *Example:*
            *   `body_width=80` (default-ish for some tools):
                ```markdown
                This is a longer sentence that will be wrapped by html2text if the body_width is
                set to a value like 80 characters.
                ```
            *   `body_width=0`:
                ```markdown
                This is a longer sentence that will not be wrapped by html2text if body_width is 0, allowing the LLM to handle line breaks.
                ```
    *   6.2.2. `ignore_links`:
        *   **What it does:** If `True`, all hyperlink information (`[text](url)`) is removed, leaving only the link text.
        *   **Why configure it:** Set to `True` if links are considered noise for your LLM task and you don't need source attribution. If `False` (default for Crawl4AI's `CustomHTML2Text` unless overridden), links are preserved and can then be converted to citations by `DefaultMarkdownGenerator`.
        *   *Example:*
            *   `ignore_links=False` (then processed for citations): `Visit [Crawl4AI](https://crawl4ai.com)` -> `Visit Crawl4AI [^1^]`
            *   `ignore_links=True`: `Visit [Crawl4AI](https://crawl4ai.com)` -> `Visit Crawl4AI`
    *   6.2.3. `ignore_images`:
        *   **What it does:** If `True`, image tags (`<img>`) are completely ignored, and no Markdown image syntax (`![alt](src)`) is generated.
        *   **Why configure it:** Useful if image information is irrelevant to your LLM task and you want cleaner, more text-focused Markdown.
        *   *Example:*
            *   HTML: `<img src="logo.png" alt="My Logo">`
            *   `ignore_images=False`: `![My Logo](logo.png)`
            *   `ignore_images=True`: (nothing is output for the image)
    *   6.2.4. `protect_links`:
        *   **What it does:** If `True`, surrounds link URLs with `<` and `>`. E.g., `[text](<url>)`.
        *   **Why configure it:** This can sometimes help Markdown parsers that might misinterpret URLs containing special characters. However, with Crawl4AI's citation handling, this is generally not needed, as the raw URLs are moved to the reference section.
    *   6.2.5. `mark_code`:
        *   **What it does:** Controls how `<pre>` and `<code>` tags are handled. If `True`, it attempts to use Markdown code block syntax (backticks).
        *   **Why configure it:** Essential for preserving code snippets correctly. Usually, you'd want this to be `True`.
    *   6.2.6. `default_image_alt`:
        *   **What it does:** Provides a default alt text string if an `<img>` tag is missing an `alt` attribute.
        *   **Why configure it:** Can make Markdown more consistent if you choose to include images.
    *   6.2.7. `bypass_tables`:
        *   **What it does:** If `True`, `<table>` elements are not converted into Markdown table syntax. Their content might be rendered as plain text or omitted, depending on other settings.
        *   **Why configure it:** Standard Markdown table syntax is limited and may not handle complex tables (with `colspan`, `rowspan`, nested tables) well. If you encounter mangled tables, setting this to `True` and processing the table HTML separately (e.g., by extracting the `<table>` HTML and using a specialized table-to-text or table-to-JSON library) might be a better approach.
    *   6.2.8. `pad_tables`:
        *   **What it does:** If `True`, adds padding spaces around cell content in Markdown tables for better visual alignment in raw Markdown.
        *   **Why configure it:** Mostly an aesthetic choice for human readability of the raw Markdown; LLMs typically don't care about this padding.
    *   *Other relevant options identified from `CustomHTML2Text` (or base `html2text`) source:*
        *   `escape_snob`: If `True`, escapes `>` and `&` characters. Default is `False`.
        *   `skip_internal_links`: If `True`, ignores links that start with `#`. Default is `False`.
        *   `links_each_paragraph`: If `True`, puts a link list after each paragraph. Default is `False`. Crawl4AI's citation system provides a better alternative.
        *   `unicode_snob`: If `True`, uses Unicode characters instead of ASCII approximations. Default is `False` in base `html2text`, but `CustomHTML2Text` might behave differently or Crawl4AI ensures UTF-8 handling.
*   6.3. **Best Practices for Configuring `CustomHTML2Text`**
    *   6.3.1. **General recommendations for LLM-friendly output:**
        *   Set `body_width=0` to disable line wrapping and let paragraphs flow naturally.
        *   Consider `ignore_images=True` if images are not relevant to the LLM's task.
        *   Usually, keep `ignore_links=False` (Crawl4AI default) to allow `DefaultMarkdownGenerator` to handle citations properly.
    *   6.3.2. **How to balance information preservation with conciseness:**
        *   Be selective with `ignore_*` options. Removing too much might discard useful context.
        *   Use content filters (Section 4) for semantic reduction rather than relying solely on `html2text` options to remove large irrelevant sections.
    *   6.3.3. **Experimenting with options to achieve desired Markdown style:**
        *   Create a small test HTML snippet.
        *   Instantiate `DefaultMarkdownGenerator` with different `options` dictionaries.
        *   Call its `generate_markdown` method directly (or `_html_to_markdown` on its internal `CustomHTML2Text` instance if you want to bypass citation logic for testing) and observe the output.
*   6.4. **Handling Citations and References (`convert_links_to_citations` method in `CustomHTML2Text`)**
    *   6.4.1. **How it works:**
        *   The `convert_links_to_citations` method (called by `DefaultMarkdownGenerator` if citations are enabled) iterates through the Markdown produced by `html2text.handle()`.
        *   It uses a regular expression (`LINK_PATTERN`) to find all Markdown links (`[text](url "optional title")`).
        *   For each unique URL, it assigns an incremental citation number.
        *   It replaces the original Markdown link with `text [^N^]` (or `![text][^N^]` for images if not ignored).
        *   It builds up a list of reference strings like `[^N^]: url "optional title - text if different from title"`.
    *   6.4.2. **When it's called:** This method is invoked by `DefaultMarkdownGenerator.generate_markdown()` *after* the initial HTML-to-Markdown conversion by `CustomHTML2Text.handle()` if the `citations` flag is `True`.
    *   6.4.3. **Impact on `MarkdownGenerationResult` fields:**
        *   The modified Markdown (with `[^N^]` markers) is stored in `markdown_with_citations`.
        *   The collected reference list is stored in `references_markdown`.
        *   `raw_markdown` remains the version *before* citation processing.
    *   6.4.4. **Customizing Citation Behavior (if possible through options or by subclassing)**.
        *   **Explanation:** Direct customization of the citation format (e.g., changing `[^N^]` to `(N)`) via options is not explicitly provided in `CustomHTML2Text`.
        *   To change this, you would need to:
            1.  Create your own class inheriting from `DefaultMarkdownGenerator`.
            2.  Override the `generate_markdown` method.
            3.  In your override, you could either:
                *   Call the parent's `generate_markdown`, get the `MarkdownGenerationResult`, and then post-process `markdown_with_citations` and `references_markdown` to your desired format.
                *   Or, more invasively, replicate the logic but modify the citation generation part. This might involve creating a custom version of `CustomHTML2Text` or its `convert_links_to_citations` method.
        *   For most users, the default citation format is standard and widely accepted.

## 7. Advanced Markdown Generation Techniques & Best Practices

*   7.1. **Achieving LLM-Friendly Markdown Output**
    *   7.1.1. Prioritizing semantic structure (headings, lists, paragraphs).
        *   **Why:** LLMs leverage structural cues to understand context and hierarchy. Ensure your `html2text_options` (e.g., for headings, list indentation) preserve this structure faithfully.
        *   **How:** Rely on `CustomHTML2Text`'s default handling of semantic HTML tags. If specific tags are problematic, consider pre-processing the HTML.
    *   7.1.2. Handling complex HTML structures (nested tables, complex layouts).
        *   **Strategies for simplifying or selectively extracting from them:**
            *   **Tables:** For very complex tables, consider `html2text_options={'bypass_tables': True}`. Then, extract the table HTML separately (e.g., using `CrawlResult.html` and a CSS selector for the table) and process it with a specialized table parsing library or even an LLM call focused just on table interpretation.
            *   **Layouts:** Aggressive `RelevantContentFilter` strategies can help. If parts of a complex layout are consistently noise, use `CrawlerRunConfig.excluded_selector` to remove them before they even reach the Markdown generator.
    *   7.1.3. When to prefer `fit_markdown` over `raw_markdown` (or `markdown_with_citations`).
        *   **Reasoning:**
            *   **`fit_markdown`:** Best for tasks requiring high relevance and conciseness (e.g., RAG context, focused summarization). It reflects the output of your content filtering.
            *   **`raw_markdown` / `markdown_with_citations`:** Better when you need a broader representation of the page's textual content, or when the filtering might be too aggressive and discard potentially useful context. Also, if your `content_source` is already very clean (e.g., from a targeted CSS selector), the difference might be minimal.
    *   7.1.4. Balancing detail vs. conciseness for different LLM tasks (e.g., summarization vs. Q&A).
        *   **Summarization:** `fit_markdown` from a well-configured `LLMContentFilter` or `BM25ContentFilter` is often ideal. You might also use more aggressive `html2text_options` to remove minor elements.
        *   **Q&A / RAG:** You might prefer a slightly less aggressive filter or even `raw_markdown` (if `content_source` is clean) to ensure all potentially relevant details are available. Citations (`markdown_with_citations` and `references_markdown`) are crucial here for source tracking.

*   7.2. **Pre-processing HTML for Better Markdown**
    *   7.2.1. Using `CrawlerRunConfig.excluded_tags` or `excluded_selector` to remove noise before Markdown generation.
        *   **How:** These parameters in `CrawlerRunConfig` are applied by the `ContentScrapingStrategy` *before* the HTML even reaches the `MarkdownGenerationStrategy`.
        *   **Why:** This is the most efficient way to remove large, consistently irrelevant sections (like global headers, footers, sidebars, ad blocks) across all outputs (HTML, Markdown, etc.).
        *   *Code Example:*
            ```python
            # In CrawlerRunConfig
            # config = CrawlerRunConfig(
            #     excluded_tags=["nav", "footer", "script", "style"],
            #     excluded_selector=".ads, #social-share-buttons"
            # )
            ```
    *   7.2.2. The role of `ContentScrapingStrategy` (e.g., `LXMLWebScrapingStrategy` or the default `WebScrapingStrategy` using BeautifulSoup) in preparing the HTML that `DefaultMarkdownGenerator` receives.
        *   **Explanation:** The `ContentScrapingStrategy` is responsible for the initial cleaning of the HTML. Its output (what becomes `cleaned_html`) is the direct input to `DefaultMarkdownGenerator` if `content_source` is `"cleaned_html"`. Understanding how your chosen scraping strategy cleans HTML is key to predicting the input for Markdown generation. `LXMLWebScrapingStrategy` is generally faster and can be more robust for heavily malformed HTML.

*   7.3. **Post-processing Generated Markdown**
    *   7.3.1. When and why you might need to further process Markdown from `MarkdownGenerationResult`.
        *   **Scenarios:**
            *   Custom formatting not achievable with `html2text` options (e.g., specific table styles, unique list markers).
            *   Domain-specific transformations (e.g., converting certain patterns to custom shortcodes).
            *   Further cleaning or condensing based on rules `html2text` or content filters don't cover.
    *   7.3.2. *Example:* [Python snippet for custom regex replacements or structural adjustments on `raw_markdown`].
        ```python
        import re

        def custom_post_process_markdown(markdown_text):
            # Example: Replace all occurrences of "Crawl4AI" with "**Crawl4AI**"
            markdown_text = re.sub(r"Crawl4AI", r"**Crawl4AI**", markdown_text)
            
            # Example: Add a horizontal rule after every H2 heading
            markdown_text = re.sub(r"(^## .*)", r"\1\n\n---", markdown_text, flags=re.MULTILINE)
            return markdown_text

        # result = await crawler.arun(...)
        # if result.success:
        #     final_markdown = custom_post_process_markdown(result.markdown.raw_markdown)
        #     print(final_markdown)
        ```

*   7.4. **Combining Different Strategies for Optimal Results**
    *   7.4.1. *Scenario:* Using a `RelevantContentFilter` to get `fit_html`, then passing `fit_html` to a custom Markdown generator that expects highly focused input.
        *   **How:**
            1.  Instantiate your filter (e.g., `LLMContentFilter`).
            2.  Instantiate your custom Markdown generator (`MyCustomMarkdownStrategy`).
            3.  In `CrawlerRunConfig`, set `markdown_generator` to your custom generator.
            4.  Crucially, within your custom generator's `generate_markdown` method, ensure you *first* apply the `content_filter` (passed as an argument) to the `input_html` to get the `fit_html`, and then process this `fit_html` with your custom logic. Or, configure your custom generator's `content_source="fit_html"` and pass the filter during its initialization.
    *   7.4.2. *Scenario:* Using one set of `html2text_options` for `raw_markdown` and another for generating an alternative Markdown representation (perhaps for a different LLM or purpose).
        *   **How:** This would typically require two separate calls to `crawler.arun()` with different `CrawlerRunConfig` objects, each specifying a `DefaultMarkdownGenerator` with different `options`. Alternatively, a custom `MarkdownGenerationStrategy` could internally generate multiple Markdown versions with different settings and include them in custom fields within `MarkdownGenerationResult` (though this would require modifying or extending `MarkdownGenerationResult`).

## 8. Troubleshooting Common Markdown Generation Issues

*   8.1. **Problem: Markdown is too noisy / includes boilerplate**
    *   8.1.1. **Solutions:**
        *   **Use a `RelevantContentFilter`**:
            *   Start with `PruningContentFilter`. It's fast and good for common boilerplate.
                ```python
                # from crawl4ai.content_filter_strategy import PruningContentFilter
                # from crawl4ai import DefaultMarkdownGenerator
                # md_generator = DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.5))
                ```
            *   If more precision is needed, try `BM25ContentFilter` with a relevant query or `LLMContentFilter` with clear instructions.
        *   **Refine `excluded_tags` or `excluded_selector` in `CrawlerRunConfig`**: This removes elements *before* any Markdown strategy sees them.
            ```python
            # run_config = CrawlerRunConfig(
            #     excluded_tags=["nav", "footer", "aside", "script"],
            #     excluded_selector=".ad-banner, #social-links"
            # )
            ```
        *   **Adjust `html2text_options`**: Options like `ignore_links`, `ignore_images`, `skip_internal_links` can reduce clutter.
            ```python
            # from crawl4ai import DefaultMarkdownGenerator
            # md_generator = DefaultMarkdownGenerator(options={"ignore_images": True, "ignore_links": True})
            ```

*   8.2. **Problem: Important content is missing from Markdown**
    *   8.2.1. **Solutions:**
        *   **Check if `content_filter` is too aggressive**: If using a filter, try lowering its threshold (e.g., `bm25_threshold` for `BM25ContentFilter`) or simplifying instructions for `LLMContentFilter`. Temporarily disable the filter to see if the content appears in `raw_markdown`.
        *   **Ensure `word_count_threshold` in `CrawlerRunConfig` (or scraping strategy) is not too high**: The default `WebScrapingStrategy` might have its own cleaning. If `CrawlerRunConfig.word_count_threshold` is too high, it might remove short but important paragraphs.
        *   **Verify `html2text_options` are not inadvertently removing desired content**: For example, if `ignore_links=True` is set, link text itself might still be there, but the link URL will be gone.
        *   **Examine `cleaned_html` or `fit_html`**: Inspect `result.markdown.fit_html` (if a filter was used) or `result.cleaned_html` (if no filter and `content_source` was `cleaned_html`). If the content is missing here, the issue is with HTML cleaning or filtering, not the Markdown conversion itself. If it's present in these HTML versions but not in the final Markdown, the issue is likely with `html2text_options` or the conversion process.

*   8.3. **Problem: Tables are mangled or poorly formatted**
    *   8.3.1. **Solutions:**
        *   **Try `html2text_options={'bypass_tables': True}`**: This tells `html2text` to skip converting tables.
            ```python
            # from crawl4ai import DefaultMarkdownGenerator
            # md_generator = DefaultMarkdownGenerator(options={"bypass_tables": True})
            # run_config = CrawlerRunConfig(markdown_generator=md_generator)
            # result = await crawler.arun(...)
            # # Now result.markdown.raw_markdown will not have Markdown tables.
            # # You'd need to parse tables from result.cleaned_html or result.markdown.fit_html
            ```
            You can then extract the table HTML directly from `result.cleaned_html` (or `result.markdown.fit_html`) using BeautifulSoup or lxml and parse it with a library better suited for complex tables (e.g., pandas `read_html`, or a custom parser).
        *   **Experiment with other `html2text` table formatting options**: Options like `pad_tables` might slightly improve appearance, but won't fix fundamentally complex table structures.
        *   **Consider if the table is truly a data table or a layout table**: Layout tables are often problematic for Markdown conversion and should ideally be filtered out by `PruningContentFilter` or more aggressive cleaning.

*   8.4. **Problem: Citations or references are incorrect/missing**
    *   8.4.1. **Solutions:**
        *   **Ensure links are present in the HTML input to `DefaultMarkdownGenerator`**: If the links were removed during an earlier HTML cleaning stage (e.g., by an aggressive `ContentScrapingStrategy` or `excluded_tags`), they can't be converted to citations.
        *   **Verify `ignore_links` is not `True` in `html2text_options`**: `DefaultMarkdownGenerator` relies on `CustomHTML2Text` to see the links to convert them. If `ignore_links=True`, the links are stripped before citation processing can occur.
        *   **Check for unusual link structures in the HTML**: Very non-standard link formats (e.g., heavily JavaScript-driven links without `href` attributes) might not be picked up. `CustomHTML2Text` primarily looks for standard `<a href="...">` tags.

*   8.5. **Problem: Markdown formatting is not ideal for a specific LLM**
    *   8.5.1. **Solutions:**
        *   **Fine-tune `html2text_options` extensively**: This is the first line of defense. Experiment with all available options (see Section 6.2) to control aspects like heading styles, list formatting, code block rendering, etc.
        *   **Consider a custom `MarkdownGenerationStrategy`**: If `html2text` options are insufficient, you might need to build your own strategy, possibly using a different Markdown conversion library or implementing custom transformation logic (see Section 2.1.3).
        *   **Implement post-processing steps**: After getting the Markdown from `MarkdownGenerationResult`, apply your own Python scripts (e.g., using regex) to further refine the formatting (see Section 7.3.2).

*   8.6. **Debugging Workflow**
    *   8.6.1. **Start with `raw_html` from `CrawlResult`**: `print(result.html)` This is the very first HTML fetched, before any processing. Is your target content even here?
    *   8.6.2. **Examine `cleaned_html` (or `fit_html`)**:
        *   If no content filter is used in `MarkdownGenerationStrategy`, inspect `result.cleaned_html`. This is what `DefaultMarkdownGenerator` (with `content_source="cleaned_html"`) will use.
        *   If a content filter *is* used, inspect `result.markdown.fit_html`. This is what `DefaultMarkdownGenerator` will use to produce `fit_markdown`.
        *   Is your target content present in these intermediate HTML stages?
    *   8.6.3. **Isolate the issue**:
        *   **HTML Cleaning/Scraping:** If content is missing from `cleaned_html` (but present in `raw_html`), the issue lies with the `ContentScrapingStrategy` or `CrawlerRunConfig` parameters like `excluded_tags`, `css_selector`, `target_elements`.
        *   **Content Filtering:** If content is in `cleaned_html` but missing from `fit_html`, the issue is with your `RelevantContentFilter` configuration.
        *   **Markdown Conversion:** If content is in `cleaned_html`/`fit_html` but malformed or missing in the final Markdown fields (`raw_markdown`, `fit_markdown`), the issue is likely with `html2text_options` or the `CustomHTML2Text` conversion process.
    *   8.6.4. **Use `verbose=True` in relevant configs**: Set `verbose=True` in `BrowserConfig` and `CrawlerRunConfig` for more detailed logging output from Crawl4AI, which can provide clues.

## 9. Conclusion and Next Steps

*   9.1. Recap of key strategies for effective Markdown generation.
    *   **Summary:** Crawl4AI provides a flexible Markdown generation pipeline. Start with `DefaultMarkdownGenerator`. Use `html2text_options` for stylistic control. Employ `RelevantContentFilter` strategies (`PruningContentFilter`, `BM25ContentFilter`, `LLMContentFilter`) to create focused `fit_markdown` for LLMs. Choose the appropriate `content_source` based on your needs. For highly custom requirements, implement your own `MarkdownGenerationStrategy`.
*   9.2. Pointers to other relevant documentation sections (e.g., `RelevantContentFilter` deep dive, `CustomHTML2Text` options in API reference).
    *   **Suggestion:** For a detailed breakdown of each `RelevantContentFilter`, see the "Content Filtering Strategies" guide. For an exhaustive list of `html2text` options, refer to the `CustomHTML2Text` API documentation or the original `html2text` library's documentation.
*   9.3. Encouragement for experimentation and community contributions.
    *   **Call to Action:** The best way to master Markdown generation is to experiment with different configurations and content types. If you develop useful custom strategies or identify improvements, consider contributing them back to the Crawl4AI community!

---
```

---


## Markdown Generation - Examples
Source: crawl4ai_markdown_examples_content.llm.md

# Examples Outline for crawl4ai - markdown Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_markdown.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

This document provides practical, runnable code examples for the `markdown` component of the `crawl4ai` library, focusing on the `DefaultMarkdownGenerator` and its various configurations.

## 1. Basic Markdown Generation with `DefaultMarkdownGenerator`

### 1.1. Example: Generating Markdown with default `DefaultMarkdownGenerator` settings via `AsyncWebCrawler`.
This example demonstrates the most basic usage of `DefaultMarkdownGenerator` within an `AsyncWebCrawler` run.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode

async def basic_markdown_generation_via_crawler():
    # DefaultMarkdownGenerator will be used by default if markdown_generator is not specified,
    # but we explicitly set it here for clarity.
    md_generator = DefaultMarkdownGenerator()
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS # Use BYPASS for fresh content in examples
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        if result.success and result.markdown:
            print("--- Raw Markdown (First 300 chars) ---")
            print(result.markdown.raw_markdown[:300])
            print("\n--- Markdown with Citations (First 300 chars) ---")
            print(result.markdown.markdown_with_citations[:300])
            print("\n--- References Markdown ---")
            print(result.markdown.references_markdown) # example.com has no outbound links usually
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(basic_markdown_generation_via_crawler())
```
---

### 1.2. Example: Direct instantiation and use of `DefaultMarkdownGenerator`.
You can use `DefaultMarkdownGenerator` directly if you already have HTML content.

```python
from crawl4ai import DefaultMarkdownGenerator

def direct_markdown_generation():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <h1>Welcome to Example</h1>
            <p>This is a paragraph with a <a href="https://example.org/another-page">link</a>.</p>
            <p>Another paragraph follows.</p>
        </body>
    </html>
    """
    # base_url is important for resolving relative links if any, and for citation context
    result_md = generator.generate_markdown(input_html=html_content, base_url="https://example.com")

    print("--- Raw Markdown (Direct Generation) ---")
    print(result_md.raw_markdown)
    print("\n--- Markdown with Citations (Direct Generation) ---")
    print(result_md.markdown_with_citations)
    print("\n--- References Markdown (Direct Generation) ---")
    print(result_md.references_markdown)

if __name__ == "__main__":
    direct_markdown_generation()
```
---

## 2. Citation Management in Markdown

### 2.1. Example: Default citation behavior (citations enabled).
By default, `DefaultMarkdownGenerator` generates citations for links.

```python
from crawl4ai import DefaultMarkdownGenerator

def default_citation_behavior():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html><body>
        <p>Check out <a href="https://crawl4ai.com" title="Crawl4ai Homepage">Crawl4ai</a> and
        <a href="/docs">our documentation</a>.</p>
    </body></html>
    """
    result_md = generator.generate_markdown(input_html=html_content, base_url="https://example.com")

    print("--- Raw Markdown ---")
    print(result_md.raw_markdown)
    print("\n--- Markdown with Citations ---")
    print(result_md.markdown_with_citations)
    print("\n--- References Markdown ---")
    print(result_md.references_markdown)

if __name__ == "__main__":
    default_citation_behavior()
```
---

### 2.2. Example: Disabling citations in `DefaultMarkdownGenerator`.
You can disable citation generation by setting `citations=False` in the `generate_markdown` method.

```python
from crawl4ai import DefaultMarkdownGenerator

def disabling_citations():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html><body>
        <p>A link to <a href="https://anothersite.com">another site</a> will not be cited.</p>
    </body></html>
    """
    # Disable citations for this specific call
    result_md_no_citations = generator.generate_markdown(
        input_html=html_content,
        base_url="https://example.com",
        citations=False
    )

    print("--- Raw Markdown (Citations Disabled) ---")
    print(result_md_no_citations.raw_markdown)
    print("\n--- Markdown with Citations (Citations Disabled) ---")
    # This should be the same as raw_markdown when citations=False
    print(result_md_no_citations.markdown_with_citations)
    print("\n--- References Markdown (Citations Disabled) ---")
    # This should be empty or minimal
    print(result_md_no_citations.references_markdown)

    # For comparison, with citations enabled (default)
    result_md_with_citations = generator.generate_markdown(
        input_html=html_content,
        base_url="https://example.com",
        citations=True # Default
    )
    print("\n--- For Comparison: Markdown with Citations (Enabled) ---")
    print(result_md_with_citations.markdown_with_citations)
    print("\n--- For Comparison: References Markdown (Enabled) ---")
    print(result_md_with_citations.references_markdown)


if __name__ == "__main__":
    disabling_citations()
```
---

### 2.3. Example: Impact of `base_url` on citation links for relative URLs.
The `base_url` parameter is crucial for correctly resolving relative URLs in your HTML content into absolute URLs in the references.

```python
from crawl4ai import DefaultMarkdownGenerator

def base_url_impact_on_citations():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html><body>
        <p>Links: <a href="/features">Features</a>, <a href="pricing.html">Pricing</a>,
        and an absolute link to <a href="https://external.com/resource">External Resource</a>.</p>
    </body></html>
    """

    print("--- Case 1: With base_url='https://example.com/products/' ---")
    result_md_case1 = generator.generate_markdown(
        input_html=html_content,
        base_url="https://example.com/products/"
    )
    print(result_md_case1.references_markdown)

    print("\n--- Case 2: With base_url='https://another-domain.net/' ---")
    result_md_case2 = generator.generate_markdown(
        input_html=html_content,
        base_url="https://another-domain.net/"
    )
    print(result_md_case2.references_markdown)

    print("\n--- Case 3: Without base_url (relative links might be incomplete) ---")
    result_md_case3 = generator.generate_markdown(input_html=html_content)
    print(result_md_case3.references_markdown)

if __name__ == "__main__":
    base_url_impact_on_citations()
```
---

### 2.4. Example: Handling HTML with no links (empty `references_markdown`).
If the input HTML contains no hyperlinks, the `references_markdown` will be empty.

```python
from crawl4ai import DefaultMarkdownGenerator

def no_links_in_html():
    generator = DefaultMarkdownGenerator()
    html_content = "<html><body><p>This is a paragraph with no links at all.</p><b>Just some bold text.</b></body></html>"
    result_md = generator.generate_markdown(input_html=html_content, base_url="https://example.com")

    print("--- Raw Markdown ---")
    print(result_md.raw_markdown)
    print("\n--- Markdown with Citations ---")
    print(result_md.markdown_with_citations) # Should be same as raw_markdown
    print("\n--- References Markdown ---")
    print(f"'{result_md.references_markdown}'") # Should be empty or contain minimal boilerplate

if __name__ == "__main__":
    no_links_in_html()
```
---

## 3. Controlling `html2text` Conversion Options
The `DefaultMarkdownGenerator` uses the `html2text` library internally. You can pass options to `html2text` either during generator initialization (`options` parameter) or during the `generate_markdown` call (`html2text_options` parameter).

### 3.1. Example: Initializing `DefaultMarkdownGenerator` with `options` to ignore links.
This will prevent links from appearing in the Markdown output altogether (different from `citations=False` which keeps link text but omits citation markers).

```python
from crawl4ai import DefaultMarkdownGenerator

def ignore_links_option():
    # Initialize with html2text option to ignore links
    generator = DefaultMarkdownGenerator(options={"ignore_links": True})
    html_content = "<html><body><p>A link to <a href='https://example.com'>Example Site</a> and some text.</p></body></html>"
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown (ignore_links=True) ---")
    print(result_md.raw_markdown) # Link text might be present or absent based on html2text behavior
    print("--- Markdown with Citations (ignore_links=True) ---")
    print(result_md.markdown_with_citations) # No citations as links are ignored
    print("--- References (ignore_links=True) ---")
    print(f"'{result_md.references_markdown}'") # Should be empty

if __name__ == "__main__":
    ignore_links_option()
```
---

### 3.2. Example: Initializing `DefaultMarkdownGenerator` with `options` to ignore images.
This will prevent image references (like `![alt text](src)`) from appearing in the Markdown.

```python
from crawl4ai import DefaultMarkdownGenerator

def ignore_images_option():
    generator = DefaultMarkdownGenerator(options={"ignore_images": True})
    html_content = "<html><body><p>An image: <img src='image.png' alt='My Test Image'></p></body></html>"
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown (ignore_images=True) ---")
    print(result_md.raw_markdown) # Image markdown should be absent

if __name__ == "__main__":
    ignore_images_option()
```
---

### 3.3. Example: Initializing `DefaultMarkdownGenerator` with `options` for `body_width=0` (no line wrapping).
`body_width=0` tells `html2text` not to wrap lines.

```python
from crawl4ai import DefaultMarkdownGenerator

def no_line_wrapping_option():
    generator = DefaultMarkdownGenerator(options={"body_width": 0})
    long_text = "This is a very long line of text that would normally be wrapped by html2text. " * 5
    html_content = f"<html><body><p>{long_text}</p></body></html>"
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown (body_width=0) ---")
    print(result_md.raw_markdown) # Observe the long line without soft wraps

if __name__ == "__main__":
    no_line_wrapping_option()
```
---

### 3.4. Example: Initializing `DefaultMarkdownGenerator` to disable emphasis.
This will remove formatting for `<em>` and `<strong>` tags.

```python
from crawl4ai import DefaultMarkdownGenerator

def ignore_emphasis_option():
    generator = DefaultMarkdownGenerator(options={"ignore_emphasis": True})
    html_content = "<html><body><p>Normal, <em>emphasized</em>, and <strong>strongly emphasized</strong> text.</p></body></html>"
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown (ignore_emphasis=True) ---")
    print(result_md.raw_markdown) # Emphasis should be gone

if __name__ == "__main__":
    ignore_emphasis_option()
```
---

### 3.5. Example: Overriding `html2text_options` at `generate_markdown` call time.
Options passed to `generate_markdown` via `html2text_options` take precedence.

```python
from crawl4ai import DefaultMarkdownGenerator

def override_html2text_options():
    # Initial generator might have some defaults
    generator = DefaultMarkdownGenerator(options={"ignore_links": False})
    html_content = "<html><body><p>Link: <a href='https://example.com'>Example</a>.</p></body></html>"

    # Override at call time to protect links
    result_md = generator.generate_markdown(
        input_html=html_content,
        html2text_options={"protect_links": True} # Links will be <URL>
    )

    print("--- Markdown (protect_links=True via call-time override) ---")
    print(result_md.raw_markdown)

if __name__ == "__main__":
    override_html2text_options()
```
---

### 3.6. Example: Combining multiple `html2text` options.
Multiple options can be combined for fine-grained control over the Markdown output.

```python
from crawl4ai import DefaultMarkdownGenerator

def combined_html2text_options():
    generator = DefaultMarkdownGenerator(options={
        "ignore_links": True,
        "ignore_images": True,
        "body_width": 60  # Wrap at 60 characters
    })
    html_content = """
    <html><body>
        <p>This is a paragraph with a <a href='https://example.com'>link to ignore</a> and an
        <img src='image.png' alt='image to ignore'>. It also has some long text to demonstrate wrapping.
        Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
        </p>
    </body></html>
    """
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown (Combined Options: ignore_links, ignore_images, body_width=60) ---")
    print(result_md.raw_markdown)

if __name__ == "__main__":
    combined_html2text_options()
```
---

## 4. Selecting the HTML Content Source for Markdown Generation
The `DefaultMarkdownGenerator` can generate Markdown from different HTML sources within the `CrawlResult`.

### 4.1. Example: Markdown from `cleaned_html` (default `content_source`).
This is the default behavior. `cleaned_html` is the HTML after `WebScrapingStrategy` (e.g., `LXMLWebScrapingStrategy`) has processed it.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode

async def markdown_from_cleaned_html():
    # Default content_source is "cleaned_html"
    md_generator = DefaultMarkdownGenerator()
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        # Using a more complex page to see the effect of cleaning
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        if result.success and result.markdown:
            print("--- Markdown from Cleaned HTML (Default - First 300 chars) ---")
            print(result.markdown.raw_markdown[:300])
            # For comparison, show a snippet of cleaned_html
            print("\n--- Cleaned HTML (Source - First 300 chars) ---")
            print(result.cleaned_html[:300])
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(markdown_from_cleaned_html())
```
---

### 4.2. Example: Markdown from `raw_html`.
This example uses the original, unprocessed HTML fetched from the URL as the source for Markdown generation.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode

async def markdown_from_raw_html():
    md_generator = DefaultMarkdownGenerator(content_source="raw_html")
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        if result.success and result.markdown:
            print("--- Markdown from Raw HTML (First 300 chars) ---")
            print(result.markdown.raw_markdown[:300])
            print("\n--- Raw Page HTML (Source - First 300 chars for comparison) ---")
            print(result.html[:300]) # result.html contains the raw HTML
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(markdown_from_raw_html())
```
---

### 4.3. Example: Markdown from `fit_html` (requires a `ContentFilterStrategy`).
`fit_html` is the HTML content after a `ContentFilterStrategy` (like `PruningContentFilter`) has processed it.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter

async def markdown_from_fit_html():
    # A content filter must run to produce fit_html
    pruning_filter = PruningContentFilter()
    md_generator = DefaultMarkdownGenerator(
        content_filter=pruning_filter,
        content_source="fit_html" # Explicitly use the output of the filter
    )
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        # Using a news site which PruningContentFilter can work on
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        if result.success and result.markdown:
            print("--- Markdown from Fit HTML (Output of PruningFilter - First 300 chars) ---")
            # When content_source="fit_html", result.markdown.raw_markdown IS from fit_html
            print(result.markdown.raw_markdown[:300])
            print("\n--- Fit HTML itself (Source - First 300 chars for comparison) ---")
            print(result.markdown.fit_html[:300])
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(markdown_from_fit_html())
```
---

## 5. Integration with Content Filters
`DefaultMarkdownGenerator` can work in conjunction with `ContentFilterStrategy` instances. If a filter is provided, it will produce `fit_html` and `fit_markdown`.

### 5.1. Example: `DefaultMarkdownGenerator` with `PruningContentFilter`.
The `PruningContentFilter` attempts to remove boilerplate and keep main content.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter

async def md_with_pruning_filter():
    pruning_filter = PruningContentFilter()
    # By default, raw_markdown is from cleaned_html, fit_markdown is from fit_html
    md_generator = DefaultMarkdownGenerator(content_filter=pruning_filter)
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        if result.success and result.markdown:
            print("--- Raw Markdown (from cleaned_html - First 200 chars) ---")
            print(result.markdown.raw_markdown[:200])
            print("\n--- Fit Markdown (from PruningFilter's fit_html - First 200 chars) ---")
            print(result.markdown.fit_markdown[:200])
            print("\n--- Fit HTML (Source for Fit Markdown - First 200 chars) ---")
            print(result.markdown.fit_html[:200])
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(md_with_pruning_filter())
```
---

### 5.2. Example: `DefaultMarkdownGenerator` with `BM25ContentFilter`.
`BM25ContentFilter` filters content based on relevance to a user query.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
from crawl4ai.content_filter_strategy import BM25ContentFilter

async def md_with_bm25_filter():
    bm25_filter = BM25ContentFilter(user_query="Python programming language features")
    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        # Using a relevant page for the query
        result = await crawler.arun(url="https://docs.python.org/3/tutorial/classes.html", config=config)
        if result.success and result.markdown:
            print("--- Fit Markdown (from BM25Filter - First 300 chars) ---")
            print(result.markdown.fit_markdown[:300])
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(md_with_bm25_filter())
```
---

### 5.3. Example: `DefaultMarkdownGenerator` with `LLMContentFilter`.
`LLMContentFilter` uses an LLM to intelligently filter or summarize content based on instructions. (Requires API Key)

```python
import asyncio
import os
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, LLMConfig, CacheMode
from crawl4ai.content_filter_strategy import LLMContentFilter

async def md_with_llm_filter():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("OPENAI_API_KEY not found. Skipping LLMContentFilter example.")
        return

    llm_config = LLMConfig(api_token=openai_api_key, provider="openai/gpt-3.5-turbo")
    llm_filter = LLMContentFilter(
        llm_config=llm_config,
        instruction="Summarize the main arguments presented in this Hacker News discussion thread."
    )
    md_generator = DefaultMarkdownGenerator(content_filter=llm_filter)
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS # Fresh run for LLM
    )

    async with AsyncWebCrawler() as crawler:
        # Example Hacker News discussion
        result = await crawler.arun(url="https://news.ycombinator.com/item?id=39000000", config=config) # A past popular item
        if result.success and result.markdown:
            print("--- Fit Markdown (from LLMContentFilter - First 500 chars) ---")
            print(result.markdown.fit_markdown[:500])
            llm_filter.show_usage() # Show token usage
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(md_with_llm_filter())
```
---

### 5.4. Example: Forcing Markdown generation from `fit_html` when a filter is active.
This example shows how to ensure the `raw_markdown` itself is generated from the `fit_html` (output of the filter) rather than `cleaned_html`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter

async def md_forced_from_fit_html():
    pruning_filter = PruningContentFilter()
    # Explicitly set content_source to "fit_html"
    md_generator = DefaultMarkdownGenerator(
        content_filter=pruning_filter,
        content_source="fit_html"
    )
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        if result.success and result.markdown:
            print("--- Raw Markdown (forced from fit_html - First 300 chars) ---")
            # This raw_markdown is now generated from the output of PruningFilter
            print(result.markdown.raw_markdown[:300])
            print("\n--- Fit HTML (Source for Raw Markdown - First 300 chars) ---")
            print(result.markdown.fit_html[:300])
            print("\n--- Fit Markdown (should be same as Raw Markdown here - First 300 chars) ---")
            print(result.markdown.fit_markdown[:300])
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(md_forced_from_fit_html())
```
---

### 5.5. Example: Markdown generation when no filter is active.
If no `content_filter` is provided to `DefaultMarkdownGenerator`, `fit_markdown` and `fit_html` will be empty or None.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode

async def md_no_filter():
    md_generator = DefaultMarkdownGenerator() # No filter provided
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        if result.success and result.markdown:
            print("--- Raw Markdown (First 300 chars) ---")
            print(result.markdown.raw_markdown[:300])
            print("\n--- Fit Markdown (Expected: None or empty) ---")
            print(result.markdown.fit_markdown)
            print("\n--- Fit HTML (Expected: None or empty) ---")
            print(result.markdown.fit_html)
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(md_no_filter())
```
---

## 6. Understanding `MarkdownGenerationResult` Output Fields

### 6.1. Example: Accessing all fields of `MarkdownGenerationResult`.
This example demonstrates how to access all the different Markdown and HTML outputs available in the `MarkdownGenerationResult` object.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter # Using a filter to populate fit_html/fit_markdown

async def access_all_markdown_fields():
    # Setup with a filter to ensure fit_html and fit_markdown are generated
    content_filter = PruningContentFilter()
    md_generator = DefaultMarkdownGenerator(
        content_filter=content_filter,
        content_source="cleaned_html" # raw_markdown will be from cleaned_html
    )
    config = CrawlerRunConfig(
        markdown_generator=md_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        # Using a content-rich page
        result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=config)
        if result.success and result.markdown:
            md_result = result.markdown

            print("--- Accessing MarkdownGenerationResult Fields ---")

            print(f"\n1. Raw Markdown (from '{md_generator.content_source}' - snippet):")
            print(md_result.raw_markdown[:300] + "...")

            print(f"\n2. Markdown with Citations (snippet):")
            print(md_result.markdown_with_citations[:300] + "...")

            print(f"\n3. References Markdown (snippet):")
            print(md_result.references_markdown[:200] + "...")

            print(f"\n4. Fit HTML (from ContentFilter - snippet):")
            if md_result.fit_html:
                print(md_result.fit_html[:300] + "...")
            else:
                print("None (No filter or filter produced no output)")

            print(f"\n5. Fit Markdown (from fit_html - snippet):")
            if md_result.fit_markdown:
                print(md_result.fit_markdown[:300] + "...")
            else:
                print("None (No filter or filter produced no output)")
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(access_all_markdown_fields())
```
---

## 7. Advanced and Specific Scenarios

### 7.1. Example: Handling HTML with complex table structures.
`DefaultMarkdownGenerator` (via `html2text`) attempts to render HTML tables into Markdown tables.

```python
from crawl4ai import DefaultMarkdownGenerator

def markdown_for_tables():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html><body>
        <h3>Product Comparison</h3>
        <table>
            <thead>
                <tr><th>Feature</th><th>Product A</th><th>Product B</th></tr>
            </thead>
            <tbody>
                <tr><td>Price</td><td>$100</td><td>$120</td></tr>
                <tr><td>Rating</td><td>4.5 stars</td><td>4.2 stars</td></tr>
                <tr><td>Multi-row<br/>Feature</td><td colspan="2">Supported by Both</td></tr>
            </tbody>
        </table>
    </body></html>
    """
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown for Table ---")
    print(result_md.raw_markdown)

if __name__ == "__main__":
    markdown_for_tables()
```
---

### 7.2. Example: Handling HTML with code blocks.
Code blocks are generally preserved in Markdown format.

```python
from crawl4ai import DefaultMarkdownGenerator

def markdown_for_code_blocks():
    generator = DefaultMarkdownGenerator()
    html_content = """
    <html><body>
        <p>Here is some Python code:</p>
        <pre><code class="language-python">
def greet(name):
    print(f"Hello, {name}!")

greet("World")
        </code></pre>
        <p>And an inline <code>example_function()</code>.</p>
    </body></html>
    """
    result_md = generator.generate_markdown(input_html=html_content)

    print("--- Markdown for Code Blocks ---")
    print(result_md.raw_markdown)

if __name__ == "__main__":
    markdown_for_code_blocks()
```
---

### 7.3. Example: Using a custom `MarkdownGenerationStrategy` (conceptual).
You can create your own Markdown generation logic by subclassing `MarkdownGenerationStrategy`.

```python
import asyncio
from crawl4ai import (
    AsyncWebCrawler, CrawlerRunConfig, CacheMode,
    MarkdownGenerationStrategy, MarkdownGenerationResult
)

# Define a minimal custom Markdown generator
class CustomMarkdownGenerator(MarkdownGenerationStrategy):
    def __init__(self, prefix="CUSTOM MD: ", **kwargs):
        super().__init__(**kwargs) # Pass along any other options
        self.prefix = prefix

    def generate_markdown(
        self,
        input_html: str,
        base_url: str = "",
        html2text_options: dict = None, # Can be used by html2text
        citations: bool = True, # Standard param
        **kwargs # For other potential strategy-specific params
    ) -> MarkdownGenerationResult:
        # Simplified custom logic: just prefix and take a snippet
        # A real custom generator would do more sophisticated parsing/conversion
        custom_raw_md = self.prefix + input_html[:100].strip() + "..."

        # For simplicity, we'll just return the custom raw markdown for all fields
        return MarkdownGenerationResult(
            raw_markdown=custom_raw_md,
            markdown_with_citations=custom_raw_md, # No real citation logic here
            references_markdown="",
            fit_markdown=None, # Not implementing filtering here
            fit_html=None
        )

async def use_custom_markdown_generator():
    custom_generator = CustomMarkdownGenerator(prefix="[MyGenerator Says]: ")
    config = CrawlerRunConfig(
        markdown_generator=custom_generator,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        if result.success and result.markdown:
            print("--- Output from CustomMarkdownGenerator ---")
            print(result.markdown.raw_markdown)
            # Since our custom generator doesn't really do citations or filtering:
            print(f"Citations: '{result.markdown.markdown_with_citations}'")
            print(f"References: '{result.markdown.references_markdown}'")
            print(f"Fit Markdown: '{result.markdown.fit_markdown}'")

        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(use_custom_markdown_generator())
```
---
**End of Examples Document**
```

---


## Vibe Coding - Memory
Source: crawl4ai_vibe_memory_content.llm.md

Okay, I have read the "vibe" description for `crawl4ai`. Based on this, and adhering to the "memory" document type requirements, here is the detailed Markdown outline:

```markdown
# Detailed Outline for crawl4ai - vibe Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_vibe_coding.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Vibe Coding with Crawl4AI: Core Concept

*   1.1. Purpose:
    *   Provides a conceptual framework for interacting with the `crawl4ai` library, particularly when using AI coding assistants.
    *   Aims to simplify the process of building web data applications by focusing on high-level capabilities and key building blocks, enabling users to guide AI assistants effectively even with limited direct `crawl4ai` API knowledge.
*   1.2. Principle:
    *   Describes how users can communicate their web scraping and data extraction goals to an AI assistant, which then translates these "vibes" or high-level intentions into `crawl4ai` Python code by leveraging knowledge of the library's core components and configurations.

## 2. `crawl4ai` High-Level Capabilities (for Vibe Prompts)

*   2.1. Fetching Webpages
    *   2.1.1. Description: The library can retrieve content from specified web URLs.
*   2.2. Converting Web Content to Clean Markdown
    *   2.2.1. Description: The library can process raw HTML content and convert it into a cleaned, structured Markdown format.
    *   2.2.2. Applications: Suitable for content summarization, input for Question & Answering systems, and as a pre-processing step for other LLMs.
*   2.3. Extracting Specific Information (JSON)
    *   2.3.1. Description: The library can extract targeted data elements from webpages and organize them into a JSON structure.
    *   2.3.2. Examples: Can be used to extract product names, prices from e-commerce sites, article headlines, author names, etc.
*   2.4. Crawling Multiple Pages
    *   2.4.1. Description: The library supports concurrent fetching and processing of a list of URLs.
*   2.5. Taking Screenshots and Generating PDFs
    *   2.5.1. Description: The library can capture visual representations of webpages as PNG screenshots or generate PDF documents.
*   2.6. Handling Simple Page Interactions
    *   2.6.1. Description: The library can execute JavaScript to simulate basic user interactions on a webpage, such as clicking buttons (e.g., "load more") or scrolling.

## 3. Key `crawl4ai` Building Blocks (API Reference for Vibe Coding Context)

*   3.1. Class `AsyncWebCrawler`
    *   3.1.1. Purpose: The primary entry point and main tool within `crawl4ai` for orchestrating web crawling and data extraction tasks.
    *   3.1.2. Initialization (`__init__`):
        *   Signature: `AsyncWebCrawler(self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, base_directory: str = ..., thread_safe: bool = False, logger: Optional[AsyncLoggerBase] = None, **kwargs)`
        *   Parameters:
            *   `crawler_strategy (Optional[AsyncCrawlerStrategy])`: The underlying strategy for web crawling (e.g., `AsyncPlaywrightCrawlerStrategy`). Defaults to `AsyncPlaywrightCrawlerStrategy`.
            *   `config (Optional[BrowserConfig])`: Configuration for the browser instance. See section 3.5 for details.
            *   Other parameters are generally handled by defaults for vibe coding.
*   3.2. Method `AsyncWebCrawler.arun()`
    *   3.2.1. Purpose: Executes a crawl operation on a single URL or resource.
    *   3.2.2. Signature: `async def arun(self, url: str, config: Optional[CrawlerRunConfig] = None, **kwargs) -> RunManyReturn`
    *   3.2.3. Parameters:
        *   `url (str)`: The target resource.
            *   Description: Can be a standard web URL (e.g., "https://example.com"), a local file path (e.g., "file:///path/to/file.html"), or raw HTML content (e.g., "raw:<html>...</html>").
        *   `config (Optional[CrawlerRunConfig])`: An instance of `CrawlerRunConfig` specifying how this particular crawl run should be executed. See section 3.4 for details.
*   3.3. Method `AsyncWebCrawler.arun_many()`
    *   3.3.1. Purpose: Executes crawl operations on a list of URLs or resources, often concurrently.
    *   3.3.2. Signature: `async def arun_many(self, urls: List[str], config: Optional[CrawlerRunConfig] = None, dispatcher: Optional[BaseDispatcher] = None, **kwargs) -> RunManyReturn`
    *   3.3.3. Parameters:
        *   `urls (List[str])`: A list of target resources (URLs, file paths, raw HTML strings).
        *   `config (Optional[CrawlerRunConfig])`: An instance of `CrawlerRunConfig` applied to all URLs in the list. See section 3.4 for details.
*   3.4. Class `CrawlerRunConfig`
    *   3.4.1. Purpose: Configuration object for individual crawl runs, controlling aspects like content extraction, page interaction, and output formats.
    *   3.4.2. Key Parameters for Vibe Coding Context:
        *   `markdown_generator (Optional[MarkdownGenerationStrategy])`:
            *   Description: Specifies the strategy for generating Markdown.
            *   Default: An instance of `DefaultMarkdownGenerator`.
            *   Note for Vibe Coding: Can be `DefaultMarkdownGenerator(content_filter=PruningContentFilter())` for cleaner output.
        *   `extraction_strategy (Optional[ExtractionStrategy])`:
            *   Description: Specifies the strategy for extracting structured data.
            *   Supported Strategies (for Vibe Coding):
                *   `JsonCssExtractionStrategy`: For extracting data based on CSS selectors from structured HTML. Requires a `schema` dictionary.
                *   `LLMExtractionStrategy`: For extracting data using an LLM, often for complex or unstructured HTML. Requires an `LLMConfig` and an `instruction` or Pydantic model defining the desired output.
        *   `js_code (Optional[Union[str, List[str]]])`:
            *   Description: JavaScript code (or a list of code snippets) to be executed on the page after it loads.
        *   `wait_for (Optional[str])`:
            *   Description: A CSS selector or JavaScript expression. The crawler will wait for this condition to be met after `js_code` execution before proceeding.
        *   `session_id (Optional[str])`:
            *   Description: An identifier used to maintain the state of a browser page across multiple `arun` calls. Essential for multi-step interactions on the same page.
        *   `js_only (bool)`:
            *   Description: If `True` (and `session_id` is used), only executes `js_code` on the existing page without a full navigation/reload. Default is `False`.
        *   `screenshot (bool)`:
            *   Description: If `True`, captures a screenshot of the page. Result in `CrawlResult.screenshot`. Default is `False`.
        *   `pdf (bool)`:
            *   Description: If `True`, generates a PDF of the page. Result in `CrawlResult.pdf`. Default is `False`.
        *   `cache_mode (Optional[CacheMode])`:
            *   Description: Controls caching behavior.
            *   Type: `crawl4ai.cache_context.CacheMode` (Enum).
            *   Common Values: `CacheMode.ENABLED`, `CacheMode.BYPASS`.
*   3.5. Class `BrowserConfig`
    *   3.5.1. Purpose: Configures persistent browser-level settings for an `AsyncWebCrawler` instance.
    *   3.5.2. Key Parameters for Vibe Coding Context:
        *   `headless (bool)`:
            *   Description: If `True`, the browser runs without a visible UI. If `False`, the browser UI is shown.
            *   Default: `True`.
        *   `proxy_config (Optional[Union[ProxyConfig, Dict[str, str]]])`:
            *   Description: Configuration for using a proxy server.
            *   Structure (if dict): `{"server": "http://<host>:<port>", "username": "<user>", "password": "<pass>"}`.
        *   `user_agent (Optional[str])`:
            *   Description: Custom User-Agent string to be used by the browser.
*   3.6. Class `LLMConfig`
    *   3.6.1. Purpose: Configures settings for interacting with Large Language Models, used by `LLMExtractionStrategy`.
    *   3.6.2. Key Parameters:
        *   `provider (str)`:
            *   Description: Specifies the LLM provider and model identifier.
            *   Examples: "openai/gpt-4o-mini", "ollama/llama3", "anthropic/claude-3-opus-20240229".
        *   `api_token (Optional[str])`:
            *   Description: API key for the LLM provider. Can be the actual key or an environment variable reference (e.g., "env:OPENAI_API_KEY").
*   3.7. Class `CrawlResult`
    *   3.7.1. Purpose: The data object returned by `crawl4ai` operations, containing the results and metadata of a crawl.
    *   3.7.2. Key Attributes:
        *   `success (bool)`: `True` if the crawl was successful, `False` otherwise.
        *   `markdown (MarkdownGenerationResult)`: Object containing Markdown representations.
            *   `markdown.raw_markdown (str)`: Markdown generated directly from the cleaned HTML.
            *   `markdown.fit_markdown (str)`: Markdown potentially further processed by content filters.
        *   `extracted_content (Optional[str])`: JSON string of structured data if an `ExtractionStrategy` was used and successful.
        *   `links (Links)`: Object containing `internal` and `external` lists of `Link` objects. Each `Link` object has `href`, `text`, `title`.
        *   `media (Media)`: Object containing lists of `MediaItem` for `images`, `videos`, `audios`, and `tables`. Each `MediaItem` has `src`, `alt`, `score`, etc.
        *   `screenshot (Optional[str])`: Base64 encoded string of the PNG screenshot, if `screenshot=True`.
        *   `pdf (Optional[bytes])`: Raw bytes of the PDF document, if `pdf=True`.
        *   `error_message (Optional[str])`: Description of the error if `success` is `False`.

## 4. Common `crawl4ai` Usage Patterns (Vibe Recipes Mapped to Components)

*   4.1. Task: Get Clean Markdown from a Page
    *   4.1.1. Description: Fetch a single webpage and convert its main content into clean Markdown.
    *   4.1.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `markdown_generator`: Typically `DefaultMarkdownGenerator()`. For very clean output, `DefaultMarkdownGenerator(content_filter=PruningContentFilter())`.
*   4.2. Task: Extract All Product Names and Prices from an E-commerce Category Page
    *   4.2.1. Description: Scrape structured data (e.g., product names, prices) from a page with repeating elements.
    *   4.2.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `extraction_strategy`: `JsonCssExtractionStrategy(schema={"name_field": "h2.product-title", "price_field": "span.price"})`. The schema's CSS selectors identify where to find the data.
*   4.3. Task: Extract Key Information from an Article using an LLM
    *   4.3.1. Description: Use an LLM to parse an article and extract specific fields like author, date, and a summary into a JSON format.
    *   4.3.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `extraction_strategy`: `LLMExtractionStrategy(llm_config=..., instruction=..., schema=...)`.
        *   `LLMConfig`: Instance specifying `provider` (e.g., "openai/gpt-4o-mini") and `api_token`.
        *   Schema for `LLMExtractionStrategy`: Can be a Pydantic model definition or a dictionary describing the target JSON structure.
*   4.4. Task: Crawl Multiple Pages of a Blog (Clicking "Next Page")
    *   4.4.1. Description: Navigate through paginated content by simulating clicks on "Next Page" or similar links, collecting data from each page.
    *   4.4.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   Multiple sequential calls to `arun()` (typically in a loop).
        *   `CrawlerRunConfig` (reused or cloned for each step):
            *   `session_id`: A consistent identifier (e.g., "blog_pagination_session") to maintain the browser state across `arun` calls.
            *   `js_code`: JavaScript to trigger the "Next Page" action (e.g., `document.querySelector('a.next-page-link').click();`).
            *   `wait_for`: A CSS selector or JavaScript condition to ensure the new page content has loaded before proceeding.
            *   `js_only=True`: For subsequent `arun` calls after the initial page load to indicate only JS interaction without full navigation.
*   4.5. Task: Get Screenshots of a List of URLs
    *   4.5.1. Description: Capture screenshots for a batch of URLs.
    *   4.5.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun_many()` method.
        *   `CrawlerRunConfig`:
            *   `screenshot=True`.

## 5. Key Input Considerations for `crawl4ai` Operations (Inferred from Vibe Prompting Tips)

*   5.1. Clear Objective: `crawl4ai` operations are guided by the configuration. The configuration should reflect the user's goal (e.g., Markdown generation, specific data extraction, media capture).
*   5.2. URL Input: The `arun` method requires a single `url` string. `arun_many` requires a `List[str]` of URLs.
*   5.3. Structured Data Extraction Guidance:
    *   For `JsonCssExtractionStrategy`, the `schema` parameter (a dictionary mapping desired field names to CSS selectors) is essential.
    *   For `LLMExtractionStrategy`, the `instruction` parameter (natural language description of desired data) and/or a `schema` (Pydantic model or dictionary) are crucial, along with a configured `LLMConfig`.
*   5.4. LLM Configuration: When `LLMExtractionStrategy` is used, an `LLMConfig` instance specifying `provider` and `api_token` (if applicable) must be provided.
*   5.5. Dynamic Page Handling: For pages requiring interaction, `CrawlerRunConfig` parameters like `js_code`, `wait_for`, `session_id`, and `js_only` are used.

## 6. Expected Output Data from `crawl4ai` Operations (Accessing `CrawlResult`)

*   6.1. Generated Python Code: When using an AI assistant with `crawl4ai` context, the AI is expected to generate Python code that utilizes `crawl4ai` classes and methods.
*   6.2. `CrawlResult` Object: The primary output of `arun()` and `arun_many()` calls.
    *   `result.success (bool)`: Indicates if the individual crawl operation was successful.
    *   `result.markdown.raw_markdown (str)` / `result.markdown.fit_markdown (str)`: Contains the generated Markdown content.
    *   `result.extracted_content (Optional[str])`: Contains the JSON string of structured data if an extraction strategy was successful.
    *   `result.links (Links)`: Provides access to lists of internal and external links.
    *   `result.media (Media)`: Provides access to lists of images, videos, audio files, and tables.
    *   `result.screenshot (Optional[str])`: Base64 encoded screenshot data.
    *   `result.pdf (Optional[bytes])`: Raw PDF data.
    *   `result.error_message (Optional[str])`: Error details if `success` is `False`.
*   6.3. Files on Disk: Operations like screenshot or PDF generation, or custom code within an AI-generated script, might save files to the local disk (e.g., PNGs, PDFs, JSON files). The paths depend on the configuration or the custom code.

```

---


## Vibe Coding - Reasoning
Source: crawl4ai_vibe_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - vibe Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_vibe.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

# Vibe Coding with Crawl4AI: A Guide for AI Coding Assistants

## 1. Introduction: What is "Vibe Coding" with Crawl4AI?

*   **1.1. Explaining the Core Idea:**
    "Vibe Coding" with Crawl4AI is about empowering users, even those with limited direct coding experience, to build powerful web data applications. Instead of meticulously writing every line of Python, you focus on clearly describing your data extraction or web interaction goals to an AI coding assistant. This guide teaches you how to provide that AI assistant with the *right context* about Crawl4AI, so it can generate effective and efficient code for you. The goal is to translate your "vibe" – your high-level intent – into working Crawl4AI solutions quickly.

*   **1.2. Who is this Guide For?**
    This guide is designed for:
    *   **Users new to web scraping or `crawl4ai`:** If you prefer to articulate your needs in natural language and have an AI assistant handle the code generation, this guide is for you.
    *   **Data analysts, researchers, and product managers:** Anyone who needs web data but doesn't want to get bogged down in the intricacies of web scraping libraries.
    *   **Developers looking for rapid prototyping:** Even experienced developers can use "vibe coding" to quickly generate boilerplate or test ideas with `crawl4ai` before refining the code.
    *   **AI Coding Assistant Users:** This guide helps you understand what information to feed your AI to get the best `crawl4ai` code.

*   **1.3. How this Guide Helps You (and Your AI Assistant):**
    By understanding the concepts in this guide, you (and by extension, your AI assistant) will:
    *   Grasp the high-level capabilities of `crawl4ai` that are most relevant for prompting an AI.
    *   Learn the key terminology and building blocks of `crawl4ai` to include in your prompts for precise code generation.
    *   Discover common "vibe recipes" – typical data extraction tasks and how to prompt an AI to solve them using `crawl4ai`.
    *   Pick up effective prompting patterns to maximize the quality of AI-generated `crawl4ai` code.

## 2. High-Level Capabilities of Crawl4AI (What to Tell Your AI Assistant Crawl4AI Can Do)

When you're "vibe coding" with your AI assistant, you don't need to explain every nuance of `crawl4ai`. Instead, focus on what it *can do* for you. Here's a high-level overview of capabilities you can confidently tell your AI assistant about:

*   **2.1. Fetching Any Webpage:**
    *   **How to tell your AI:** "Crawl4AI can fetch the content of any webpage, whether it's a simple static page or a complex JavaScript-heavy application."
    *   **Why it's important:** This establishes the fundamental capability – getting the raw HTML from a target URL.

*   **2.2. Converting Web Content into Clean Markdown:**
    *   **How to tell your AI:** "Crawl4AI is great at turning messy web pages into clean, readable Markdown. This is perfect if I need to summarize an article, feed content into another LLM for Q&A, or just get the main text."
    *   **Why it's important:** Markdown is often the desired end-format for LLM-based tasks, and `crawl4ai` simplifies this conversion.

*   **2.3. Extracting Specific Pieces of Information (Structured Data/JSON):**
    *   **How to tell your AI:** "If I need specific data from a page, like all the product names and prices from an e-commerce site, or all the headlines from a news page, Crawl4AI can extract that and give it to me as structured JSON."
    *   **Why it's important:** This highlights `crawl4ai`'s ability to go beyond simple text extraction and pull out specific, targeted information.

*   **2.4. Crawling Multiple Pages at Once:**
    *   **How to tell your AI:** "If I have a list of URLs, Crawl4AI can process them all efficiently, often in parallel, instead of me having to do them one by one."
    *   **Why it's important:** This points to the library's capability for batch processing, saving significant time for larger tasks.

*   **2.5. Taking Screenshots or Making PDFs of Pages:**
    *   **How to tell your AI:** "Crawl4AI can take a screenshot of a webpage or save it as a PDF document. This is useful for archiving, visual checks, or sharing."
    *   **Why it's important:** Visual capture and PDF generation are common requirements that `crawl4ai` handles easily.

*   **2.6. Handling Simple Page Interactions:**
    *   **How to tell your AI:** "For pages that need a bit of interaction, like clicking a 'load more' button to reveal more content or scrolling down, Crawl4AI can execute JavaScript to do that."
    *   **Why it'S important:** This shows that `crawl4ai` isn't limited to static content and can handle basic dynamic page elements to access more data.

## 3. Key Building Blocks: Communicating Crawl4AI Concepts to Your AI Assistant

To get the best results from your AI assistant, you'll want to mention the key components of `crawl4ai` it should use. Think of these as the essential vocabulary for your "vibe" prompts.

*   **3.1. `AsyncWebCrawler`**: The Main Tool
    *   **3.1.1. Explaining its Role:**
        *   **What to tell your AI:** "The main tool we'll use in `crawl4ai` is called `AsyncWebCrawler`. We always need to create an instance of this first. It's best to use it with an `async with` statement to make sure resources are managed correctly."
        *   **Why it's important for the AI:** This directs the AI to the central class for all crawling operations and hints at the asynchronous nature and proper resource management.
    *   **3.1.2. Basic Usage Pattern:**
        *   **Vibe Prompt:** "Let's start a `crawl4ai` script. We'll need an `AsyncWebCrawler`."
        *   **Expected AI action:** The AI should generate the basic setup:
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig # Common imports

            async def main():
                # browser_config = BrowserConfig(...) # Optional, if needed
                async with AsyncWebCrawler(# config=browser_config
                                          ) as crawler:
                    # ... your crawl logic will go here ...
                    pass

            if __name__ == "__main__":
                asyncio.run(main())
            ```

*   **3.2. `arun(url, config)`**: Crawling a Single Page
    *   **3.2.1. Explaining its Purpose:**
        *   **What to tell your AI:** "To crawl a single page, we'll use the `arun` method of the `AsyncWebCrawler` instance. It takes the URL and an optional configuration object."
        *   **Why it's important for the AI:** This points to the primary method for individual page crawls.
    *   **3.2.2. Guiding the AI on `url` Specification:**
        *   **What to tell your AI:** "The `url` for `arun` can be a regular web address (like 'https://example.com'), a local file path (if it starts with `file://`), or even raw HTML content (if it starts with `raw:`)."
        *   **Vibe Prompts & AI Interpretation:**
            *   "Crawl this URL: https://my-target-site.com" -> AI uses `url="https://my-target-site.com"`
            *   "Process this local HTML file: /path/to/my/page.html" -> AI uses `url="file:///path/to/my/page.html"`
            *   "I have some raw HTML I want to parse: `<html><body><h1>Test</h1></body></html>`" -> AI uses `url="raw:<html><body><h1>Test</h1></body></html>"`
    *   **3.2.3. Emphasizing `config` (`CrawlerRunConfig`):**
        *   **What to tell your AI:** "The most important part for customizing a crawl is the `config` parameter of `arun`. This is where we'll use `CrawlerRunConfig` to tell `crawl4ai` *how* to crawl, what to extract, and what to do with the page."
        *   **Vibe Prompt:** "...and when you call `arun`, pass in a `CrawlerRunConfig` object with these settings: [describe settings]."

*   **3.3. `arun_many(urls, config)`**: Crawling Multiple Pages
    *   **3.3.1. Explaining its Purpose:**
        *   **What to tell your AI:** "If I have a list of URLs, we should use `arun_many` instead of calling `arun` in a loop. It's more efficient for batch crawling."
        *   **Why it's important for the AI:** Guides the AI to use the optimized method for multiple URLs, which handles concurrency internally.
    *   **3.3.2. Vibe Prompt:**
        *   "Crawl all these pages: https://example.com/page1, https://example.com/page2, https://example.com/page3."
        *   **Expected AI action:**
            ```python
            # ... inside async def main() and async with AsyncWebCrawler ...
            list_of_urls = [
                "https://example.com/page1",
                "https://example.com/page2",
                "https://example.com/page3"
            ]
            # run_config = CrawlerRunConfig(...) # Define config for all URLs
            results = await crawler.arun_many(urls=list_of_urls, config=run_config)
            for result in results:
                # process each result
                pass
            ```

*   **3.4. `CrawlerRunConfig`**: Tailoring the Crawl (Simplified for Vibe Coding)
    *   **3.4.1. Getting Markdown:**
        *   **Basic Request Vibe:** "Get the Markdown for this page [URL]."
            *   **AI's understanding:** The AI should know that `crawl4ai` produces Markdown by default, so a very simple `CrawlerRunConfig()` or even `None` might suffice if no other settings are needed.
        *   **For Cleaner Markdown Vibe:** "I need very clean Markdown from [URL], suitable for feeding into another LLM. Remove boilerplate like headers, footers, and navigation."
            *   **AI's understanding:** "Ah, the user wants cleaner output. I should suggest or use `DefaultMarkdownGenerator` combined with `PruningContentFilter`. The `PruningContentFilter` heuristically tries to remove common non-content sections."
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import CrawlerRunConfig, DefaultMarkdownGenerator
                from crawl4ai.content_filter_strategy import PruningContentFilter

                markdown_generator = DefaultMarkdownGenerator(
                    content_filter=PruningContentFilter()
                )
                run_config = CrawlerRunConfig(
                    markdown_generator=markdown_generator
                )
                # result = await crawler.arun(url="...", config=run_config)
                # print(result.markdown.fit_markdown) # Access the filtered markdown
                ```
            *   **Reasoning to convey (implicitly or explicitly):** `PruningContentFilter` is a good first step for general boilerplate removal. `DefaultMarkdownGenerator` then converts the filtered HTML to Markdown. Accessing `result.markdown.fit_markdown` gives this cleaner version.
    *   **3.4.2. Extracting Specific Data (JSON):**
        *   **3.4.2.1. Using `JsonCssExtractionStrategy` for Structured Pages:**
            *   **When to suggest it (Vibe):** "The page I want to crawl ([URL]) has a list of items, like products on an e-commerce category page, and each item has a similar layout. I want to extract these repeating items into a JSON list."
            *   **Information to provide the AI (Vibe):** "For each item, I want to get the 'product_name', which is usually in an `<h2>` tag, and the 'price', which seems to be in a `<span>` tag with a class like 'price-tag' or 'current-price'."
            *   **AI's Role & Reasoning:** The AI should recognize this pattern and suggest `JsonCssExtractionStrategy`. It understands that the user is describing a schema. The AI's job is to translate "name from h2" into `{"name": "product_name", "selector": "h2", "type": "text"}` within the `fields` list of a schema dictionary, and the overall repeating item selector into `baseSelector`. The AI should also know to set `extraction_type="schema"` on `LLMExtractionStrategy` if it were using that for schema generation, but here it's direct CSS.
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import CrawlerRunConfig
                from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

                # AI would help construct this schema based on user's description
                schema = {
                    "name": "ProductList",
                    "baseSelector": "div.product-item", # Example selector for each product block
                    "fields": [
                        {"name": "product_name", "selector": "h2.product-title", "type": "text"},
                        {"name": "price", "selector": "span.price-tag", "type": "text"}
                    ]
                }
                extraction_strategy = JsonCssExtractionStrategy(schema=schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     products = json.loads(result.extracted_content)
                #     for product in products:
                #         print(f"Name: {product.get('product_name')}, Price: {product.get('price')}")
                ```
        *   **3.4.2.2. Using `LLMExtractionStrategy` for Complex/Unclear Structures:**
            *   **When to suggest it (Vibe):** "The page ([URL]) has the information I want, but it's not in a clear, repeating list, or it's mixed in with a lot of text. I need the AI to understand the content to pull out specific details." Or, "I want to extract information that requires some interpretation, like summarizing a paragraph."
            *   **Information to provide the AI (Vibe):**
                *   "Use `LLMExtractionStrategy` for this."
                *   "The LLM I want to use is [LLM provider/model, e.g., 'openai/gpt-4o-mini'] and my API key is [YOUR_API_KEY_OR_ENV_VAR_NAME] (or tell it to look for an env var)."
                *   **Option A (Describing fields):** "I need a JSON object with the following fields: 'author_name', 'article_publish_date', and a 'short_summary' (about 2 sentences)."
                *   **Option B (Example JSON):** "The JSON output should look something like this: `{\"author\": \"Jane Doe\", \"published_on\": \"2024-05-23\", \"summary\": \"This article discusses...\"}`."
                *   **Option C (Pydantic Model - more advanced but best for AI):** "Here's a Pydantic model that defines the structure I want: [Pydantic Class Code Snippet]. Use this for the schema."
            *   **AI's Role & Reasoning:** The AI needs to construct an `LLMConfig` and an `LLMExtractionStrategy`. If the user provides field descriptions or an example JSON, the AI can generate a simple schema dictionary. If a Pydantic model is provided, the AI should use `MyPydanticModel.model_json_schema()` to create the schema for `LLMExtractionStrategy`. This strategy is powerful because it leverages the LLM's understanding.
            *   **Code Snippet AI might generate (with Pydantic example):**
                ```python
                from crawl4ai import CrawlerRunConfig, LLMConfig
                from crawl4ai.extraction_strategy import LLMExtractionStrategy
                from pydantic import BaseModel, Field # Assuming user might provide this

                # User might provide this, or AI generates it from description
                class ArticleInfo(BaseModel):
                    author_name: str = Field(description="The main author of the article")
                    publication_date: str = Field(description="The date the article was published, e.g., YYYY-MM-DD")
                    short_summary: str = Field(description="A concise 2-3 sentence summary of the article")

                llm_config = LLMConfig(
                    provider="openai/gpt-4o-mini", # Or user's choice
                    api_token="env:OPENAI_API_KEY" # Or direct key if user insists and understands risk
                )
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleInfo.model_json_schema(),
                    # instruction="Extract author, publication date, and a summary." # Could also be used
                    extraction_type="schema" # Important for Pydantic/JSON schema
                )
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     article_data = json.loads(result.extracted_content) # Or ArticleInfo.model_validate_json(result.extracted_content)
                #     print(article_data)
                ```
    *   **3.4.3. Interacting with Pages (Dynamic Content):**
        *   **How to tell your AI (Vibe):** "This page ([URL]) loads more content when you scroll down, or when you click a 'Show More' button. `crawl4ai` needs to perform this interaction."
        *   **For clicking (Vibe):** "To get all the data, we need to click the button with text 'Load All Comments'."
            *   **AI's understanding:** This requires `js_code` to find and click the button. The AI should be guided that finding elements by text might involve more complex JS like `Array.from(document.querySelectorAll('button')).find(btn => btn.textContent.includes('Load All Comments')).click();`.
        *   **For scrolling (Vibe):** "Scroll to the bottom of the page to make sure everything loads."
            *   **AI's understanding:** `js_code` like `window.scrollTo(0, document.body.scrollHeight);`
        *   **Ensuring actions complete (Vibe):** "After clicking 'Load More', wait for the new items to appear. They usually show up in a `div` with class `comment-list` and we expect more than 10 comments."
            *   **AI's understanding:** Use `wait_for`. This can be a CSS selector (e.g., `wait_for="css:.comment-list .comment-item:nth-child(11)"`) or a JS condition (e.g., `wait_for="js:() => document.querySelectorAll('.comment-item').length > 10"`).
        *   **For multi-step interactions on the same page (Vibe):** "I need to first click 'Agree to Cookies', then click 'Show Details', then extract the text. Make sure these happen on the same page view."
            *   **AI's understanding:** "This requires a `session_id` to persist the page state across multiple `arun` calls. Each `arun` call will perform one step of the interaction."
        *   **If only JS interaction is needed (Vibe):** "After the first page load, the next actions (like clicking 'Next Page') only update part of the page with JavaScript, they don't reload everything."
            *   **AI's understanding:** "For these subsequent `arun` calls within the same session, set `js_only=True` in `CrawlerRunConfig` to prevent unnecessary full page navigations, making it faster."
    *   **3.4.4. Taking Screenshots or PDFs:**
        *   **Simple request (Vibe):** "Get me a screenshot of [URL]."
            *   **AI's understanding:** Set `screenshot=True` in `CrawlerRunConfig`.
        *   **Or (Vibe):** "I need a PDF version of [URL]."
            *   **AI's understanding:** Set `pdf=True` in `CrawlerRunConfig`.
    *   **3.4.5. Caching for Speed and Freshness:**
        *   **For faster testing/development (Vibe):** "I'm testing my script for [URL]. Can you make it use the cache so it's faster after the first run?"
            *   **AI's understanding:** Use `cache_mode=CacheMode.ENABLED`.
            *   **Reasoning:** This saves time during development by re-using previously fetched content.
        *   **To get the latest data (Vibe):** "I need the absolute latest version of [URL], don't use any cached data."
            *   **AI's understanding:** Use `cache_mode=CacheMode.BYPASS`.
            *   **Reasoning:** Ensures `crawl4ai` fetches a fresh copy from the web, ignoring any local cache. This is crucial for production or when data changes frequently.

*   **3.5. `BrowserConfig`**: Browser-Level Settings (Simplified for Vibe Coding)
    *   **3.5.1. When it's usually not needed:**
        *   **What to tell your AI (or what it should infer):** "For most simple crawls where you just need to fetch a page and get its content, you probably don't need to worry about `BrowserConfig`. `crawl4ai` uses sensible defaults."
    *   **3.5.2. Headless Mode:**
        *   **Default behavior (Vibe):** "By default, `crawl4ai` runs 'headless', which means you won't see a browser window pop up. This is usually what we want for automated scripts."
        *   **To see the browser (Vibe):** "I'm having trouble with [URL], and I want to see what the browser is doing. Can you make it run in non-headless mode?"
            *   **AI's understanding:** The user wants to visually debug. Set `headless=False` in `BrowserConfig`.
    *   **3.5.3. Using a Proxy:**
        *   **How to ask (Vibe):** "I need to make `crawl4ai` use a proxy server for this crawl."
        *   **Information to provide (Vibe):** "The proxy server address is `http://myproxy.server.com:8080`. If it needs a username and password, they are `myuser` and `mypass`. Make sure to use the `proxy_config` setting in `BrowserConfig`."
            *   **AI's understanding:** The AI should construct a `ProxyConfig` object (or dictionary that `BrowserConfig` can handle) and pass it to `BrowserConfig`.
    *   **3.5.4. Changing User Agent:**
        *   **How to ask (Vibe):** "The website [URL] might be blocking default user agents. Can we make `crawl4ai` look like it's Firefox on a Mac?"
        *   **Information to provide (Vibe):** "You can set a custom `user_agent` string in `BrowserConfig`. For example, 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/115.0'."
            *   **AI's understanding:** The AI should pass the provided string to the `user_agent` parameter of `BrowserConfig`.

*   **3.6. `LLMConfig`**: Configuring Language Models (Simplified for Vibe Coding)
    *   **3.6.1. When it's needed:**
        *   **What to tell your AI:** "If we're using `LLMExtractionStrategy` to extract structured data or `LLMContentFilter` to clean up content, we need to tell `crawl4ai` which language model to use. This is done with an `LLMConfig` object."
    *   **3.6.2. Information to provide the AI (Vibe):**
        *   **Model choice:** "For this task, let's use the `provider` called 'openai/gpt-4o-mini'." (Other examples: 'ollama/llama3', 'anthropic/claude-3-opus-20240229').
        *   **API Key:** "My `api_token` for this provider is [YOUR_API_KEY_PLACEHOLDER]. (Best practice is to tell the AI to get it from an environment variable, e.g., 'env:OPENAI_API_KEY')."
            *   **AI's understanding:** The AI will create an `LLMConfig(provider="...", api_token="...")` and pass it to the relevant strategy.
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import LLMConfig
                # For OpenAI
                llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token="env:OPENAI_API_KEY")
                # For Ollama (locally running Llama3)
                # llm_conf = LLMConfig(provider="ollama/llama3") # api_token often not needed for local Ollama
                ```

*   **3.7. The `CrawlResult`**: Understanding What You Get Back
    *   **3.7.1. Checking for Success:**
        *   **What to tell your AI (Crucial Vibe):** "When `crawl4ai` finishes an `arun` or `arun_many` call, the most important first step is to check if it was successful. Tell the AI to always generate code that checks `result.success`. This will be `True` or `False`."
        *   **If `False` (Vibe):** "If `result.success` is `False`, the AI should print or log `result.error_message` to tell us what went wrong."
    *   **3.7.2. Accessing Markdown Content:**
        *   **Raw Markdown (Vibe):** "The main text content of the page, converted to Markdown, is usually in `result.markdown.raw_markdown`."
        *   **Filtered Markdown (Vibe):** "If we used a content filter (like `PruningContentFilter`), the cleaner, more focused Markdown will be in `result.markdown.fit_markdown`."
    *   **3.7.3. Accessing Extracted Structured Data (JSON):**
        *   **Where to find it (Vibe):** "If we asked `crawl4ai` to extract specific structured data (using `JsonCssExtractionStrategy` or `LLMExtractionStrategy`), that data will be in `result.extracted_content`."
        *   **How to use it (Vibe):** "The `result.extracted_content` is a JSON string. To use it in Python, tell the AI to parse it using `json.loads()`."
    *   **3.7.4. Accessing Links:**
        *   **What it contains (Vibe):** "`result.links` is a dictionary. It usually has two keys: 'internal' and 'external'. Each of these is a list of links found on the page, with details like the link text and URL."
    *   **3.7.5. Accessing Media Information:**
        *   **What it contains (Vibe):** "`result.media` is another dictionary. It can contain lists of 'images', 'videos', or 'audios' found on the page, along with their URLs and sometimes alt text or descriptions."
    *   **3.7.6. Accessing Screenshots and PDFs:**
        *   **Screenshots (Vibe):** "If we set `screenshot=True` in the config, the screenshot image data (which is a base64 encoded string) will be in `result.screenshot`."
        *   **PDFs (Vibe):** "If we set `pdf=True`, the PDF data (as raw bytes) will be in `result.pdf`."

## 4. Common "Vibe" Recipes & Prompting Patterns for AI Assistants

Here are some common scenarios and how you can prompt your AI assistant to use `crawl4ai` to solve them.

*   **4.1. Recipe 1: Get Clean Markdown from a Page**
    *   **4.1.1. User Vibe Prompt to AI:**
        *   *Example 1 (Basic):* "Hey AI, can you use `crawl4ai` to get me the clean Markdown content of `https://blog.example.com/my-latest-post`?"
        *   *Example 2 (Requesting more cleaning):* "AI, I need *very* clean Markdown from `https://some.news.site/article123`. It's for an LLM, so please try to remove ads, navigation, and footers."
    *   **4.1.2. Key `crawl4ai` elements the AI should use (and why):**
        *   **`AsyncWebCrawler`:** The starting point for any crawl.
        *   **`arun`:** Since it's a single URL.
        *   **`CrawlerRunConfig`:**
            *   For Example 1, the default `DefaultMarkdownGenerator` might be sufficient.
            *   For Example 2 ("very clean"), the AI should be guided to use `DefaultMarkdownGenerator(content_filter=PruningContentFilter())`.
            *   **Reasoning:** `PruningContentFilter` is designed to heuristically remove common boilerplate like headers, footers, and navigation bars before Markdown conversion, leading to cleaner text suitable for LLMs.
    *   **4.1.3. Expected Output from AI-generated code:**
        *   A Python script that initializes `AsyncWebCrawler`, calls `arun` with the appropriate URL and config.
        *   The script should then access and print (or save) `result.markdown.raw_markdown` (for basic) or `result.markdown.fit_markdown` (if `PruningContentFilter` was used).
        *   **Code Example (for "very clean"):**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator
            from crawl4ai.content_filter_strategy import PruningContentFilter

            async def get_clean_markdown(url_to_crawl):
                markdown_generator = DefaultMarkdownGenerator(
                    content_filter=PruningContentFilter()
                )
                run_config = CrawlerRunConfig(
                    markdown_generator=markdown_generator,
                    cache_mode="BYPASS" # Ensure fresh crawl for demo
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success:
                        print(f"--- Fit Markdown for {url_to_crawl} ---")
                        print(result.markdown.fit_markdown)
                        # You might also want to see raw_markdown to compare
                        # print(f"--- Raw Markdown for {url_to_crawl} ---")
                        # print(result.markdown.raw_markdown)
                    else:
                        print(f"Failed to crawl {url_to_crawl}: {result.error_message}")

            # asyncio.run(get_clean_markdown("https://en.wikipedia.org/wiki/Python_(programming_language)"))
            ```

*   **4.2. Recipe 2: Extract All Product Names and Prices from an E-commerce Category Page**
    *   **4.2.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, I need to use `crawl4ai` to get all product names and their prices from `https://www.example-store.com/laptops`. On that page, product names look like they are in `<h3>` tags with a class `product-title`, and prices are in `<span>` elements with the class `final-price`."
    *   **4.2.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**, **`arun`**.
        *   **`CrawlerRunConfig`** with **`JsonCssExtractionStrategy`**.
            *   **Reasoning:** The user described a page with repeating structured items. `JsonCssExtractionStrategy` is ideal for this as it uses CSS selectors to pinpoint the data. The AI's task is to translate the user's description of element locations into a valid schema for the strategy.
            *   The AI needs to understand that `baseSelector` in the schema should target the container for each product, and `fields` will target individual pieces of data within that container.
    *   **4.2.3. Expected Output from AI-generated code:**
        *   A Python script that defines the schema dictionary.
        *   Initializes `JsonCssExtractionStrategy` with this schema.
        *   Passes the strategy to `CrawlerRunConfig`.
        *   After `arun`, it parses `result.extracted_content` using `json.loads()` and likely iterates through the list of extracted product dictionaries.
        *   **Code Example:**
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
            from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

            async def extract_products(url_to_crawl):
                # AI helps create this schema based on user's description
                product_schema = {
                    "name": "LaptopList",
                    "baseSelector": "div.product-listing-item", # Hypothetical selector for each product's container
                    "fields": [
                        {"name": "product_name", "selector": "h3.product-title", "type": "text"},
                        {"name": "price", "selector": "span.final-price", "type": "text"}
                    ]
                }
                extraction_strategy = JsonCssExtractionStrategy(schema=product_schema)
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode="BYPASS"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success and result.extracted_content:
                        products = json.loads(result.extracted_content)
                        print(f"Found {len(products)} products:")
                        for i, product in enumerate(products[:3]): # Print first 3
                            print(f"  Product {i+1}: Name='{product.get('product_name')}', Price='{product.get('price')}'")
                    else:
                        print(f"Failed to extract products from {url_to_crawl}: {result.error_message}")

            # asyncio.run(extract_products("https://www.example-store.com/laptops")) # Replace with a real URL for testing
            ```

*   **4.3. Recipe 3: Extract Key Information from an Article using an LLM**
    *   **4.3.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, I want `crawl4ai` to read this article: `https://example.com/news/ai-breakthrough`. Use `openai/gpt-4o-mini` to extract the author's name, the publication date, and a short (2-3 sentence) summary. The output should be JSON. My OpenAI API key is in the `OPENAI_API_KEY` environment variable."
    *   **4.3.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**, **`arun`**.
        *   **`CrawlerRunConfig`** with **`LLMExtractionStrategy`**.
        *   **`LLMConfig`**: To specify the `provider` ("openai/gpt-4o-mini") and `api_token` ("env:OPENAI_API_KEY").
            *   **Reasoning:** The task requires understanding and summarization, making `LLMExtractionStrategy` suitable. The AI needs to construct a schema (either a simple dictionary or a Pydantic model `model_json_schema()`) that tells the LLM what fields to populate. The instruction to the LLM will be implicitly derived from the schema field descriptions or can be explicitly provided.
    *   **4.3.3. Expected Output from AI-generated code:**
        *   Python script that defines a Pydantic model (or a dictionary schema).
        *   Initializes `LLMConfig` and `LLMExtractionStrategy`.
        *   Parses `result.extracted_content`.
        *   **Code Example (using Pydantic):**
            ```python
            import asyncio
            import json
            import os
            from pydantic import BaseModel, Field
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
            from crawl4ai.extraction_strategy import LLMExtractionStrategy

            class ArticleDetails(BaseModel):
                author_name: str = Field(..., description="The main author of the article.")
                publication_date: str = Field(..., description="The date the article was published (e.g., YYYY-MM-DD).")
                summary: str = Field(..., description="A concise 2-3 sentence summary of the article.")

            async def extract_article_info_llm(url_to_crawl):
                if not os.getenv("OPENAI_API_KEY"): # Or your specific key variable
                    print("API key environment variable not set. Skipping LLM extraction.")
                    return

                llm_config = LLMConfig(
                    provider="openai/gpt-4o-mini", # Use a cost-effective model for demos
                    api_token="env:OPENAI_API_KEY"
                )
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleDetails.model_json_schema(),
                    extraction_type="schema" # Crucial for Pydantic/JSON schema
                )
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode="BYPASS"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success and result.extracted_content:
                        try:
                            article_data = ArticleDetails.model_validate_json(result.extracted_content)
                            print(f"Extracted Article Info for {url_to_crawl}:")
                            print(json.dumps(article_data.model_dump(), indent=2))
                        except Exception as e:
                            print(f"Error parsing LLM output: {e}")
                            print(f"Raw LLM output: {result.extracted_content}")
                    else:
                        print(f"Failed to extract article info from {url_to_crawl}: {result.error_message}")

            # asyncio.run(extract_article_info_llm("https://www.example.com/news/ai-breakthrough")) # Replace with real article
            ```

*   **4.4. Recipe 4: Crawl the first 3 pages of a blog (clicking "Next Page")**
    *   **4.4.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, can you use `crawl4ai` to get the Markdown from the first 3 pages of `https://myblog.example.com/archive`? To get to the next page, I think you need to click a link that says 'Older Posts'."
    *   **4.4.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**.
        *   **Multiple `arun` calls** in a loop (3 iterations).
        *   **`CrawlerRunConfig`** with:
            *   `session_id="blog_session"`: **Crucial** for maintaining the browser state (cookies, current page) across the multiple clicks.
            *   `js_code`: JavaScript to find and click the "Older Posts" link. The AI might need to generate robust JS like:
                `Array.from(document.querySelectorAll('a')).find(a => a.textContent.trim() === 'Older Posts')?.click();`
            *   `wait_for`: After clicking, wait for a condition that indicates the next page has loaded (e.g., a specific element on the new page, or a change in an existing element). This can be tricky and might require some iteration. A simple `wait_for` for a few seconds could also be a starting point, like `wait_for=3000` (milliseconds).
            *   `js_only=True`: For the second and third `arun` calls, after the initial page load. This tells `crawl4ai` to only execute the JS and not perform a full new navigation to the original URL.
    *   **4.4.3. Expected Output from AI-generated code:**
        *   A Python script with a loop that calls `arun` three times.
        *   The script should collect and potentially print or save the Markdown from each page.
        *   **Code Example:**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_blog_pages(start_url, num_pages=3):
                session_id = "my_blog_crawl_session"
                all_markdowns = []

                # JavaScript to find and click "Older Posts" (example)
                js_click_older_posts = """
                (() => {
                    const links = Array.from(document.querySelectorAll('a'));
                    const olderPostsLink = links.find(a => a.textContent.trim().toLowerCase() === 'older posts');
                    if (olderPostsLink) {
                        olderPostsLink.click();
                        return true; // Indicate click was attempted
                    }
                    return false; // Indicate link not found
                })();
                """

                async with AsyncWebCrawler() as crawler:
                    current_url = start_url
                    for i in range(num_pages):
                        print(f"Crawling page {i+1}...")
                        run_config_dict = {
                            "session_id": session_id,
                            "cache_mode": CacheMode.BYPASS,
                            "wait_for": 2000 # Wait 2s for content to potentially load after click
                        }
                        if i > 0: # For subsequent pages, click and don't re-navigate
                            run_config_dict["js_code"] = js_click_older_posts
                            run_config_dict["js_only"] = True
                        
                        run_config = CrawlerRunConfig(**run_config_dict)
                        
                        result = await crawler.arun(url=current_url, config=run_config) # URL is mainly for context in js_only
                        
                        if result.success:
                            print(f"  Page {i+1} ({result.url}) - Markdown length: {len(result.markdown.raw_markdown)}")
                            all_markdowns.append({"url": result.url, "markdown": result.markdown.raw_markdown})
                            if i < num_pages - 1 and i > 0 and not run_config_dict.get("js_code_executed_successfully", True): # Hypothetical flag
                                print(f"  'Older Posts' link might not have been found or clicked on page {i+1}. Stopping.")
                                break
                        else:
                            print(f"  Failed to crawl page {i+1}: {result.error_message}")
                            break
                    
                    # Important: Clean up the session
                    await crawler.crawler_strategy.kill_session(session_id) 
                
                print(f"\nCollected markdown for {len(all_markdowns)} pages.")
                # For demo, print first 100 chars of each
                # for i, md_data in enumerate(all_markdowns):
                #     print(f"\n--- Page {i+1} URL: {md_data['url']} ---")
                #     print(md_data['markdown'][:100] + "...")

            # asyncio.run(crawl_blog_pages("YOUR_BLOG_START_URL_HERE"))
            ```

*   **4.5. Recipe 5: Get Screenshots of a List of URLs**
    *   **4.5.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, use `crawl4ai` to take a screenshot of each of these pages: `https://example.com`, `https://crawl4ai.com`, `https://github.com`. Save them as `example_com.png`, `crawl4ai_com.png`, and `github_com.png`."
    *   **4.5.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**.
        *   **`arun_many`**: Efficient for processing a list of URLs.
        *   **`CrawlerRunConfig`** with `screenshot=True`.
            *   **Reasoning:** `arun_many` will process each URL with the same config. The AI needs to add logic to iterate through the results and save each `result.screenshot` (which is base64 data) to a uniquely named file.
    *   **4.5.3. Expected Output from AI-generated code:**
        *   Python script.
        *   PNG files saved to the current directory or a specified output directory.
        *   **Code Example:**
            ```python
            import asyncio
            import base64
            import os
            from urllib.parse import urlparse
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def take_screenshots(urls_to_screenshot):
                run_config = CrawlerRunConfig(
                    screenshot=True,
                    cache_mode=CacheMode.BYPASS # Get fresh screenshots
                )
                output_dir = "screenshots_output"
                os.makedirs(output_dir, exist_ok=True)

                async with AsyncWebCrawler() as crawler:
                    results = await crawler.arun_many(urls=urls_to_screenshot, config=run_config)
                    
                    for result in results:
                        if result.success and result.screenshot:
                            # Create a filename from the URL
                            parsed_url = urlparse(result.url)
                            filename = "".join(c if c.isalnum() else '_' for c in parsed_url.netloc + parsed_url.path)
                            if not filename or filename == "_": # Handle root path or empty paths
                                filename = "homepage"
                            filepath = os.path.join(output_dir, f"{filename}.png")
                            
                            try:
                                screenshot_data = base64.b64decode(result.screenshot)
                                with open(filepath, "wb") as f:
                                    f.write(screenshot_data)
                                print(f"Screenshot saved to {filepath}")
                            except Exception as e:
                                print(f"Error saving screenshot for {result.url}: {e}")
                        elif not result.success:
                            print(f"Failed to crawl {result.url}: {result.error_message}")
                        elif not result.screenshot:
                            print(f"Crawled {result.url} but no screenshot data was returned.")
            
            # urls = ["https://example.com", "https://crawl4ai.com", "https://github.com"]
            # asyncio.run(take_screenshots(urls))
            ```

## 5. Tips for Effective Prompting Your AI Assistant for Crawl4AI Tasks

To get the best code from your AI assistant when working with `crawl4ai`, consider these prompting tips:

*   **5.1. Be Clear About Your Goal:**
    *   Start with a high-level objective. Instead of just "Crawl a page," say "I need to extract all article titles from the homepage of this news site," or "Get the main content of this blog post as clean Markdown," or "Take full-page screenshots of these product pages." This helps the AI choose the right strategies and configurations.

*   **5.2. Always Provide the URL(s):**
    *   This seems obvious, but be precise. If it's a list, provide the list.
    *   Remember to use the `file:///` prefix for local files (e.g., `file:///Users/me/Documents/mypage.html`) and `raw:` for inline HTML (e.g., `raw:<html><body>...</body></html>`). The AI might not always infer this correctly without a hint.

*   **5.3. Describe Data for Extraction (Especially for `JsonCssExtractionStrategy` or `LLMExtractionStrategy`):**
    *   **What you want:** List the specific pieces of information you need (e.g., "product name," "price," "author," "publication_date," "article summary").
    *   **Where to find it (for CSS/XPath):** If you have an idea of the HTML structure, share it. "Product names seem to be in `<h2>` tags with class `item-title`." "The price is always in a `<span>` element right after a `<strong>` tag that says 'Price:'." This helps the AI generate accurate CSS selectors or XPath expressions for `JsonCssExtractionStrategy`.
    *   **Desired structure (for LLM):** For `LLMExtractionStrategy`, tell the AI the desired JSON structure. "I want a list of objects, where each object has a 'title' and a 'link'." Or even better, "Can you define a Pydantic model for me that has 'title' as a string and 'link' as a string, and then use that for extraction?"

*   **5.4. Specify LLM Details for LLM Extraction or Filtering:**
    *   **Model/Provider:** "Use `openai/gpt-4o-mini` for this extraction." or "I want to use my local Ollama model, `ollama/llama3`."
    *   **API Key:** Clearly state where the API key should come from. "My API key is in the environment variable `OPENAI_API_KEY`." (This is safer than putting the key directly in the prompt). If you must provide it directly, be aware of the security implications.

*   **5.5. Mention Page Dynamics and Interactions:**
    *   "This page loads more items when you scroll down."
    *   "You need to click the 'View All Reviews' button to see all the reviews."
    *   "The data I want only appears after selecting 'Category X' from a dropdown."
    *   This signals to the AI that `js_code`, `wait_for`, and possibly `session_id` will be necessary. You might need to guide it on *how* to identify the elements to interact with (e.g., "The 'Load More' button has the ID `load-more-btn`").

*   **5.6. Iterative Refinement is Key:**
    *   Your first prompt might not yield perfect code. That's okay!
    *   Treat it as a conversation. If the AI-generated code misses something or makes a mistake:
        *   "That was close, but it missed extracting the product ratings. Ratings seem to be in a `div` with class `star-rating` inside each product item."
        *   "The script timed out. Can we increase the `page_timeout` in `CrawlerRunConfig` to 90 seconds?"
        *   "It didn't click the 'Next' button correctly. The button actually has the text '>>' instead of 'Next Page'."
    *   Provide the error messages or incorrect output back to the AI for context.

## 6. What to Expect as Output (From AI-Generated Code)

When you use "Vibe Coding" with an AI assistant for `crawl4ai`, you should generally expect the following:

*   **6.1. Python Code:**
    *   The primary output will be a Python script that uses the `crawl4ai` library.
    *   It should include necessary imports like `asyncio`, `AsyncWebCrawler`, `CrawlerRunConfig`, etc.
    *   It will typically define an `async def main():` function and run it with `asyncio.run(main())`.

*   **6.2. Accessing the `CrawlResult`:**
    *   The core of the script will involve one or more calls to `crawler.arun(...)` or `crawler.arun_many(...)`.
    *   These calls return `CrawlResult` objects (or a list of them for `arun_many`).
    *   The AI-generated code should then show you how to access the specific data you asked for from these `CrawlResult` objects. For example:
        *   `print(result.markdown.raw_markdown)` or `print(result.markdown.fit_markdown)`
        *   `data = json.loads(result.extracted_content)`
        *   `screenshot_data = base64.b64decode(result.screenshot)`
        *   `if not result.success: print(result.error_message)`

*   **6.3. Files Saved to Disk (if requested):**
    *   If your vibe prompt included saving data (e.g., "save the screenshots as PNG files," "write the extracted JSON to `output.json`"), the AI-generated code should include the Python logic to perform these file operations.
    *   **Example for saving a screenshot:**
        ```python
        import base64
        # ... inside your async function, after getting 'result' ...
        if result.success and result.screenshot:
            with open("myscreenshot.png", "wb") as f:
                f.write(base64.b64decode(result.screenshot))
            print("Screenshot saved to myscreenshot.png")
        ```

## 7. Conclusion: Vibe Your Way to Web Data!

*   **7.1. Recap of "Vibe Coding" Benefits with `crawl4ai`:**
    "Vibe Coding" empowers you to leverage the full capabilities of `crawl4ai` without needing to memorize every API detail. By understanding the high-level concepts and key building blocks outlined in this guide, you can effectively communicate your data extraction and web interaction needs to an AI coding assistant. This leads to faster prototyping, easier access to web data for non-programmers, and a more intuitive way to build data-driven applications.

*   **7.2. Encouragement to experiment with different prompts and `crawl4ai` features:**
    The key to successful "Vibe Coding" is experimentation. Try different ways of describing your goals to your AI assistant. If the first attempt doesn't yield the perfect `crawl4ai` code, refine your prompt with more specific details or hints. Don't be afraid to mention `crawl4ai` specific terms like `CrawlerRunConfig`, `js_code`, or `LLMExtractionStrategy` – this guide has equipped you with the essential vocabulary. The more context you provide, the better the AI can assist you.

*   **7.3. Pointers to more detailed `crawl4ai` documentation for users who want to learn direct coding or advanced configurations:**
    While "Vibe Coding" is a great way to get started and be productive quickly, you might eventually want to dive deeper into `crawl4ai`'s capabilities or fine-tune the generated code yourself. For that, refer to:
    *   **The Official Crawl4AI API Reference:** (Assuming this exists or will exist - replace with actual link if available, e.g., `https://docs.crawl4ai.com/api/`) For detailed information on all classes, methods, and parameters.
    *   **Specific "Reasoning & Problem-Solving" Guides:** Check the `crawl4ai` documentation for other guides that delve into specific components like advanced `CrawlerRunConfig` options, deep crawling strategies, or custom extraction techniques.

Happy Vibe Coding, and may your web data adventures be fruitful!
```

---


## Vibe Coding - Examples
Source: crawl4ai_vibe_examples_content.llm.md

# Examples Outline for crawl4ai - vibe Component

**Target Document Type:** Examples Collection
**Target Output Filename Suggestion:** `llm_examples_vibe.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

This document provides a collection of runnable code examples for the `vibe` component of the `crawl4ai` library, focusing on its deep crawling capabilities, filtering, and scoring mechanisms.

**Note on URLs:** Most examples use placeholder URLs like `https://docs.crawl4ai.com/vibe-examples/pageN.html`. These are for demonstration and will be mocked to return predefined content. Replace them with actual URLs for real-world use.

**Common Imports (assumed for many examples below, but will be included in each runnable block):**
```python
import asyncio
import time
import re
from pathlib import Path
import os # For local file examples
from crawl4ai import (
    AsyncWebCrawler,
    CrawlerRunConfig,
    CrawlResult,
    BrowserConfig,
    CacheMode,
    # Deep Crawling Strategies
    BFSDeePCrawlStrategy,
    DFSDeePCrawlStrategy,
    BestFirstCrawlingStrategy,
    DeepCrawlStrategy, # For custom strategy
    # Filters
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter,
    URLFilter,
    ContentRelevanceFilter, # Conceptual
    SEOFilter,            # Conceptual
    FilterStats,
    # Scorers
    URLScorer, # For custom scorer
    KeywordRelevanceScorer,
    PathDepthScorer,
    ContentTypeScorer,
    DomainAuthorityScorer, # Conceptual
    FreshnessScorer,       # Conceptual
    CompositeScorer,
    # Other
    LLMExtractionStrategy, # For combination example
    AsyncLogger          # For custom logger example
)
from unittest.mock import patch, AsyncMock # For mocking network calls

# --- Mock Website Data ---
# This data will be used by the MockAsyncWebCrawler to simulate a website
MOCK_SITE_DATA = {
    "https://docs.crawl4ai.com/vibe-examples/index.html": {
        "html_content": """
            <html><head><title>Index</title></head><body>
                <h1>Main Page</h1>
                <a href="page1.html">Page 1</a>
                <a href="page2.html">Page 2 (Feature)</a>
                <a href="https://external-site.com/pageA.html">External Site</a>
                <a href="/vibe-examples/archive/old_page.html">Archive</a>
                <a href="/vibe-examples/blog/post1.html">Blog Post 1</a>
                <a href="/vibe-examples/login.html">Login</a>
                <a href="javascript:void(0);" onclick="document.body.innerHTML += '<a href=js_page.html>JS Link</a>'">Load JS Link</a>
            </body></html>
        """,
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/page1.html": {
        "html_content": """
            <html><head><title>Page 1</title></head><body>
                <h2>Page One</h2>
                <p>This is page 1. It has some core content about crawl strategies.</p>
                <a href="page1_sub1.html">Sub Page 1.1</a>
                <a href="page1_sub2.pdf">Sub Page 1.2 (PDF)</a>
                <a href="index.html">Back to Index</a>
            </body></html>
        """,
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/page1_sub1.html": {
        "html_content": "<html><head><title>Sub Page 1.1</title></head><body><p>Sub page 1.1 content. More on core concepts.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/page1_sub2.pdf": {
        "html_content": "%PDF-1.4 ... (Mock PDF Content: Crawl examples)", # Mock PDF content
        "response_headers": {"Content-Type": "application/pdf"}
    },
    "https://docs.crawl4ai.com/vibe-examples/page2.html": {
        "html_content": """
            <html><head><title>Page 2 - Feature Rich</title></head><body>
                <h2>Page Two with Feature</h2>
                <p>This page discusses a key feature and advanced configuration for async tasks.</p>
                <a href="page2_sub1.html">Sub Page 2.1</a>
            </body></html>
        """,
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/page2_sub1.html": {
        "html_content": "<html><head><title>Sub Page 2.1</title></head><body><p>More about the feature and JavaScript interaction.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/archive/old_page.html": {
        "html_content": "<html><head><title>Old Page</title></head><body><p>Archived content, less relevant.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/blog/post1.html": {
        "html_content": "<html><head><title>Blog Post 1</title></head><body><p>This is a blog post about core ideas and examples.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
     "https://docs.crawl4ai.com/vibe-examples/login.html": {
        "html_content": "<html><head><title>Login</title></head><body><form>...</form></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://docs.crawl4ai.com/vibe-examples/js_page.html": {
        "html_content": "<html><head><title>JS Page</title></head><body><p>Content loaded by JavaScript.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    "https://external-site.com/pageA.html": {
        "html_content": "<html><head><title>External Page A</title></head><body><p>Content from external site about other topics.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    },
    # For local file examples
    "file:" + str(Path(os.getcwd()) / "test_local_index.html"): {
         "html_content": """
            <html><head><title>Local Index</title></head><body>
                <h1>Local Main Page</h1>
                <a href="test_local_page1.html">Local Page 1</a>
                <a href="https://docs.crawl4ai.com/vibe-examples/index.html">Web Index</a>
            </body></html>
        """,
        "response_headers": {"Content-Type": "text/html"}
    },
    "file:" + str(Path(os.getcwd()) / "test_local_page1.html"): {
        "html_content": "<html><head><title>Local Page 1</title></head><body><p>Local page 1 content.</p></body></html>",
        "response_headers": {"Content-Type": "text/html"}
    }
}

# Create a dummy local file for testing
Path("test_local_index.html").write_text(MOCK_SITE_DATA["file:" + str(Path(os.getcwd()) / "test_local_index.html")]["html_content"])
Path("test_local_page1.html").write_text(MOCK_SITE_DATA["file:" + str(Path(os.getcwd()) / "test_local_page1.html")]["html_content"])


# --- Mock AsyncWebCrawler ---
# This mock crawler will simulate fetching pages from MOCK_SITE_DATA
class MockAsyncWebCrawler(AsyncWebCrawler):
    async def _fetch_page(self, url: str, config: CrawlerRunConfig):
        # Simulate network delay
        await asyncio.sleep(0.01)
        
        # Normalize URL for lookup (e.g. relative to absolute)
        if not url.startswith("file:") and not url.startswith("http"):
            # This is a simplified relative URL resolver for the mock
            base_parts = self.current_url.split('/')[:-1] if hasattr(self, 'current_url') and self.current_url else []
            normalized_url = "/".join(base_parts + [url])
            if "docs.crawl4ai.com" not in normalized_url and not normalized_url.startswith("file:"): # ensure base domain
                 normalized_url = "https://docs.crawl4ai.com/vibe-examples/" + url.lstrip("/")
        else:
            normalized_url = url

        if normalized_url in MOCK_SITE_DATA:
            page_data = MOCK_SITE_DATA[normalized_url]
            self.current_url = normalized_url # Store for relative path resolution
            
            # Basic link extraction for deep crawling
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_data["html_content"], 'html.parser')
            links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                # Simple relative to absolute conversion for mock
                if not href.startswith("http") and not href.startswith("file:") and not href.startswith("javascript:"):
                    abs_href = "/".join(normalized_url.split('/')[:-1]) + "/" + href.lstrip("./")
                     # Further simplify to ensure it hits mock data, very basic
                    if "docs.crawl4ai.com" in abs_href: # if it's a vibe-example page
                        abs_href = "https://docs.crawl4ai.com/vibe-examples/" + Path(href).name
                    elif "external-site.com" in abs_href:
                        abs_href = "https://external-site.com/" + Path(href).name

                elif href.startswith("file:"): # Keep file URLs as is
                    abs_href = href
                elif href.startswith("javascript:"):
                    abs_href = None # Skip JS links for this mock
                else:
                    abs_href = href
                
                if abs_href:
                    links.append({"href": abs_href, "text": a_tag.get_text(strip=True)})

            return CrawlResult(
                url=normalized_url,
                html_content=page_data["html_content"],
                success=True,
                status_code=200,
                response_headers=page_data.get("response_headers", {"Content-Type": "text/html"}),
                links={"internal": [l for l in links if "docs.crawl4ai.com/vibe-examples" in l["href"] or l["href"].startswith("file:")], 
                       "external": [l for l in links if "external-site.com" in l["href"]]}
            )
        else:
            # print(f"Mock Warning: URL not found in MOCK_SITE_DATA: {normalized_url} (Original: {url})")
            return CrawlResult(
                url=url, html_content="", success=False, status_code=404, error_message="Mock URL not found"
            )

    async def arun(self, url: str, config: CrawlerRunConfig = None, **kwargs):
        # This is the method called by DeepCrawlStrategy instances
        # For deep crawls, the strategy itself calls this multiple times.
        # For a single arun call with a deep_crawl_strategy, the decorator handles it.
        
        if config and config.deep_crawl_strategy:
             # The decorator usually handles this part. For direct strategy.arun() tests:
            return await config.deep_crawl_strategy.arun(
                crawler=self, # Pass the mock crawler instance
                start_url=url,
                config=config
            )
        
        # Fallback to single page fetch if no deep crawl strategy
        self.current_url = url # Set for relative path resolution in _fetch_page
        return await self._fetch_page(url, config)

    async def arun_many(self, urls: list[str], config: CrawlerRunConfig = None, **kwargs):
        results = []
        for url_item in urls:
            # In BestFirst, arun_many is called with tuples of (score, depth, url, parent_url)
            # For simplicity in mock, we assume url_item is just the URL string here or a tuple where url is at index 2
            current_url_to_crawl = url_item
            if isinstance(url_item, tuple) and len(url_item) >=3 :
                 current_url_to_crawl = url_item[2]

            self.current_url = current_url_to_crawl # Set for relative path resolution
            result = await self._fetch_page(current_url_to_crawl, config)
            results.append(result)
        if config and config.stream:
            async def result_generator():
                for res in results:
                    yield res
            return result_generator()
        return results

    async def __aenter__(self):
        # print("MockAsyncWebCrawler entered")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # print("MockAsyncWebCrawler exited")
        pass
    
    async def start(self): # Add start method
        # print("MockAsyncWebCrawler started")
        self.ready = True
        return self

    async def close(self): # Add close method
        # print("MockAsyncWebCrawler closed")
        self.ready = False

# --- End Mock ---
```

---
## 1. Introduction to Deep Crawling (`vibe`)

The `vibe` component of Crawl4ai provides powerful deep crawling capabilities, allowing you to traverse websites by following links and processing multiple pages.

### 1.1. Example: Enabling Basic Deep Crawl with `BFSDeePCrawlStrategy` via `CrawlerRunConfig`.
This example demonstrates how to enable a basic Breadth-First Search (BFS) deep crawl by setting the `deep_crawl_strategy` in `CrawlerRunConfig`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

# Using the MockAsyncWebCrawler defined in the preamble
@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def basic_bfs_deep_crawl():
    # Configure BFS to crawl up to 1 level deep from the start URL
    bfs_strategy = BFSDeePCrawlStrategy(max_depth=1)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=bfs_strategy,
        # For mock, ensure cache is bypassed to see fresh mock results
        cache_mode=CacheMode.BYPASS 
    )

    # The actual AsyncWebCrawler is replaced by MockAsyncWebCrawler via @patch
    async with AsyncWebCrawler() as crawler: # This will be MockAsyncWebCrawler
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- Basic BFS Deep Crawl (max_depth=1) ---")
        print(f"Crawled {len(results)} pages starting from {start_url}:")
        for i, result in enumerate(results):
            if result.success:
                print(f"  {i+1}. URL: {result.url}, Depth: {result.metadata.get('depth')}, Parent: {result.metadata.get('parent_url')}")
            else:
                print(f"  {i+1}. FAILED: {result.url}, Error: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(basic_bfs_deep_crawl())
```

### 1.2. Example: Understanding `CrawlResult.metadata` (depth, parent_url, score) in Deep Crawl Results.
Each `CrawlResult` from a deep crawl contains useful metadata like the crawl `depth`, the `parent_url` from which it was discovered, and a `score` (if applicable, e.g., with `BestFirstCrawlingStrategy`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, KeywordRelevanceScorer, BestFirstCrawlingStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def understand_metadata():
    # Using BestFirstCrawlingStrategy to demonstrate scores
    scorer = KeywordRelevanceScorer(keywords=["feature", "core"])
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- Understanding CrawlResult.metadata ---")
        for result in results:
            if result.success:
                depth = result.metadata.get('depth', 'N/A')
                parent = result.metadata.get('parent_url', 'N/A')
                score = result.metadata.get('score', 'N/A') # Score comes from BestFirst strategy
                print(f"URL: {result.url}")
                print(f"  Depth: {depth}")
                print(f"  Parent URL: {parent}")
                print(f"  Score: {score if score != 'N/A' else 'N/A (not scored or BFS/DFS)'}")
                print("-" * 20)

if __name__ == "__main__":
    asyncio.run(understand_metadata())
```

### 1.3. Example: Minimal setup for deep crawling a single level deep.
This demonstrates the most straightforward way to perform a shallow deep crawl (depth 1).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def minimal_single_level_deep_crawl():
    # BFS strategy, max_depth=1 means start_url + its direct links
    strategy = BFSDeePCrawlStrategy(max_depth=1) 
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- Minimal Single Level Deep Crawl (max_depth=1) ---")
        print(f"Total pages crawled: {len(results)}")
        for result in results:
            if result.success:
                print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(minimal_single_level_deep_crawl())
```

---
## 2. Breadth-First Search (`BFSDeePCrawlStrategy`) Examples

`BFSDeePCrawlStrategy` explores the website level by level.

### 2.1. Example: Basic `BFSDeePCrawlStrategy` with default depth.
The default `max_depth` for `BFSDeePCrawlStrategy` is often 1 if not specified, meaning it crawls the start URL and its direct links.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_default_depth():
    # Default max_depth is typically 1 (start_url + its direct children)
    # but let's be explicit for clarity or test with a higher default if library changes
    strategy = BFSDeePCrawlStrategy() # Default max_depth is 1
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with Default Depth (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(bfs_default_depth())
```

### 2.2. Example: `BFSDeePCrawlStrategy` - Setting `max_depth` to control crawl depth (e.g., 3 levels).
Control how many levels deep the BFS crawler will go from the start URL. `max_depth=0` means only the start URL. `max_depth=1` means start URL + its direct links.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_set_max_depth():
    strategy = BFSDeePCrawlStrategy(max_depth=2) # Start URL (0), its links (1), and their links (2)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with max_depth=2 ---")
        print(f"Crawled {len(results)} pages.")
        for result in sorted(results, key=lambda r: (r.metadata.get('depth', 0), r.url)):
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
        
        # Verify that no pages with depth > 2 are present
        assert all(r.metadata.get('depth', 0) <= 2 for r in results if r.success)

if __name__ == "__main__":
    asyncio.run(bfs_set_max_depth())
```

### 2.3. Example: `BFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages crawled (e.g., 10 pages).
Limit the crawl to a maximum number of pages, regardless of depth.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch
import math # for math.inf

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_set_max_pages():
    strategy = BFSDeePCrawlStrategy(
        max_depth=math.inf, # Effectively no depth limit for this test
        max_pages=3         # Limit to 3 pages
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with max_pages=3 ---")
        print(f"Crawled {len(results)} pages (should be at most 3).")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
        
        assert len(results) <= 3

if __name__ == "__main__":
    asyncio.run(bfs_set_max_pages())
```

### 2.4. Example: `BFSDeePCrawlStrategy` - Using `include_external=True` to follow links to external domains.
Allow the BFS crawler to follow links that lead to different domains than the start URL.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_include_external():
    strategy = BFSDeePCrawlStrategy(
        max_depth=1, 
        include_external=True
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with include_external=True (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        found_external = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            if "external-site.com" in result.url:
                found_external = True
        
        assert found_external, "Expected to crawl an external link."

if __name__ == "__main__":
    asyncio.run(bfs_include_external())
```

### 2.5. Example: `BFSDeePCrawlStrategy` - Using `include_external=False` (default) to stay within the starting domain.
The default behavior is to only crawl links within the same domain as the start URL.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_exclude_external():
    strategy = BFSDeePCrawlStrategy(
        max_depth=1, 
        include_external=False # Default, but explicit for clarity
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with include_external=False (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        found_external = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            if "external-site.com" in result.url:
                found_external = True
        
        assert not found_external, "Should not have crawled external links."

if __name__ == "__main__":
    asyncio.run(bfs_exclude_external())
```

### 2.6. Example: `BFSDeePCrawlStrategy` - Streaming results using `CrawlerRunConfig(stream=True)`.
Process results as they become available, useful for long crawls.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_streaming_results():
    strategy = BFSDeePCrawlStrategy(max_depth=1)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True, # Enable streaming
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- BFS with Streaming Results (max_depth=1) ---")
        count = 0
        async for result in await crawler.arun(url=start_url, config=run_config):
            count += 1
            if result.success:
                print(f"  Streamed Result {count}: {result.url}, Depth: {result.metadata.get('depth')}")
            else:
                print(f"  Streamed FAILED Result {count}: {result.url}, Error: {result.error_message}")
        print(f"Total results streamed: {count}")

if __name__ == "__main__":
    asyncio.run(bfs_streaming_results())
```

### 2.7. Example: `BFSDeePCrawlStrategy` - Batch results using `CrawlerRunConfig(stream=False)` (default).
The default behavior is to return all results as a list after the crawl completes.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_batch_results():
    strategy = BFSDeePCrawlStrategy(max_depth=1)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=False, # Default, but explicit for clarity
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config) # Returns a list
        
        print(f"--- BFS with Batch Results (max_depth=1) ---")
        print(f"Received {len(results)} pages in a batch.")
        for result in results:
            if result.success:
                print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(bfs_batch_results())
```

### 2.8. Example: `BFSDeePCrawlStrategy` - Integrating a `FilterChain` with `URLPatternFilter` to crawl specific paths.
Use filters to guide the crawler, for instance, to only explore URLs matching `/blog/*`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_with_url_pattern_filter():
    # Only crawl URLs containing '/blog/'
    url_filter = URLPatternFilter(patterns=["*/blog/*"])
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(
        max_depth=1, 
        filter_chain=filter_chain
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with URLPatternFilter ('*/blog/*') ---")
        print(f"Crawled {len(results)} pages.")
        all_match_pattern = True
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            # The start URL itself might not match, but discovered links should
            if result.metadata.get('depth', 0) > 0 and "/blog/" not in result.url:
                all_match_pattern = False
        
        # The start_url itself is always processed, then its links are filtered.
        # So, we check if all *discovered* pages match the pattern.
        discovered_pages = [r for r in results if r.metadata.get('depth',0) > 0]
        if discovered_pages: # only assert if any pages beyond start_url were processed
            assert all("/blog/" in r.url for r in discovered_pages), "Not all crawled pages matched the /blog/ pattern"
        print("Filter applied successfully (start URL is always processed, subsequent links are filtered).")


if __name__ == "__main__":
    asyncio.run(bfs_with_url_pattern_filter())
```

### 2.9. Example: `BFSDeePCrawlStrategy` - Demonstrating `shutdown()` to gracefully stop an ongoing crawl.
Showcase how to stop a crawl prematurely using the strategy's `shutdown()` method.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_demonstrate_shutdown():
    strategy = BFSDeePCrawlStrategy(
        max_depth=5, # A potentially long crawl
        max_pages=100 
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True, # Streaming is good to see partial results before shutdown
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html" # A site with enough links
        
        print(f"--- BFS with shutdown() demonstration ---")
        
        crawl_task = asyncio.create_task(crawler.arun(url=start_url, config=run_config))
        
        # Let the crawl run for a very short time
        await asyncio.sleep(0.1) 
        
        print("Attempting to shut down the crawl...")
        await strategy.shutdown() 
        
        results_list = []
        try:
            # Await the results from the crawl task
            # If streaming, this will iterate through what was processed before shutdown
            async for res in await crawl_task:
                results_list.append(res)
                print(f"  Collected result (post-shutdown signal): {res.url}")
        except asyncio.CancelledError:
            print("Crawl task was cancelled.")
        
        print(f"Crawl shut down. Processed {len(results_list)} pages before/during shutdown.")
        # The number of pages will be less than if it ran to completion
        assert len(results_list) < 10, "Crawl likely didn't shut down early enough or mock site too small."

if __name__ == "__main__":
    asyncio.run(bfs_demonstrate_shutdown())
```

### 2.10. Example: `BFSDeePCrawlStrategy` - Crawling with no `max_depth` limit but a `max_pages` limit.
Demonstrate a scenario where depth is unlimited (or very high) but the crawl stops after a certain number of pages.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch
import math

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def bfs_no_depth_limit_max_pages():
    strategy = BFSDeePCrawlStrategy(
        max_depth=math.inf, # Unlimited depth
        max_pages=4        # But only 4 pages
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BFS with no depth limit, max_pages=4 ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
        
        assert len(results) <= 4, "More pages crawled than max_pages limit."

if __name__ == "__main__":
    asyncio.run(bfs_no_depth_limit_max_pages())
```

---
## 3. Depth-First Search (`DFSDeePCrawlStrategy`) Examples

`DFSDeePCrawlStrategy` explores as far down one branch as possible before backtracking.

### 3.1. Example: Basic `DFSDeePCrawlStrategy` with default depth.
The default `max_depth` for `DFSDeePCrawlStrategy` is typically 10 if not specified.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_default_depth():
    # Default max_depth for DFS is typically higher (e.g., 10)
    strategy = DFSDeePCrawlStrategy() 
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        max_pages=5, # Limit pages to keep example short with default depth
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with Default Depth (max_pages=5 to limit output) ---")
        print(f"Crawled {len(results)} pages.")
        for result in results: # Order might be less predictable than BFS for small mock
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(dfs_default_depth())
```

### 3.2. Example: `DFSDeePCrawlStrategy` - Setting `max_depth` to control how deep each branch goes.
Set `max_depth` to 2 for a DFS crawl.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_set_max_depth():
    strategy = DFSDeePCrawlStrategy(max_depth=2)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with max_depth=2 ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
        assert all(r.metadata.get('depth', 0) <= 2 for r in results if r.success)


if __name__ == "__main__":
    asyncio.run(dfs_set_max_depth())
```

### 3.3. Example: `DFSDeePCrawlStrategy` - Setting `max_pages` to limit the total number of pages.
Limit the total number of pages crawled by DFS to 3.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch
import math

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_set_max_pages():
    strategy = DFSDeePCrawlStrategy(
        max_depth=math.inf, # No depth limit for this test
        max_pages=3
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with max_pages=3 ---")
        print(f"Crawled {len(results)} pages (should be at most 3).")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
        assert len(results) <= 3

if __name__ == "__main__":
    asyncio.run(dfs_set_max_pages())
```

### 3.4. Example: `DFSDeePCrawlStrategy` - Following external links with `include_external=True`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_include_external():
    strategy = DFSDeePCrawlStrategy(
        max_depth=1, 
        include_external=True,
        max_pages=5 # Limit pages as external can be vast
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with include_external=True (max_depth=1, max_pages=5) ---")
        print(f"Crawled {len(results)} pages.")
        found_external = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            if "external-site.com" in result.url:
                found_external = True
        
        assert found_external, "Expected to crawl an external link."

if __name__ == "__main__":
    asyncio.run(dfs_include_external())
```

### 3.5. Example: `DFSDeePCrawlStrategy` - Staying within the domain with `include_external=False`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_exclude_external():
    strategy = DFSDeePCrawlStrategy(
        max_depth=1, 
        include_external=False # Default
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with include_external=False (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        found_external = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            if "external-site.com" in result.url:
                found_external = True
        
        assert not found_external, "Should not have crawled external links."

if __name__ == "__main__":
    asyncio.run(dfs_exclude_external())
```

### 3.6. Example: `DFSDeePCrawlStrategy` - Streaming results.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_streaming_results():
    strategy = DFSDeePCrawlStrategy(max_depth=1)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- DFS with Streaming Results (max_depth=1) ---")
        count = 0
        async for result in await crawler.arun(url=start_url, config=run_config):
            count +=1
            if result.success:
                print(f"  Streamed Result {count}: {result.url}, Depth: {result.metadata.get('depth')}")
        print(f"Total results streamed: {count}")


if __name__ == "__main__":
    asyncio.run(dfs_streaming_results())
```

### 3.7. Example: `DFSDeePCrawlStrategy` - Batch results.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_batch_results():
    strategy = DFSDeePCrawlStrategy(max_depth=1)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=False, # Default
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with Batch Results (max_depth=1) ---")
        print(f"Received {len(results)} pages in a batch.")
        for result in results:
            if result.success:
                print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(dfs_batch_results())
```

### 3.8. Example: `DFSDeePCrawlStrategy` - Integrating a `FilterChain` with `DomainFilter` to restrict to subdomains.
This example is conceptual for subdomains as MOCK_SITE_DATA doesn't have distinct subdomains. The filter setup is key.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DFSDeePCrawlStrategy, FilterChain, DomainFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def dfs_with_domain_filter_subdomains():
    # Allow only the start domain and its subdomains
    # For this mock, 'docs.crawl4ai.com' will be the main domain.
    # If we had e.g., 'blog.docs.crawl4ai.com', this filter would allow it.
    domain_filter = DomainFilter(
        allowed_domains=["docs.crawl4ai.com"], 
        allow_subdomains=True
    )
    filter_chain = FilterChain(filters=[domain_filter])
    
    strategy = DFSDeePCrawlStrategy(
        max_depth=1, 
        filter_chain=filter_chain,
        include_external=True # Necessary to even consider other (sub)domains
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DFS with DomainFilter (allow subdomains of docs.crawl4ai.com) ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            # In a real scenario, you'd assert that only allowed domains/subdomains are present.
            # Our mock data doesn't have true subdomains to test this effectively.
            assert "docs.crawl4ai.com" in result.url or "external-site.com" not in result.url

if __name__ == "__main__":
    asyncio.run(dfs_with_domain_filter_subdomains())
```

---
## 4. Best-First Crawling (`BestFirstCrawlingStrategy`) Examples

`BestFirstCrawlingStrategy` uses a priority queue, guided by scorers, to decide which URLs to crawl next.

### 4.1. Example: Basic `BestFirstCrawlingStrategy` with default parameters.
If no `url_scorer` is provided, it behaves somewhat like BFS but might have different internal queue management.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_default_params():
    strategy = BestFirstCrawlingStrategy(max_depth=1) # Default scorer (often scores 0)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy with default parameters (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 0.0):.2f}")

if __name__ == "__main__":
    asyncio.run(best_first_default_params())
```

### 4.2. Example: `BestFirstCrawlingStrategy` - Setting `max_depth` to limit crawl depth.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_max_depth():
    strategy = BestFirstCrawlingStrategy(max_depth=2)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy with max_depth=2 ---")
        print(f"Crawled {len(results)} pages.")
        for result in sorted(results, key=lambda r: (r.metadata.get('depth', 0), r.url)):
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 0.0):.2f}")
        assert all(r.metadata.get('depth', 0) <= 2 for r in results if r.success)

if __name__ == "__main__":
    asyncio.run(best_first_max_depth())
```

### 4.3. Example: `BestFirstCrawlingStrategy` - Setting `max_pages` to limit total pages crawled.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from unittest.mock import patch
import math

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_max_pages():
    strategy = BestFirstCrawlingStrategy(
        max_depth=math.inf, 
        max_pages=3
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy with max_pages=3 ---")
        print(f"Crawled {len(results)} pages.")
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 0.0):.2f}")
        assert len(results) <= 3

if __name__ == "__main__":
    asyncio.run(best_first_max_pages())
```

### 4.4. Example: `BestFirstCrawlingStrategy` - Using `include_external=True`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_include_external():
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        include_external=True,
        max_pages=5 # To keep it manageable
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy with include_external=True (max_depth=1) ---")
        print(f"Crawled {len(results)} pages.")
        found_external = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}, Score: {result.metadata.get('score', 0.0):.2f}")
            if "external-site.com" in result.url:
                found_external = True
        
        assert found_external, "Expected to crawl an external link."

if __name__ == "__main__":
    asyncio.run(best_first_include_external())
```

### 4.5. Example: `BestFirstCrawlingStrategy` - Using `KeywordRelevanceScorer` to prioritize URLs containing specific keywords.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_keyword_scorer():
    scorer = KeywordRelevanceScorer(keywords=["feature", "advanced", "core"])
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        url_scorer=scorer,
        max_pages=4 # Limit for example clarity
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS,
        stream=True # Stream to see order
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- BestFirstCrawlingStrategy with KeywordRelevanceScorer ---")
        results_list = []
        async for result in await crawler.arun(url=start_url, config=run_config):
            results_list.append(result)
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f} (Depth: {result.metadata.get('depth')})")
        
        # Check if pages with keywords like "feature" or "core" were prioritized (appeared earlier/higher score)
        # This is a soft check as actual order depends on many factors in a real crawl
        # and the mock site's link structure.
        print("\nNote: Higher scores should ideally correspond to URLs with keywords 'feature', 'advanced', 'core'.")
        feature_page_crawled = any("page2.html" in r.url for r in results_list) # page2 has "feature"
        assert feature_page_crawled, "Page with 'feature' keyword was expected."


if __name__ == "__main__":
    asyncio.run(best_first_keyword_scorer())
```

### 4.6. Example: `BestFirstCrawlingStrategy` - Using `PathDepthScorer` to influence priority based on URL path depth.
This scorer penalizes deeper paths by default.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, PathDepthScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_path_depth_scorer():
    # Penalizes deeper paths (lower score for deeper paths)
    scorer = PathDepthScorer(higher_score_is_better=False) 
    strategy = BestFirstCrawlingStrategy(
        max_depth=2, # Allow some depth to see scorer effect
        url_scorer=scorer
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS,
        stream=True
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- BestFirstCrawlingStrategy with PathDepthScorer (favoring shallower paths) ---")
        
        results_list = []
        async for result in await crawler.arun(url=start_url, config=run_config):
            results_list.append(result)
            if result.success:
                 print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}")
        
        # A simple check: depth 1 pages should generally have higher (less negative) scores than depth 2
        # (if scores are negative due to penalty) or simply appear earlier if scores are positive.
        # With default scoring, higher score_is_better = True, so higher depth = lower score.
        # With higher_score_is_better=False, higher depth = higher (less negative) score.
        # The mock PathDepthScorer will need to be implemented or this test adjusted based on actual scorer logic.
        # For now, let's assume the scorer penalizes, so deeper paths have lower (more negative) scores.
        print("\nNote: Shallower pages should ideally have higher scores.")


if __name__ == "__main__":
    asyncio.run(best_first_path_depth_scorer())
```

### 4.7. Example: `BestFirstCrawlingStrategy` - Using `ContentTypeScorer` to prioritize HTML pages over PDFs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, ContentTypeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_content_type_scorer():
    # Prioritize HTML, penalize PDF
    scorer = ContentTypeScorer(content_type_weights={"text/html": 1.0, "application/pdf": -0.5})
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        url_scorer=scorer
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS,
        stream=True
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # This page links to HTML and PDF
        print(f"--- BestFirstCrawlingStrategy with ContentTypeScorer (HTML > PDF) ---")
        
        results_list = []
        async for result in await crawler.arun(url=start_url, config=run_config):
            results_list.append(result)
            if result.success:
                 print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Content-Type: {result.response_headers.get('Content-Type')}")

        html_page_score = next((r.metadata.get('score') for r in results_list if "page1_sub1.html" in r.url), None)
        pdf_page_score = next((r.metadata.get('score') for r in results_list if "page1_sub2.pdf" in r.url), None)

        print(f"HTML page score: {html_page_score}, PDF page score: {pdf_page_score}")
        if html_page_score is not None and pdf_page_score is not None:
            assert html_page_score > pdf_page_score, "HTML page should have a higher score than PDF."
        elif html_page_score is None or pdf_page_score is None:
            print("Warning: Could not find both HTML and PDF pages in results to compare scores.")


if __name__ == "__main__":
    asyncio.run(best_first_content_type_scorer())
```

### 4.8. Example: `BestFirstCrawlingStrategy` - Using `CompositeScorer` to combine `KeywordRelevanceScorer` and `PathDepthScorer`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer, PathDepthScorer, CompositeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_composite_scorer():
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature", "core"], weight=0.7)
    path_scorer = PathDepthScorer(weight=0.3, higher_score_is_better=False) # Penalize depth slightly
    
    composite_scorer = CompositeScorer(scorers=[keyword_scorer, path_scorer])
    
    strategy = BestFirstCrawlingStrategy(
        max_depth=2, 
        url_scorer=composite_scorer,
        max_pages=6
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS,
        stream=True
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- BestFirstCrawlingStrategy with CompositeScorer ---")
        
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}")
        print("\nNote: Scores are a combination of keyword relevance and path depth penalty.")

if __name__ == "__main__":
    asyncio.run(best_first_composite_scorer())
```

### 4.9. Example: `BestFirstCrawlingStrategy` - Integrating a `FilterChain` with `ContentTypeFilter` to only process HTML.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, FilterChain, ContentTypeFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_with_content_type_filter():
    content_filter = ContentTypeFilter(allowed_types=["text/html"])
    filter_chain = FilterChain(filters=[content_filter])
    
    # Scorer is optional here, just demonstrating filter integration
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        filter_chain=filter_chain
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # This page links to HTML and PDF
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy with ContentTypeFilter (HTML only) ---")
        print(f"Crawled {len(results)} pages.")
        all_html = True
        for result in results:
            content_type = result.response_headers.get('Content-Type', '')
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}, Content-Type: {content_type}")
            if result.metadata.get('depth',0) > 0 and "text/html" not in content_type : # Start URL is not filtered
                 all_html = False
        
        discovered_pages = [r for r in results if r.metadata.get('depth',0) > 0]
        if discovered_pages:
            assert all("text/html" in r.response_headers.get('Content-Type','') for r in discovered_pages), "Non-HTML page found among discovered pages."
        print("Filter for HTML content type applied successfully to discovered pages.")

if __name__ == "__main__":
    asyncio.run(best_first_with_content_type_filter())
```

### 4.10. Example: `BestFirstCrawlingStrategy` - Streaming results and observing the order based on scores.
This example will use a scorer and stream results to demonstrate that higher-scored URLs are (generally) processed earlier.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_streaming_order():
    scorer = KeywordRelevanceScorer(keywords=["feature", "advanced"])
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        url_scorer=scorer,
        max_pages=5
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        print(f"--- BestFirstCrawlingStrategy - Streaming and Observing Order ---")
        
        previous_score = float('inf') # Assuming scores are positive and higher is better
        processed_urls = []
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                current_score = result.metadata.get('score', 0.0)
                print(f"  Streamed: {result.url}, Score: {current_score:.2f}, Depth: {result.metadata.get('depth')}")
                # Note: Due to batching (BATCH_SIZE) and async nature, strict descending order isn't guaranteed
                # but generally higher scored items should appear earlier.
                # assert current_score <= previous_score + 1e-9, f"Scores not in generally descending order: {previous_score} then {current_score}"
                # previous_score = current_score
                processed_urls.append((result.url, current_score))

        print("\nProcessed URLs and their scores (order of processing):")
        for url, score in processed_urls:
            print(f"  {url} (Score: {score:.2f})")
        print("Note: Higher scored URLs are prioritized but strict order depends on batching and concurrency.")

if __name__ == "__main__":
    asyncio.run(best_first_streaming_order())
```

### 4.11. Example: `BestFirstCrawlingStrategy` - Batch results and analyzing scores post-crawl.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_batch_analysis():
    scorer = KeywordRelevanceScorer(keywords=["feature", "core"])
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        url_scorer=scorer,
        max_pages=5
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=False, # Batch mode
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy - Batch Results Analysis ---")
        print(f"Received {len(results)} pages.")
        
        # Sort by score for analysis (higher score first)
        sorted_results = sorted(results, key=lambda r: r.metadata.get('score', 0.0), reverse=True)
        
        for result in sorted_results:
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}")

if __name__ == "__main__":
    asyncio.run(best_first_batch_analysis())
```

### 4.12. Example: `BestFirstCrawlingStrategy` - Accessing and interpreting `score`, `depth`, and `parent_url` from `CrawlResult.metadata`.
This explicitly shows how to get these specific metadata fields.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_access_metadata():
    scorer = KeywordRelevanceScorer(keywords=["feature"])
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- BestFirstCrawlingStrategy - Accessing Metadata ---")
        for result in results:
            if result.success:
                url = result.url
                metadata = result.metadata
                depth = metadata.get('depth', 'N/A')
                parent_url = metadata.get('parent_url', 'N/A')
                score = metadata.get('score', 'N/A')
                
                print(f"URL: {url}")
                print(f"  Depth: {depth}")
                print(f"  Parent URL: {parent_url}")
                print(f"  Score: {score:.2f}" if isinstance(score, float) else f"  Score: {score}")
                print("-" * 10)

if __name__ == "__main__":
    asyncio.run(best_first_access_metadata())
```

### 4.13. Example: `BestFirstCrawlingStrategy` - Demonstrating `shutdown()` to stop an ongoing prioritized crawl.

```python
import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_demonstrate_shutdown():
    scorer = KeywordRelevanceScorer(keywords=["feature", "core", "example"])
    strategy = BestFirstCrawlingStrategy(
        max_depth=5, # A potentially long crawl
        max_pages=100,
        url_scorer=scorer
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True, 
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        
        print(f"--- BestFirstCrawlingStrategy with shutdown() demonstration ---")
        
        crawl_task = asyncio.create_task(crawler.arun(url=start_url, config=run_config))
        
        await asyncio.sleep(0.1) 
        
        print("Attempting to shut down the BestFirst crawl...")
        await strategy.shutdown() 
        
        results_list = []
        try:
            async for res in await crawl_task:
                results_list.append(res)
                print(f"  Collected result (post-shutdown signal): {res.url} (Score: {res.metadata.get('score', 0.0):.2f})")
        except asyncio.CancelledError:
            print("Crawl task was cancelled.")
        
        print(f"Crawl shut down. Processed {len(results_list)} pages before/during shutdown.")
        assert len(results_list) < 10, "Crawl likely didn't shut down early enough or mock site too small."

if __name__ == "__main__":
    asyncio.run(best_first_demonstrate_shutdown())
```

### 4.14. Example: `BestFirstCrawlingStrategy` - Explaining the effect of `BATCH_SIZE` on `arun_many`.
`BATCH_SIZE` is an internal constant in `bbf_strategy.py` (typically 10). This example explains its role rather than making it directly configurable by the user through the strategy's constructor, as it's an internal implementation detail of how the strategy uses `crawler.arun_many`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

# Note: BATCH_SIZE is internal to BestFirstCrawlingStrategy, usually 10.
# We can't directly set it, but we can explain its effect.

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def best_first_batch_size_effect():
    print("--- Explaining BATCH_SIZE in BestFirstCrawlingStrategy ---")
    print("BestFirstCrawlingStrategy processes URLs in batches for efficiency.")
    print("Internally, it retrieves a batch of highest-priority URLs (typically up to BATCH_SIZE, e.g., 10) from its queue.")
    print("It then calls `crawler.arun_many()` with this batch.")
    print("This means that while URLs are prioritized, the order within a small batch might not be strictly descending by score,")
    print("especially if `stream=True`, as results from `arun_many` can arrive slightly out of strict submission order.")
    print("The overall crawl still heavily favors higher-scored URLs first over many batches.")

    # To simulate observing this, let's run a crawl and see if groups of results are processed.
    scorer = KeywordRelevanceScorer(keywords=["feature", "core", "page1", "page2"])
    strategy = BestFirstCrawlingStrategy(
        max_depth=2, 
        url_scorer=scorer,
        max_pages=6 # Small enough to potentially see batching effects if BATCH_SIZE was smaller
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        
        print("\n--- Crawl Example (max_pages=6) ---")
        results_in_order = []
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                results_in_order.append(result.metadata.get('score',0.0))
                print(f"  Streamed: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}")
        
        # This assertion is hard to make definitively without knowing the exact internal BATCH_SIZE
        # and perfect mock site behavior. The print statements are more illustrative.
        print("\nScores in order of processing:", [f"{s:.2f}" for s in results_in_order])
        print("Observe if there are small groups where order might not be strictly descending due to batch processing.")


if __name__ == "__main__":
    asyncio.run(best_first_batch_size_effect())
```

---
## 5. Configuring Filters (`FilterChain`) for Deep Crawling

Filters allow you to control which URLs are processed during a deep crawl. They are applied *before* a URL is added to the crawl queue (except for the start URL).

### 5.1. `URLPatternFilter`

#### 5.1.1. Example: Using `URLPatternFilter` to allow URLs matching specific patterns (e.g., `/blog/*`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allow_pattern():
    # Allow only URLs containing '/blog/'
    url_filter = URLPatternFilter(patterns=["*/blog/*"])
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- URLPatternFilter: Allowing '*/blog/*' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url} (Depth: {r.metadata.get('depth')})")
            if r.metadata.get('depth', 0) > 0: # Check discovered URLs
                assert "/blog/" in r.url, f"Page {r.url} does not match pattern."
        print("All discovered pages match the allowed pattern.")

if __name__ == "__main__":
    asyncio.run(filter_allow_pattern())
```

#### 5.1.2. Example: Using `URLPatternFilter` to block URLs matching specific patterns (e.g., `*/login/*`, `*/archive/*`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_block_pattern():
    # Block URLs containing '/login/' or '/archive/'
    url_filter = URLPatternFilter(patterns=["*/login/*", "*/archive/*"], block_list=True)
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- URLPatternFilter: Blocking '*/login/*' and '*/archive/*' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url} (Depth: {r.metadata.get('depth')})")
            assert "/login/" not in r.url, f"Page {r.url} should have been blocked (login)."
            assert "/archive/" not in r.url, f"Page {r.url} should have been blocked (archive)."
        print("No pages matching blocked patterns were crawled.")

if __name__ == "__main__":
    asyncio.run(filter_block_pattern())
```

#### 5.1.3. Example: `URLPatternFilter` with `case_sensitive=True` vs. `case_sensitive=False`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter
from unittest.mock import patch

# Add a case-specific URL to MOCK_SITE_DATA
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/Page1.html"] = {
    "html_content": "<html><head><title>Page 1 Case Test</title></head><body><p>Content for case test.</p></body></html>",
    "response_headers": {"Content-Type": "text/html"}
}
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] += '<a href="Page1.html">Page 1 Case Test</a>'


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_pattern_case_sensitivity():
    start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"

    # Case-sensitive: should only match 'page1.html'
    print("\n--- URLPatternFilter: Case Sensitive (Allow '*/page1.html*') ---")
    url_filter_sensitive = URLPatternFilter(patterns=["*/page1.html*"], case_sensitive=True)
    filter_chain_sensitive = FilterChain(filters=[url_filter_sensitive])
    strategy_sensitive = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain_sensitive)
    run_config_sensitive = CrawlerRunConfig(deep_crawl_strategy=strategy_sensitive, cache_mode=CacheMode.BYPASS)
    
    async with AsyncWebCrawler() as crawler:
        results_sensitive = await crawler.arun(url=start_url, config=run_config_sensitive)
        print(f"Crawled {len(results_sensitive)} pages.")
        for r in results_sensitive:
            print(f"  URL: {r.url}")
            if r.metadata.get('depth',0) > 0:
                assert "page1.html" in r.url and "Page1.html" not in r.url, "Case-sensitive filter failed."
    
    # Case-insensitive: should match both 'page1.html' and 'Page1.html'
    print("\n--- URLPatternFilter: Case Insensitive (Allow '*/page1.html*') ---")
    url_filter_insensitive = URLPatternFilter(patterns=["*/page1.html*"], case_sensitive=False)
    filter_chain_insensitive = FilterChain(filters=[url_filter_insensitive])
    strategy_insensitive = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain_insensitive)
    run_config_insensitive = CrawlerRunConfig(deep_crawl_strategy=strategy_insensitive, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        results_insensitive = await crawler.arun(url=start_url, config=run_config_insensitive)
        print(f"Crawled {len(results_insensitive)} pages.")
        found_page1_lower = False
        found_page1_upper = False
        for r in results_insensitive:
            print(f"  URL: {r.url}")
            if "page1.html" in r.url.lower(): # Check lower to catch both
                 if "page1.html" == Path(r.url).name: found_page1_lower = True
                 if "Page1.html" == Path(r.url).name: found_page1_upper = True
        
        assert found_page1_lower and found_page1_upper, "Case-insensitive filter should have matched both cases."

if __name__ == "__main__":
    asyncio.run(filter_pattern_case_sensitivity())
```

### 5.2. `DomainFilter`

#### 5.2.1. Example: Using `DomainFilter` with `allowed_domains` to restrict crawling to a list of specific domains.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, DomainFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allowed_domains():
    # Only crawl within 'docs.crawl4ai.com'
    domain_filter = DomainFilter(allowed_domains=["docs.crawl4ai.com"])
    filter_chain = FilterChain(filters=[domain_filter])
    
    # include_external needs to be True for DomainFilter to even consider other domains for blocking/allowing
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html" # This links to external-site.com
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DomainFilter: Allowing only 'docs.crawl4ai.com' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url}")
            assert "docs.crawl4ai.com" in r.url, f"Page {r.url} is not from an allowed domain."
        print("All crawled pages are from 'docs.crawl4ai.com'.")

if __name__ == "__main__":
    asyncio.run(filter_allowed_domains())
```

#### 5.2.2. Example: Using `DomainFilter` with `blocked_domains` to avoid crawling certain domains.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, DomainFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_blocked_domains():
    # Block 'external-site.com'
    domain_filter = DomainFilter(blocked_domains=["external-site.com"])
    filter_chain = FilterChain(filters=[domain_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DomainFilter: Blocking 'external-site.com' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url}")
            assert "external-site.com" not in r.url, f"Page {r.url} from blocked domain was crawled."
        print("No pages from 'external-site.com' were crawled.")

if __name__ == "__main__":
    asyncio.run(filter_blocked_domains())
```

#### 5.2.3. Example: `DomainFilter` configured to allow subdomains (`allow_subdomains=True`).
(Conceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, DomainFilter
from unittest.mock import patch

# Imagine MOCK_SITE_DATA also has:
# "https://blog.docs.crawl4ai.com/vibe-examples/post.html": { ... }
# And index.html links to it.

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allow_subdomains():
    domain_filter = DomainFilter(allowed_domains=["docs.crawl4ai.com"], allow_subdomains=True)
    filter_chain = FilterChain(filters=[domain_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DomainFilter: Allowing subdomains of 'docs.crawl4ai.com' (Conceptual) ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url}")
            # In a real test, you'd check if blog.docs.crawl4ai.com was included
        print("This example is conceptual; for a real test, ensure mock data includes subdomains.")

if __name__ == "__main__":
    asyncio.run(filter_allow_subdomains())
```

#### 5.2.4. Example: `DomainFilter` configured to disallow subdomains (`allow_subdomains=False`).
(Conceptual as MOCK_SITE_DATA doesn't have subdomains for `docs.crawl4ai.com`.)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, DomainFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_disallow_subdomains():
    domain_filter = DomainFilter(allowed_domains=["docs.crawl4ai.com"], allow_subdomains=False) # Default
    filter_chain = FilterChain(filters=[domain_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- DomainFilter: Disallowing subdomains of 'docs.crawl4ai.com' (Conceptual) ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url}")
            # In a real test, you'd check if blog.docs.crawl4ai.com was NOT included
        print("This example is conceptual; for a real test, ensure mock data includes subdomains to be excluded.")

if __name__ == "__main__":
    asyncio.run(filter_disallow_subdomains())
```

### 5.3. `ContentTypeFilter`

#### 5.3.1. Example: Using `ContentTypeFilter` to allow only `text/html` pages.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, ContentTypeFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allow_html_only():
    content_filter = ContentTypeFilter(allowed_types=["text/html"])
    filter_chain = FilterChain(filters=[content_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # Links to HTML and PDF
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- ContentTypeFilter: Allowing only 'text/html' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            content_type = r.response_headers.get('Content-Type', '')
            print(f"  URL: {r.url}, Content-Type: {content_type}")
            if r.metadata.get('depth', 0) > 0: # Check discovered URLs
                assert "text/html" in content_type, f"Page {r.url} has wrong content type: {content_type}"
        print("All discovered pages are 'text/html'.")

if __name__ == "__main__":
    asyncio.run(filter_allow_html_only())
```

#### 5.3.2. Example: Using `ContentTypeFilter` with multiple `allowed_types` (e.g., `text/html`, `application/json`).
(Conceptual, as MOCK_SITE_DATA only has html/pdf)

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, ContentTypeFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allow_multiple_types():
    content_filter = ContentTypeFilter(allowed_types=["text/html", "application/json"])
    filter_chain = FilterChain(filters=[content_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" 
        # Imagine page1.html also links to a page1_sub3.json
        MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1_sub3.json"] = {
            "html_content": '{"key": "value"}',
            "response_headers": {"Content-Type": "application/json"}
        }
        MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"] += '<a href="page1_sub3.json">JSON Data</a>'


        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- ContentTypeFilter: Allowing 'text/html', 'application/json' ---")
        print(f"Crawled {len(results)} pages.")
        found_json = False
        for r in results:
            content_type = r.response_headers.get('Content-Type', '')
            print(f"  URL: {r.url}, Content-Type: {content_type}")
            if r.metadata.get('depth',0) > 0:
                assert "text/html" in content_type or "application/json" in content_type
            if "application/json" in content_type:
                found_json = True
        assert found_json, "Expected to find a JSON page."
        print("All discovered pages are either 'text/html' or 'application/json'.")
        
        # Clean up mock data
        del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1_sub3.json"]
        MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"].replace('<a href="page1_sub3.json">JSON Data</a>', '')


if __name__ == "__main__":
    asyncio.run(filter_allow_multiple_types())
```

#### 5.3.3. Example: Using `ContentTypeFilter` with `blocked_types` (e.g., blocking `application/pdf`).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, ContentTypeFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_block_pdf():
    content_filter = ContentTypeFilter(blocked_types=["application/pdf"])
    filter_chain = FilterChain(filters=[content_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # Links to HTML and PDF
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- ContentTypeFilter: Blocking 'application/pdf' ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            content_type = r.response_headers.get('Content-Type', '')
            print(f"  URL: {r.url}, Content-Type: {content_type}")
            assert "application/pdf" not in content_type, f"PDF page {r.url} was not blocked."
        print("No 'application/pdf' pages were crawled (beyond start URL if it was PDF).")

if __name__ == "__main__":
    asyncio.run(filter_block_pdf())
```

### 5.4. `URLFilter` (Simple exact match)

#### 5.4.1. Example: `URLFilter` to allow a specific list of exact URLs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_allow_exact_urls():
    allowed_urls = [
        "https://docs.crawl4ai.com/vibe-examples/page1.html",
        "https://docs.crawl4ai.com/vibe-examples/page1_sub1.html"
    ]
    url_filter = URLFilter(urls=allowed_urls, block_list=False) # Allow list
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=2, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- URLFilter: Allowing specific URLs ---")
        print(f"Crawled {len(results)} pages.")
        crawled_urls = {r.url for r in results}
        # The start URL is always crawled initially, then its links are filtered.
        # So we check that all *other* crawled URLs are in the allowed list.
        for r_url in crawled_urls:
            if r_url != start_url: # Exclude start_url from this assertion
                 assert r_url in allowed_urls, f"URL {r_url} was not in the allowed list."
        print("Only URLs from the allowed list (plus start_url) were crawled.")

if __name__ == "__main__":
    asyncio.run(filter_allow_exact_urls())
```

#### 5.4.2. Example: `URLFilter` to block a specific list of exact URLs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLFilter
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_block_exact_urls():
    blocked_urls = [
        "https://docs.crawl4ai.com/vibe-examples/page2.html",
        "https://docs.crawl4ai.com/vibe-examples/archive/old_page.html"
    ]
    url_filter = URLFilter(urls=blocked_urls, block_list=True) # Block list
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- URLFilter: Blocking specific URLs ---")
        print(f"Crawled {len(results)} pages.")
        crawled_urls = {r.url for r in results}
        for blocked_url in blocked_urls:
            assert blocked_url not in crawled_urls, f"URL {blocked_url} should have been blocked."
        print("Blocked URLs were not crawled.")

if __name__ == "__main__":
    asyncio.run(filter_block_exact_urls())
```

### 5.5. `ContentRelevanceFilter`
This filter uses an LLM to determine relevance. The example focuses on setup, as a full run requires an LLM.

#### 5.5.1. Example: Setting up `ContentRelevanceFilter` with target keywords (conceptual, focusing on setup).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, ContentRelevanceFilter, LLMConfig

# This is a conceptual example showing setup.
# A real run would require an LLM provider to be configured.
async def setup_content_relevance_filter():
    print("--- Setting up ContentRelevanceFilter (Conceptual) ---")
    
    # Define keywords and context for relevance
    keywords = ["artificial intelligence", "web crawling", "data extraction"]
    context_query = "Articles related to AI-powered web scraping tools and techniques."

    # Configure LLM (replace with your actual provider and API key)
    llm_config = LLMConfig(provider="openai/gpt-3.5-turbo", api_token="YOUR_OPENAI_API_KEY")
    
    relevance_filter = ContentRelevanceFilter(
        llm_config=llm_config,
        keywords=keywords,
        context_query=context_query,
        threshold=0.6 # Adjust threshold as needed
    )
    filter_chain = FilterChain(filters=[relevance_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    print("ContentRelevanceFilter configured. To run this example:")
    print("1. Replace 'YOUR_OPENAI_API_KEY' with your actual OpenAI API key.")
    print("2. (Optional) Install OpenAI client: pip install openai")
    print("3. Uncomment the crawler execution part below.")

    # # Example of how it would be used (requires actual LLM call)
    # async with AsyncWebCrawler() as crawler:
    #     # Mock or use a real URL that would trigger the LLM
    #     start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" 
    #     print(f"Attempting to crawl {start_url} with ContentRelevanceFilter...")
    #     # results = await crawler.arun(url=start_url, config=run_config)
    #     # print(f"Crawled {len(results)} pages after relevance filtering.")
    #     # for r in results:
    #     #     print(f"  URL: {r.url}, Relevance Score: {r.metadata.get('relevance_score')}")
    print("Conceptual setup complete.")

if __name__ == "__main__":
    asyncio.run(setup_content_relevance_filter())
```

#### 5.5.2. Example: `ContentRelevanceFilter` with a custom `threshold`.

```python
import asyncio
from crawl4ai import ContentRelevanceFilter, LLMConfig

async def content_relevance_custom_threshold():
    print("--- ContentRelevanceFilter with custom threshold (Conceptual Setup) ---")
    llm_config = LLMConfig(provider="openai/gpt-3.5-turbo", api_token="YOUR_OPENAI_API_KEY") # Replace
    
    # A higher threshold means stricter relevance checking
    strict_filter = ContentRelevanceFilter(
        llm_config=llm_config,
        keywords=["specific technical term"],
        threshold=0.8 
    )
    print(f"Strict filter created with threshold: {strict_filter.threshold}")

    # A lower threshold is more lenient
    lenient_filter = ContentRelevanceFilter(
        llm_config=llm_config,
        keywords=["general topic"],
        threshold=0.4
    )
    print(f"Lenient filter created with threshold: {lenient_filter.threshold}")
    print("Note: Actual filtering behavior depends on LLM responses to content.")

if __name__ == "__main__":
    asyncio.run(content_relevance_custom_threshold())
```

### 5.6. `SEOFilter`
This filter checks for common SEO issues. The example is conceptual, focusing on setup.

#### 5.6.1. Example: Basic `SEOFilter` with default SEO checks (conceptual, focusing on setup).

```python
import asyncio
from crawl4ai import SEOFilter

async def setup_basic_seo_filter():
    print("--- Basic SEOFilter with default checks (Conceptual Setup) ---")
    
    # Default checks might include missing title, short meta description, etc.
    seo_filter = SEOFilter() 
    
    print(f"SEOFilter created with default settings:")
    print(f"  Min Title Length: {seo_filter.min_title_length}")
    print(f"  Max Title Length: {seo_filter.max_title_length}")
    print(f"  Min Meta Description Length: {seo_filter.min_meta_description_length}")
    # ... and other default parameters
    print("This filter would be added to a FilterChain and used in a DeepCrawlStrategy.")
    print("It would then check each page against these SEO criteria.")

if __name__ == "__main__":
    asyncio.run(setup_basic_seo_filter())
```

#### 5.6.2. Example: `SEOFilter` configuring specific checks like `min_title_length`, `max_meta_description_length`, or `keyword_in_title_check` (conceptual).

```python
import asyncio
from crawl4ai import SEOFilter

async def setup_custom_seo_filter():
    print("--- SEOFilter with custom checks (Conceptual Setup) ---")
    
    custom_seo_filter = SEOFilter(
        min_title_length=20,
        max_meta_description_length=150,
        keyword_in_title_check=True,
        target_keywords_for_seo=["crawl4ai", "web scraping"] # if keyword_in_title_check is True
    )
    
    print(f"Custom SEOFilter created with:")
    print(f"  Min Title Length: {custom_seo_filter.min_title_length}")
    print(f"  Max Meta Description Length: {custom_seo_filter.max_meta_description_length}")
    print(f"  Keyword in Title Check: {custom_seo_filter.keyword_in_title_check}")
    print(f"  Target SEO Keywords: {custom_seo_filter.target_keywords_for_seo}")
    print("This filter would apply these specific criteria during a crawl.")

if __name__ == "__main__":
    asyncio.run(setup_custom_seo_filter())
```

### 5.7. `FilterChain`

#### 5.7.1. Example: Combining `URLPatternFilter` (allow `/products/*`) and `DomainFilter` (only `example.com`) in a `FilterChain`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter, DomainFilter
from unittest.mock import patch

# Add mock data for this scenario
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/products/productA.html"] = {
    "html_content": "<html><title>Product A</title><body>Product A details</body></html>",
    "response_headers": {"Content-Type": "text/html"}
}
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] += '<a href="products/productA.html">Product A</a>'


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_chain_combination():
    product_filter = URLPatternFilter(patterns=["*/products/*"])
    domain_filter = DomainFilter(allowed_domains=["docs.crawl4ai.com"])
    
    combined_filter_chain = FilterChain(filters=[product_filter, domain_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=2, filter_chain=combined_filter_chain, include_external=True)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- FilterChain: URLPatternFilter + DomainFilter ---")
        print(f"Crawled {len(results)} pages.")
        for r in results:
            print(f"  URL: {r.url}")
            if r.metadata.get('depth', 0) > 0: # Discovered URLs
                assert "docs.crawl4ai.com" in r.url, "Domain filter failed."
                assert "/products/" in r.url, "URL pattern filter failed."
        print("All discovered pages are from 'docs.crawl4ai.com' and match '*/products/*'.")
        
        # Clean up mock data
        del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/products/productA.html"]
        MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"].replace('<a href="products/productA.html">Product A</a>', '')


if __name__ == "__main__":
    asyncio.run(filter_chain_combination())
```

#### 5.7.2. Example: Using `FilterChain` with `FilterStats` to retrieve and display statistics about filtered URLs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter, FilterStats
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_chain_with_stats():
    url_filter = URLPatternFilter(patterns=["*/blog/*"], block_list=False) # Allow only blog
    filter_stats = FilterStats() # Create a stats object
    filter_chain = FilterChain(filters=[url_filter], stats=filter_stats) # Pass stats to chain
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- FilterChain with FilterStats ---")
        print(f"Crawled {len(results)} pages.")
        
        print("\nFilter Statistics:")
        print(f"  Total URLs considered by filters: {filter_stats.total_considered}")
        print(f"  Total URLs allowed: {filter_stats.total_allowed}")
        print(f"  Total URLs blocked: {filter_stats.total_blocked}")
        
        # Based on MOCK_SITE_DATA, index links to one /blog/ page and several non-blog pages.
        # Start URL itself is not subject to filter_chain in this strategy logic.
        # Links from start URL: page1, page2, external, archive, blog, login
        # Only /blog/post1.html should pass. 5 should be blocked.
        assert filter_stats.total_considered >= 5 # Links from index.html
        assert filter_stats.total_allowed >= 1    # /blog/post1.html
        assert filter_stats.total_blocked >= 4    # page1, page2, external (if not implicitly blocked), archive, login

if __name__ == "__main__":
    asyncio.run(filter_chain_with_stats())
```

#### 5.7.3. Example: `FilterChain` with `allow_empty=True` vs `allow_empty=False`.
This shows how `allow_empty` on the `FilterChain` itself works. If `allow_empty=True` (default), an empty chain allows all URLs. If `False`, an empty chain blocks all.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def filter_chain_allow_empty():
    start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
    
    # Case 1: allow_empty=True (default) - empty chain allows all
    print("\n--- FilterChain with allow_empty=True (empty chain) ---")
    empty_chain_allow = FilterChain(filters=[], allow_empty=True)
    strategy_allow = BFSDeePCrawlStrategy(max_depth=1, filter_chain=empty_chain_allow)
    run_config_allow = CrawlerRunConfig(deep_crawl_strategy=strategy_allow, cache_mode=CacheMode.BYPASS)
    async with AsyncWebCrawler() as crawler:
        results_allow = await crawler.arun(url=start_url, config=run_config_allow)
        print(f"Crawled {len(results_allow)} pages. (Expected > 1 as all links from index should be allowed)")
        assert len(results_allow) > 1 # Start URL + its links

    # Case 2: allow_empty=False - empty chain blocks all (except start URL)
    print("\n--- FilterChain with allow_empty=False (empty chain) ---")
    empty_chain_block = FilterChain(filters=[], allow_empty=False)
    strategy_block = BFSDeePCrawlStrategy(max_depth=1, filter_chain=empty_chain_block)
    run_config_block = CrawlerRunConfig(deep_crawl_strategy=strategy_block, cache_mode=CacheMode.BYPASS)
    async with AsyncWebCrawler() as crawler:
        results_block = await crawler.arun(url=start_url, config=run_config_block)
        print(f"Crawled {len(results_block)} pages. (Expected 1, only start URL)")
        assert len(results_block) == 1 # Only start_url, as all its links are blocked by empty chain


if __name__ == "__main__":
    asyncio.run(filter_chain_allow_empty())
```

---
## 6. Configuring Scorers (`URLScorer`) for `BestFirstCrawlingStrategy`

Scorers are used by `BestFirstCrawlingStrategy` to prioritize URLs in its crawl queue.

### 6.1. `KeywordRelevanceScorer`

#### 6.1.1. Example: `KeywordRelevanceScorer` with a list of keywords and default weight.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_keyword_default_weight():
    scorer = KeywordRelevanceScorer(keywords=["feature", "core concepts"]) # Default weight is 1.0
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer, max_pages=4)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- KeywordRelevanceScorer with default weight ---")
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun("https://docs.crawl4ai.com/vibe-examples/index.html", config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}")
    print("Pages containing 'feature' or 'core concepts' in their URL should have higher scores.")

if __name__ == "__main__":
    asyncio.run(scorer_keyword_default_weight())
```

#### 6.1.2. Example: `KeywordRelevanceScorer` adjusting the `weight` parameter to influence its importance.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer, PathDepthScorer, CompositeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_keyword_custom_weight():
    # High weight for keywords, low for path depth
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature"], weight=2.0) 
    path_scorer = PathDepthScorer(weight=0.1, higher_score_is_better=False) # Less penalty
    
    composite_scorer = CompositeScorer(scorers=[keyword_scorer, path_scorer])
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=composite_scorer, max_pages=4)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- KeywordRelevanceScorer with adjusted weight (weight=2.0) in CompositeScorer ---")
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun("https://docs.crawl4ai.com/vibe-examples/index.html", config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}")
    print("Keyword relevance should have a stronger impact on the final score.")

if __name__ == "__main__":
    asyncio.run(scorer_keyword_custom_weight())
```

#### 6.1.3. Example: `KeywordRelevanceScorer` with `case_sensitive=True`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

# Modify mock data to have case-specific keywords in URLs
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/FEATUREpage.html"] = {
    "html_content": "<html><title>FEATURE Page</title><body>Uppercase FEATURE</body></html>",
    "response_headers": {"Content-Type": "text/html"}
}
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] += '<a href="FEATUREpage.html">FEATURE Page</a>'


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_keyword_case_sensitive():
    # Case-sensitive: will only score URLs with 'feature' (lowercase)
    scorer_sensitive = KeywordRelevanceScorer(keywords=["feature"], case_sensitive=True)
    strategy_sensitive = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer_sensitive, max_pages=5)
    run_config_sensitive = CrawlerRunConfig(deep_crawl_strategy=strategy_sensitive, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- KeywordRelevanceScorer with case_sensitive=True (keyword: 'feature') ---")
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun("https://docs.crawl4ai.com/vibe-examples/index.html", config=run_config_sensitive):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}")
                if "FEATUREpage.html" in result.url: # Uppercase 'FEATURE'
                    assert result.metadata.get('score', 0.0) == 0.0, "Uppercase keyword should not be scored."
                elif "page2.html" in result.url: # Contains lowercase 'feature' in title/mock
                     assert result.metadata.get('score', 0.0) > 0.0, "Lowercase keyword should be scored."

    # Clean up mock data
    del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/FEATUREpage.html"]
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"].replace('<a href="FEATUREpage.html">FEATURE Page</a>', '')


if __name__ == "__main__":
    asyncio.run(scorer_keyword_case_sensitive())
```

### 6.2. `PathDepthScorer`

#### 6.2.1. Example: `PathDepthScorer` with default behavior (penalizing deeper paths).
By default, `PathDepthScorer` gives higher scores to shallower paths (depth 0 > depth 1 > depth 2).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, PathDepthScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_path_depth_default():
    scorer = PathDepthScorer() # Default: higher_score_is_better=True, depth_penalty_factor=0.1
    strategy = BestFirstCrawlingStrategy(max_depth=2, url_scorer=scorer, max_pages=6)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- PathDepthScorer with default behavior (shallower is better) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        
        depth_scores = {}
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                depth = result.metadata.get('depth')
                score = result.metadata.get('score', 0.0)
                print(f"  URL: {result.url}, Depth: {depth}, Score: {score:.2f}")
                if depth not in depth_scores:
                    depth_scores[depth] = []
                depth_scores[depth].append(score)
        
        if 1 in depth_scores and 2 in depth_scores and depth_scores[1] and depth_scores[2]:
           avg_score_depth1 = sum(depth_scores[1]) / len(depth_scores[1])
           avg_score_depth2 = sum(depth_scores[2]) / len(depth_scores[2])
           print(f"Avg score depth 1: {avg_score_depth1:.2f}, Avg score depth 2: {avg_score_depth2:.2f}")
           assert avg_score_depth1 > avg_score_depth2, "Shallower paths should have higher scores."

if __name__ == "__main__":
    asyncio.run(scorer_path_depth_default())
```

#### 6.2.2. Example: `PathDepthScorer` with custom `depth_penalty_factor`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, PathDepthScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_path_depth_custom_penalty():
    # Higher penalty factor means deeper paths are penalized more severely
    scorer = PathDepthScorer(depth_penalty_factor=0.5, higher_score_is_better=True) 
    strategy = BestFirstCrawlingStrategy(max_depth=2, url_scorer=scorer, max_pages=6)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- PathDepthScorer with custom depth_penalty_factor=0.5 ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        
        depth_scores = {}
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                depth = result.metadata.get('depth')
                score = result.metadata.get('score', 0.0)
                print(f"  URL: {result.url}, Depth: {depth}, Score: {score:.2f}")
                if depth not in depth_scores:
                    depth_scores[depth] = []
                depth_scores[depth].append(score)

        if 1 in depth_scores and 2 in depth_scores and depth_scores[1] and depth_scores[2]:
           avg_score_depth1 = sum(depth_scores[1]) / len(depth_scores[1])
           avg_score_depth2 = sum(depth_scores[2]) / len(depth_scores[2])
           print(f"Avg score depth 1: {avg_score_depth1:.2f}, Avg score depth 2: {avg_score_depth2:.2f}")
           # Expect a larger difference due to higher penalty
           assert (avg_score_depth1 - avg_score_depth2) > 0.05, "Higher penalty factor should result in a larger score drop for deeper paths."


if __name__ == "__main__":
    asyncio.run(scorer_path_depth_custom_penalty())
```

#### 6.2.3. Example: `PathDepthScorer` with `higher_score_is_better=False` (to favor deeper paths).

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, PathDepthScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_path_depth_favor_deep():
    # Now, deeper paths will get higher (less negative or more positive) scores
    scorer = PathDepthScorer(higher_score_is_better=False) 
    strategy = BestFirstCrawlingStrategy(max_depth=2, url_scorer=scorer, max_pages=6)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- PathDepthScorer with higher_score_is_better=False (favoring deeper paths) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        
        depth_scores = {}
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                depth = result.metadata.get('depth')
                score = result.metadata.get('score', 0.0)
                print(f"  URL: {result.url}, Depth: {depth}, Score: {score:.2f}")
                if depth not in depth_scores:
                    depth_scores[depth] = []
                depth_scores[depth].append(score)
        
        if 1 in depth_scores and 2 in depth_scores and depth_scores[1] and depth_scores[2]:
           avg_score_depth1 = sum(depth_scores[1]) / len(depth_scores[1])
           avg_score_depth2 = sum(depth_scores[2]) / len(depth_scores[2])
           print(f"Avg score depth 1: {avg_score_depth1:.2f}, Avg score depth 2: {avg_score_depth2:.2f}")
           assert avg_score_depth2 > avg_score_depth1, "Deeper paths should have higher scores with higher_score_is_better=False."

if __name__ == "__main__":
    asyncio.run(scorer_path_depth_favor_deep())
```

### 6.3. `ContentTypeScorer`

#### 6.3.1. Example: `ContentTypeScorer` prioritizing `text/html` and penalizing `application/pdf`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, ContentTypeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_content_type_html_vs_pdf():
    scorer = ContentTypeScorer(
        content_type_weights={"text/html": 1.0, "application/pdf": -1.0, "image/jpeg": 0.2}
    )
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- ContentTypeScorer (HTML: 1.0, PDF: -1.0) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # Links to HTML and PDF
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                content_type = result.response_headers.get('Content-Type', 'unknown')
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Type: {content_type}")

if __name__ == "__main__":
    asyncio.run(scorer_content_type_html_vs_pdf())
```

#### 6.3.2. Example: `ContentTypeScorer` with custom `content_type_weights`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, ContentTypeScorer
from unittest.mock import patch

# Add a JSON page to mock data
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/data.json"] = {
    "html_content": '{"data": "sample"}', "response_headers": {"Content-Type": "application/json"}
}
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] += '<a href="data.json">JSON Data</a>'


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def scorer_content_type_custom_weights():
    scorer = ContentTypeScorer(
        content_type_weights={
            "application/json": 2.0, # Highly prioritize JSON
            "text/html": 0.5,
            "application/pdf": -2.0 # Strongly penalize PDF
        }
    )
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=scorer, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- ContentTypeScorer with custom weights (JSON: 2.0, HTML: 0.5, PDF: -2.0) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/page1.html" # Links to HTML, PDF. Index links to JSON.
        
        # We'll crawl index to ensure JSON is discoverable
        async for result in await crawler.arun("https://docs.crawl4ai.com/vibe-examples/index.html", config=run_config):
            if result.success:
                content_type = result.response_headers.get('Content-Type', 'unknown')
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Type: {content_type}")
    
    del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/data.json"]
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"].replace('<a href="data.json">JSON Data</a>', '')

if __name__ == "__main__":
    asyncio.run(scorer_content_type_custom_weights())
```

### 6.4. `DomainAuthorityScorer`

#### 6.4.1. Example: Setting up `DomainAuthorityScorer` (conceptual, as DA often requires an external API or dataset).
This example shows how to instantiate and potentially use it, but actual scoring depends on external data.

```python
import asyncio
from crawl4ai import DomainAuthorityScorer

async def setup_domain_authority_scorer():
    print("--- DomainAuthorityScorer (Conceptual Setup) ---")
    
    # Conceptual: imagine you have a way to get DA scores
    # da_scores = {"example.com": 90, "anotherexample.net": 70}
    # scorer = DomainAuthorityScorer(domain_authority_map=da_scores, weight=1.5)
    
    # For this example, we'll just instantiate it
    scorer = DomainAuthorityScorer(weight=1.5)
    print(f"DomainAuthorityScorer created with weight: {scorer.weight}")
    print("To use this scorer effectively, you'd need a 'domain_authority_map' or a way to fetch DA scores.")
    print("Example URL score (conceptual): ", scorer.score("https://highly-authoritative-site.com/page"))

if __name__ == "__main__":
    asyncio.run(setup_domain_authority_scorer())
```

### 6.5. `FreshnessScorer`

#### 6.5.1. Example: Setting up `FreshnessScorer` (conceptual, as freshness often requires parsing dates from content or headers).
This example focuses on instantiation. Actual scoring would need date extraction.

```python
import asyncio
from crawl4ai import FreshnessScorer
from datetime import datetime, timedelta

async def setup_freshness_scorer():
    print("--- FreshnessScorer (Conceptual Setup) ---")
    
    # Conceptual: the scorer would need a way to get the publication date of a URL
    # For this example, we'll just instantiate it
    scorer = FreshnessScorer(
        max_age_days=30,      # Pages older than 30 days get lower scores
        date_penalty_factor=0.1 # How much to penalize per day older
    )
    print(f"FreshnessScorer created with max_age_days: {scorer.max_age_days}")
    print("To use this, the crawling process or a pre-processor would need to extract and provide publication dates for URLs.")
    
    # Conceptual scoring:
    # recent_date = datetime.now() - timedelta(days=5)
    # old_date = datetime.now() - timedelta(days=60)
    # print(f"Score for recent page (mock date): {scorer.score('https://example.com/recent', publication_date=recent_date)}")
    # print(f"Score for old page (mock date): {scorer.score('https://example.com/old', publication_date=old_date)}")


if __name__ == "__main__":
    asyncio.run(setup_freshness_scorer())
```

### 6.6. `CompositeScorer`

#### 6.6.1. Example: Combining `KeywordRelevanceScorer` and `PathDepthScorer` using `CompositeScorer` with equal weights.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from crawl4ai import KeywordRelevanceScorer, PathDepthScorer, CompositeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def composite_scorer_equal_weights():
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature"]) # Default weight 1.0
    path_scorer = PathDepthScorer(higher_score_is_better=False)  # Default weight 1.0, penalizes depth
    
    # Equal weighting by default if weights list not provided or all weights are same
    composite_scorer = CompositeScorer(scorers=[keyword_scorer, path_scorer])
    
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=composite_scorer, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- CompositeScorer with equal weights for Keyword and PathDepth ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}")
    print("Scores are an equal combination of keyword relevance and path depth penalty.")

if __name__ == "__main__":
    asyncio.run(composite_scorer_equal_weights())
```

#### 6.6.2. Example: `CompositeScorer` assigning different `weights` to prioritize one scorer over another.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from crawl4ai import KeywordRelevanceScorer, PathDepthScorer, CompositeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def composite_scorer_different_weights():
    # Keyword relevance is more important
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature"]) 
    path_scorer = PathDepthScorer(higher_score_is_better=False)
    
    composite_scorer = CompositeScorer(
        scorers=[keyword_scorer, path_scorer],
        weights=[0.8, 0.2] # Keyword scorer has 80% influence, PathDepth 20%
    )
    
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=composite_scorer, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- CompositeScorer with different weights (Keyword: 0.8, PathDepth: 0.2) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}")
    print("Keyword relevance should more heavily influence scores.")

if __name__ == "__main__":
    asyncio.run(composite_scorer_different_weights())
```

#### 6.6.3. Example: Nesting `CompositeScorer` for more complex scoring logic.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy
from crawl4ai import KeywordRelevanceScorer, PathDepthScorer, ContentTypeScorer, CompositeScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def composite_scorer_nesting():
    keyword_scorer = KeywordRelevanceScorer(keywords=["feature"])
    path_scorer = PathDepthScorer(higher_score_is_better=False)
    content_type_scorer = ContentTypeScorer(content_type_weights={"text/html": 1.0, "application/pdf": -1.0})

    # First level composite: keyword and path
    relevance_and_structure_scorer = CompositeScorer(
        scorers=[keyword_scorer, path_scorer],
        weights=[0.7, 0.3]
    )

    # Second level composite: combine above with content type
    final_scorer = CompositeScorer(
        scorers=[relevance_and_structure_scorer, content_type_scorer],
        weights=[0.8, 0.2] # Relevance/structure is 80%, content type 20%
    )
    
    strategy = BestFirstCrawlingStrategy(max_depth=1, url_scorer=final_scorer, max_pages=5)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- Nested CompositeScorer ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                 print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}, Depth: {result.metadata.get('depth')}, Type: {result.response_headers.get('Content-Type')}")
    print("Scores reflect a nested combination of keyword, path, and content type.")

if __name__ == "__main__":
    asyncio.run(composite_scorer_nesting())
```

---
## 7. General Deep Crawl Configuration and Usage

### 7.1. Example: Deep crawling a site that relies heavily on JavaScript for link generation.
This example demonstrates the setup. A real JS-heavy site would be needed for full verification.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, BrowserConfig
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_js_heavy_site():
    # BrowserConfig enables JS by default.
    # For very JS-heavy sites, ensure headless=False if debugging, and consider timeouts.
    browser_cfg = BrowserConfig(headless=True) # Keep headless for automated tests

    # CrawlerRunConfig might need adjustments for JS execution time
    run_cfg = CrawlerRunConfig(
        page_timeout=30000, # 30 seconds, might need more for complex JS
        # js_code can be used to trigger actions if needed before link discovery
        # js_code="window.scrollTo(0, document.body.scrollHeight);", # Example to scroll
        deep_crawl_strategy=BFSDeePCrawlStrategy(max_depth=1, max_pages=3),
        cache_mode=CacheMode.BYPASS
    )

    print("--- Deep Crawling a JS-Heavy Site (Conceptual: JS execution is enabled by default) ---")
    # Using index.html which has a JS-triggered link via onclick
    start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        results = await crawler.arun(url=start_url, config=run_cfg)
        
        print(f"Crawled {len(results)} pages.")
        js_link_found = False
        for result in results:
            print(f"  URL: {result.url}")
            if "js_page.html" in result.url:
                js_link_found = True
        
        # This assertion relies on the MockAsyncWebCrawler's _fetch_page
        # correctly parsing links from html_content, even if added by mock JS.
        # A more robust test would involve Playwright's own JS execution.
        # For now, we assume the mock crawler finds links from the final HTML state.
        # To truly test JS-driven links, one would need to modify MockAsyncWebCrawler
        # to simulate JS execution or use a real browser test.
        # This example mainly shows the configuration for enabling JS.
        print("Note: True JS-link discovery depends on Playwright's execution within the crawler.")
        print("The mock crawler simulates link finding from final HTML state.")
        # assert js_link_found, "JS-generated link was not found. Mock might need adjustment or real browser test."


if __name__ == "__main__":
    asyncio.run(deep_crawl_js_heavy_site())
```

### 7.2. Example: How `CrawlerRunConfig` parameters (e.g., `page_timeout`) and `BrowserConfig` (e.g., `user_agent`, `proxy_config`) affect underlying page fetches.
This shows how `BrowserConfig` (passed to `AsyncWebCrawler`) and `CrawlerRunConfig` (passed to `arun`) influence individual page fetches.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, BrowserConfig, ProxyConfig
from unittest.mock import patch

# Mocking a proxy server check - in reality, you'd use a real proxy
async def mock_check_ip_via_proxy(url, config):
    # This function would normally make a request through the proxy
    # and return the perceived IP. For mock, we'll just simulate.
    if config and config.proxy_config and config.proxy_config.server == "http://mockproxy.com:8080":
        return "1.2.3.4" # Mocked IP if proxy is used
    return "9.8.7.6" # Mocked direct IP

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_with_configs():
    browser_cfg = BrowserConfig(
        user_agent="MyCustomDeepCrawler/1.0",
        proxy_config=ProxyConfig(server="http://mockproxy.com:8080") # This should be used by crawler
    )
    
    # For deep crawl, the page_timeout in CrawlerRunConfig applies to each page fetch
    run_cfg = CrawlerRunConfig(
        page_timeout=15000, # 15s timeout for each page in the deep crawl
        deep_crawl_strategy=BFSDeePCrawlStrategy(max_depth=0), # Just the start URL
        cache_mode=CacheMode.BYPASS
    )

    print("--- Deep Crawl with Custom Browser & Run Configs ---")
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # The crawler instance now has the browser_cfg settings.
        # We expect its internal page fetches to use these.
        
        # We'd need to inspect logs or mock `crawler.strategy._fetch_page` to truly verify user_agent/proxy.
        # For this example, we'll conceptually check based on setup.
        print(f"Browser User-Agent set to: {crawler.browser_config.user_agent}")
        if crawler.browser_config.proxy_config:
            print(f"Browser Proxy set to: {crawler.browser_config.proxy_config.server}")
        
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_cfg)
        
        if results and results[0].success:
            print(f"Crawled {results[0].url} successfully with page_timeout={run_cfg.page_timeout}ms")
            # In a real scenario with a proxy, you'd verify the source IP.
            # For mock:
            # perceived_ip = await mock_check_ip_via_proxy(start_url, browser_cfg) 
            # print(f"Perceived IP (mocked): {perceived_ip}")
            # assert perceived_ip == "1.2.3.4" # Assuming proxy was used
        else:
            print(f"Crawl failed for {start_url}")

if __name__ == "__main__":
    asyncio.run(deep_crawl_with_configs())
```

### 7.3. Example: Iterating through deep crawl results and handling cases where some pages failed to crawl or were filtered out.
A robust deep crawl should handle partial failures gracefully.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain, URLPatternFilter
from unittest.mock import patch

# Add a URL that will "fail" in our mock
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/failing_page.html"] = {
    "html_content": None, # Simulate failure by not providing content
    "success": False,
    "status_code": 500,
    "error_message": "Mock Server Error"
}
MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] += '<a href="failing_page.html">Failing Page</a>'


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_handling_failures():
    # Filter out '/archive/' pages, and one page will fail
    url_filter = URLPatternFilter(patterns=["*/archive/*"], block_list=True)
    filter_chain = FilterChain(filters=[url_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"--- Deep Crawl - Handling Failures and Filtered Pages ---")
        successful_pages = 0
        failed_pages = 0
        
        for result in results:
            if result.success:
                successful_pages += 1
                print(f"  SUCCESS: {result.url} (Depth: {result.metadata.get('depth')})")
                assert "/archive/" not in result.url
            else:
                failed_pages += 1
                print(f"  FAILURE: {result.url} (Error: {result.error_message}, Status: {result.status_code})")
        
        print(f"\nTotal Successful: {successful_pages}, Total Failed/Filtered Out by crawler: {failed_pages}")
        # Start URL + index links (page1, page2, external, blog, login, failing) = 7 initial candidates
        # - external might be skipped by default include_external=False (depends on strategy)
        # - /archive/ is filtered by URLPatternFilter
        # - failing_page.html will fail
        # So, we expect start_url + page1, page2, blog, login. Failing page is in results but success=False.
        # The number of results includes the start_url and pages that were attempted.
        # Filters apply to links *discovered* from a page.
        
        # One page (/archive/old_page.html) should be filtered by the filter chain.
        # One page (failing_page.html) should be in results but with success=False.
        assert any("failing_page.html" in r.url and not r.success for r in results)
        assert not any("/archive/" in r.url for r in results)

    # Clean up mock data
    del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/failing_page.html"]
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/index.html"]["html_content"].replace('<a href="failing_page.html">Failing Page</a>', '')

if __name__ == "__main__":
    asyncio.run(deep_crawl_handling_failures())
```

### 7.4. Example: Using a custom `logger` instance passed to a `DeepCrawlStrategy`.

```python
import asyncio
import logging
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, AsyncLogger
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_custom_logger():
    # Setup a custom logger
    custom_logger = AsyncLogger(log_file="custom_deep_crawl.log", name="MyDeepCrawler", level="DEBUG")
    
    strategy = BFSDeePCrawlStrategy(max_depth=0, logger=custom_logger) # Pass logger to strategy
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    print("--- Deep Crawl with Custom Logger ---")
    async with AsyncWebCrawler() as crawler: # Main crawler logger can be default
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        await crawler.arun(url=start_url, config=run_config)
        
    print("Crawl complete. Check 'custom_deep_crawl.log' for logs from the strategy.")
    # You can verify the log file content here if needed
    # e.g., with open("custom_deep_crawl.log", "r") as f: assert "MyDeepCrawler" in f.read()
    # For this example, just visual confirmation is sufficient.

if __name__ == "__main__":
    asyncio.run(deep_crawl_custom_logger())
```

### 7.5. Example: Deep crawling starting from a local HTML file that contains links to other local files or web URLs.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch
from pathlib import Path
import os

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_from_local_file():
    # Ensure the mock local files exist for the test
    local_index_path = Path(os.getcwd()) / "test_local_index.html"
    local_page1_path = Path(os.getcwd()) / "test_local_page1.html"
    
    # If not created by preamble, create them
    if not local_index_path.exists():
        local_index_path.write_text(MOCK_SITE_DATA[f"file://{local_index_path}"]["html_content"])
    if not local_page1_path.exists():
        local_page1_path.write_text(MOCK_SITE_DATA[f"file://{local_page1_path}"]["html_content"])

    start_file_url = f"file://{local_index_path.resolve()}"
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, include_external=True) # Allow following to web URLs
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    print(f"--- Deep Crawling from Local File: {start_file_url} ---")
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun(url=start_file_url, config=run_config)
        
        print(f"Crawled {len(results)} pages.")
        found_local_link = False
        found_web_link = False
        for result in results:
            print(f"  URL: {result.url}, Depth: {result.metadata.get('depth')}")
            if result.url == f"file://{local_page1_path.resolve()}":
                found_local_link = True
            if result.url == "https://docs.crawl4ai.com/vibe-examples/index.html":
                found_web_link = True
        
        assert found_local_link, "Did not follow local file link."
        assert found_web_link, "Did not follow web link from local file."
    
    # Clean up dummy files
    if local_index_path.exists(): os.remove(local_index_path)
    if local_page1_path.exists(): os.remove(local_page1_path)


if __name__ == "__main__":
    asyncio.run(deep_crawl_from_local_file())
```

### 7.6. Example: Comparing outputs from `BFSDeePCrawlStrategy`, `DFSDeePCrawlStrategy`, and `BestFirstCrawlingStrategy`.
This example runs all three main strategies with similar settings to highlight differences in traversal and results.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai import BFSDeePCrawlStrategy, DFSDeePCrawlStrategy, BestFirstCrawlingStrategy, KeywordRelevanceScorer
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def compare_deep_crawl_strategies():
    start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
    max_depth = 2
    max_pages = 7 # Keep it manageable for comparison
    
    common_config_params = {
        "max_depth": max_depth,
        "max_pages": max_pages,
        "include_external": False, # Keep it simple for comparison
    }
    
    scorer = KeywordRelevanceScorer(keywords=["feature", "core"])

    strategies_to_compare = {
        "BFS": BFSDeePCrawlStrategy(**common_config_params),
        "DFS": DFSDeePCrawlStrategy(**common_config_params),
        "Best-First": BestFirstCrawlingStrategy(**common_config_params, url_scorer=scorer)
    }

    print(f"--- Comparing Deep Crawl Strategies (max_depth={max_depth}, max_pages={max_pages}) ---")

    async with AsyncWebCrawler() as crawler:
        for name, strategy_instance in strategies_to_compare.items():
            print(f"\n-- Running {name} Strategy --")
            run_config = CrawlerRunConfig(
                deep_crawl_strategy=strategy_instance,
                cache_mode=CacheMode.BYPASS,
                stream=False # Batch for easier comparison of final set
            )
            
            start_time = time.perf_counter()
            results = await crawler.arun(url=start_url, config=run_config)
            duration = time.perf_counter() - start_time
            
            print(f"  {name} crawled {len(results)} pages in {duration:.2f}s.")
            # Sort by depth then URL for consistent output for BFS/DFS
            # For Best-First, sort by score (desc) then depth then URL
            if name == "Best-First":
                 sorted_results = sorted(results, key=lambda r: (r.metadata.get('score', 0.0), -r.metadata.get('depth', 0), r.url), reverse=True)
            else:
                 sorted_results = sorted(results, key=lambda r: (r.metadata.get('depth', 0), r.url))


            for i, r in enumerate(sorted_results):
                if i < 5 or i > len(sorted_results) - 3 : # Show first 5 and last 2
                    score_str = f", Score: {r.metadata.get('score', 0.0):.2f}" if name == "Best-First" else ""
                    print(f"    URL: {r.url} (Depth: {r.metadata.get('depth')}{score_str})")
                elif i == 5:
                    print(f"    ... ({len(sorted_results) - 5 -2 } more results) ...")
            print("-" * 30)

if __name__ == "__main__":
    asyncio.run(compare_deep_crawl_strategies())
```

---
## 8. Advanced Scenarios & Customization

### 8.1. Example: Implementing a custom `DeepCrawlStrategy` by subclassing `DeepCrawlStrategy`.
This provides a skeleton for creating your own crawl logic.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DeepCrawlStrategy, CrawlResult
from typing import List, Set, Dict, AsyncGenerator, Tuple
from unittest.mock import patch

class MyCustomDeepCrawlStrategy(DeepCrawlStrategy):
    def __init__(self, max_depth=1, **kwargs):
        self.max_depth = max_depth
        # Potentially other custom init params
        super().__init__(**kwargs) # Pass along other kwargs if base class uses them
        print("MyCustomDeepCrawlStrategy Initialized")

    async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]:
        print(f"[Custom Strategy] _arun_batch called for: {start_url}")
        # Implement batch crawling logic (e.g., BFS-like)
        # This is a simplified version. A real one needs queue, visited set, depth tracking etc.
        results = []
        initial_result_container = await crawler.arun(url=start_url, config=config.clone(deep_crawl_strategy=None))
        initial_result = initial_result_container[0] # arun returns a list
        
        if not initial_result.success: return [initial_result]
        results.append(initial_result)
        
        if self.max_depth > 0 and initial_result.links.get("internal"):
            for link_info in initial_result.links["internal"][:2]: # Crawl first 2 internal links
                link_url = link_info["href"]
                # Pass metadata for depth and parent
                link_config = config.clone(deep_crawl_strategy=None)
                
                # In a real strategy, you'd manage metadata directly or pass it for crawler.arun
                # For this mock, we simplify as crawler.arun normally doesn't take depth/parent for single page
                print(f"  [Custom Strategy] Crawling linked URL: {link_url} at depth 1")
                linked_result_container = await crawler.arun(url=link_url, config=link_config)
                linked_result = linked_result_container[0]
                # Manually add metadata for this example
                if linked_result.metadata is None: linked_result.metadata = {}
                linked_result.metadata['depth'] = 1
                linked_result.metadata['parent_url'] = start_url
                results.append(linked_result)
        return results

    async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]:
        print(f"[Custom Strategy] _arun_stream called for: {start_url}")
        # Implement streaming crawling logic
        # Simplified: yields results from a batch-like process for this example
        batch_results = await self._arun_batch(start_url, crawler, config)
        for result in batch_results:
            yield result
            
    async def can_process_url(self, url: str, depth: int) -> bool:
        # Example: only process URLs not containing "archive" and within max_depth
        print(f"[Custom Strategy] can_process_url called for: {url}, depth: {depth}")
        if "archive" in url:
            return False
        return depth <= self.max_depth

    async def link_discovery(
        self, result: CrawlResult, source_url: str, current_depth: int, 
        visited: Set[str], next_level: List[Tuple[str, str]], depths: Dict[str, int]
    ) -> None:
        # This method is crucial for discovering and queuing new links.
        # The base class might have a default implementation, or you might need to call
        # crawler.arun to get links if result.links is not populated.
        # For this example, we'll assume result.links is populated by the crawler.
        print(f"[Custom Strategy] link_discovery for: {source_url} at depth {current_depth}")
        new_depth = current_depth + 1
        if new_depth > self.max_depth:
            return

        for link_info in result.links.get("internal", [])[:3]: # Limit for example
            link_url = link_info["href"]
            if link_url not in visited and await self.can_process_url(link_url, new_depth):
                next_level.append((link_url, source_url)) # (url, parent_url)
                depths[link_url] = new_depth
                print(f"  [Custom Strategy] Discovered and added to queue: {link_url}")
    
    async def shutdown(self):
        print("[Custom Strategy] Shutdown called.")
        # Implement any cleanup or signal to stop crawling loops


@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def custom_deep_crawl_strategy_example():
    custom_strategy = MyCustomDeepCrawlStrategy(max_depth=1)
    run_config = CrawlerRunConfig(deep_crawl_strategy=custom_strategy, cache_mode=CacheMode.BYPASS)

    print("--- Using Custom DeepCrawlStrategy ---")
    async with AsyncWebCrawler() as crawler: # This will be MockAsyncWebCrawler
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"\nCustom strategy crawled {len(results)} pages:")
        for r in results:
            print(f"  URL: {r.url}, Success: {r.success}, Depth: {r.metadata.get('depth') if r.metadata else 'N/A'}")

if __name__ == "__main__":
    asyncio.run(custom_deep_crawl_strategy_example())
```

### 8.2. Example: Implementing a custom `URLFilter`.
`URLFilter` itself is a concrete class, but you can create custom logic by making a callable class or function that adheres to the expected filter signature `(url: str) -> bool`. For more complex stateful filters, subclassing a base might be an option if one is provided or creating your own structure.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, FilterChain
from unittest.mock import patch

class MyCustomURLFilter:
    def __init__(self, forbidden_keyword: str):
        self.forbidden_keyword = forbidden_keyword.lower()
        print(f"MyCustomURLFilter initialized to block URLs with '{self.forbidden_keyword}'")

    async def __call__(self, url: str) -> bool: # Filters must be async
        """Return True if URL should be allowed, False if blocked."""
        if self.forbidden_keyword in url.lower():
            print(f"[CustomFilter] Blocking URL: {url} (contains '{self.forbidden_keyword}')")
            return False # Block if keyword found
        print(f"[CustomFilter] Allowing URL: {url}")
        return True # Allow otherwise

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def custom_url_filter_example():
    custom_filter = MyCustomURLFilter(forbidden_keyword="archive")
    filter_chain = FilterChain(filters=[custom_filter])
    
    strategy = BFSDeePCrawlStrategy(max_depth=1, filter_chain=filter_chain)
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS)

    print("--- Using Custom URLFilter (blocking 'archive') ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"\nCustom filter crawl resulted in {len(results)} pages:")
        for r in results:
            print(f"  URL: {r.url}")
            assert "archive" not in r.url.lower(), f"Custom filter failed to block {r.url}"
        print("Successfully blocked URLs containing 'archive'.")

if __name__ == "__main__":
    asyncio.run(custom_url_filter_example())
```

### 8.3. Example: Implementing a custom `URLScorer` for `BestFirstCrawlingStrategy`.
Subclass `URLScorer` and implement the `score` method.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BestFirstCrawlingStrategy, URLScorer
from urllib.parse import urlparse
from unittest.mock import patch

class MyCustomURLScorer(URLScorer):
    def __init__(self, preferred_domain: str, weight: float = 1.0):
        super().__init__(weight)
        self.preferred_domain = preferred_domain
        print(f"MyCustomURLScorer initialized, preferring domain: {self.preferred_domain}")

    def score(self, url: str, **kwargs) -> float:
        """Scores URL based on whether it matches the preferred domain."""
        parsed_url = urlparse(url)
        score = 0.0
        if parsed_url.netloc == self.preferred_domain:
            score = 1.0 * self.weight
            print(f"[CustomScorer] URL {url} matches preferred domain. Score: {score}")
        else:
            score = 0.1 * self.weight # Lower score for other domains
            print(f"[CustomScorer] URL {url} does NOT match preferred domain. Score: {score}")
        return score

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def custom_url_scorer_example():
    custom_scorer = MyCustomURLScorer(preferred_domain="docs.crawl4ai.com", weight=2.0)
    
    strategy = BestFirstCrawlingStrategy(
        max_depth=1, 
        url_scorer=custom_scorer,
        include_external=True, # To allow scoring external domains differently
        max_pages=5
    )
    run_config = CrawlerRunConfig(deep_crawl_strategy=strategy, cache_mode=CacheMode.BYPASS, stream=True)

    print("--- Using Custom URLScorer (preferring 'docs.crawl4ai.com') ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        async for result in await crawler.arun(url=start_url, config=run_config):
            if result.success:
                print(f"  URL: {result.url}, Score: {result.metadata.get('score', 0.0):.2f}")
    print("Pages from 'docs.crawl4ai.com' should generally have higher scores.")

if __name__ == "__main__":
    asyncio.run(custom_url_scorer_example())
```

### 8.4. Example: Deep crawling a site with very large number of pages efficiently using `max_pages` and streaming.
This combines `max_pages` to limit the scope and `stream=True` to process results incrementally, which is crucial for very large crawls to manage memory and get feedback sooner.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy
from unittest.mock import patch

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_large_site_efficiently():
    # Simulate a large site by setting a high conceptual depth,
    # but limit actual work with max_pages.
    strategy = BFSDeePCrawlStrategy(
        max_depth=10,      # Imagine this could lead to thousands of pages
        max_pages=10,      # But we only want the first 10 found by BFS
        include_external=False 
    )
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=strategy,
        stream=True,       # Process results as they come
        cache_mode=CacheMode.BYPASS # Or CacheMode.ENABLED for subsequent partial crawls
    )

    print("--- Efficiently Crawling a 'Large' Site (max_pages=10, stream=True) ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html" # Use our mock site
        
        crawled_count = 0
        async for result in await crawler.arun(url=start_url, config=run_config):
            crawled_count += 1
            if result.success:
                print(f"  Processed ({crawled_count}/{strategy.max_pages}): {result.url} at depth {result.metadata.get('depth')}")
            else:
                print(f"  Failed ({crawled_count}/{strategy.max_pages}): {result.url} - {result.error_message}")
            
            if crawled_count >= strategy.max_pages:
                print(f"Reached max_pages limit of {strategy.max_pages}. Stopping.")
                # In a real scenario, you might need to call strategy.shutdown() if the crawler
                # doesn't automatically stop precisely at max_pages when streaming.
                # However, strategies are designed to respect max_pages.
                break 
                
        print(f"\nTotal pages processed: {crawled_count}")
        assert crawled_count <= strategy.max_pages

if __name__ == "__main__":
    asyncio.run(deep_crawl_large_site_efficiently())
```

### 8.5. Example: Combining deep crawling with `LLMExtractionStrategy` to extract structured data from each crawled page.
This example shows setting up a deep crawl where each successfully crawled page's content is then passed to an `LLMExtractionStrategy`.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, LLMExtractionStrategy, LLMConfig
from pydantic import BaseModel, Field
from unittest.mock import patch

class PageSummary(BaseModel):
    title: str = Field(description="The main title of the page.")
    brief_summary: str = Field(description="A one-sentence summary of the page content.")

# Mock the LLM call within the extraction strategy for this example
async def mock_llm_extract(self, url: str, sections: list[str]):
    print(f"[Mock LLM] Extracting from {url}, first section: {sections[0][:50]}...")
    # Based on the URL from MOCK_SITE_DATA, return a plausible mock summary
    if "index.html" in url:
        return [{"title": "Index", "brief_summary": "This is the main page."}]
    elif "page1.html" in url:
        return [{"title": "Page 1", "brief_summary": "Content about crawl strategies."}]
    elif "page2.html" in url:
        return [{"title": "Page 2 - Feature Rich", "brief_summary": "Discusses a key feature."}]
    return [{"title": "Unknown Title", "brief_summary": "Could not summarize."}]

@patch('crawl4ai.extraction_strategy.LLMExtractionStrategy.run', side_effect=mock_llm_extract)
@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def deep_crawl_with_llm_extraction(mock_llm_run): # mock_llm_run is from the patch
    llm_config = LLMConfig(provider="mock/mock-model") # Mock provider
    
    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        schema=PageSummary.model_json_schema(), # Use Pydantic model for schema
        extraction_type="schema",
        instruction="Extract the title and a brief summary for the provided HTML content."
    )
    
    deep_crawl_config = BFSDeePCrawlStrategy(max_depth=1, max_pages=3)
    
    run_config = CrawlerRunConfig(
        deep_crawl_strategy=deep_crawl_config,
        extraction_strategy=extraction_strategy, # Apply this to each crawled page
        cache_mode=CacheMode.BYPASS
    )

    print("--- Deep Crawl with LLM Extraction on Each Page ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        for result in results:
            if result.success:
                print(f"\nCrawled URL: {result.url}")
                if result.extracted_content:
                    print(f"  Extracted Data: {result.extracted_content}")
                else:
                    print("  No data extracted (or LLM mock returned empty).")
            else:
                print(f"\nFailed to crawl URL: {result.url} - {result.error_message}")
        
        assert mock_llm_run.called, "LLM Extraction strategy's run method was not called."

if __name__ == "__main__":
    asyncio.run(deep_crawl_with_llm_extraction())
```

### 8.6. Example: Scenario for using `can_process_url` within a strategy to dynamically decide if a URL should be added to the queue.
Override `can_process_url` in a custom strategy to implement dynamic filtering logic based on URL and current depth.

```python
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeePCrawlStrategy, CrawlResult
from typing import List, Set, Dict, Tuple
from unittest.mock import patch

class DepthAndPatternAwareBFSStrategy(BFSDeePCrawlStrategy):
    async def can_process_url(self, url: str, depth: int) -> bool:
        # Standard checks from parent (like filter_chain)
        if not await super().can_process_url(url, depth):
            print(f"[Custom can_process_url] Blocked by parent: {url}")
            return False
        
        # Custom logic: Do not process '/archive/' pages if depth is > 1
        if depth > 1 and "/archive/" in url:
            print(f"[Custom can_process_url] Blocking deep archive page: {url} at depth {depth}")
            return False
        
        print(f"[Custom can_process_url] Allowing: {url} at depth {depth}")
        return True

@patch('crawl4ai.AsyncWebCrawler', MockAsyncWebCrawler)
async def custom_can_process_url_example():
    # Add a deeper archive link for testing
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"] += '<a href="archive/deep_archive.html">Deep Archive</a>'
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/archive/deep_archive.html"] = {
        "html_content": "<html><title>Deep Archive</title><body>Very old stuff.</body></html>",
        "response_headers": {"Content-Type": "text/html"}
    }

    custom_strategy = DepthAndPatternAwareBFSStrategy(max_depth=2) # Crawl up to depth 2
    run_config = CrawlerRunConfig(deep_crawl_strategy=custom_strategy, cache_mode=CacheMode.BYPASS)

    print("--- Custom Strategy with Dynamic can_process_url ---")
    async with AsyncWebCrawler() as crawler:
        start_url = "https://docs.crawl4ai.com/vibe-examples/index.html"
        results = await crawler.arun(url=start_url, config=run_config)
        
        print(f"\nCrawled {len(results)} pages:")
        archive_at_depth_1_crawled = False
        deep_archive_blocked = True

        for r in results:
            print(f"  URL: {r.url}, Depth: {r.metadata.get('depth')}")
            if "/archive/old_page.html" in r.url and r.metadata.get('depth') == 1:
                archive_at_depth_1_crawled = True
            if "/archive/deep_archive.html" in r.url and r.metadata.get('depth') == 2:
                 # This should not happen due to our custom can_process_url
                deep_archive_blocked = False 
        
        assert archive_at_depth_1_crawled, "Archive page at depth 1 should have been crawled."
        assert deep_archive_blocked, "Deep archive page at depth 2 should have been blocked by custom can_process_url."
        print("Dynamic URL processing logic worked as expected.")

    # Clean up mock data
    MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"] = MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/page1.html"]["html_content"].replace('<a href="archive/deep_archive.html">Deep Archive</a>', '')
    del MOCK_SITE_DATA["https://docs.crawl4ai.com/vibe-examples/archive/deep_archive.html"]


if __name__ == "__main__":
    asyncio.run(custom_can_process_url_example())
    # Clean up dummy files after all examples run
    if (Path(os.getcwd()) / "test_local_index.html").exists():
        os.remove(Path(os.getcwd()) / "test_local_index.html")
    if (Path(os.getcwd()) / "test_local_page1.html").exists():
        os.remove(Path(os.getcwd()) / "test_local_page1.html")
    if Path("custom_deep_crawl.log").exists():
        os.remove("custom_deep_crawl.log")

```

---

